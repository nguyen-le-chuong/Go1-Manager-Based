nohup: ignoring input
MESA: warning: Driver does not support the 0x7d67 PCI ID.
MESA: warning: Driver does not support the 0x7d67 PCI ID.
[INFO][AppLauncher]: Using device: cuda:0
[INFO][AppLauncher]: Loading experience file: /home/ad/setup/IsaacLab/apps/isaaclab.python.headless.kit
[INFO]: Parsing configuration from: go1.tasks.manager_based.go1.go1_env_cfg:UnitreeGo1RoughEnvCfg
[INFO]: Parsing configuration from: go1.tasks.manager_based.go1.agents.rsl_rl_ppo_cfg:UnitreeGo1RoughPPORunnerCfg
[Warning] [simulation_app.simulation_app] Modules: ['omni.kit_app'] were loaded before SimulationApp was started and might not be loaded correctly.
[Warning] [simulation_app.simulation_app] Please check to make sure no extra omniverse or pxr modules are imported before the call to SimulationApp(...)
Loading user config located at: '/home/ad/miniconda3/envs/isaaclab/lib/python3.10/site-packages/omni/data/Kit/Isaac-Sim/4.5/user.config.json'
[Info] [carb] Logging to file: /home/ad/miniconda3/envs/isaaclab/lib/python3.10/site-packages/omni/logs/Kit/Isaac-Sim/4.5/kit_20250603_135000.log
2025-06-03 06:50:00 [0ms] [Warning] [omni.kit.app.plugin] No crash reporter present, dumps uploading isn't available.
2025-06-03 06:50:00 [3ms] [Warning] [omni.ext.plugin] [ext: rendering_modes] Extensions config 'extension.toml' doesn't exist '/home/ad/setup/IsaacLab/apps/rendering_modes' or '/home/ad/setup/IsaacLab/apps/rendering_modes/config'
2025-06-03 06:50:00 [69ms] [Warning] [omni.usd_config.extension] Enable omni.materialx.libs extension to use MaterialX
2025-06-03 06:50:00 [253ms] [Warning] [omni.datastore] OmniHub is inaccessible
2025-06-03 06:50:00 [296ms] [Warning] [omni.isaac.dynamic_control] omni.isaac.dynamic_control is deprecated as of Isaac Sim 4.5. No action is needed from end-users.
2025-06-03 06:50:03 [3,349ms] [Warning] [pxr.Semantics] pxr.Semantics is deprecated - please use Semantics instead

|---------------------------------------------------------------------------------------------|
| Driver Version: 570.133.07    | Graphics API: Vulkan
|=============================================================================================|
| GPU | Name                             | Active | LDA | GPU Memory | Vendor-ID | LUID       |
|     |                                  |        |     |            | Device-ID | UUID       |
|     |                                  |        |     |            | Bus-ID    |            |
|---------------------------------------------------------------------------------------------|
| 0   | NVIDIA GeForce RTX 5070 Ti       | Yes: 0 |     | 16303   MB | 10de      | 0          |
|     |                                  |        |     |            | 2c05      | 1c6fda92.. |
|     |                                  |        |     |            | 2         |            |
|=============================================================================================|
| OS: 22.04.5 LTS (Jammy Jellyfish) ubuntu, Version: 22.04.5, Kernel: 6.8.0-60-generic
| XServer Vendor: The X.Org Foundation, XServer Version: 12101004 (1.21.1.4)
| Processor: Intel(R) Core(TM) Ultra 7 265K
| Cores: 20 | Logical Cores: 20
|---------------------------------------------------------------------------------------------|
| Total Memory (MB): 31543 | Free Memory: 22918
| Total Page/Swap (MB): 2047 | Free Page/Swap: 2047
|---------------------------------------------------------------------------------------------|
2025-06-03 06:50:05 [4,428ms] [Warning] [gpu.foundation.plugin] IOMMU is enabled.
2025-06-03 06:50:10 [10,416ms] [Warning] [omni.usd] Warning: in GetNodeDiscoveryResults at line 136 of /builds/omniverse/usd-ci/USD/pxr/usd/usdShade/shaderDefUtils.cpp -- Unable to resolve info:sourceAsset </UsdPreviewSurface.info:mdl:sourceAsset> with value @UsdPreviewSurface.mdl@.

2025-06-03 06:50:10 [10,416ms] [Warning] [omni.usd] Warning: in GetNodeDiscoveryResults at line 136 of /builds/omniverse/usd-ci/USD/pxr/usd/usdShade/shaderDefUtils.cpp -- Unable to resolve info:sourceAsset </UsdUVTexture.info:mdl:sourceAsset> with value @UsdPreviewSurface.mdl@.

2025-06-03 06:50:10 [10,416ms] [Warning] [omni.usd] Warning: in GetNodeDiscoveryResults at line 136 of /builds/omniverse/usd-ci/USD/pxr/usd/usdShade/shaderDefUtils.cpp -- Unable to resolve info:sourceAsset </UsdPrimvarReader_float.info:mdl:sourceAsset> with value @UsdPreviewSurface.mdl@.

2025-06-03 06:50:10 [10,416ms] [Warning] [omni.usd] Warning: in GetNodeDiscoveryResults at line 136 of /builds/omniverse/usd-ci/USD/pxr/usd/usdShade/shaderDefUtils.cpp -- Unable to resolve info:so[INFO] Logging experiment in directory: /home/ad/setup/manager-based/go1/logs/rsl_rl/unitree_go1_rough
Exact experiment name requested from command line: 2025-06-03_13-50-05
Setting seed: 42
[INFO]: Base environment:
	Environment device    : cuda:0
	Environment seed      : 42
	Physics step-size     : 0.005
	Rendering step-size   : 0.02
	Environment step-size : 0.02
[INFO] Generating terrains based on curriculum took : 0.773604 seconds
[INFO]: Time taken for scene creation : 2.988687 seconds
[INFO]: Scene manager:  <class InteractiveScene>
	Number of environments: 1500
	Environment spacing   : 2.5
	Source prim name      : /World/envs/env_0
	Global prim paths     : ['/World/ground']
	Replicate physics     : True
[INFO]: Starting the simulation. This may take a few seconds. Please wait...
[INFO]: Time taken for simulation start : 2.171251 seconds
[INFO] Command Manager:  <CommandManager> contains 1 active terms.
+------------------------------------------------+
|              Active Command Terms              |
+-------+---------------+------------------------+
| Index | Name          |          Type          |
+-------+---------------+------------------------+
|   0   | base_velocity | UniformVelocityCommand |
+-------+---------------+------------------------+

[INFO] Event Manager:  <EventManager> contains 2 active terms.
+--------------------------------------+
| Active Event Terms in Mode: 'startup' |
+----------+---------------------------+
|  Index   | Name                      |
+----------+---------------------------+
|    0     | physics_material          |
|    1     | add_base_mass             |
+----------+---------------------------+
+---------------------------------------+
|  Active Event Terms in Mode: 'reset'  |
+--------+------------------------------+
| Index  | Name                         |
+--------+------------------------------+
|   0    | base_external_force_torque   |
|   1    | reset_base                   |
|   2    | reset_robot_joints           |
+--------+------------------------------+

[INFO] Recorder Manager:  <RecorderManager> contains 0 active terms.
+---------------------+
| Active Recorder Terms |
+-----------+---------+
|   Index   | Name    |
+-----------+---------+
+-----------+---------+

[INFO] Action Manager:  <ActionManager> contains 1 active terms.
+------------------------------------+
|  Active Action Terms (shape: 12)   |
+--------+-------------+-------------+
| Index  | Name        |   Dimension |
+--------+-------------+-------------+
|   0    | joint_pos   |          12 |
+--------+-------------+-------------+

[INFO] Observation Manager: <ObservationManager> contains 1 groups.
+----------------------------------------------------------+
| Active Observation Terms in Group: 'policy' (shape: (235,)) |
+-----------+--------------------------------+-------------+
|   Index   | Name                           |    Shape    |
+-----------+--------------------------------+-------------+
|     0     | base_lin_vel                   |     (3,)    |
|     1     | base_ang_vel                   |     (3,)    |
|     2     | projected_gravity              |     (3,)    |
|     3     | velocity_commands              |     (3,)    |
|     4     | joint_pos                      |    (12,)    |
|     5     | joint_vel                      |    (12,)    |
|     6     | actions                        |    (12,)    |
|     7     | height_scan                    |    (187,)   |
+-----------+--------------------------------+-------------+

[INFO] Termination Manager:  <TerminationManager> contains 2 active terms.
+---------------------------------+
|     Active Termination Terms    |
+-------+--------------+----------+
| Index | Name         | Time Out |
+-------+--------------+----------+
|   0   | time_out     |   True   |
|   1   | base_contact |  False   |
+-------+--------------+----------+

[INFO] Reward Manager:  <RewardManager> contains 10 active terms.
+-----------------------------------------+
|           Active Reward Terms           |
+-------+----------------------+----------+
| Index | Name                 |   Weight |
+-------+----------------------+----------+
|   0   | track_lin_vel_xy_exp |      1.5 |
|   1   | track_ang_vel_z_exp  |     0.75 |
|   2   | lin_vel_z_l2         |     -2.0 |
|   3   | ang_vel_xy_l2        |    -0.05 |
|   4   | dof_torques_l2       |  -0.0002 |
|   5   | dof_acc_l2           | -2.5e-07 |
|   6   | action_rate_l2       |    -0.01 |
|   7   | feet_air_time        |     0.01 |
|   8   | flat_orientation_l2  |      0.0 |
|   9   | dof_pos_limits       |      0.0 |
+-------+----------------------+----------+

[INFO] Curriculum Manager:  <CurriculumManager> contains 1 active terms.
+---------------------------+
|  Active Curriculum Terms  |
+--------+------------------+
| Index  | Name             |
+--------+------------------+
|   0    | terrain_levels   |
+--------+------------------+

[INFO]: Completed setting up the environment...
Actor MLP: Sequential(
  (0): Linear(in_features=235, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=12, bias=True)
)
Critic MLP: Sequential(
  (0): Linear(in_features=235, out_features=512, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=512, out_features=256, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=256, out_features=128, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=128, out_features=1, bias=True)
)
################################################################################
                      [1m Learning iteration 0/1500 [0m                       

                       Computation: 18596 steps/s (collection: 1.761s, learning 0.175s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0700
               Mean surrogate loss: 0.0382
                 Mean entropy loss: 17.0208
                       Mean reward: -0.58
               Mean episode length: 13.63
Episode_Reward/track_lin_vel_xy_exp: 0.0039
Episode_Reward/track_ang_vel_z_exp: 0.0028
       Episode_Reward/lin_vel_z_l2: -0.0160
      Episode_Reward/ang_vel_xy_l2: -0.0044
     Episode_Reward/dof_torques_l2: -0.0010
         Episode_Reward/dof_acc_l2: -0.0054
     Episode_Reward/action_rate_l2: -0.0028
      Episode_Reward/feet_air_time: -0.0000
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5052
Metrics/base_velocity/error_vel_xy: 0.0173
Metrics/base_velocity/error_vel_yaw: 0.0167
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 36000
                    Iteration time: 1.94s
                      Time elapsed: 00:00:01
                               ETA: 00:48:23

Could not find git repository in /home/ad/miniconda3/envs/isaaclab/lib/python3.10/site-packages/rsl_rl/__init__.py. Skipping.
Storing git diff for 'go1' in: /home/ad/setup/manager-based/go1/logs/rsl_rl/unitree_go1_rough/2025-06-03_13-50-05/git/go1.diff
################################################################################
                      [1m Learning iteration 1/1500 [0m                       

                       Computation: 34890 steps/s (collection: 0.979s, learning 0.053s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0135
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.0088
                       Mean reward: -0.69
               Mean episode length: 24.85
Episode_Reward/track_lin_vel_xy_exp: 0.0143
Episode_Reward/track_ang_vel_z_exp: 0.0077
       Episode_Reward/lin_vel_z_l2: -0.0223
      Episode_Reward/ang_vel_xy_l2: -0.0130
     Episode_Reward/dof_torques_l2: -0.0029
         Episode_Reward/dof_acc_l2: -0.0098
     Episode_Reward/action_rate_l2: -0.0085
      Episode_Reward/feet_air_time: -0.0001
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4647
Metrics/base_velocity/error_vel_xy: 0.0495
Metrics/base_velocity/error_vel_yaw: 0.0600
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 72000
                    Iteration time: 1.03s
                      Time elapsed: 00:00:02
                               ETA: 00:37:04

################################################################################
                      [1m Learning iteration 2/1500 [0m                       

                       Computation: 34939 steps/s (collection: 0.979s, learning 0.051s)
             Mean action noise std: 0.99
          Mean value_function loss: 0.0110
               Mean surrogate loss: -0.0070
                 Mean entropy loss: 16.9166
                       Mean reward: -0.87
               Mean episode length: 47.70
Episode_Reward/track_lin_vel_xy_exp: 0.0135
Episode_Reward/track_ang_vel_z_exp: 0.0125
       Episode_Reward/lin_vel_z_l2: -0.0258
      Episode_Reward/ang_vel_xy_l2: -0.0200
     Episode_Reward/dof_torques_l2: -0.0051
         Episode_Reward/dof_acc_l2: -0.0139
     Episode_Reward/action_rate_l2: -0.0143
      Episode_Reward/feet_air_time: -0.0001
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4349
Metrics/base_velocity/error_vel_xy: 0.0997
Metrics/base_velocity/error_vel_yaw: 0.1033
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 108000
                    Iteration time: 1.03s
                      Time elapsed: 00:00:03
                               ETA: 00:33:16

################################################################################
                      [1m Learning iteration 3/1500 [0m                       

                       Computation: 34218 steps/s (collection: 0.999s, learning 0.053s)
             Mean action noise std: 0.97
          Mean value_function loss: 0.0088
               Mean surrogate loss: -0.0074
                 Mean entropy loss: 16.7442
                       Mean reward: -1.08
               Mean episode length: 75.03
Episode_Reward/track_lin_vel_xy_exp: 0.0237
Episode_Reward/track_ang_vel_z_exp: 0.0187
       Episode_Reward/lin_vel_z_l2: -0.0276
      Episode_Reward/ang_vel_xy_l2: -0.0305
     Episode_Reward/dof_torques_l2: -0.0066
         Episode_Reward/dof_acc_l2: -0.0166
     Episode_Reward/action_rate_l2: -0.0198
      Episode_Reward/feet_air_time: -0.0002
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3985
Metrics/base_velocity/error_vel_xy: 0.1354
Metrics/base_velocity/error_vel_yaw: 0.1319
      Episode_Termination/time_out: 2.0833
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 144000
                    Iteration time: 1.05s
                      Time elapsed: 00:00:05
                               ETA: 00:31:29

################################################################################
                      [1m Learning iteration 4/1500 [0m                       

                       Computation: 34695 steps/s (collection: 0.986s, learning 0.052s)
             Mean action noise std: 0.96
          Mean value_function loss: 0.0084
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 16.5983
                       Mean reward: -1.14
               Mean episode length: 94.86
Episode_Reward/track_lin_vel_xy_exp: 0.0325
Episode_Reward/track_ang_vel_z_exp: 0.0252
       Episode_Reward/lin_vel_z_l2: -0.0262
      Episode_Reward/ang_vel_xy_l2: -0.0363
     Episode_Reward/dof_torques_l2: -0.0086
         Episode_Reward/dof_acc_l2: -0.0194
     Episode_Reward/action_rate_l2: -0.0260
      Episode_Reward/feet_air_time: -0.0003
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3632
Metrics/base_velocity/error_vel_xy: 0.1630
Metrics/base_velocity/error_vel_yaw: 0.1720
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 180000
                    Iteration time: 1.04s
                      Time elapsed: 00:00:06
                               ETA: 00:30:21

################################################################################
                      [1m Learning iteration 5/1500 [0m                       

                       Computation: 34717 steps/s (collection: 0.984s, learning 0.053s)
             Mean action noise std: 0.95
          Mean value_function loss: 0.0081
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 16.4572
                       Mean reward: -1.26
               Mean episode length: 118.52
Episode_Reward/track_lin_vel_xy_exp: 0.0295
Episode_Reward/track_ang_vel_z_exp: 0.0322
       Episode_Reward/lin_vel_z_l2: -0.0293
      Episode_Reward/ang_vel_xy_l2: -0.0408
     Episode_Reward/dof_torques_l2: -0.0100
         Episode_Reward/dof_acc_l2: -0.0208
     Episode_Reward/action_rate_l2: -0.0297
      Episode_Reward/feet_air_time: -0.0003
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3336
Metrics/base_velocity/error_vel_xy: 0.2107
Metrics/base_velocity/error_vel_yaw: 0.1933
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 216000
                    Iteration time: 1.04s
                      Time elapsed: 00:00:07
                               ETA: 00:29:35

################################################################################
                      [1m Learning iteration 6/1500 [0m                       

                       Computation: 33742 steps/s (collection: 1.015s, learning 0.052s)
             Mean action noise std: 0.94
          Mean value_function loss: 0.0077
               Mean surrogate loss: -0.0062
                 Mean entropy loss: 16.2931
                       Mean reward: -1.25
               Mean episode length: 142.11
Episode_Reward/track_lin_vel_xy_exp: 0.0602
Episode_Reward/track_ang_vel_z_exp: 0.0366
       Episode_Reward/lin_vel_z_l2: -0.0285
      Episode_Reward/ang_vel_xy_l2: -0.0474
     Episode_Reward/dof_torques_l2: -0.0124
         Episode_Reward/dof_acc_l2: -0.0251
     Episode_Reward/action_rate_l2: -0.0358
      Episode_Reward/feet_air_time: -0.0004
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2997
Metrics/base_velocity/error_vel_xy: 0.2154
Metrics/base_velocity/error_vel_yaw: 0.2463
      Episode_Termination/time_out: 2.0417
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 252000
                    Iteration time: 1.07s
                      Time elapsed: 00:00:08
                               ETA: 00:29:08

################################################################################
                      [1m Learning iteration 7/1500 [0m                       

                       Computation: 32212 steps/s (collection: 1.063s, learning 0.054s)
             Mean action noise std: 0.93
          Mean value_function loss: 0.0064
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 16.1968
                       Mean reward: -1.31
               Mean episode length: 160.58
Episode_Reward/track_lin_vel_xy_exp: 0.0567
Episode_Reward/track_ang_vel_z_exp: 0.0389
       Episode_Reward/lin_vel_z_l2: -0.0288
      Episode_Reward/ang_vel_xy_l2: -0.0554
     Episode_Reward/dof_torques_l2: -0.0132
         Episode_Reward/dof_acc_l2: -0.0271
     Episode_Reward/action_rate_l2: -0.0399
      Episode_Reward/feet_air_time: -0.0004
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2719
Metrics/base_velocity/error_vel_xy: 0.2654
Metrics/base_velocity/error_vel_yaw: 0.2799
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 288000
                    Iteration time: 1.12s
                      Time elapsed: 00:00:09
                               ETA: 00:28:57

################################################################################
                      [1m Learning iteration 8/1500 [0m                       

                       Computation: 31079 steps/s (collection: 1.104s, learning 0.054s)
             Mean action noise std: 0.92
          Mean value_function loss: 0.0127
               Mean surrogate loss: -0.0074
                 Mean entropy loss: 16.0408
                       Mean reward: -1.43
               Mean episode length: 182.23
Episode_Reward/track_lin_vel_xy_exp: 0.0645
Episode_Reward/track_ang_vel_z_exp: 0.0491
       Episode_Reward/lin_vel_z_l2: -0.0323
      Episode_Reward/ang_vel_xy_l2: -0.0639
     Episode_Reward/dof_torques_l2: -0.0161
         Episode_Reward/dof_acc_l2: -0.0285
     Episode_Reward/action_rate_l2: -0.0446
      Episode_Reward/feet_air_time: -0.0005
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2439
Metrics/base_velocity/error_vel_xy: 0.2823
Metrics/base_velocity/error_vel_yaw: 0.3058
      Episode_Termination/time_out: 1.1667
  Episode_Termination/base_contact: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 324000
                    Iteration time: 1.16s
                      Time elapsed: 00:00:10
                               ETA: 00:28:55

################################################################################
                      [1m Learning iteration 9/1500 [0m                       

                       Computation: 32917 steps/s (collection: 1.041s, learning 0.053s)
             Mean action noise std: 0.91
          Mean value_function loss: 0.0078
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 15.9093
                       Mean reward: -1.80
               Mean episode length: 212.38
Episode_Reward/track_lin_vel_xy_exp: 0.0550
Episode_Reward/track_ang_vel_z_exp: 0.0501
       Episode_Reward/lin_vel_z_l2: -0.0354
      Episode_Reward/ang_vel_xy_l2: -0.0715
     Episode_Reward/dof_torques_l2: -0.0164
         Episode_Reward/dof_acc_l2: -0.0336
     Episode_Reward/action_rate_l2: -0.0487
      Episode_Reward/feet_air_time: -0.0005
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2096
Metrics/base_velocity/error_vel_xy: 0.3551
Metrics/base_velocity/error_vel_yaw: 0.3524
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 360000
                    Iteration time: 1.09s
                      Time elapsed: 00:00:11
                               ETA: 00:28:43

################################################################################
                      [1m Learning iteration 10/1500 [0m                      

                       Computation: 31167 steps/s (collection: 1.103s, learning 0.052s)
             Mean action noise std: 0.90
          Mean value_function loss: 0.0056
               Mean surrogate loss: -0.0072
                 Mean entropy loss: 15.7750
                       Mean reward: -1.74
               Mean episode length: 232.31
Episode_Reward/track_lin_vel_xy_exp: 0.0855
Episode_Reward/track_ang_vel_z_exp: 0.0570
       Episode_Reward/lin_vel_z_l2: -0.0337
      Episode_Reward/ang_vel_xy_l2: -0.0736
     Episode_Reward/dof_torques_l2: -0.0182
         Episode_Reward/dof_acc_l2: -0.0356
     Episode_Reward/action_rate_l2: -0.0537
      Episode_Reward/feet_air_time: -0.0006
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1664
Metrics/base_velocity/error_vel_xy: 0.3515
Metrics/base_velocity/error_vel_yaw: 0.3774
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 396000
                    Iteration time: 1.16s
                      Time elapsed: 00:00:12
                               ETA: 00:28:42

################################################################################
                      [1m Learning iteration 11/1500 [0m                      

                       Computation: 32941 steps/s (collection: 1.040s, learning 0.053s)
             Mean action noise std: 0.89
          Mean value_function loss: 0.0053
               Mean surrogate loss: -0.0073
                 Mean entropy loss: 15.6732
                       Mean reward: -1.63
               Mean episode length: 250.59
Episode_Reward/track_lin_vel_xy_exp: 0.0826
Episode_Reward/track_ang_vel_z_exp: 0.0680
       Episode_Reward/lin_vel_z_l2: -0.0338
      Episode_Reward/ang_vel_xy_l2: -0.0778
     Episode_Reward/dof_torques_l2: -0.0198
         Episode_Reward/dof_acc_l2: -0.0348
     Episode_Reward/action_rate_l2: -0.0568
      Episode_Reward/feet_air_time: -0.0006
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1267
Metrics/base_velocity/error_vel_xy: 0.3993
Metrics/base_velocity/error_vel_yaw: 0.3856
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 432000
                    Iteration time: 1.09s
                      Time elapsed: 00:00:13
                               ETA: 00:28:33

################################################################################
                      [1m Learning iteration 12/1500 [0m                      

                       Computation: 33990 steps/s (collection: 1.007s, learning 0.052s)
             Mean action noise std: 0.88
          Mean value_function loss: 0.0084
               Mean surrogate loss: -0.0072
                 Mean entropy loss: 15.5557
                       Mean reward: -1.59
               Mean episode length: 264.67
Episode_Reward/track_lin_vel_xy_exp: 0.0882
Episode_Reward/track_ang_vel_z_exp: 0.0645
       Episode_Reward/lin_vel_z_l2: -0.0455
      Episode_Reward/ang_vel_xy_l2: -0.0856
     Episode_Reward/dof_torques_l2: -0.0206
         Episode_Reward/dof_acc_l2: -0.0376
     Episode_Reward/action_rate_l2: -0.0585
      Episode_Reward/feet_air_time: -0.0006
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0945
Metrics/base_velocity/error_vel_xy: 0.4072
Metrics/base_velocity/error_vel_yaw: 0.4189
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 468000
                    Iteration time: 1.06s
                      Time elapsed: 00:00:14
                               ETA: 00:28:21

################################################################################
                      [1m Learning iteration 13/1500 [0m                      

                       Computation: 32828 steps/s (collection: 1.042s, learning 0.054s)
             Mean action noise std: 0.87
          Mean value_function loss: 0.0054
               Mean surrogate loss: -0.0068
                 Mean entropy loss: 15.4450
                       Mean reward: -1.77
               Mean episode length: 280.94
Episode_Reward/track_lin_vel_xy_exp: 0.0825
Episode_Reward/track_ang_vel_z_exp: 0.0722
       Episode_Reward/lin_vel_z_l2: -0.0333
      Episode_Reward/ang_vel_xy_l2: -0.0916
     Episode_Reward/dof_torques_l2: -0.0233
         Episode_Reward/dof_acc_l2: -0.0444
     Episode_Reward/action_rate_l2: -0.0675
      Episode_Reward/feet_air_time: -0.0008
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0651
Metrics/base_velocity/error_vel_xy: 0.5163
Metrics/base_velocity/error_vel_yaw: 0.4875
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 504000
                    Iteration time: 1.10s
                      Time elapsed: 00:00:15
                               ETA: 00:28:15

################################################################################
                      [1m Learning iteration 14/1500 [0m                      

                       Computation: 32326 steps/s (collection: 1.062s, learning 0.052s)
             Mean action noise std: 0.87
          Mean value_function loss: 0.0043
               Mean surrogate loss: -0.0075
                 Mean entropy loss: 15.3279
                       Mean reward: -2.02
               Mean episode length: 309.01
Episode_Reward/track_lin_vel_xy_exp: 0.0908
Episode_Reward/track_ang_vel_z_exp: 0.0808
       Episode_Reward/lin_vel_z_l2: -0.0350
      Episode_Reward/ang_vel_xy_l2: -0.0957
     Episode_Reward/dof_torques_l2: -0.0242
         Episode_Reward/dof_acc_l2: -0.0417
     Episode_Reward/action_rate_l2: -0.0697
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0356
Metrics/base_velocity/error_vel_xy: 0.5258
Metrics/base_velocity/error_vel_yaw: 0.5019
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 540000
                    Iteration time: 1.11s
                      Time elapsed: 00:00:17
                               ETA: 00:28:11

################################################################################
                      [1m Learning iteration 15/1500 [0m                      

                       Computation: 33209 steps/s (collection: 1.031s, learning 0.053s)
             Mean action noise std: 0.86
          Mean value_function loss: 0.0048
               Mean surrogate loss: -0.0080
                 Mean entropy loss: 15.2197
                       Mean reward: -2.27
               Mean episode length: 319.88
Episode_Reward/track_lin_vel_xy_exp: 0.0635
Episode_Reward/track_ang_vel_z_exp: 0.0706
       Episode_Reward/lin_vel_z_l2: -0.0384
      Episode_Reward/ang_vel_xy_l2: -0.0895
     Episode_Reward/dof_torques_l2: -0.0244
         Episode_Reward/dof_acc_l2: -0.0406
     Episode_Reward/action_rate_l2: -0.0663
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9996
Metrics/base_velocity/error_vel_xy: 0.5449
Metrics/base_velocity/error_vel_yaw: 0.5080
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 576000
                    Iteration time: 1.08s
                      Time elapsed: 00:00:18
                               ETA: 00:28:05

################################################################################
                      [1m Learning iteration 16/1500 [0m                      

                       Computation: 32466 steps/s (collection: 1.055s, learning 0.053s)
             Mean action noise std: 0.85
          Mean value_function loss: 0.0353
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 15.1077
                       Mean reward: -2.30
               Mean episode length: 330.17
Episode_Reward/track_lin_vel_xy_exp: 0.0817
Episode_Reward/track_ang_vel_z_exp: 0.0792
       Episode_Reward/lin_vel_z_l2: -0.0364
      Episode_Reward/ang_vel_xy_l2: -0.0912
     Episode_Reward/dof_torques_l2: -0.0252
         Episode_Reward/dof_acc_l2: -0.0440
     Episode_Reward/action_rate_l2: -0.0691
      Episode_Reward/feet_air_time: -0.0008
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9617
Metrics/base_velocity/error_vel_xy: 0.5953
Metrics/base_velocity/error_vel_yaw: 0.5262
      Episode_Termination/time_out: 1.0833
  Episode_Termination/base_contact: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 612000
                    Iteration time: 1.11s
                      Time elapsed: 00:00:19
                               ETA: 00:28:02

################################################################################
                      [1m Learning iteration 17/1500 [0m                      

                       Computation: 33701 steps/s (collection: 1.015s, learning 0.054s)
             Mean action noise std: 0.85
          Mean value_function loss: 0.0087
               Mean surrogate loss: -0.0078
                 Mean entropy loss: 15.0357
                       Mean reward: -2.43
               Mean episode length: 347.42
Episode_Reward/track_lin_vel_xy_exp: 0.0652
Episode_Reward/track_ang_vel_z_exp: 0.0798
       Episode_Reward/lin_vel_z_l2: -0.0394
      Episode_Reward/ang_vel_xy_l2: -0.0921
     Episode_Reward/dof_torques_l2: -0.0248
         Episode_Reward/dof_acc_l2: -0.0419
     Episode_Reward/action_rate_l2: -0.0693
      Episode_Reward/feet_air_time: -0.0008
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9158
Metrics/base_velocity/error_vel_xy: 0.6014
Metrics/base_velocity/error_vel_yaw: 0.5332
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 648000
                    Iteration time: 1.07s
                      Time elapsed: 00:00:20
                               ETA: 00:27:55

################################################################################
                      [1m Learning iteration 18/1500 [0m                      

                       Computation: 33102 steps/s (collection: 1.035s, learning 0.052s)
             Mean action noise std: 0.84
          Mean value_function loss: 0.0064
               Mean surrogate loss: -0.0089
                 Mean entropy loss: 14.9671
                       Mean reward: -1.50
               Mean episode length: 341.39
Episode_Reward/track_lin_vel_xy_exp: 0.1182
Episode_Reward/track_ang_vel_z_exp: 0.0821
       Episode_Reward/lin_vel_z_l2: -0.0346
      Episode_Reward/ang_vel_xy_l2: -0.0919
     Episode_Reward/dof_torques_l2: -0.0261
         Episode_Reward/dof_acc_l2: -0.0436
     Episode_Reward/action_rate_l2: -0.0706
      Episode_Reward/feet_air_time: -0.0008
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.8629
Metrics/base_velocity/error_vel_xy: 0.5367
Metrics/base_velocity/error_vel_yaw: 0.5592
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 684000
                    Iteration time: 1.09s
                      Time elapsed: 00:00:21
                               ETA: 00:27:51

################################################################################
                      [1m Learning iteration 19/1500 [0m                      

                       Computation: 32212 steps/s (collection: 1.066s, learning 0.052s)
             Mean action noise std: 0.84
          Mean value_function loss: 0.0066
               Mean surrogate loss: -0.0074
                 Mean entropy loss: 14.9156
                       Mean reward: -1.76
               Mean episode length: 341.49
Episode_Reward/track_lin_vel_xy_exp: 0.0826
Episode_Reward/track_ang_vel_z_exp: 0.0844
       Episode_Reward/lin_vel_z_l2: -0.0341
      Episode_Reward/ang_vel_xy_l2: -0.0876
     Episode_Reward/dof_torques_l2: -0.0252
         Episode_Reward/dof_acc_l2: -0.0411
     Episode_Reward/action_rate_l2: -0.0679
      Episode_Reward/feet_air_time: -0.0008
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.8014
Metrics/base_velocity/error_vel_xy: 0.5798
Metrics/base_velocity/error_vel_yaw: 0.5205
      Episode_Termination/time_out: 1.0833
  Episode_Termination/base_contact: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 720000
                    Iteration time: 1.12s
                      Time elapsed: 00:00:22
                               ETA: 00:27:49

################################################################################
                      [1m Learning iteration 20/1500 [0m                      

                       Computation: 33594 steps/s (collection: 1.018s, learning 0.054s)
             Mean action noise std: 0.84
          Mean value_function loss: 0.0248
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 14.8890
                       Mean reward: -0.95
               Mean episode length: 293.17
Episode_Reward/track_lin_vel_xy_exp: 0.0957
Episode_Reward/track_ang_vel_z_exp: 0.0771
       Episode_Reward/lin_vel_z_l2: -0.0307
      Episode_Reward/ang_vel_xy_l2: -0.0754
     Episode_Reward/dof_torques_l2: -0.0210
         Episode_Reward/dof_acc_l2: -0.0372
     Episode_Reward/action_rate_l2: -0.0583
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7327
Metrics/base_velocity/error_vel_xy: 0.4660
Metrics/base_velocity/error_vel_yaw: 0.4359
      Episode_Termination/time_out: 0.6667
  Episode_Termination/base_contact: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 756000
                    Iteration time: 1.07s
                      Time elapsed: 00:00:23
                               ETA: 00:27:44

################################################################################
                      [1m Learning iteration 21/1500 [0m                      

                       Computation: 32003 steps/s (collection: 1.072s, learning 0.053s)
             Mean action noise std: 0.84
          Mean value_function loss: 0.0083
               Mean surrogate loss: -0.0075
                 Mean entropy loss: 14.8778
                       Mean reward: -0.76
               Mean episode length: 294.18
Episode_Reward/track_lin_vel_xy_exp: 0.0966
Episode_Reward/track_ang_vel_z_exp: 0.0835
       Episode_Reward/lin_vel_z_l2: -0.0313
      Episode_Reward/ang_vel_xy_l2: -0.0766
     Episode_Reward/dof_torques_l2: -0.0226
         Episode_Reward/dof_acc_l2: -0.0360
     Episode_Reward/action_rate_l2: -0.0599
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6520
Metrics/base_velocity/error_vel_xy: 0.4668
Metrics/base_velocity/error_vel_yaw: 0.4429
      Episode_Termination/time_out: 0.8333
  Episode_Termination/base_contact: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 792000
                    Iteration time: 1.12s
                      Time elapsed: 00:00:24
                               ETA: 00:27:43

################################################################################
                      [1m Learning iteration 22/1500 [0m                      

                       Computation: 33123 steps/s (collection: 1.035s, learning 0.052s)
             Mean action noise std: 0.83
          Mean value_function loss: 0.0089
               Mean surrogate loss: -0.0077
                 Mean entropy loss: 14.8225
                       Mean reward: -0.69
               Mean episode length: 359.69
Episode_Reward/track_lin_vel_xy_exp: 0.1165
Episode_Reward/track_ang_vel_z_exp: 0.0867
       Episode_Reward/lin_vel_z_l2: -0.0320
      Episode_Reward/ang_vel_xy_l2: -0.0807
     Episode_Reward/dof_torques_l2: -0.0241
         Episode_Reward/dof_acc_l2: -0.0376
     Episode_Reward/action_rate_l2: -0.0634
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5563
Metrics/base_velocity/error_vel_xy: 0.4716
Metrics/base_velocity/error_vel_yaw: 0.4722
      Episode_Termination/time_out: 0.9583
  Episode_Termination/base_contact: 7.0000
--------------------------------------------------------------------------------
                   Total timesteps: 828000
                    Iteration time: 1.09s
                      Time elapsed: 00:00:25
                               ETA: 00:27:39

################################################################################
                      [1m Learning iteration 23/1500 [0m                      

                       Computation: 33340 steps/s (collection: 1.025s, learning 0.054s)
             Mean action noise std: 0.83
          Mean value_function loss: 0.0082
               Mean surrogate loss: -0.0093
                 Mean entropy loss: 14.7751
                       Mean reward: -0.58
               Mean episode length: 327.33
Episode_Reward/track_lin_vel_xy_exp: 0.1149
Episode_Reward/track_ang_vel_z_exp: 0.0801
       Episode_Reward/lin_vel_z_l2: -0.0312
      Episode_Reward/ang_vel_xy_l2: -0.0757
     Episode_Reward/dof_torques_l2: -0.0231
         Episode_Reward/dof_acc_l2: -0.0352
     Episode_Reward/action_rate_l2: -0.0593
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.4561
Metrics/base_velocity/error_vel_xy: 0.4397
Metrics/base_velocity/error_vel_yaw: 0.4560
      Episode_Termination/time_out: 0.9167
  Episode_Termination/base_contact: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 864000
                    Iteration time: 1.08s
                      Time elapsed: 00:00:26
                               ETA: 00:27:35

################################################################################
                      [1m Learning iteration 24/1500 [0m                      

                       Computation: 33379 steps/s (collection: 1.027s, learning 0.051s)
             Mean action noise std: 0.82
          Mean value_function loss: 0.0083
               Mean surrogate loss: -0.0071
                 Mean entropy loss: 14.6972
                       Mean reward: -0.13
               Mean episode length: 261.24
Episode_Reward/track_lin_vel_xy_exp: 0.1032
Episode_Reward/track_ang_vel_z_exp: 0.0720
       Episode_Reward/lin_vel_z_l2: -0.0274
      Episode_Reward/ang_vel_xy_l2: -0.0623
     Episode_Reward/dof_torques_l2: -0.0194
         Episode_Reward/dof_acc_l2: -0.0302
     Episode_Reward/action_rate_l2: -0.0494
      Episode_Reward/feet_air_time: -0.0006
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.3686
Metrics/base_velocity/error_vel_xy: 0.3685
Metrics/base_velocity/error_vel_yaw: 0.3662
      Episode_Termination/time_out: 0.4583
  Episode_Termination/base_contact: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 900000
                    Iteration time: 1.08s
                      Time elapsed: 00:00:27
                               ETA: 00:27:32

################################################################################
                      [1m Learning iteration 25/1500 [0m                      

                       Computation: 32677 steps/s (collection: 1.050s, learning 0.052s)
             Mean action noise std: 0.82
          Mean value_function loss: 0.0157
               Mean surrogate loss: -0.0089
                 Mean entropy loss: 14.6466
                       Mean reward: -0.35
               Mean episode length: 248.42
Episode_Reward/track_lin_vel_xy_exp: 0.1051
Episode_Reward/track_ang_vel_z_exp: 0.0647
       Episode_Reward/lin_vel_z_l2: -0.0256
      Episode_Reward/ang_vel_xy_l2: -0.0571
     Episode_Reward/dof_torques_l2: -0.0178
         Episode_Reward/dof_acc_l2: -0.0278
     Episode_Reward/action_rate_l2: -0.0446
      Episode_Reward/feet_air_time: -0.0005
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.2925
Metrics/base_velocity/error_vel_xy: 0.3197
Metrics/base_velocity/error_vel_yaw: 0.3446
      Episode_Termination/time_out: 0.3750
  Episode_Termination/base_contact: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 936000
                    Iteration time: 1.10s
                      Time elapsed: 00:00:29
                               ETA: 00:27:30

################################################################################
                      [1m Learning iteration 26/1500 [0m                      

                       Computation: 33653 steps/s (collection: 1.017s, learning 0.053s)
             Mean action noise std: 0.82
          Mean value_function loss: 0.0101
               Mean surrogate loss: -0.0080
                 Mean entropy loss: 14.5915
                       Mean reward: -0.19
               Mean episode length: 228.99
Episode_Reward/track_lin_vel_xy_exp: 0.0849
Episode_Reward/track_ang_vel_z_exp: 0.0635
       Episode_Reward/lin_vel_z_l2: -0.0241
      Episode_Reward/ang_vel_xy_l2: -0.0491
     Episode_Reward/dof_torques_l2: -0.0164
         Episode_Reward/dof_acc_l2: -0.0239
     Episode_Reward/action_rate_l2: -0.0393
      Episode_Reward/feet_air_time: -0.0005
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.2240
Metrics/base_velocity/error_vel_xy: 0.3096
Metrics/base_velocity/error_vel_yaw: 0.2876
      Episode_Termination/time_out: 0.3750
  Episode_Termination/base_contact: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 972000
                    Iteration time: 1.07s
                      Time elapsed: 00:00:30
                               ETA: 00:27:26

################################################################################
                      [1m Learning iteration 27/1500 [0m                      

                       Computation: 32928 steps/s (collection: 1.041s, learning 0.052s)
             Mean action noise std: 0.81
          Mean value_function loss: 0.0081
               Mean surrogate loss: -0.0073
                 Mean entropy loss: 14.5267
                       Mean reward: 0.29
               Mean episode length: 238.70
Episode_Reward/track_lin_vel_xy_exp: 0.1255
Episode_Reward/track_ang_vel_z_exp: 0.0749
       Episode_Reward/lin_vel_z_l2: -0.0264
      Episode_Reward/ang_vel_xy_l2: -0.0561
     Episode_Reward/dof_torques_l2: -0.0178
         Episode_Reward/dof_acc_l2: -0.0271
     Episode_Reward/action_rate_l2: -0.0440
      Episode_Reward/feet_air_time: -0.0005
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.1720
Metrics/base_velocity/error_vel_xy: 0.3032
Metrics/base_velocity/error_vel_yaw: 0.3020
      Episode_Termination/time_out: 0.4167
  Episode_Termination/base_contact: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 1008000
                    Iteration time: 1.09s
                      Time elapsed: 00:00:31
                               ETA: 00:27:24

################################################################################
                      [1m Learning iteration 28/1500 [0m                      

                       Computation: 33987 steps/s (collection: 1.008s, learning 0.051s)
             Mean action noise std: 0.81
          Mean value_function loss: 0.0103
               Mean surrogate loss: -0.0072
                 Mean entropy loss: 14.4545
                       Mean reward: -0.33
               Mean episode length: 290.39
Episode_Reward/track_lin_vel_xy_exp: 0.1054
Episode_Reward/track_ang_vel_z_exp: 0.0788
       Episode_Reward/lin_vel_z_l2: -0.0255
      Episode_Reward/ang_vel_xy_l2: -0.0628
     Episode_Reward/dof_torques_l2: -0.0214
         Episode_Reward/dof_acc_l2: -0.0299
     Episode_Reward/action_rate_l2: -0.0513
      Episode_Reward/feet_air_time: -0.0006
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.1226
Metrics/base_velocity/error_vel_xy: 0.3887
Metrics/base_velocity/error_vel_yaw: 0.3880
      Episode_Termination/time_out: 0.5833
  Episode_Termination/base_contact: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 1044000
                    Iteration time: 1.06s
                      Time elapsed: 00:00:32
                               ETA: 00:27:20

################################################################################
                      [1m Learning iteration 29/1500 [0m                      

                       Computation: 32745 steps/s (collection: 1.047s, learning 0.052s)
             Mean action noise std: 0.80
          Mean value_function loss: 0.0077
               Mean surrogate loss: -0.0083
                 Mean entropy loss: 14.4053
                       Mean reward: 0.05
               Mean episode length: 299.62
Episode_Reward/track_lin_vel_xy_exp: 0.1123
Episode_Reward/track_ang_vel_z_exp: 0.0766
       Episode_Reward/lin_vel_z_l2: -0.0266
      Episode_Reward/ang_vel_xy_l2: -0.0629
     Episode_Reward/dof_torques_l2: -0.0212
         Episode_Reward/dof_acc_l2: -0.0300
     Episode_Reward/action_rate_l2: -0.0508
      Episode_Reward/feet_air_time: -0.0006
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.0808
Metrics/base_velocity/error_vel_xy: 0.4125
Metrics/base_velocity/error_vel_yaw: 0.3977
      Episode_Termination/time_out: 0.5833
  Episode_Termination/base_contact: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 1080000
                    Iteration time: 1.10s
                      Time elapsed: 00:00:33
                               ETA: 00:27:18

################################################################################
                      [1m Learning iteration 30/1500 [0m                      

                       Computation: 32868 steps/s (collection: 1.042s, learning 0.053s)
             Mean action noise std: 0.80
          Mean value_function loss: 0.0074
               Mean surrogate loss: -0.0085
                 Mean entropy loss: 14.3635
                       Mean reward: 0.26
               Mean episode length: 304.38
Episode_Reward/track_lin_vel_xy_exp: 0.1460
Episode_Reward/track_ang_vel_z_exp: 0.0922
       Episode_Reward/lin_vel_z_l2: -0.0268
      Episode_Reward/ang_vel_xy_l2: -0.0660
     Episode_Reward/dof_torques_l2: -0.0219
         Episode_Reward/dof_acc_l2: -0.0325
     Episode_Reward/action_rate_l2: -0.0529
      Episode_Reward/feet_air_time: -0.0006
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.0468
Metrics/base_velocity/error_vel_xy: 0.3835
Metrics/base_velocity/error_vel_yaw: 0.3591
      Episode_Termination/time_out: 0.5417
  Episode_Termination/base_contact: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 1116000
                    Iteration time: 1.10s
                      Time elapsed: 00:00:34
                               ETA: 00:27:16

################################################################################
                      [1m Learning iteration 31/1500 [0m                      

                       Computation: 33325 steps/s (collection: 1.027s, learning 0.053s)
             Mean action noise std: 0.80
          Mean value_function loss: 0.0065
               Mean surrogate loss: -0.0081
                 Mean entropy loss: 14.3366
                       Mean reward: 0.17
               Mean episode length: 278.33
Episode_Reward/track_lin_vel_xy_exp: 0.1112
Episode_Reward/track_ang_vel_z_exp: 0.0893
       Episode_Reward/lin_vel_z_l2: -0.0271
      Episode_Reward/ang_vel_xy_l2: -0.0677
     Episode_Reward/dof_torques_l2: -0.0228
         Episode_Reward/dof_acc_l2: -0.0315
     Episode_Reward/action_rate_l2: -0.0543
      Episode_Reward/feet_air_time: -0.0006
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.0166
Metrics/base_velocity/error_vel_xy: 0.4466
Metrics/base_velocity/error_vel_yaw: 0.4013
      Episode_Termination/time_out: 0.5417
  Episode_Termination/base_contact: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 1152000
                    Iteration time: 1.08s
                      Time elapsed: 00:00:35
                               ETA: 00:27:13

################################################################################
                      [1m Learning iteration 32/1500 [0m                      

                       Computation: 33451 steps/s (collection: 1.025s, learning 0.051s)
             Mean action noise std: 0.80
          Mean value_function loss: 0.0091
               Mean surrogate loss: -0.0077
                 Mean entropy loss: 14.3065
                       Mean reward: 0.04
               Mean episode length: 302.70
Episode_Reward/track_lin_vel_xy_exp: 0.0838
Episode_Reward/track_ang_vel_z_exp: 0.0703
       Episode_Reward/lin_vel_z_l2: -0.0236
      Episode_Reward/ang_vel_xy_l2: -0.0561
     Episode_Reward/dof_torques_l2: -0.0184
         Episode_Reward/dof_acc_l2: -0.0285
     Episode_Reward/action_rate_l2: -0.0453
      Episode_Reward/feet_air_time: -0.0005
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.9934
Metrics/base_velocity/error_vel_xy: 0.3771
Metrics/base_velocity/error_vel_yaw: 0.3556
      Episode_Termination/time_out: 0.2917
  Episode_Termination/base_contact: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 1188000
                    Iteration time: 1.08s
                      Time elapsed: 00:00:36
                               ETA: 00:27:10

################################################################################
                      [1m Learning iteration 33/1500 [0m                      

                       Computation: 28903 steps/s (collection: 1.193s, learning 0.052s)
             Mean action noise std: 0.80
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0080
                 Mean entropy loss: 14.2620
                       Mean reward: -0.02
               Mean episode length: 325.86
Episode_Reward/track_lin_vel_xy_exp: 0.1162
Episode_Reward/track_ang_vel_z_exp: 0.1005
       Episode_Reward/lin_vel_z_l2: -0.0285
      Episode_Reward/ang_vel_xy_l2: -0.0737
     Episode_Reward/dof_torques_l2: -0.0263
         Episode_Reward/dof_acc_l2: -0.0328
     Episode_Reward/action_rate_l2: -0.0620
      Episode_Reward/feet_air_time: -0.0007
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.9720
Metrics/base_velocity/error_vel_xy: 0.5326
Metrics/base_velocity/error_vel_yaw: 0.4663
      Episode_Termination/time_out: 0.4583
  Episode_Termination/base_contact: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 1224000
                    Iteration time: 1.25s
                      Time elapsed: 00:00:37
                               ETA: 00:27:15

################################################################################
                      [1m Learning iteration 34/1500 [0m                      

                       Computation: 32411 steps/s (collection: 1.059s, learning 0.051s)
             Mean action noise std: 0.79
          Mean value_function loss: 0.0059
               Mean surrogate loss: -0.0075
                 Mean entropy loss: 14.2013
                       Mean reward: -0.43
               Mean episode length: 379.37
Episode_Reward/track_lin_vel_xy_exp: 0.1146
Episode_Reward/track_ang_vel_z_exp: 0.0831
       Episode_Reward/lin_vel_z_l2: -0.0283
      Episode_Reward/ang_vel_xy_l2: -0.0788
     Episode_Reward/dof_torques_l2: -0.0278
         Episode_Reward/dof_acc_l2: -0.0351
     Episode_Reward/action_rate_l2: -0.0646
      Episode_Reward/feet_air_time: -0.0008
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.9474
Metrics/base_velocity/error_vel_xy: 0.5617
Metrics/base_velocity/error_vel_yaw: 0.5867
      Episode_Termination/time_out: 0.6667
  Episode_Termination/base_contact: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 1260000
                    Iteration time: 1.11s
                      Time elapsed: 00:00:39
                               ETA: 00:27:14

################################################################################
                      [1m Learning iteration 35/1500 [0m                      

                       Computation: 32010 steps/s (collection: 1.073s, learning 0.052s)
             Mean action noise std: 0.79
          Mean value_function loss: 0.0100
               Mean surrogate loss: -0.0074
                 Mean entropy loss: 14.1486
                       Mean reward: -0.47
               Mean episode length: 441.73
Episode_Reward/track_lin_vel_xy_exp: 0.1557
Episode_Reward/track_ang_vel_z_exp: 0.1189
       Episode_Reward/lin_vel_z_l2: -0.0345
      Episode_Reward/ang_vel_xy_l2: -0.0944
     Episode_Reward/dof_torques_l2: -0.0327
         Episode_Reward/dof_acc_l2: -0.0432
     Episode_Reward/action_rate_l2: -0.0779
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.9235
Metrics/base_velocity/error_vel_xy: 0.6544
Metrics/base_velocity/error_vel_yaw: 0.6377
      Episode_Termination/time_out: 0.6667
  Episode_Termination/base_contact: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 1296000
                    Iteration time: 1.12s
                      Time elapsed: 00:00:40
                               ETA: 00:27:13

################################################################################
                      [1m Learning iteration 36/1500 [0m                      

                       Computation: 32523 steps/s (collection: 1.053s, learning 0.054s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0060
               Mean surrogate loss: -0.0072
                 Mean entropy loss: 14.1018
                       Mean reward: -0.28
               Mean episode length: 427.00
Episode_Reward/track_lin_vel_xy_exp: 0.1314
Episode_Reward/track_ang_vel_z_exp: 0.0852
       Episode_Reward/lin_vel_z_l2: -0.0257
      Episode_Reward/ang_vel_xy_l2: -0.0742
     Episode_Reward/dof_torques_l2: -0.0291
         Episode_Reward/dof_acc_l2: -0.0361
     Episode_Reward/action_rate_l2: -0.0642
      Episode_Reward/feet_air_time: -0.0008
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.8996
Metrics/base_velocity/error_vel_xy: 0.5409
Metrics/base_velocity/error_vel_yaw: 0.5955
      Episode_Termination/time_out: 0.7083
  Episode_Termination/base_contact: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 1332000
                    Iteration time: 1.11s
                      Time elapsed: 00:00:41
                               ETA: 00:27:12

################################################################################
                      [1m Learning iteration 37/1500 [0m                      

                       Computation: 31983 steps/s (collection: 1.071s, learning 0.055s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0074
               Mean surrogate loss: -0.0078
                 Mean entropy loss: 14.0474
                       Mean reward: -0.25
               Mean episode length: 424.70
Episode_Reward/track_lin_vel_xy_exp: 0.1450
Episode_Reward/track_ang_vel_z_exp: 0.1125
       Episode_Reward/lin_vel_z_l2: -0.0311
      Episode_Reward/ang_vel_xy_l2: -0.0903
     Episode_Reward/dof_torques_l2: -0.0349
         Episode_Reward/dof_acc_l2: -0.0400
     Episode_Reward/action_rate_l2: -0.0778
      Episode_Reward/feet_air_time: -0.0009
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.8773
Metrics/base_velocity/error_vel_xy: 0.6652
Metrics/base_velocity/error_vel_yaw: 0.6765
      Episode_Termination/time_out: 0.5417
  Episode_Termination/base_contact: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 1368000
                    Iteration time: 1.13s
                      Time elapsed: 00:00:42
                               ETA: 00:27:11

################################################################################
                      [1m Learning iteration 38/1500 [0m                      

                       Computation: 32976 steps/s (collection: 1.039s, learning 0.053s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0094
               Mean surrogate loss: -0.0091
                 Mean entropy loss: 13.9868
                       Mean reward: 0.08
               Mean episode length: 469.58
Episode_Reward/track_lin_vel_xy_exp: 0.1935
Episode_Reward/track_ang_vel_z_exp: 0.1273
       Episode_Reward/lin_vel_z_l2: -0.0326
      Episode_Reward/ang_vel_xy_l2: -0.1032
     Episode_Reward/dof_torques_l2: -0.0372
         Episode_Reward/dof_acc_l2: -0.0471
     Episode_Reward/action_rate_l2: -0.0870
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.8589
Metrics/base_velocity/error_vel_xy: 0.7040
Metrics/base_velocity/error_vel_yaw: 0.7700
      Episode_Termination/time_out: 0.4583
  Episode_Termination/base_contact: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 1404000
                    Iteration time: 1.09s
                      Time elapsed: 00:00:43
                               ETA: 00:27:09

################################################################################
                      [1m Learning iteration 39/1500 [0m                      

                       Computation: 32869 steps/s (collection: 1.044s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0070
               Mean surrogate loss: -0.0079
                 Mean entropy loss: 13.9513
                       Mean reward: 0.17
               Mean episode length: 483.77
Episode_Reward/track_lin_vel_xy_exp: 0.1999
Episode_Reward/track_ang_vel_z_exp: 0.1077
       Episode_Reward/lin_vel_z_l2: -0.0300
      Episode_Reward/ang_vel_xy_l2: -0.0860
     Episode_Reward/dof_torques_l2: -0.0319
         Episode_Reward/dof_acc_l2: -0.0411
     Episode_Reward/action_rate_l2: -0.0748
      Episode_Reward/feet_air_time: -0.0009
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.8392
Metrics/base_velocity/error_vel_xy: 0.6354
Metrics/base_velocity/error_vel_yaw: 0.6827
      Episode_Termination/time_out: 0.3750
  Episode_Termination/base_contact: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 1440000
                    Iteration time: 1.10s
                      Time elapsed: 00:00:44
                               ETA: 00:27:07

################################################################################
                      [1m Learning iteration 40/1500 [0m                      

                       Computation: 32757 steps/s (collection: 1.048s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0131
               Mean surrogate loss: -0.0072
                 Mean entropy loss: 13.9329
                       Mean reward: 0.32
               Mean episode length: 499.16
Episode_Reward/track_lin_vel_xy_exp: 0.2000
Episode_Reward/track_ang_vel_z_exp: 0.1790
       Episode_Reward/lin_vel_z_l2: -0.0352
      Episode_Reward/ang_vel_xy_l2: -0.1137
     Episode_Reward/dof_torques_l2: -0.0410
         Episode_Reward/dof_acc_l2: -0.0519
     Episode_Reward/action_rate_l2: -0.0963
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.8217
Metrics/base_velocity/error_vel_xy: 0.8254
Metrics/base_velocity/error_vel_yaw: 0.6976
      Episode_Termination/time_out: 0.7083
  Episode_Termination/base_contact: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 1476000
                    Iteration time: 1.10s
                      Time elapsed: 00:00:45
                               ETA: 00:27:05

################################################################################
                      [1m Learning iteration 41/1500 [0m                      

                       Computation: 32762 steps/s (collection: 1.047s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0114
               Mean surrogate loss: -0.0075
                 Mean entropy loss: 13.9123
                       Mean reward: 0.44
               Mean episode length: 544.87
Episode_Reward/track_lin_vel_xy_exp: 0.1365
Episode_Reward/track_ang_vel_z_exp: 0.1710
       Episode_Reward/lin_vel_z_l2: -0.0331
      Episode_Reward/ang_vel_xy_l2: -0.1051
     Episode_Reward/dof_torques_l2: -0.0393
         Episode_Reward/dof_acc_l2: -0.0472
     Episode_Reward/action_rate_l2: -0.0893
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.8019
Metrics/base_velocity/error_vel_xy: 0.8509
Metrics/base_velocity/error_vel_yaw: 0.6595
      Episode_Termination/time_out: 0.5833
  Episode_Termination/base_contact: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 1512000
                    Iteration time: 1.10s
                      Time elapsed: 00:00:46
                               ETA: 00:27:04

################################################################################
                      [1m Learning iteration 42/1500 [0m                      

                       Computation: 32441 steps/s (collection: 1.058s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0172
               Mean surrogate loss: -0.0066
                 Mean entropy loss: 13.9069
                       Mean reward: 0.17
               Mean episode length: 554.04
Episode_Reward/track_lin_vel_xy_exp: 0.1850
Episode_Reward/track_ang_vel_z_exp: 0.1408
       Episode_Reward/lin_vel_z_l2: -0.0348
      Episode_Reward/ang_vel_xy_l2: -0.1069
     Episode_Reward/dof_torques_l2: -0.0426
         Episode_Reward/dof_acc_l2: -0.0446
     Episode_Reward/action_rate_l2: -0.0920
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.7836
Metrics/base_velocity/error_vel_xy: 0.8387
Metrics/base_velocity/error_vel_yaw: 0.7995
      Episode_Termination/time_out: 0.8333
  Episode_Termination/base_contact: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 1548000
                    Iteration time: 1.11s
                      Time elapsed: 00:00:47
                               ETA: 00:27:03

################################################################################
                      [1m Learning iteration 43/1500 [0m                      

                       Computation: 31767 steps/s (collection: 1.080s, learning 0.054s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0174
               Mean surrogate loss: -0.0069
                 Mean entropy loss: 13.9130
                       Mean reward: 0.58
               Mean episode length: 558.41
Episode_Reward/track_lin_vel_xy_exp: 0.2047
Episode_Reward/track_ang_vel_z_exp: 0.1386
       Episode_Reward/lin_vel_z_l2: -0.0297
      Episode_Reward/ang_vel_xy_l2: -0.0959
     Episode_Reward/dof_torques_l2: -0.0380
         Episode_Reward/dof_acc_l2: -0.0408
     Episode_Reward/action_rate_l2: -0.0838
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.7643
Metrics/base_velocity/error_vel_xy: 0.6904
Metrics/base_velocity/error_vel_yaw: 0.7280
      Episode_Termination/time_out: 0.5833
  Episode_Termination/base_contact: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 1584000
                    Iteration time: 1.13s
                      Time elapsed: 00:00:49
                               ETA: 00:27:02

################################################################################
                      [1m Learning iteration 44/1500 [0m                      

                       Computation: 31645 steps/s (collection: 1.082s, learning 0.055s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0198
               Mean surrogate loss: -0.0071
                 Mean entropy loss: 13.9241
                       Mean reward: 0.94
               Mean episode length: 522.97
Episode_Reward/track_lin_vel_xy_exp: 0.1658
Episode_Reward/track_ang_vel_z_exp: 0.1817
       Episode_Reward/lin_vel_z_l2: -0.0309
      Episode_Reward/ang_vel_xy_l2: -0.1019
     Episode_Reward/dof_torques_l2: -0.0406
         Episode_Reward/dof_acc_l2: -0.0449
     Episode_Reward/action_rate_l2: -0.0892
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.7497
Metrics/base_velocity/error_vel_xy: 0.8230
Metrics/base_velocity/error_vel_yaw: 0.6648
      Episode_Termination/time_out: 0.6250
  Episode_Termination/base_contact: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 1620000
                    Iteration time: 1.14s
                      Time elapsed: 00:00:50
                               ETA: 00:27:02

################################################################################
                      [1m Learning iteration 45/1500 [0m                      

                       Computation: 31479 steps/s (collection: 1.091s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0085
                 Mean entropy loss: 13.9042
                       Mean reward: 0.76
               Mean episode length: 560.14
Episode_Reward/track_lin_vel_xy_exp: 0.1857
Episode_Reward/track_ang_vel_z_exp: 0.1889
       Episode_Reward/lin_vel_z_l2: -0.0345
      Episode_Reward/ang_vel_xy_l2: -0.1141
     Episode_Reward/dof_torques_l2: -0.0481
         Episode_Reward/dof_acc_l2: -0.0495
     Episode_Reward/action_rate_l2: -0.1013
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.7270
Metrics/base_velocity/error_vel_xy: 0.9755
Metrics/base_velocity/error_vel_yaw: 0.8204
      Episode_Termination/time_out: 0.6250
  Episode_Termination/base_contact: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 1656000
                    Iteration time: 1.14s
                      Time elapsed: 00:00:51
                               ETA: 00:27:02

################################################################################
                      [1m Learning iteration 46/1500 [0m                      

                       Computation: 33272 steps/s (collection: 1.031s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0192
               Mean surrogate loss: -0.0074
                 Mean entropy loss: 13.8602
                       Mean reward: 0.74
               Mean episode length: 554.00
Episode_Reward/track_lin_vel_xy_exp: 0.1902
Episode_Reward/track_ang_vel_z_exp: 0.1293
       Episode_Reward/lin_vel_z_l2: -0.0303
      Episode_Reward/ang_vel_xy_l2: -0.0888
     Episode_Reward/dof_torques_l2: -0.0367
         Episode_Reward/dof_acc_l2: -0.0393
     Episode_Reward/action_rate_l2: -0.0773
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.7048
Metrics/base_velocity/error_vel_xy: 0.6806
Metrics/base_velocity/error_vel_yaw: 0.6963
      Episode_Termination/time_out: 0.6250
  Episode_Termination/base_contact: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 1692000
                    Iteration time: 1.08s
                      Time elapsed: 00:00:52
                               ETA: 00:26:59

################################################################################
                      [1m Learning iteration 47/1500 [0m                      

                       Computation: 33477 steps/s (collection: 1.023s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0139
               Mean surrogate loss: -0.0066
                 Mean entropy loss: 13.8521
                       Mean reward: 1.71
               Mean episode length: 534.75
Episode_Reward/track_lin_vel_xy_exp: 0.2937
Episode_Reward/track_ang_vel_z_exp: 0.1750
       Episode_Reward/lin_vel_z_l2: -0.0273
      Episode_Reward/ang_vel_xy_l2: -0.0944
     Episode_Reward/dof_torques_l2: -0.0434
         Episode_Reward/dof_acc_l2: -0.0391
     Episode_Reward/action_rate_l2: -0.0876
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.6882
Metrics/base_velocity/error_vel_xy: 0.7103
Metrics/base_velocity/error_vel_yaw: 0.7076
      Episode_Termination/time_out: 0.4583
  Episode_Termination/base_contact: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 1728000
                    Iteration time: 1.08s
                      Time elapsed: 00:00:53
                               ETA: 00:26:57

################################################################################
                      [1m Learning iteration 48/1500 [0m                      

                       Computation: 32941 steps/s (collection: 1.040s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0103
               Mean surrogate loss: -0.0065
                 Mean entropy loss: 13.8283
                       Mean reward: 1.63
               Mean episode length: 548.92
Episode_Reward/track_lin_vel_xy_exp: 0.2159
Episode_Reward/track_ang_vel_z_exp: 0.1697
       Episode_Reward/lin_vel_z_l2: -0.0295
      Episode_Reward/ang_vel_xy_l2: -0.1016
     Episode_Reward/dof_torques_l2: -0.0425
         Episode_Reward/dof_acc_l2: -0.0413
     Episode_Reward/action_rate_l2: -0.0906
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.6628
Metrics/base_velocity/error_vel_xy: 0.8366
Metrics/base_velocity/error_vel_yaw: 0.7543
      Episode_Termination/time_out: 0.8333
  Episode_Termination/base_contact: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 1764000
                    Iteration time: 1.09s
                      Time elapsed: 00:00:54
                               ETA: 00:26:55

################################################################################
                      [1m Learning iteration 49/1500 [0m                      

                       Computation: 30617 steps/s (collection: 1.124s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0140
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 13.8094
                       Mean reward: 1.11
               Mean episode length: 506.20
Episode_Reward/track_lin_vel_xy_exp: 0.1729
Episode_Reward/track_ang_vel_z_exp: 0.1303
       Episode_Reward/lin_vel_z_l2: -0.0260
      Episode_Reward/ang_vel_xy_l2: -0.0769
     Episode_Reward/dof_torques_l2: -0.0328
         Episode_Reward/dof_acc_l2: -0.0357
     Episode_Reward/action_rate_l2: -0.0698
      Episode_Reward/feet_air_time: -0.0009
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.6404
Metrics/base_velocity/error_vel_xy: 0.6350
Metrics/base_velocity/error_vel_yaw: 0.6178
      Episode_Termination/time_out: 0.4583
  Episode_Termination/base_contact: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 1800000
                    Iteration time: 1.18s
                      Time elapsed: 00:00:55
                               ETA: 00:26:56

################################################################################
                      [1m Learning iteration 50/1500 [0m                      

                       Computation: 33076 steps/s (collection: 1.035s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0173
               Mean surrogate loss: -0.0072
                 Mean entropy loss: 13.8023
                       Mean reward: 1.86
               Mean episode length: 543.18
Episode_Reward/track_lin_vel_xy_exp: 0.1900
Episode_Reward/track_ang_vel_z_exp: 0.1714
       Episode_Reward/lin_vel_z_l2: -0.0278
      Episode_Reward/ang_vel_xy_l2: -0.0924
     Episode_Reward/dof_torques_l2: -0.0409
         Episode_Reward/dof_acc_l2: -0.0397
     Episode_Reward/action_rate_l2: -0.0851
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.6178
Metrics/base_velocity/error_vel_xy: 0.7824
Metrics/base_velocity/error_vel_yaw: 0.7234
      Episode_Termination/time_out: 0.7083
  Episode_Termination/base_contact: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 1836000
                    Iteration time: 1.09s
                      Time elapsed: 00:00:56
                               ETA: 00:26:54

################################################################################
                      [1m Learning iteration 51/1500 [0m                      

                       Computation: 32859 steps/s (collection: 1.042s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0126
               Mean surrogate loss: -0.0080
                 Mean entropy loss: 13.7997
                       Mean reward: 1.31
               Mean episode length: 606.44
Episode_Reward/track_lin_vel_xy_exp: 0.1583
Episode_Reward/track_ang_vel_z_exp: 0.1715
       Episode_Reward/lin_vel_z_l2: -0.0292
      Episode_Reward/ang_vel_xy_l2: -0.0993
     Episode_Reward/dof_torques_l2: -0.0442
         Episode_Reward/dof_acc_l2: -0.0433
     Episode_Reward/action_rate_l2: -0.0922
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.5878
Metrics/base_velocity/error_vel_xy: 0.9563
Metrics/base_velocity/error_vel_yaw: 0.8228
      Episode_Termination/time_out: 0.9583
  Episode_Termination/base_contact: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 1872000
                    Iteration time: 1.10s
                      Time elapsed: 00:00:57
                               ETA: 00:26:53

################################################################################
                      [1m Learning iteration 52/1500 [0m                      

                       Computation: 32823 steps/s (collection: 1.045s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0125
               Mean surrogate loss: -0.0074
                 Mean entropy loss: 13.7787
                       Mean reward: 1.24
               Mean episode length: 577.84
Episode_Reward/track_lin_vel_xy_exp: 0.1772
Episode_Reward/track_ang_vel_z_exp: 0.1595
       Episode_Reward/lin_vel_z_l2: -0.0257
      Episode_Reward/ang_vel_xy_l2: -0.0861
     Episode_Reward/dof_torques_l2: -0.0400
         Episode_Reward/dof_acc_l2: -0.0371
     Episode_Reward/action_rate_l2: -0.0801
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.5668
Metrics/base_velocity/error_vel_xy: 0.7786
Metrics/base_velocity/error_vel_yaw: 0.6816
      Episode_Termination/time_out: 0.9167
  Episode_Termination/base_contact: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 1908000
                    Iteration time: 1.10s
                      Time elapsed: 00:00:58
                               ETA: 00:26:51

################################################################################
                      [1m Learning iteration 53/1500 [0m                      

                       Computation: 33045 steps/s (collection: 1.038s, learning 0.051s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0095
               Mean surrogate loss: -0.0075
                 Mean entropy loss: 13.7510
                       Mean reward: 1.93
               Mean episode length: 561.44
Episode_Reward/track_lin_vel_xy_exp: 0.1928
Episode_Reward/track_ang_vel_z_exp: 0.1773
       Episode_Reward/lin_vel_z_l2: -0.0256
      Episode_Reward/ang_vel_xy_l2: -0.0846
     Episode_Reward/dof_torques_l2: -0.0418
         Episode_Reward/dof_acc_l2: -0.0377
     Episode_Reward/action_rate_l2: -0.0805
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.5419
Metrics/base_velocity/error_vel_xy: 0.7548
Metrics/base_velocity/error_vel_yaw: 0.6623
      Episode_Termination/time_out: 0.4583
  Episode_Termination/base_contact: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 1944000
                    Iteration time: 1.09s
                      Time elapsed: 00:01:00
                               ETA: 00:26:49

################################################################################
                      [1m Learning iteration 54/1500 [0m                      

                       Computation: 32363 steps/s (collection: 1.060s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0110
               Mean surrogate loss: -0.0079
                 Mean entropy loss: 13.7210
                       Mean reward: 2.46
               Mean episode length: 564.32
Episode_Reward/track_lin_vel_xy_exp: 0.2170
Episode_Reward/track_ang_vel_z_exp: 0.1863
       Episode_Reward/lin_vel_z_l2: -0.0292
      Episode_Reward/ang_vel_xy_l2: -0.0950
     Episode_Reward/dof_torques_l2: -0.0435
         Episode_Reward/dof_acc_l2: -0.0406
     Episode_Reward/action_rate_l2: -0.0882
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.5213
Metrics/base_velocity/error_vel_xy: 0.8073
Metrics/base_velocity/error_vel_yaw: 0.7185
      Episode_Termination/time_out: 1.0417
  Episode_Termination/base_contact: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 1980000
                    Iteration time: 1.11s
                      Time elapsed: 00:01:01
                               ETA: 00:26:48

################################################################################
                      [1m Learning iteration 55/1500 [0m                      

                       Computation: 32975 steps/s (collection: 1.039s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0095
               Mean surrogate loss: -0.0074
                 Mean entropy loss: 13.6998
                       Mean reward: 2.70
               Mean episode length: 554.52
Episode_Reward/track_lin_vel_xy_exp: 0.2171
Episode_Reward/track_ang_vel_z_exp: 0.1776
       Episode_Reward/lin_vel_z_l2: -0.0261
      Episode_Reward/ang_vel_xy_l2: -0.0876
     Episode_Reward/dof_torques_l2: -0.0381
         Episode_Reward/dof_acc_l2: -0.0436
     Episode_Reward/action_rate_l2: -0.0812
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.5036
Metrics/base_velocity/error_vel_xy: 0.7463
Metrics/base_velocity/error_vel_yaw: 0.6633
      Episode_Termination/time_out: 0.5000
  Episode_Termination/base_contact: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 2016000
                    Iteration time: 1.09s
                      Time elapsed: 00:01:02
                               ETA: 00:26:47

################################################################################
                      [1m Learning iteration 56/1500 [0m                      

                       Computation: 33254 steps/s (collection: 1.031s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0114
               Mean surrogate loss: -0.0071
                 Mean entropy loss: 13.6789
                       Mean reward: 2.08
               Mean episode length: 542.54
Episode_Reward/track_lin_vel_xy_exp: 0.1960
Episode_Reward/track_ang_vel_z_exp: 0.1734
       Episode_Reward/lin_vel_z_l2: -0.0267
      Episode_Reward/ang_vel_xy_l2: -0.0879
     Episode_Reward/dof_torques_l2: -0.0411
         Episode_Reward/dof_acc_l2: -0.0382
     Episode_Reward/action_rate_l2: -0.0822
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.4836
Metrics/base_velocity/error_vel_xy: 0.8044
Metrics/base_velocity/error_vel_yaw: 0.6946
      Episode_Termination/time_out: 0.7917
  Episode_Termination/base_contact: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2052000
                    Iteration time: 1.08s
                      Time elapsed: 00:01:03
                               ETA: 00:26:45

################################################################################
                      [1m Learning iteration 57/1500 [0m                      

                       Computation: 33199 steps/s (collection: 1.032s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0098
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 13.6563
                       Mean reward: 1.34
               Mean episode length: 514.28
Episode_Reward/track_lin_vel_xy_exp: 0.1637
Episode_Reward/track_ang_vel_z_exp: 0.1966
       Episode_Reward/lin_vel_z_l2: -0.0257
      Episode_Reward/ang_vel_xy_l2: -0.0868
     Episode_Reward/dof_torques_l2: -0.0370
         Episode_Reward/dof_acc_l2: -0.0396
     Episode_Reward/action_rate_l2: -0.0805
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.4662
Metrics/base_velocity/error_vel_xy: 0.8189
Metrics/base_velocity/error_vel_yaw: 0.5893
      Episode_Termination/time_out: 0.7500
  Episode_Termination/base_contact: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 2088000
                    Iteration time: 1.08s
                      Time elapsed: 00:01:04
                               ETA: 00:26:43

################################################################################
                      [1m Learning iteration 58/1500 [0m                      

                       Computation: 32687 steps/s (collection: 1.051s, learning 0.050s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0119
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 13.6322
                       Mean reward: 2.39
               Mean episode length: 563.48
Episode_Reward/track_lin_vel_xy_exp: 0.2228
Episode_Reward/track_ang_vel_z_exp: 0.1862
       Episode_Reward/lin_vel_z_l2: -0.0268
      Episode_Reward/ang_vel_xy_l2: -0.0880
     Episode_Reward/dof_torques_l2: -0.0398
         Episode_Reward/dof_acc_l2: -0.0395
     Episode_Reward/action_rate_l2: -0.0818
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.4465
Metrics/base_velocity/error_vel_xy: 0.7805
Metrics/base_velocity/error_vel_yaw: 0.6490
      Episode_Termination/time_out: 0.8333
  Episode_Termination/base_contact: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 2124000
                    Iteration time: 1.10s
                      Time elapsed: 00:01:05
                               ETA: 00:26:42

################################################################################
                      [1m Learning iteration 59/1500 [0m                      

                       Computation: 33582 steps/s (collection: 1.022s, learning 0.050s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0110
               Mean surrogate loss: -0.0071
                 Mean entropy loss: 13.6125
                       Mean reward: 2.65
               Mean episode length: 614.26
Episode_Reward/track_lin_vel_xy_exp: 0.2947
Episode_Reward/track_ang_vel_z_exp: 0.1937
       Episode_Reward/lin_vel_z_l2: -0.0265
      Episode_Reward/ang_vel_xy_l2: -0.0946
     Episode_Reward/dof_torques_l2: -0.0468
         Episode_Reward/dof_acc_l2: -0.0419
     Episode_Reward/action_rate_l2: -0.0906
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.4216
Metrics/base_velocity/error_vel_xy: 0.8001
Metrics/base_velocity/error_vel_yaw: 0.7895
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 2160000
                    Iteration time: 1.07s
                      Time elapsed: 00:01:06
                               ETA: 00:26:40

################################################################################
                      [1m Learning iteration 60/1500 [0m                      

                       Computation: 32363 steps/s (collection: 1.059s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0090
               Mean surrogate loss: -0.0074
                 Mean entropy loss: 13.5997
                       Mean reward: 2.78
               Mean episode length: 630.09
Episode_Reward/track_lin_vel_xy_exp: 0.2325
Episode_Reward/track_ang_vel_z_exp: 0.2120
       Episode_Reward/lin_vel_z_l2: -0.0280
      Episode_Reward/ang_vel_xy_l2: -0.0979
     Episode_Reward/dof_torques_l2: -0.0477
         Episode_Reward/dof_acc_l2: -0.0436
     Episode_Reward/action_rate_l2: -0.0934
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.4006
Metrics/base_velocity/error_vel_xy: 0.9141
Metrics/base_velocity/error_vel_yaw: 0.7817
      Episode_Termination/time_out: 1.1667
  Episode_Termination/base_contact: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 2196000
                    Iteration time: 1.11s
                      Time elapsed: 00:01:07
                               ETA: 00:26:39

################################################################################
                      [1m Learning iteration 61/1500 [0m                      

                       Computation: 33557 steps/s (collection: 1.020s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0067
                 Mean entropy loss: 13.5820
                       Mean reward: 3.07
               Mean episode length: 680.87
Episode_Reward/track_lin_vel_xy_exp: 0.2460
Episode_Reward/track_ang_vel_z_exp: 0.2709
       Episode_Reward/lin_vel_z_l2: -0.0294
      Episode_Reward/ang_vel_xy_l2: -0.1102
     Episode_Reward/dof_torques_l2: -0.0544
         Episode_Reward/dof_acc_l2: -0.0482
     Episode_Reward/action_rate_l2: -0.1083
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.3768
Metrics/base_velocity/error_vel_xy: 1.1094
Metrics/base_velocity/error_vel_yaw: 0.8144
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 2232000
                    Iteration time: 1.07s
                      Time elapsed: 00:01:08
                               ETA: 00:26:37

################################################################################
                      [1m Learning iteration 62/1500 [0m                      

                       Computation: 32868 steps/s (collection: 1.043s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0117
               Mean surrogate loss: -0.0076
                 Mean entropy loss: 13.5894
                       Mean reward: 3.39
               Mean episode length: 721.20
Episode_Reward/track_lin_vel_xy_exp: 0.2676
Episode_Reward/track_ang_vel_z_exp: 0.2141
       Episode_Reward/lin_vel_z_l2: -0.0260
      Episode_Reward/ang_vel_xy_l2: -0.0971
     Episode_Reward/dof_torques_l2: -0.0482
         Episode_Reward/dof_acc_l2: -0.0421
     Episode_Reward/action_rate_l2: -0.0949
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.3465
Metrics/base_velocity/error_vel_xy: 0.8970
Metrics/base_velocity/error_vel_yaw: 0.8009
      Episode_Termination/time_out: 2.1667
  Episode_Termination/base_contact: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 2268000
                    Iteration time: 1.10s
                      Time elapsed: 00:01:09
                               ETA: 00:26:35

################################################################################
                      [1m Learning iteration 63/1500 [0m                      

                       Computation: 33008 steps/s (collection: 1.038s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0212
               Mean surrogate loss: -0.0067
                 Mean entropy loss: 13.5918
                       Mean reward: 4.61
               Mean episode length: 821.33
Episode_Reward/track_lin_vel_xy_exp: 0.3212
Episode_Reward/track_ang_vel_z_exp: 0.2648
       Episode_Reward/lin_vel_z_l2: -0.0302
      Episode_Reward/ang_vel_xy_l2: -0.1189
     Episode_Reward/dof_torques_l2: -0.0587
         Episode_Reward/dof_acc_l2: -0.0508
     Episode_Reward/action_rate_l2: -0.1174
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.3086
Metrics/base_velocity/error_vel_xy: 1.1197
Metrics/base_velocity/error_vel_yaw: 1.0053
      Episode_Termination/time_out: 2.3333
  Episode_Termination/base_contact: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 2304000
                    Iteration time: 1.09s
                      Time elapsed: 00:01:10
                               ETA: 00:26:34

################################################################################
                      [1m Learning iteration 64/1500 [0m                      

                       Computation: 33104 steps/s (collection: 1.037s, learning 0.050s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0120
               Mean surrogate loss: -0.0081
                 Mean entropy loss: 13.6068
                       Mean reward: 4.09
               Mean episode length: 879.39
Episode_Reward/track_lin_vel_xy_exp: 0.3373
Episode_Reward/track_ang_vel_z_exp: 0.2960
       Episode_Reward/lin_vel_z_l2: -0.0332
      Episode_Reward/ang_vel_xy_l2: -0.1348
     Episode_Reward/dof_torques_l2: -0.0619
         Episode_Reward/dof_acc_l2: -0.0587
     Episode_Reward/action_rate_l2: -0.1294
      Episode_Reward/feet_air_time: -0.0018
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.2730
Metrics/base_velocity/error_vel_xy: 1.2608
Metrics/base_velocity/error_vel_yaw: 1.0879
      Episode_Termination/time_out: 2.7083
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 2340000
                    Iteration time: 1.09s
                      Time elapsed: 00:01:12
                               ETA: 00:26:32

################################################################################
                      [1m Learning iteration 65/1500 [0m                      

                       Computation: 32458 steps/s (collection: 1.056s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0152
               Mean surrogate loss: -0.0069
                 Mean entropy loss: 13.6005
                       Mean reward: 3.19
               Mean episode length: 682.23
Episode_Reward/track_lin_vel_xy_exp: 0.2033
Episode_Reward/track_ang_vel_z_exp: 0.1903
       Episode_Reward/lin_vel_z_l2: -0.0258
      Episode_Reward/ang_vel_xy_l2: -0.0885
     Episode_Reward/dof_torques_l2: -0.0435
         Episode_Reward/dof_acc_l2: -0.0406
     Episode_Reward/action_rate_l2: -0.0875
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.2351
Metrics/base_velocity/error_vel_xy: 0.8887
Metrics/base_velocity/error_vel_yaw: 0.7955
      Episode_Termination/time_out: 2.0417
  Episode_Termination/base_contact: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 2376000
                    Iteration time: 1.11s
                      Time elapsed: 00:01:13
                               ETA: 00:26:31

################################################################################
                      [1m Learning iteration 66/1500 [0m                      

                       Computation: 32824 steps/s (collection: 1.044s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0171
               Mean surrogate loss: -0.0076
                 Mean entropy loss: 13.5978
                       Mean reward: 3.37
               Mean episode length: 660.19
Episode_Reward/track_lin_vel_xy_exp: 0.2450
Episode_Reward/track_ang_vel_z_exp: 0.2189
       Episode_Reward/lin_vel_z_l2: -0.0266
      Episode_Reward/ang_vel_xy_l2: -0.0931
     Episode_Reward/dof_torques_l2: -0.0471
         Episode_Reward/dof_acc_l2: -0.0417
     Episode_Reward/action_rate_l2: -0.0920
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.1953
Metrics/base_velocity/error_vel_xy: 0.8742
Metrics/base_velocity/error_vel_yaw: 0.7568
      Episode_Termination/time_out: 2.5417
  Episode_Termination/base_contact: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 2412000
                    Iteration time: 1.10s
                      Time elapsed: 00:01:14
                               ETA: 00:26:29

################################################################################
                      [1m Learning iteration 67/1500 [0m                      

                       Computation: 32864 steps/s (collection: 1.044s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0152
               Mean surrogate loss: -0.0079
                 Mean entropy loss: 13.6011
                       Mean reward: 4.77
               Mean episode length: 721.59
Episode_Reward/track_lin_vel_xy_exp: 0.3368
Episode_Reward/track_ang_vel_z_exp: 0.2529
       Episode_Reward/lin_vel_z_l2: -0.0274
      Episode_Reward/ang_vel_xy_l2: -0.1023
     Episode_Reward/dof_torques_l2: -0.0497
         Episode_Reward/dof_acc_l2: -0.0466
     Episode_Reward/action_rate_l2: -0.1009
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.1622
Metrics/base_velocity/error_vel_xy: 0.8938
Metrics/base_velocity/error_vel_yaw: 0.7948
      Episode_Termination/time_out: 2.0833
  Episode_Termination/base_contact: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 2448000
                    Iteration time: 1.10s
                      Time elapsed: 00:01:15
                               ETA: 00:26:28

################################################################################
                      [1m Learning iteration 68/1500 [0m                      

                       Computation: 32619 steps/s (collection: 1.050s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0141
               Mean surrogate loss: -0.0068
                 Mean entropy loss: 13.5993
                       Mean reward: 3.72
               Mean episode length: 610.12
Episode_Reward/track_lin_vel_xy_exp: 0.2794
Episode_Reward/track_ang_vel_z_exp: 0.1956
       Episode_Reward/lin_vel_z_l2: -0.0240
      Episode_Reward/ang_vel_xy_l2: -0.0823
     Episode_Reward/dof_torques_l2: -0.0428
         Episode_Reward/dof_acc_l2: -0.0390
     Episode_Reward/action_rate_l2: -0.0827
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.1338
Metrics/base_velocity/error_vel_xy: 0.7196
Metrics/base_velocity/error_vel_yaw: 0.7027
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2484000
                    Iteration time: 1.10s
                      Time elapsed: 00:01:16
                               ETA: 00:26:27

################################################################################
                      [1m Learning iteration 69/1500 [0m                      

                       Computation: 32653 steps/s (collection: 1.049s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0129
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 13.6022
                       Mean reward: 3.71
               Mean episode length: 580.17
Episode_Reward/track_lin_vel_xy_exp: 0.2292
Episode_Reward/track_ang_vel_z_exp: 0.1932
       Episode_Reward/lin_vel_z_l2: -0.0229
      Episode_Reward/ang_vel_xy_l2: -0.0758
     Episode_Reward/dof_torques_l2: -0.0384
         Episode_Reward/dof_acc_l2: -0.0359
     Episode_Reward/action_rate_l2: -0.0762
      Episode_Reward/feet_air_time: -0.0009
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.1106
Metrics/base_velocity/error_vel_xy: 0.7122
Metrics/base_velocity/error_vel_yaw: 0.5881
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 2520000
                    Iteration time: 1.10s
                      Time elapsed: 00:01:17
                               ETA: 00:26:26

################################################################################
                      [1m Learning iteration 70/1500 [0m                      

                       Computation: 32864 steps/s (collection: 1.044s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0194
               Mean surrogate loss: -0.0080
                 Mean entropy loss: 13.6104
                       Mean reward: 3.69
               Mean episode length: 600.97
Episode_Reward/track_lin_vel_xy_exp: 0.2445
Episode_Reward/track_ang_vel_z_exp: 0.1976
       Episode_Reward/lin_vel_z_l2: -0.0232
      Episode_Reward/ang_vel_xy_l2: -0.0781
     Episode_Reward/dof_torques_l2: -0.0409
         Episode_Reward/dof_acc_l2: -0.0364
     Episode_Reward/action_rate_l2: -0.0783
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0877
Metrics/base_velocity/error_vel_xy: 0.7365
Metrics/base_velocity/error_vel_yaw: 0.6269
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 2556000
                    Iteration time: 1.10s
                      Time elapsed: 00:01:18
                               ETA: 00:26:24

################################################################################
                      [1m Learning iteration 71/1500 [0m                      

                       Computation: 32891 steps/s (collection: 1.031s, learning 0.064s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0152
               Mean surrogate loss: -0.0077
                 Mean entropy loss: 13.6135
                       Mean reward: 4.56
               Mean episode length: 663.94
Episode_Reward/track_lin_vel_xy_exp: 0.3307
Episode_Reward/track_ang_vel_z_exp: 0.2601
       Episode_Reward/lin_vel_z_l2: -0.0279
      Episode_Reward/ang_vel_xy_l2: -0.1034
     Episode_Reward/dof_torques_l2: -0.0525
         Episode_Reward/dof_acc_l2: -0.0470
     Episode_Reward/action_rate_l2: -0.1046
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0685
Metrics/base_velocity/error_vel_xy: 0.9643
Metrics/base_velocity/error_vel_yaw: 0.8390
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 2592000
                    Iteration time: 1.09s
                      Time elapsed: 00:01:19
                               ETA: 00:26:23

################################################################################
                      [1m Learning iteration 72/1500 [0m                      

                       Computation: 33173 steps/s (collection: 1.033s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0179
               Mean surrogate loss: -0.0079
                 Mean entropy loss: 13.6091
                       Mean reward: 4.57
               Mean episode length: 672.27
Episode_Reward/track_lin_vel_xy_exp: 0.2671
Episode_Reward/track_ang_vel_z_exp: 0.2464
       Episode_Reward/lin_vel_z_l2: -0.0257
      Episode_Reward/ang_vel_xy_l2: -0.0919
     Episode_Reward/dof_torques_l2: -0.0454
         Episode_Reward/dof_acc_l2: -0.0443
     Episode_Reward/action_rate_l2: -0.0916
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0488
Metrics/base_velocity/error_vel_xy: 0.8659
Metrics/base_velocity/error_vel_yaw: 0.6753
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 2628000
                    Iteration time: 1.09s
                      Time elapsed: 00:01:20
                               ETA: 00:26:21

################################################################################
                      [1m Learning iteration 73/1500 [0m                      

                       Computation: 33297 steps/s (collection: 1.028s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0172
               Mean surrogate loss: -0.0066
                 Mean entropy loss: 13.6138
                       Mean reward: 4.05
               Mean episode length: 637.89
Episode_Reward/track_lin_vel_xy_exp: 0.2483
Episode_Reward/track_ang_vel_z_exp: 0.2503
       Episode_Reward/lin_vel_z_l2: -0.0250
      Episode_Reward/ang_vel_xy_l2: -0.0906
     Episode_Reward/dof_torques_l2: -0.0457
         Episode_Reward/dof_acc_l2: -0.0453
     Episode_Reward/action_rate_l2: -0.0929
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0302
Metrics/base_velocity/error_vel_xy: 0.9062
Metrics/base_velocity/error_vel_yaw: 0.6929
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 2664000
                    Iteration time: 1.08s
                      Time elapsed: 00:01:21
                               ETA: 00:26:20

################################################################################
                      [1m Learning iteration 74/1500 [0m                      

                       Computation: 34084 steps/s (collection: 1.004s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0118
               Mean surrogate loss: -0.0064
                 Mean entropy loss: 13.6118
                       Mean reward: 3.63
               Mean episode length: 634.78
Episode_Reward/track_lin_vel_xy_exp: 0.1898
Episode_Reward/track_ang_vel_z_exp: 0.1952
       Episode_Reward/lin_vel_z_l2: -0.0236
      Episode_Reward/ang_vel_xy_l2: -0.0794
     Episode_Reward/dof_torques_l2: -0.0448
         Episode_Reward/dof_acc_l2: -0.0361
     Episode_Reward/action_rate_l2: -0.0817
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0148
Metrics/base_velocity/error_vel_xy: 0.8746
Metrics/base_velocity/error_vel_yaw: 0.7298
      Episode_Termination/time_out: 0.8750
  Episode_Termination/base_contact: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 2700000
                    Iteration time: 1.06s
                      Time elapsed: 00:01:23
                               ETA: 00:26:18

################################################################################
                      [1m Learning iteration 75/1500 [0m                      

                       Computation: 33018 steps/s (collection: 1.037s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0111
               Mean surrogate loss: -0.0067
                 Mean entropy loss: 13.5985
                       Mean reward: 2.73
               Mean episode length: 608.52
Episode_Reward/track_lin_vel_xy_exp: 0.1744
Episode_Reward/track_ang_vel_z_exp: 0.2105
       Episode_Reward/lin_vel_z_l2: -0.0253
      Episode_Reward/ang_vel_xy_l2: -0.0864
     Episode_Reward/dof_torques_l2: -0.0437
         Episode_Reward/dof_acc_l2: -0.0424
     Episode_Reward/action_rate_l2: -0.0864
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9976
Metrics/base_velocity/error_vel_xy: 0.9381
Metrics/base_velocity/error_vel_yaw: 0.7343
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 2736000
                    Iteration time: 1.09s
                      Time elapsed: 00:01:24
                               ETA: 00:26:16

################################################################################
                      [1m Learning iteration 76/1500 [0m                      

                       Computation: 32937 steps/s (collection: 1.041s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0093
               Mean surrogate loss: -0.0075
                 Mean entropy loss: 13.5779
                       Mean reward: 3.25
               Mean episode length: 611.85
Episode_Reward/track_lin_vel_xy_exp: 0.2430
Episode_Reward/track_ang_vel_z_exp: 0.2171
       Episode_Reward/lin_vel_z_l2: -0.0248
      Episode_Reward/ang_vel_xy_l2: -0.0836
     Episode_Reward/dof_torques_l2: -0.0436
         Episode_Reward/dof_acc_l2: -0.0420
     Episode_Reward/action_rate_l2: -0.0856
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9832
Metrics/base_velocity/error_vel_xy: 0.8298
Metrics/base_velocity/error_vel_yaw: 0.7055
      Episode_Termination/time_out: 0.7083
  Episode_Termination/base_contact: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 2772000
                    Iteration time: 1.09s
                      Time elapsed: 00:01:25
                               ETA: 00:26:15

################################################################################
                      [1m Learning iteration 77/1500 [0m                      

                       Computation: 32919 steps/s (collection: 1.040s, learning 0.054s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0156
               Mean surrogate loss: -0.0078
                 Mean entropy loss: 13.5513
                       Mean reward: 3.34
               Mean episode length: 631.99
Episode_Reward/track_lin_vel_xy_exp: 0.1989
Episode_Reward/track_ang_vel_z_exp: 0.2184
       Episode_Reward/lin_vel_z_l2: -0.0263
      Episode_Reward/ang_vel_xy_l2: -0.0843
     Episode_Reward/dof_torques_l2: -0.0432
         Episode_Reward/dof_acc_l2: -0.0412
     Episode_Reward/action_rate_l2: -0.0859
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9688
Metrics/base_velocity/error_vel_xy: 0.9548
Metrics/base_velocity/error_vel_yaw: 0.7401
      Episode_Termination/time_out: 1.0833
  Episode_Termination/base_contact: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 2808000
                    Iteration time: 1.09s
                      Time elapsed: 00:01:26
                               ETA: 00:26:14

################################################################################
                      [1m Learning iteration 78/1500 [0m                      

                       Computation: 33552 steps/s (collection: 1.022s, learning 0.051s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0107
               Mean surrogate loss: -0.0071
                 Mean entropy loss: 13.5302
                       Mean reward: 3.22
               Mean episode length: 646.32
Episode_Reward/track_lin_vel_xy_exp: 0.1708
Episode_Reward/track_ang_vel_z_exp: 0.1982
       Episode_Reward/lin_vel_z_l2: -0.0228
      Episode_Reward/ang_vel_xy_l2: -0.0681
     Episode_Reward/dof_torques_l2: -0.0367
         Episode_Reward/dof_acc_l2: -0.0364
     Episode_Reward/action_rate_l2: -0.0706
      Episode_Reward/feet_air_time: -0.0009
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9503
Metrics/base_velocity/error_vel_xy: 0.7530
Metrics/base_velocity/error_vel_yaw: 0.5172
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 2844000
                    Iteration time: 1.07s
                      Time elapsed: 00:01:27
                               ETA: 00:26:12

################################################################################
                      [1m Learning iteration 79/1500 [0m                      

                       Computation: 33381 steps/s (collection: 1.026s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0120
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 13.5145
                       Mean reward: 4.07
               Mean episode length: 635.85
Episode_Reward/track_lin_vel_xy_exp: 0.3425
Episode_Reward/track_ang_vel_z_exp: 0.2580
       Episode_Reward/lin_vel_z_l2: -0.0267
      Episode_Reward/ang_vel_xy_l2: -0.0912
     Episode_Reward/dof_torques_l2: -0.0469
         Episode_Reward/dof_acc_l2: -0.0472
     Episode_Reward/action_rate_l2: -0.0946
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9355
Metrics/base_velocity/error_vel_xy: 0.8018
Metrics/base_velocity/error_vel_yaw: 0.7232
      Episode_Termination/time_out: 0.6667
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2880000
                    Iteration time: 1.08s
                      Time elapsed: 00:01:28
                               ETA: 00:26:10

################################################################################
                      [1m Learning iteration 80/1500 [0m                      

                       Computation: 33893 steps/s (collection: 1.011s, learning 0.051s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0146
               Mean surrogate loss: -0.0062
                 Mean entropy loss: 13.5098
                       Mean reward: 4.56
               Mean episode length: 635.91
Episode_Reward/track_lin_vel_xy_exp: 0.2215
Episode_Reward/track_ang_vel_z_exp: 0.2046
       Episode_Reward/lin_vel_z_l2: -0.0234
      Episode_Reward/ang_vel_xy_l2: -0.0739
     Episode_Reward/dof_torques_l2: -0.0373
         Episode_Reward/dof_acc_l2: -0.0393
     Episode_Reward/action_rate_l2: -0.0755
      Episode_Reward/feet_air_time: -0.0009
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9241
Metrics/base_velocity/error_vel_xy: 0.7453
Metrics/base_velocity/error_vel_yaw: 0.5675
      Episode_Termination/time_out: 0.7083
  Episode_Termination/base_contact: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 2916000
                    Iteration time: 1.06s
                      Time elapsed: 00:01:29
                               ETA: 00:26:08

################################################################################
                      [1m Learning iteration 81/1500 [0m                      

                       Computation: 32428 steps/s (collection: 1.057s, learning 0.054s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0099
               Mean surrogate loss: -0.0069
                 Mean entropy loss: 13.5004
                       Mean reward: 5.05
               Mean episode length: 648.55
Episode_Reward/track_lin_vel_xy_exp: 0.3229
Episode_Reward/track_ang_vel_z_exp: 0.2678
       Episode_Reward/lin_vel_z_l2: -0.0278
      Episode_Reward/ang_vel_xy_l2: -0.0929
     Episode_Reward/dof_torques_l2: -0.0487
         Episode_Reward/dof_acc_l2: -0.0468
     Episode_Reward/action_rate_l2: -0.0969
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9144
Metrics/base_velocity/error_vel_xy: 0.8702
Metrics/base_velocity/error_vel_yaw: 0.7553
      Episode_Termination/time_out: 0.6250
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2952000
                    Iteration time: 1.11s
                      Time elapsed: 00:01:30
                               ETA: 00:26:07

################################################################################
                      [1m Learning iteration 82/1500 [0m                      

                       Computation: 33015 steps/s (collection: 1.038s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0132
               Mean surrogate loss: -0.0064
                 Mean entropy loss: 13.4847
                       Mean reward: 4.18
               Mean episode length: 660.91
Episode_Reward/track_lin_vel_xy_exp: 0.2128
Episode_Reward/track_ang_vel_z_exp: 0.2726
       Episode_Reward/lin_vel_z_l2: -0.0266
      Episode_Reward/ang_vel_xy_l2: -0.0917
     Episode_Reward/dof_torques_l2: -0.0485
         Episode_Reward/dof_acc_l2: -0.0464
     Episode_Reward/action_rate_l2: -0.0960
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9007
Metrics/base_velocity/error_vel_xy: 1.0449
Metrics/base_velocity/error_vel_yaw: 0.6964
      Episode_Termination/time_out: 0.9583
  Episode_Termination/base_contact: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 2988000
                    Iteration time: 1.09s
                      Time elapsed: 00:01:31
                               ETA: 00:26:06

################################################################################
                      [1m Learning iteration 83/1500 [0m                      

                       Computation: 33394 steps/s (collection: 1.024s, learning 0.054s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0065
                 Mean entropy loss: 13.4754
                       Mean reward: 4.69
               Mean episode length: 640.34
Episode_Reward/track_lin_vel_xy_exp: 0.2386
Episode_Reward/track_ang_vel_z_exp: 0.2264
       Episode_Reward/lin_vel_z_l2: -0.0232
      Episode_Reward/ang_vel_xy_l2: -0.0734
     Episode_Reward/dof_torques_l2: -0.0397
         Episode_Reward/dof_acc_l2: -0.0385
     Episode_Reward/action_rate_l2: -0.0770
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.8911
Metrics/base_velocity/error_vel_xy: 0.6959
Metrics/base_velocity/error_vel_yaw: 0.5367
      Episode_Termination/time_out: 0.9583
  Episode_Termination/base_contact: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 3024000
                    Iteration time: 1.08s
                      Time elapsed: 00:01:32
                               ETA: 00:26:04

################################################################################
                      [1m Learning iteration 84/1500 [0m                      

                       Computation: 33630 steps/s (collection: 1.018s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0133
               Mean surrogate loss: -0.0062
                 Mean entropy loss: 13.4732
                       Mean reward: 4.99
               Mean episode length: 638.32
Episode_Reward/track_lin_vel_xy_exp: 0.2904
Episode_Reward/track_ang_vel_z_exp: 0.2752
       Episode_Reward/lin_vel_z_l2: -0.0266
      Episode_Reward/ang_vel_xy_l2: -0.0882
     Episode_Reward/dof_torques_l2: -0.0463
         Episode_Reward/dof_acc_l2: -0.0445
     Episode_Reward/action_rate_l2: -0.0922
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.8804
Metrics/base_velocity/error_vel_xy: 0.8765
Metrics/base_velocity/error_vel_yaw: 0.6324
      Episode_Termination/time_out: 0.8333
  Episode_Termination/base_contact: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 3060000
                    Iteration time: 1.07s
                      Time elapsed: 00:01:33
                               ETA: 00:26:03

################################################################################
                      [1m Learning iteration 85/1500 [0m                      

                       Computation: 33600 steps/s (collection: 1.020s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 13.4709
                       Mean reward: 4.83
               Mean episode length: 621.92
Episode_Reward/track_lin_vel_xy_exp: 0.1853
Episode_Reward/track_ang_vel_z_exp: 0.2638
       Episode_Reward/lin_vel_z_l2: -0.0249
      Episode_Reward/ang_vel_xy_l2: -0.0834
     Episode_Reward/dof_torques_l2: -0.0473
         Episode_Reward/dof_acc_l2: -0.0413
     Episode_Reward/action_rate_l2: -0.0893
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.8698
Metrics/base_velocity/error_vel_xy: 0.9788
Metrics/base_velocity/error_vel_yaw: 0.6303
      Episode_Termination/time_out: 0.8750
  Episode_Termination/base_contact: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 3096000
                    Iteration time: 1.07s
                      Time elapsed: 00:01:34
                               ETA: 00:26:01

################################################################################
                      [1m Learning iteration 86/1500 [0m                      

                       Computation: 33097 steps/s (collection: 1.037s, learning 0.051s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0116
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 13.4691
                       Mean reward: 4.29
               Mean episode length: 610.14
Episode_Reward/track_lin_vel_xy_exp: 0.3057
Episode_Reward/track_ang_vel_z_exp: 0.2803
       Episode_Reward/lin_vel_z_l2: -0.0257
      Episode_Reward/ang_vel_xy_l2: -0.0843
     Episode_Reward/dof_torques_l2: -0.0449
         Episode_Reward/dof_acc_l2: -0.0450
     Episode_Reward/action_rate_l2: -0.0913
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.8538
Metrics/base_velocity/error_vel_xy: 0.8386
Metrics/base_velocity/error_vel_yaw: 0.5852
      Episode_Termination/time_out: 1.0000
  Episode_Termination/base_contact: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 3132000
                    Iteration time: 1.09s
                      Time elapsed: 00:01:36
                               ETA: 00:26:00

################################################################################
                      [1m Learning iteration 87/1500 [0m                      

                       Computation: 33738 steps/s (collection: 1.016s, learning 0.051s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0212
               Mean surrogate loss: -0.0065
                 Mean entropy loss: 13.4722
                       Mean reward: 5.58
               Mean episode length: 656.96
Episode_Reward/track_lin_vel_xy_exp: 0.3740
Episode_Reward/track_ang_vel_z_exp: 0.3114
       Episode_Reward/lin_vel_z_l2: -0.0263
      Episode_Reward/ang_vel_xy_l2: -0.0919
     Episode_Reward/dof_torques_l2: -0.0490
         Episode_Reward/dof_acc_l2: -0.0486
     Episode_Reward/action_rate_l2: -0.0982
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.8371
Metrics/base_velocity/error_vel_xy: 0.8019
Metrics/base_velocity/error_vel_yaw: 0.5947
      Episode_Termination/time_out: 1.1667
  Episode_Termination/base_contact: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 3168000
                    Iteration time: 1.07s
                      Time elapsed: 00:01:37
                               ETA: 00:25:58

################################################################################
                      [1m Learning iteration 88/1500 [0m                      

                       Computation: 33375 steps/s (collection: 1.025s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0171
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 13.4713
                       Mean reward: 5.54
               Mean episode length: 636.22
Episode_Reward/track_lin_vel_xy_exp: 0.3267
Episode_Reward/track_ang_vel_z_exp: 0.2826
       Episode_Reward/lin_vel_z_l2: -0.0256
      Episode_Reward/ang_vel_xy_l2: -0.0868
     Episode_Reward/dof_torques_l2: -0.0444
         Episode_Reward/dof_acc_l2: -0.0482
     Episode_Reward/action_rate_l2: -0.0922
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.8236
Metrics/base_velocity/error_vel_xy: 0.8097
Metrics/base_velocity/error_vel_yaw: 0.5976
      Episode_Termination/time_out: 0.9167
  Episode_Termination/base_contact: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 3204000
                    Iteration time: 1.08s
                      Time elapsed: 00:01:38
                               ETA: 00:25:57

################################################################################
                      [1m Learning iteration 89/1500 [0m                      

                       Computation: 33386 steps/s (collection: 1.026s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0172
               Mean surrogate loss: -0.0067
                 Mean entropy loss: 13.4677
                       Mean reward: 5.41
               Mean episode length: 635.23
Episode_Reward/track_lin_vel_xy_exp: 0.2993
Episode_Reward/track_ang_vel_z_exp: 0.2928
       Episode_Reward/lin_vel_z_l2: -0.0276
      Episode_Reward/ang_vel_xy_l2: -0.0911
     Episode_Reward/dof_torques_l2: -0.0459
         Episode_Reward/dof_acc_l2: -0.0511
     Episode_Reward/action_rate_l2: -0.0973
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.8109
Metrics/base_velocity/error_vel_xy: 0.9422
Metrics/base_velocity/error_vel_yaw: 0.6447
      Episode_Termination/time_out: 1.1250
  Episode_Termination/base_contact: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 3240000
                    Iteration time: 1.08s
                      Time elapsed: 00:01:39
                               ETA: 00:25:55

################################################################################
                      [1m Learning iteration 90/1500 [0m                      

                       Computation: 32960 steps/s (collection: 1.038s, learning 0.055s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0137
               Mean surrogate loss: -0.0063
                 Mean entropy loss: 13.4615
                       Mean reward: 5.67
               Mean episode length: 686.97
Episode_Reward/track_lin_vel_xy_exp: 0.2519
Episode_Reward/track_ang_vel_z_exp: 0.3150
       Episode_Reward/lin_vel_z_l2: -0.0275
      Episode_Reward/ang_vel_xy_l2: -0.0940
     Episode_Reward/dof_torques_l2: -0.0492
         Episode_Reward/dof_acc_l2: -0.0509
     Episode_Reward/action_rate_l2: -0.1015
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7922
Metrics/base_velocity/error_vel_xy: 1.0218
Metrics/base_velocity/error_vel_yaw: 0.6403
      Episode_Termination/time_out: 1.1250
  Episode_Termination/base_contact: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 3276000
                    Iteration time: 1.09s
                      Time elapsed: 00:01:40
                               ETA: 00:25:54

################################################################################
                      [1m Learning iteration 91/1500 [0m                      

                       Computation: 32803 steps/s (collection: 1.043s, learning 0.054s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0142
               Mean surrogate loss: -0.0077
                 Mean entropy loss: 13.4594
                       Mean reward: 5.29
               Mean episode length: 704.14
Episode_Reward/track_lin_vel_xy_exp: 0.2762
Episode_Reward/track_ang_vel_z_exp: 0.3080
       Episode_Reward/lin_vel_z_l2: -0.0277
      Episode_Reward/ang_vel_xy_l2: -0.0914
     Episode_Reward/dof_torques_l2: -0.0476
         Episode_Reward/dof_acc_l2: -0.0458
     Episode_Reward/action_rate_l2: -0.0966
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7793
Metrics/base_velocity/error_vel_xy: 0.9342
Metrics/base_velocity/error_vel_yaw: 0.5830
      Episode_Termination/time_out: 1.0833
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 3312000
                    Iteration time: 1.10s
                      Time elapsed: 00:01:41
                               ETA: 00:25:53

################################################################################
                      [1m Learning iteration 92/1500 [0m                      

                       Computation: 33248 steps/s (collection: 1.031s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0129
               Mean surrogate loss: -0.0071
                 Mean entropy loss: 13.4420
                       Mean reward: 5.67
               Mean episode length: 714.04
Episode_Reward/track_lin_vel_xy_exp: 0.2958
Episode_Reward/track_ang_vel_z_exp: 0.2918
       Episode_Reward/lin_vel_z_l2: -0.0263
      Episode_Reward/ang_vel_xy_l2: -0.0899
     Episode_Reward/dof_torques_l2: -0.0490
         Episode_Reward/dof_acc_l2: -0.0495
     Episode_Reward/action_rate_l2: -0.0966
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7625
Metrics/base_velocity/error_vel_xy: 0.9185
Metrics/base_velocity/error_vel_yaw: 0.6491
      Episode_Termination/time_out: 1.1250
  Episode_Termination/base_contact: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 3348000
                    Iteration time: 1.08s
                      Time elapsed: 00:01:42
                               ETA: 00:25:51

################################################################################
                      [1m Learning iteration 93/1500 [0m                      

                       Computation: 33233 steps/s (collection: 1.032s, learning 0.051s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0167
               Mean surrogate loss: -0.0083
                 Mean entropy loss: 13.4250
                       Mean reward: 5.63
               Mean episode length: 682.13
Episode_Reward/track_lin_vel_xy_exp: 0.3308
Episode_Reward/track_ang_vel_z_exp: 0.3163
       Episode_Reward/lin_vel_z_l2: -0.0272
      Episode_Reward/ang_vel_xy_l2: -0.0887
     Episode_Reward/dof_torques_l2: -0.0467
         Episode_Reward/dof_acc_l2: -0.0476
     Episode_Reward/action_rate_l2: -0.0951
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7437
Metrics/base_velocity/error_vel_xy: 0.8369
Metrics/base_velocity/error_vel_yaw: 0.5265
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 3384000
                    Iteration time: 1.08s
                      Time elapsed: 00:01:43
                               ETA: 00:25:50

################################################################################
                      [1m Learning iteration 94/1500 [0m                      

                       Computation: 32906 steps/s (collection: 1.043s, learning 0.051s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0134
               Mean surrogate loss: -0.0072
                 Mean entropy loss: 13.4208
                       Mean reward: 5.82
               Mean episode length: 682.74
Episode_Reward/track_lin_vel_xy_exp: 0.3105
Episode_Reward/track_ang_vel_z_exp: 0.3512
       Episode_Reward/lin_vel_z_l2: -0.0284
      Episode_Reward/ang_vel_xy_l2: -0.0987
     Episode_Reward/dof_torques_l2: -0.0558
         Episode_Reward/dof_acc_l2: -0.0547
     Episode_Reward/action_rate_l2: -0.1101
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7281
Metrics/base_velocity/error_vel_xy: 1.0321
Metrics/base_velocity/error_vel_yaw: 0.6890
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 3420000
                    Iteration time: 1.09s
                      Time elapsed: 00:01:44
                               ETA: 00:25:49

################################################################################
                      [1m Learning iteration 95/1500 [0m                      

                       Computation: 33827 steps/s (collection: 1.011s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0112
               Mean surrogate loss: -0.0073
                 Mean entropy loss: 13.3978
                       Mean reward: 5.28
               Mean episode length: 657.41
Episode_Reward/track_lin_vel_xy_exp: 0.2135
Episode_Reward/track_ang_vel_z_exp: 0.2532
       Episode_Reward/lin_vel_z_l2: -0.0243
      Episode_Reward/ang_vel_xy_l2: -0.0721
     Episode_Reward/dof_torques_l2: -0.0392
         Episode_Reward/dof_acc_l2: -0.0378
     Episode_Reward/action_rate_l2: -0.0774
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7162
Metrics/base_velocity/error_vel_xy: 0.7476
Metrics/base_velocity/error_vel_yaw: 0.4460
      Episode_Termination/time_out: 1.0417
  Episode_Termination/base_contact: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 3456000
                    Iteration time: 1.06s
                      Time elapsed: 00:01:45
                               ETA: 00:25:47

################################################################################
                      [1m Learning iteration 96/1500 [0m                      

                       Computation: 33251 steps/s (collection: 1.029s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0114
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 13.3736
                       Mean reward: 5.88
               Mean episode length: 634.01
Episode_Reward/track_lin_vel_xy_exp: 0.3634
Episode_Reward/track_ang_vel_z_exp: 0.3113
       Episode_Reward/lin_vel_z_l2: -0.0270
      Episode_Reward/ang_vel_xy_l2: -0.0894
     Episode_Reward/dof_torques_l2: -0.0472
         Episode_Reward/dof_acc_l2: -0.0507
     Episode_Reward/action_rate_l2: -0.0972
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7043
Metrics/base_velocity/error_vel_xy: 0.8172
Metrics/base_velocity/error_vel_yaw: 0.5808
      Episode_Termination/time_out: 1.0833
  Episode_Termination/base_contact: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 3492000
                    Iteration time: 1.08s
                      Time elapsed: 00:01:46
                               ETA: 00:25:46

################################################################################
                      [1m Learning iteration 97/1500 [0m                      

                       Computation: 31853 steps/s (collection: 1.076s, learning 0.055s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0097
               Mean surrogate loss: -0.0066
                 Mean entropy loss: 13.3478
                       Mean reward: 5.95
               Mean episode length: 636.18
Episode_Reward/track_lin_vel_xy_exp: 0.3217
Episode_Reward/track_ang_vel_z_exp: 0.3019
       Episode_Reward/lin_vel_z_l2: -0.0290
      Episode_Reward/ang_vel_xy_l2: -0.0926
     Episode_Reward/dof_torques_l2: -0.0464
         Episode_Reward/dof_acc_l2: -0.0521
     Episode_Reward/action_rate_l2: -0.0977
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6881
Metrics/base_velocity/error_vel_xy: 0.8584
Metrics/base_velocity/error_vel_yaw: 0.6077
      Episode_Termination/time_out: 0.8750
  Episode_Termination/base_contact: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 3528000
                    Iteration time: 1.13s
                      Time elapsed: 00:01:47
                               ETA: 00:25:45

################################################################################
                      [1m Learning iteration 98/1500 [0m                      

                       Computation: 33210 steps/s (collection: 1.031s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0197
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 13.3472
                       Mean reward: 5.54
               Mean episode length: 641.71
Episode_Reward/track_lin_vel_xy_exp: 0.2540
Episode_Reward/track_ang_vel_z_exp: 0.2856
       Episode_Reward/lin_vel_z_l2: -0.0266
      Episode_Reward/ang_vel_xy_l2: -0.0814
     Episode_Reward/dof_torques_l2: -0.0394
         Episode_Reward/dof_acc_l2: -0.0482
     Episode_Reward/action_rate_l2: -0.0857
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6769
Metrics/base_velocity/error_vel_xy: 0.8139
Metrics/base_velocity/error_vel_yaw: 0.4553
      Episode_Termination/time_out: 1.1250
  Episode_Termination/base_contact: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 3564000
                    Iteration time: 1.08s
                      Time elapsed: 00:01:49
                               ETA: 00:25:44

################################################################################
                      [1m Learning iteration 99/1500 [0m                      

                       Computation: 32919 steps/s (collection: 1.041s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0116
               Mean surrogate loss: -0.0087
                 Mean entropy loss: 13.3476
                       Mean reward: 5.66
               Mean episode length: 663.47
Episode_Reward/track_lin_vel_xy_exp: 0.3037
Episode_Reward/track_ang_vel_z_exp: 0.3444
       Episode_Reward/lin_vel_z_l2: -0.0284
      Episode_Reward/ang_vel_xy_l2: -0.0949
     Episode_Reward/dof_torques_l2: -0.0490
         Episode_Reward/dof_acc_l2: -0.0521
     Episode_Reward/action_rate_l2: -0.1025
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6668
Metrics/base_velocity/error_vel_xy: 0.9522
Metrics/base_velocity/error_vel_yaw: 0.5410
      Episode_Termination/time_out: 0.9583
  Episode_Termination/base_contact: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 3600000
                    Iteration time: 1.09s
                      Time elapsed: 00:01:50
                               ETA: 00:25:42

################################################################################
                     [1m Learning iteration 100/1500 [0m                      

                       Computation: 32497 steps/s (collection: 1.055s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0089
               Mean surrogate loss: -0.0092
                 Mean entropy loss: 13.3199
                       Mean reward: 5.59
               Mean episode length: 669.03
Episode_Reward/track_lin_vel_xy_exp: 0.2895
Episode_Reward/track_ang_vel_z_exp: 0.3094
       Episode_Reward/lin_vel_z_l2: -0.0256
      Episode_Reward/ang_vel_xy_l2: -0.0865
     Episode_Reward/dof_torques_l2: -0.0439
         Episode_Reward/dof_acc_l2: -0.0487
     Episode_Reward/action_rate_l2: -0.0934
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6537
Metrics/base_velocity/error_vel_xy: 0.8616
Metrics/base_velocity/error_vel_yaw: 0.5240
      Episode_Termination/time_out: 1.1250
  Episode_Termination/base_contact: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 3636000
                    Iteration time: 1.11s
                      Time elapsed: 00:01:51
                               ETA: 00:25:41

################################################################################
                     [1m Learning iteration 101/1500 [0m                      

                       Computation: 33232 steps/s (collection: 1.030s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0118
               Mean surrogate loss: -0.0065
                 Mean entropy loss: 13.2885
                       Mean reward: 6.02
               Mean episode length: 694.67
Episode_Reward/track_lin_vel_xy_exp: 0.3125
Episode_Reward/track_ang_vel_z_exp: 0.3431
       Episode_Reward/lin_vel_z_l2: -0.0279
      Episode_Reward/ang_vel_xy_l2: -0.0928
     Episode_Reward/dof_torques_l2: -0.0508
         Episode_Reward/dof_acc_l2: -0.0516
     Episode_Reward/action_rate_l2: -0.1026
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6353
Metrics/base_velocity/error_vel_xy: 0.9898
Metrics/base_velocity/error_vel_yaw: 0.5718
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 3672000
                    Iteration time: 1.08s
                      Time elapsed: 00:01:52
                               ETA: 00:25:40

################################################################################
                     [1m Learning iteration 102/1500 [0m                      

                       Computation: 33371 steps/s (collection: 1.027s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0078
               Mean surrogate loss: -0.0077
                 Mean entropy loss: 13.2681
                       Mean reward: 5.81
               Mean episode length: 652.48
Episode_Reward/track_lin_vel_xy_exp: 0.2330
Episode_Reward/track_ang_vel_z_exp: 0.3028
       Episode_Reward/lin_vel_z_l2: -0.0274
      Episode_Reward/ang_vel_xy_l2: -0.0826
     Episode_Reward/dof_torques_l2: -0.0456
         Episode_Reward/dof_acc_l2: -0.0498
     Episode_Reward/action_rate_l2: -0.0913
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6202
Metrics/base_velocity/error_vel_xy: 0.9060
Metrics/base_velocity/error_vel_yaw: 0.5114
      Episode_Termination/time_out: 0.9167
  Episode_Termination/base_contact: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 3708000
                    Iteration time: 1.08s
                      Time elapsed: 00:01:53
                               ETA: 00:25:39

################################################################################
                     [1m Learning iteration 103/1500 [0m                      

                       Computation: 32763 steps/s (collection: 1.046s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0142
               Mean surrogate loss: -0.0085
                 Mean entropy loss: 13.2390
                       Mean reward: 6.10
               Mean episode length: 697.28
Episode_Reward/track_lin_vel_xy_exp: 0.2756
Episode_Reward/track_ang_vel_z_exp: 0.3498
       Episode_Reward/lin_vel_z_l2: -0.0296
      Episode_Reward/ang_vel_xy_l2: -0.0950
     Episode_Reward/dof_torques_l2: -0.0497
         Episode_Reward/dof_acc_l2: -0.0567
     Episode_Reward/action_rate_l2: -0.1041
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6023
Metrics/base_velocity/error_vel_xy: 1.0357
Metrics/base_velocity/error_vel_yaw: 0.5515
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 3744000
                    Iteration time: 1.10s
                      Time elapsed: 00:01:54
                               ETA: 00:25:38

################################################################################
                     [1m Learning iteration 104/1500 [0m                      

                       Computation: 32462 steps/s (collection: 1.058s, learning 0.050s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0138
               Mean surrogate loss: -0.0084
                 Mean entropy loss: 13.2359
                       Mean reward: 6.48
               Mean episode length: 722.91
Episode_Reward/track_lin_vel_xy_exp: 0.2640
Episode_Reward/track_ang_vel_z_exp: 0.3452
       Episode_Reward/lin_vel_z_l2: -0.0252
      Episode_Reward/ang_vel_xy_l2: -0.0897
     Episode_Reward/dof_torques_l2: -0.0482
         Episode_Reward/dof_acc_l2: -0.0511
     Episode_Reward/action_rate_l2: -0.0994
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.5851
Metrics/base_velocity/error_vel_xy: 0.9792
Metrics/base_velocity/error_vel_yaw: 0.4987
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 3780000
                    Iteration time: 1.11s
                      Time elapsed: 00:01:55
                               ETA: 00:25:37

################################################################################
                     [1m Learning iteration 105/1500 [0m                      

                       Computation: 33400 steps/s (collection: 1.027s, learning 0.050s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0135
               Mean surrogate loss: -0.0075
                 Mean entropy loss: 13.2145
                       Mean reward: 6.86
               Mean episode length: 714.99
Episode_Reward/track_lin_vel_xy_exp: 0.2950
Episode_Reward/track_ang_vel_z_exp: 0.3552
       Episode_Reward/lin_vel_z_l2: -0.0269
      Episode_Reward/ang_vel_xy_l2: -0.0949
     Episode_Reward/dof_torques_l2: -0.0476
         Episode_Reward/dof_acc_l2: -0.0568
     Episode_Reward/action_rate_l2: -0.1032
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.5641
Metrics/base_velocity/error_vel_xy: 0.9815
Metrics/base_velocity/error_vel_yaw: 0.5128
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 3816000
                    Iteration time: 1.08s
                      Time elapsed: 00:01:56
                               ETA: 00:25:35

################################################################################
                     [1m Learning iteration 106/1500 [0m                      

                       Computation: 33005 steps/s (collection: 1.037s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0132
               Mean surrogate loss: -0.0071
                 Mean entropy loss: 13.1970
                       Mean reward: 7.22
               Mean episode length: 743.25
Episode_Reward/track_lin_vel_xy_exp: 0.2988
Episode_Reward/track_ang_vel_z_exp: 0.3405
       Episode_Reward/lin_vel_z_l2: -0.0254
      Episode_Reward/ang_vel_xy_l2: -0.0872
     Episode_Reward/dof_torques_l2: -0.0466
         Episode_Reward/dof_acc_l2: -0.0499
     Episode_Reward/action_rate_l2: -0.0992
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.5449
Metrics/base_velocity/error_vel_xy: 0.8850
Metrics/base_velocity/error_vel_yaw: 0.4992
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 3852000
                    Iteration time: 1.09s
                      Time elapsed: 00:01:57
                               ETA: 00:25:34

################################################################################
                     [1m Learning iteration 107/1500 [0m                      

                       Computation: 33156 steps/s (collection: 1.033s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0065
                 Mean entropy loss: 13.1928
                       Mean reward: 7.02
               Mean episode length: 751.70
Episode_Reward/track_lin_vel_xy_exp: 0.3420
Episode_Reward/track_ang_vel_z_exp: 0.3587
       Episode_Reward/lin_vel_z_l2: -0.0270
      Episode_Reward/ang_vel_xy_l2: -0.0909
     Episode_Reward/dof_torques_l2: -0.0513
         Episode_Reward/dof_acc_l2: -0.0527
     Episode_Reward/action_rate_l2: -0.1035
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.5330
Metrics/base_velocity/error_vel_xy: 0.8952
Metrics/base_velocity/error_vel_yaw: 0.5311
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 3888000
                    Iteration time: 1.09s
                      Time elapsed: 00:01:58
                               ETA: 00:25:33

################################################################################
                     [1m Learning iteration 108/1500 [0m                      

                       Computation: 33370 steps/s (collection: 1.026s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0186
               Mean surrogate loss: -0.0065
                 Mean entropy loss: 13.2015
                       Mean reward: 7.65
               Mean episode length: 764.16
Episode_Reward/track_lin_vel_xy_exp: 0.3522
Episode_Reward/track_ang_vel_z_exp: 0.3671
       Episode_Reward/lin_vel_z_l2: -0.0268
      Episode_Reward/ang_vel_xy_l2: -0.0925
     Episode_Reward/dof_torques_l2: -0.0514
         Episode_Reward/dof_acc_l2: -0.0530
     Episode_Reward/action_rate_l2: -0.1055
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.5158
Metrics/base_velocity/error_vel_xy: 0.9407
Metrics/base_velocity/error_vel_yaw: 0.5260
      Episode_Termination/time_out: 2.2917
  Episode_Termination/base_contact: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 3924000
                    Iteration time: 1.08s
                      Time elapsed: 00:01:59
                               ETA: 00:25:31

################################################################################
                     [1m Learning iteration 109/1500 [0m                      

                       Computation: 32756 steps/s (collection: 1.046s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0134
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 13.2048
                       Mean reward: 6.53
               Mean episode length: 704.30
Episode_Reward/track_lin_vel_xy_exp: 0.3311
Episode_Reward/track_ang_vel_z_exp: 0.3688
       Episode_Reward/lin_vel_z_l2: -0.0271
      Episode_Reward/ang_vel_xy_l2: -0.0955
     Episode_Reward/dof_torques_l2: -0.0549
         Episode_Reward/dof_acc_l2: -0.0572
     Episode_Reward/action_rate_l2: -0.1085
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4996
Metrics/base_velocity/error_vel_xy: 0.9881
Metrics/base_velocity/error_vel_yaw: 0.5759
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3960000
                    Iteration time: 1.10s
                      Time elapsed: 00:02:01
                               ETA: 00:25:30

################################################################################
                     [1m Learning iteration 110/1500 [0m                      

                       Computation: 33273 steps/s (collection: 1.028s, learning 0.054s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0211
               Mean surrogate loss: -0.0068
                 Mean entropy loss: 13.1988
                       Mean reward: 5.99
               Mean episode length: 720.20
Episode_Reward/track_lin_vel_xy_exp: 0.2829
Episode_Reward/track_ang_vel_z_exp: 0.3622
       Episode_Reward/lin_vel_z_l2: -0.0271
      Episode_Reward/ang_vel_xy_l2: -0.0894
     Episode_Reward/dof_torques_l2: -0.0495
         Episode_Reward/dof_acc_l2: -0.0515
     Episode_Reward/action_rate_l2: -0.1028
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4853
Metrics/base_velocity/error_vel_xy: 0.9781
Metrics/base_velocity/error_vel_yaw: 0.4866
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 3996000
                    Iteration time: 1.08s
                      Time elapsed: 00:02:02
                               ETA: 00:25:29

################################################################################
                     [1m Learning iteration 111/1500 [0m                      

                       Computation: 32989 steps/s (collection: 1.039s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0151
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 13.2151
                       Mean reward: 7.16
               Mean episode length: 719.69
Episode_Reward/track_lin_vel_xy_exp: 0.3306
Episode_Reward/track_ang_vel_z_exp: 0.3373
       Episode_Reward/lin_vel_z_l2: -0.0255
      Episode_Reward/ang_vel_xy_l2: -0.0829
     Episode_Reward/dof_torques_l2: -0.0473
         Episode_Reward/dof_acc_l2: -0.0504
     Episode_Reward/action_rate_l2: -0.0958
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4728
Metrics/base_velocity/error_vel_xy: 0.8568
Metrics/base_velocity/error_vel_yaw: 0.4740
      Episode_Termination/time_out: 1.9167
  Episode_Termination/base_contact: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 4032000
                    Iteration time: 1.09s
                      Time elapsed: 00:02:03
                               ETA: 00:25:28

################################################################################
                     [1m Learning iteration 112/1500 [0m                      

                       Computation: 32921 steps/s (collection: 1.043s, learning 0.050s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0144
               Mean surrogate loss: -0.0067
                 Mean entropy loss: 13.2163
                       Mean reward: 7.32
               Mean episode length: 689.31
Episode_Reward/track_lin_vel_xy_exp: 0.3552
Episode_Reward/track_ang_vel_z_exp: 0.3671
       Episode_Reward/lin_vel_z_l2: -0.0256
      Episode_Reward/ang_vel_xy_l2: -0.0871
     Episode_Reward/dof_torques_l2: -0.0506
         Episode_Reward/dof_acc_l2: -0.0511
     Episode_Reward/action_rate_l2: -0.1011
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4627
Metrics/base_velocity/error_vel_xy: 0.8939
Metrics/base_velocity/error_vel_yaw: 0.4603
      Episode_Termination/time_out: 2.1667
  Episode_Termination/base_contact: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 4068000
                    Iteration time: 1.09s
                      Time elapsed: 00:02:04
                               ETA: 00:25:26

################################################################################
                     [1m Learning iteration 113/1500 [0m                      

                       Computation: 32697 steps/s (collection: 1.051s, learning 0.050s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0116
               Mean surrogate loss: -0.0073
                 Mean entropy loss: 13.2126
                       Mean reward: 5.72
               Mean episode length: 651.27
Episode_Reward/track_lin_vel_xy_exp: 0.2400
Episode_Reward/track_ang_vel_z_exp: 0.3060
       Episode_Reward/lin_vel_z_l2: -0.0254
      Episode_Reward/ang_vel_xy_l2: -0.0785
     Episode_Reward/dof_torques_l2: -0.0438
         Episode_Reward/dof_acc_l2: -0.0493
     Episode_Reward/action_rate_l2: -0.0894
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4491
Metrics/base_velocity/error_vel_xy: 0.8801
Metrics/base_velocity/error_vel_yaw: 0.4766
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 4104000
                    Iteration time: 1.10s
                      Time elapsed: 00:02:05
                               ETA: 00:25:25

################################################################################
                     [1m Learning iteration 114/1500 [0m                      

                       Computation: 32307 steps/s (collection: 1.062s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0182
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 13.2225
                       Mean reward: 6.04
               Mean episode length: 644.83
Episode_Reward/track_lin_vel_xy_exp: 0.2999
Episode_Reward/track_ang_vel_z_exp: 0.3276
       Episode_Reward/lin_vel_z_l2: -0.0269
      Episode_Reward/ang_vel_xy_l2: -0.0827
     Episode_Reward/dof_torques_l2: -0.0439
         Episode_Reward/dof_acc_l2: -0.0478
     Episode_Reward/action_rate_l2: -0.0918
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4388
Metrics/base_velocity/error_vel_xy: 0.8258
Metrics/base_velocity/error_vel_yaw: 0.4163
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 4140000
                    Iteration time: 1.11s
                      Time elapsed: 00:02:06
                               ETA: 00:25:24

################################################################################
                     [1m Learning iteration 115/1500 [0m                      

                       Computation: 33168 steps/s (collection: 1.033s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 13.2367
                       Mean reward: 5.59
               Mean episode length: 616.12
Episode_Reward/track_lin_vel_xy_exp: 0.2125
Episode_Reward/track_ang_vel_z_exp: 0.3003
       Episode_Reward/lin_vel_z_l2: -0.0236
      Episode_Reward/ang_vel_xy_l2: -0.0734
     Episode_Reward/dof_torques_l2: -0.0419
         Episode_Reward/dof_acc_l2: -0.0454
     Episode_Reward/action_rate_l2: -0.0846
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4288
Metrics/base_velocity/error_vel_xy: 0.8153
Metrics/base_velocity/error_vel_yaw: 0.3948
      Episode_Termination/time_out: 1.0417
  Episode_Termination/base_contact: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 4176000
                    Iteration time: 1.09s
                      Time elapsed: 00:02:07
                               ETA: 00:25:23

################################################################################
                     [1m Learning iteration 116/1500 [0m                      

                       Computation: 32674 steps/s (collection: 1.049s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0153
               Mean surrogate loss: -0.0063
                 Mean entropy loss: 13.2275
                       Mean reward: 5.41
               Mean episode length: 569.12
Episode_Reward/track_lin_vel_xy_exp: 0.2656
Episode_Reward/track_ang_vel_z_exp: 0.2713
       Episode_Reward/lin_vel_z_l2: -0.0249
      Episode_Reward/ang_vel_xy_l2: -0.0705
     Episode_Reward/dof_torques_l2: -0.0372
         Episode_Reward/dof_acc_l2: -0.0456
     Episode_Reward/action_rate_l2: -0.0772
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4220
Metrics/base_velocity/error_vel_xy: 0.6877
Metrics/base_velocity/error_vel_yaw: 0.3837
      Episode_Termination/time_out: 0.9583
  Episode_Termination/base_contact: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 4212000
                    Iteration time: 1.10s
                      Time elapsed: 00:02:08
                               ETA: 00:25:22

################################################################################
                     [1m Learning iteration 117/1500 [0m                      

                       Computation: 32843 steps/s (collection: 1.043s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0121
               Mean surrogate loss: -0.0067
                 Mean entropy loss: 13.2205
                       Mean reward: 5.48
               Mean episode length: 574.22
Episode_Reward/track_lin_vel_xy_exp: 0.2312
Episode_Reward/track_ang_vel_z_exp: 0.2764
       Episode_Reward/lin_vel_z_l2: -0.0234
      Episode_Reward/ang_vel_xy_l2: -0.0697
     Episode_Reward/dof_torques_l2: -0.0364
         Episode_Reward/dof_acc_l2: -0.0475
     Episode_Reward/action_rate_l2: -0.0777
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4145
Metrics/base_velocity/error_vel_xy: 0.7202
Metrics/base_velocity/error_vel_yaw: 0.3661
      Episode_Termination/time_out: 1.0000
  Episode_Termination/base_contact: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 4248000
                    Iteration time: 1.10s
                      Time elapsed: 00:02:09
                               ETA: 00:25:21

################################################################################
                     [1m Learning iteration 118/1500 [0m                      

                       Computation: 33211 steps/s (collection: 1.031s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0138
               Mean surrogate loss: -0.0074
                 Mean entropy loss: 13.2061
                       Mean reward: 5.86
               Mean episode length: 607.16
Episode_Reward/track_lin_vel_xy_exp: 0.2765
Episode_Reward/track_ang_vel_z_exp: 0.3345
       Episode_Reward/lin_vel_z_l2: -0.0264
      Episode_Reward/ang_vel_xy_l2: -0.0821
     Episode_Reward/dof_torques_l2: -0.0477
         Episode_Reward/dof_acc_l2: -0.0511
     Episode_Reward/action_rate_l2: -0.0945
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4052
Metrics/base_velocity/error_vel_xy: 0.8650
Metrics/base_velocity/error_vel_yaw: 0.4621
      Episode_Termination/time_out: 1.0833
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4284000
                    Iteration time: 1.08s
                      Time elapsed: 00:02:10
                               ETA: 00:25:20

################################################################################
                     [1m Learning iteration 119/1500 [0m                      

                       Computation: 33341 steps/s (collection: 1.026s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0120
               Mean surrogate loss: -0.0081
                 Mean entropy loss: 13.1866
                       Mean reward: 7.22
               Mean episode length: 687.25
Episode_Reward/track_lin_vel_xy_exp: 0.3283
Episode_Reward/track_ang_vel_z_exp: 0.3435
       Episode_Reward/lin_vel_z_l2: -0.0254
      Episode_Reward/ang_vel_xy_l2: -0.0828
     Episode_Reward/dof_torques_l2: -0.0467
         Episode_Reward/dof_acc_l2: -0.0557
     Episode_Reward/action_rate_l2: -0.0975
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3956
Metrics/base_velocity/error_vel_xy: 0.8369
Metrics/base_velocity/error_vel_yaw: 0.4690
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 4320000
                    Iteration time: 1.08s
                      Time elapsed: 00:02:11
                               ETA: 00:25:18

################################################################################
                     [1m Learning iteration 120/1500 [0m                      

                       Computation: 32659 steps/s (collection: 1.049s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0099
               Mean surrogate loss: -0.0081
                 Mean entropy loss: 13.1694
                       Mean reward: 6.63
               Mean episode length: 633.65
Episode_Reward/track_lin_vel_xy_exp: 0.2904
Episode_Reward/track_ang_vel_z_exp: 0.2588
       Episode_Reward/lin_vel_z_l2: -0.0210
      Episode_Reward/ang_vel_xy_l2: -0.0609
     Episode_Reward/dof_torques_l2: -0.0347
         Episode_Reward/dof_acc_l2: -0.0427
     Episode_Reward/action_rate_l2: -0.0715
      Episode_Reward/feet_air_time: -0.0008
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3854
Metrics/base_velocity/error_vel_xy: 0.5619
Metrics/base_velocity/error_vel_yaw: 0.3129
      Episode_Termination/time_out: 0.4167
  Episode_Termination/base_contact: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 4356000
                    Iteration time: 1.10s
                      Time elapsed: 00:02:13
                               ETA: 00:25:17

################################################################################
                     [1m Learning iteration 121/1500 [0m                      

                       Computation: 33581 steps/s (collection: 1.021s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0136
               Mean surrogate loss: -0.0080
                 Mean entropy loss: 13.1455
                       Mean reward: 6.07
               Mean episode length: 613.20
Episode_Reward/track_lin_vel_xy_exp: 0.3141
Episode_Reward/track_ang_vel_z_exp: 0.3542
       Episode_Reward/lin_vel_z_l2: -0.0287
      Episode_Reward/ang_vel_xy_l2: -0.0863
     Episode_Reward/dof_torques_l2: -0.0465
         Episode_Reward/dof_acc_l2: -0.0550
     Episode_Reward/action_rate_l2: -0.0981
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3806
Metrics/base_velocity/error_vel_xy: 0.8636
Metrics/base_velocity/error_vel_yaw: 0.4391
      Episode_Termination/time_out: 0.7917
  Episode_Termination/base_contact: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 4392000
                    Iteration time: 1.07s
                      Time elapsed: 00:02:14
                               ETA: 00:25:16

################################################################################
                     [1m Learning iteration 122/1500 [0m                      

                       Computation: 33599 steps/s (collection: 1.020s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0109
               Mean surrogate loss: -0.0070
                 Mean entropy loss: 13.1198
                       Mean reward: 5.85
               Mean episode length: 612.34
Episode_Reward/track_lin_vel_xy_exp: 0.3005
Episode_Reward/track_ang_vel_z_exp: 0.3673
       Episode_Reward/lin_vel_z_l2: -0.0333
      Episode_Reward/ang_vel_xy_l2: -0.0907
     Episode_Reward/dof_torques_l2: -0.0490
         Episode_Reward/dof_acc_l2: -0.0595
     Episode_Reward/action_rate_l2: -0.0993
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3751
Metrics/base_velocity/error_vel_xy: 0.9192
Metrics/base_velocity/error_vel_yaw: 0.4338
      Episode_Termination/time_out: 0.6667
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4428000
                    Iteration time: 1.07s
                      Time elapsed: 00:02:15
                               ETA: 00:25:14

################################################################################
                     [1m Learning iteration 123/1500 [0m                      

                       Computation: 33295 steps/s (collection: 1.029s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0096
               Mean surrogate loss: -0.0073
                 Mean entropy loss: 13.0986
                       Mean reward: 5.51
               Mean episode length: 610.24
Episode_Reward/track_lin_vel_xy_exp: 0.1924
Episode_Reward/track_ang_vel_z_exp: 0.2392
       Episode_Reward/lin_vel_z_l2: -0.0223
      Episode_Reward/ang_vel_xy_l2: -0.0593
     Episode_Reward/dof_torques_l2: -0.0324
         Episode_Reward/dof_acc_l2: -0.0381
     Episode_Reward/action_rate_l2: -0.0663
      Episode_Reward/feet_air_time: -0.0009
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3708
Metrics/base_velocity/error_vel_xy: 0.6037
Metrics/base_velocity/error_vel_yaw: 0.2899
      Episode_Termination/time_out: 0.6250
  Episode_Termination/base_contact: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 4464000
                    Iteration time: 1.08s
                      Time elapsed: 00:02:16
                               ETA: 00:25:13

################################################################################
                     [1m Learning iteration 124/1500 [0m                      

                       Computation: 32977 steps/s (collection: 1.038s, learning 0.054s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0170
               Mean surrogate loss: -0.0073
                 Mean entropy loss: 13.0928
                       Mean reward: 5.90
               Mean episode length: 635.07
Episode_Reward/track_lin_vel_xy_exp: 0.2695
Episode_Reward/track_ang_vel_z_exp: 0.3576
       Episode_Reward/lin_vel_z_l2: -0.0275
      Episode_Reward/ang_vel_xy_l2: -0.0869
     Episode_Reward/dof_torques_l2: -0.0456
         Episode_Reward/dof_acc_l2: -0.0605
     Episode_Reward/action_rate_l2: -0.0974
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3679
Metrics/base_velocity/error_vel_xy: 0.9304
Metrics/base_velocity/error_vel_yaw: 0.4200
      Episode_Termination/time_out: 0.9167
  Episode_Termination/base_contact: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 4500000
                    Iteration time: 1.09s
                      Time elapsed: 00:02:17
                               ETA: 00:25:12

################################################################################
                     [1m Learning iteration 125/1500 [0m                      

                       Computation: 33214 steps/s (collection: 1.032s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0158
               Mean surrogate loss: -0.0068
                 Mean entropy loss: 13.1052
                       Mean reward: 6.69
               Mean episode length: 693.47
Episode_Reward/track_lin_vel_xy_exp: 0.2931
Episode_Reward/track_ang_vel_z_exp: 0.3815
       Episode_Reward/lin_vel_z_l2: -0.0269
      Episode_Reward/ang_vel_xy_l2: -0.0903
     Episode_Reward/dof_torques_l2: -0.0505
         Episode_Reward/dof_acc_l2: -0.0583
     Episode_Reward/action_rate_l2: -0.1030
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3631
Metrics/base_velocity/error_vel_xy: 0.9651
Metrics/base_velocity/error_vel_yaw: 0.4378
      Episode_Termination/time_out: 1.1250
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 4536000
                    Iteration time: 1.08s
                      Time elapsed: 00:02:18
                               ETA: 00:25:11

################################################################################
                     [1m Learning iteration 126/1500 [0m                      

                       Computation: 33728 steps/s (collection: 1.014s, learning 0.054s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0062
                 Mean entropy loss: 13.1076
                       Mean reward: 6.53
               Mean episode length: 725.92
Episode_Reward/track_lin_vel_xy_exp: 0.2379
Episode_Reward/track_ang_vel_z_exp: 0.3651
       Episode_Reward/lin_vel_z_l2: -0.0283
      Episode_Reward/ang_vel_xy_l2: -0.0883
     Episode_Reward/dof_torques_l2: -0.0495
         Episode_Reward/dof_acc_l2: -0.0619
     Episode_Reward/action_rate_l2: -0.1000
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3557
Metrics/base_velocity/error_vel_xy: 1.0183
Metrics/base_velocity/error_vel_yaw: 0.4483
      Episode_Termination/time_out: 1.1667
  Episode_Termination/base_contact: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 4572000
                    Iteration time: 1.07s
                      Time elapsed: 00:02:19
                               ETA: 00:25:09

################################################################################
                     [1m Learning iteration 127/1500 [0m                      

                       Computation: 32161 steps/s (collection: 1.069s, learning 0.050s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0216
               Mean surrogate loss: -0.0076
                 Mean entropy loss: 13.1053
                       Mean reward: 7.40
               Mean episode length: 749.98
Episode_Reward/track_lin_vel_xy_exp: 0.3572
Episode_Reward/track_ang_vel_z_exp: 0.3915
       Episode_Reward/lin_vel_z_l2: -0.0291
      Episode_Reward/ang_vel_xy_l2: -0.0932
     Episode_Reward/dof_torques_l2: -0.0524
         Episode_Reward/dof_acc_l2: -0.0639
     Episode_Reward/action_rate_l2: -0.1077
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3493
Metrics/base_velocity/error_vel_xy: 0.9266
Metrics/base_velocity/error_vel_yaw: 0.4827
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 4608000
                    Iteration time: 1.12s
                      Time elapsed: 00:02:20
                               ETA: 00:25:08

################################################################################
                     [1m Learning iteration 128/1500 [0m                      

                       Computation: 33006 steps/s (collection: 1.040s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0149
               Mean surrogate loss: -0.0077
                 Mean entropy loss: 13.1133
                       Mean reward: 7.06
               Mean episode length: 714.33
Episode_Reward/track_lin_vel_xy_exp: 0.2908
Episode_Reward/track_ang_vel_z_exp: 0.3350
       Episode_Reward/lin_vel_z_l2: -0.0264
      Episode_Reward/ang_vel_xy_l2: -0.0798
     Episode_Reward/dof_torques_l2: -0.0441
         Episode_Reward/dof_acc_l2: -0.0547
     Episode_Reward/action_rate_l2: -0.0916
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3382
Metrics/base_velocity/error_vel_xy: 0.8044
Metrics/base_velocity/error_vel_yaw: 0.3975
      Episode_Termination/time_out: 0.9167
  Episode_Termination/base_contact: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 4644000
                    Iteration time: 1.09s
                      Time elapsed: 00:02:21
                               ETA: 00:25:07

################################################################################
                     [1m Learning iteration 129/1500 [0m                      

                       Computation: 32596 steps/s (collection: 1.052s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0187
               Mean surrogate loss: -0.0069
                 Mean entropy loss: 13.1261
                       Mean reward: 6.40
               Mean episode length: 667.79
Episode_Reward/track_lin_vel_xy_exp: 0.2763
Episode_Reward/track_ang_vel_z_exp: 0.3644
       Episode_Reward/lin_vel_z_l2: -0.0281
      Episode_Reward/ang_vel_xy_l2: -0.0842
     Episode_Reward/dof_torques_l2: -0.0507
         Episode_Reward/dof_acc_l2: -0.0602
     Episode_Reward/action_rate_l2: -0.1001
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3314
Metrics/base_velocity/error_vel_xy: 0.9377
Metrics/base_velocity/error_vel_yaw: 0.4339
      Episode_Termination/time_out: 1.0000
  Episode_Termination/base_contact: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 4680000
                    Iteration time: 1.10s
                      Time elapsed: 00:02:22
                               ETA: 00:25:06

################################################################################
                     [1m Learning iteration 130/1500 [0m                      

                       Computation: 33429 steps/s (collection: 1.026s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0217
               Mean surrogate loss: -0.0071
                 Mean entropy loss: 13.1391
                       Mean reward: 7.56
               Mean episode length: 723.31
Episode_Reward/track_lin_vel_xy_exp: 0.3656
Episode_Reward/track_ang_vel_z_exp: 0.3817
       Episode_Reward/lin_vel_z_l2: -0.0282
      Episode_Reward/ang_vel_xy_l2: -0.0908
     Episode_Reward/dof_torques_l2: -0.0501
         Episode_Reward/dof_acc_l2: -0.0624
     Episode_Reward/action_rate_l2: -0.1048
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3285
Metrics/base_velocity/error_vel_xy: 0.8635
Metrics/base_velocity/error_vel_yaw: 0.4515
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4716000
                    Iteration time: 1.08s
                      Time elapsed: 00:02:23
                               ETA: 00:25:05

################################################################################
                     [1m Learning iteration 131/1500 [0m                      

                       Computation: 32932 steps/s (collection: 1.041s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0186
               Mean surrogate loss: -0.0063
                 Mean entropy loss: 13.1579
                       Mean reward: 8.55
               Mean episode length: 723.98
Episode_Reward/track_lin_vel_xy_exp: 0.3654
Episode_Reward/track_ang_vel_z_exp: 0.3740
       Episode_Reward/lin_vel_z_l2: -0.0277
      Episode_Reward/ang_vel_xy_l2: -0.0879
     Episode_Reward/dof_torques_l2: -0.0491
         Episode_Reward/dof_acc_l2: -0.0616
     Episode_Reward/action_rate_l2: -0.1025
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3204
Metrics/base_velocity/error_vel_xy: 0.8484
Metrics/base_velocity/error_vel_yaw: 0.4465
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 4752000
                    Iteration time: 1.09s
                      Time elapsed: 00:02:25
                               ETA: 00:25:04

################################################################################
                     [1m Learning iteration 132/1500 [0m                      

                       Computation: 32186 steps/s (collection: 1.067s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0156
               Mean surrogate loss: -0.0074
                 Mean entropy loss: 13.1747
                       Mean reward: 8.28
               Mean episode length: 694.92
Episode_Reward/track_lin_vel_xy_exp: 0.4250
Episode_Reward/track_ang_vel_z_exp: 0.3901
       Episode_Reward/lin_vel_z_l2: -0.0268
      Episode_Reward/ang_vel_xy_l2: -0.0902
     Episode_Reward/dof_torques_l2: -0.0508
         Episode_Reward/dof_acc_l2: -0.0643
     Episode_Reward/action_rate_l2: -0.1056
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3148
Metrics/base_velocity/error_vel_xy: 0.8288
Metrics/base_velocity/error_vel_yaw: 0.4502
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 4788000
                    Iteration time: 1.12s
                      Time elapsed: 00:02:26
                               ETA: 00:25:03

################################################################################
                     [1m Learning iteration 133/1500 [0m                      

                       Computation: 32970 steps/s (collection: 1.039s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0186
               Mean surrogate loss: -0.0078
                 Mean entropy loss: 13.1944
                       Mean reward: 8.56
               Mean episode length: 732.01
Episode_Reward/track_lin_vel_xy_exp: 0.5041
Episode_Reward/track_ang_vel_z_exp: 0.4450
       Episode_Reward/lin_vel_z_l2: -0.0322
      Episode_Reward/ang_vel_xy_l2: -0.1057
     Episode_Reward/dof_torques_l2: -0.0609
         Episode_Reward/dof_acc_l2: -0.0697
     Episode_Reward/action_rate_l2: -0.1222
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3125
Metrics/base_velocity/error_vel_xy: 0.9299
Metrics/base_velocity/error_vel_yaw: 0.5145
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 4824000
                    Iteration time: 1.09s
                      Time elapsed: 00:02:27
                               ETA: 00:25:01

################################################################################
                     [1m Learning iteration 134/1500 [0m                      

                       Computation: 33174 steps/s (collection: 1.034s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0138
               Mean surrogate loss: -0.0085
                 Mean entropy loss: 13.2078
                       Mean reward: 8.83
               Mean episode length: 746.99
Episode_Reward/track_lin_vel_xy_exp: 0.4142
Episode_Reward/track_ang_vel_z_exp: 0.4002
       Episode_Reward/lin_vel_z_l2: -0.0292
      Episode_Reward/ang_vel_xy_l2: -0.0945
     Episode_Reward/dof_torques_l2: -0.0531
         Episode_Reward/dof_acc_l2: -0.0665
     Episode_Reward/action_rate_l2: -0.1097
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3101
Metrics/base_velocity/error_vel_xy: 0.8684
Metrics/base_velocity/error_vel_yaw: 0.4644
      Episode_Termination/time_out: 1.0000
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 4860000
                    Iteration time: 1.09s
                      Time elapsed: 00:02:28
                               ETA: 00:25:00

################################################################################
                     [1m Learning iteration 135/1500 [0m                      

                       Computation: 33246 steps/s (collection: 1.032s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0125
               Mean surrogate loss: -0.0077
                 Mean entropy loss: 13.1881
                       Mean reward: 8.07
               Mean episode length: 763.45
Episode_Reward/track_lin_vel_xy_exp: 0.2841
Episode_Reward/track_ang_vel_z_exp: 0.3894
       Episode_Reward/lin_vel_z_l2: -0.0287
      Episode_Reward/ang_vel_xy_l2: -0.0905
     Episode_Reward/dof_torques_l2: -0.0558
         Episode_Reward/dof_acc_l2: -0.0621
     Episode_Reward/action_rate_l2: -0.1073
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3063
Metrics/base_velocity/error_vel_xy: 1.0121
Metrics/base_velocity/error_vel_yaw: 0.4671
      Episode_Termination/time_out: 1.1667
  Episode_Termination/base_contact: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 4896000
                    Iteration time: 1.08s
                      Time elapsed: 00:02:29
                               ETA: 00:24:59

################################################################################
                     [1m Learning iteration 136/1500 [0m                      

                       Computation: 33023 steps/s (collection: 1.038s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0173
               Mean surrogate loss: -0.0076
                 Mean entropy loss: 13.1892
                       Mean reward: 7.99
               Mean episode length: 756.74
Episode_Reward/track_lin_vel_xy_exp: 0.4338
Episode_Reward/track_ang_vel_z_exp: 0.4195
       Episode_Reward/lin_vel_z_l2: -0.0278
      Episode_Reward/ang_vel_xy_l2: -0.0952
     Episode_Reward/dof_torques_l2: -0.0579
         Episode_Reward/dof_acc_l2: -0.0634
     Episode_Reward/action_rate_l2: -0.1146
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3035
Metrics/base_velocity/error_vel_xy: 0.9267
Metrics/base_velocity/error_vel_yaw: 0.4930
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 4932000
                    Iteration time: 1.09s
                      Time elapsed: 00:02:30
                               ETA: 00:24:58

################################################################################
                     [1m Learning iteration 137/1500 [0m                      

                       Computation: 33071 steps/s (collection: 1.038s, learning 0.050s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0194
               Mean surrogate loss: -0.0073
                 Mean entropy loss: 13.1871
                       Mean reward: 8.63
               Mean episode length: 755.03
Episode_Reward/track_lin_vel_xy_exp: 0.3413
Episode_Reward/track_ang_vel_z_exp: 0.3860
       Episode_Reward/lin_vel_z_l2: -0.0245
      Episode_Reward/ang_vel_xy_l2: -0.0851
     Episode_Reward/dof_torques_l2: -0.0512
         Episode_Reward/dof_acc_l2: -0.0571
     Episode_Reward/action_rate_l2: -0.1020
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3025
Metrics/base_velocity/error_vel_xy: 0.8959
Metrics/base_velocity/error_vel_yaw: 0.4062
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 4968000
                    Iteration time: 1.09s
                      Time elapsed: 00:02:31
                               ETA: 00:24:57

################################################################################
                     [1m Learning iteration 138/1500 [0m                      

                       Computation: 32873 steps/s (collection: 1.041s, learning 0.054s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0178
               Mean surrogate loss: -0.0085
                 Mean entropy loss: 13.1868
                       Mean reward: 7.19
               Mean episode length: 703.53
Episode_Reward/track_lin_vel_xy_exp: 0.2325
Episode_Reward/track_ang_vel_z_exp: 0.3170
       Episode_Reward/lin_vel_z_l2: -0.0249
      Episode_Reward/ang_vel_xy_l2: -0.0740
     Episode_Reward/dof_torques_l2: -0.0429
         Episode_Reward/dof_acc_l2: -0.0549
     Episode_Reward/action_rate_l2: -0.0871
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.2995
Metrics/base_velocity/error_vel_xy: 0.8270
Metrics/base_velocity/error_vel_yaw: 0.3764
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 5004000
                    Iteration time: 1.10s
                      Time elapsed: 00:02:32
                               ETA: 00:24:55

################################################################################
                     [1m Learning iteration 139/1500 [0m                      

                       Computation: 32909 steps/s (collection: 1.041s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0127
               Mean surrogate loss: -0.0073
                 Mean entropy loss: 13.1755
                       Mean reward: 6.93
               Mean episode length: 736.32
Episode_Reward/track_lin_vel_xy_exp: 0.3364
Episode_Reward/track_ang_vel_z_exp: 0.4266
       Episode_Reward/lin_vel_z_l2: -0.0313
      Episode_Reward/ang_vel_xy_l2: -0.1005
     Episode_Reward/dof_torques_l2: -0.0559
         Episode_Reward/dof_acc_l2: -0.0767
     Episode_Reward/action_rate_l2: -0.1186
      Episode_Reward/feet_air_time: -0.0018
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.2963
Metrics/base_velocity/error_vel_xy: 1.0685
Metrics/base_velocity/error_vel_yaw: 0.4987
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 5040000
                    Iteration time: 1.09s
                      Time elapsed: 00:02:33
                               ETA: 00:24:54

################################################################################
                     [1m Learning iteration 140/1500 [0m                      

                       Computation: 32821 steps/s (collection: 1.045s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0097
               Mean surrogate loss: -0.0071
                 Mean entropy loss: 13.1541
                       Mean reward: 6.34
               Mean episode length: 724.36
Episode_Reward/track_lin_vel_xy_exp: 0.2643
Episode_Reward/track_ang_vel_z_exp: 0.3665
       Episode_Reward/lin_vel_z_l2: -0.0298
      Episode_Reward/ang_vel_xy_l2: -0.0869
     Episode_Reward/dof_torques_l2: -0.0475
         Episode_Reward/dof_acc_l2: -0.0722
     Episode_Reward/action_rate_l2: -0.1009
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.2961
Metrics/base_velocity/error_vel_xy: 0.9492
Metrics/base_velocity/error_vel_yaw: 0.4248
      Episode_Termination/time_out: 1.0417
  Episode_Termination/base_contact: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 5076000
                    Iteration time: 1.10s
                      Time elapsed: 00:02:34
                               ETA: 00:24:53

################################################################################
                     [1m Learning iteration 141/1500 [0m                      

                       Computation: 32950 steps/s (collection: 1.041s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0140
               Mean surrogate loss: -0.0071
                 Mean entropy loss: 13.1490
                       Mean reward: 6.59
               Mean episode length: 688.87
Episode_Reward/track_lin_vel_xy_exp: 0.2941
Episode_Reward/track_ang_vel_z_exp: 0.3346
       Episode_Reward/lin_vel_z_l2: -0.0264
      Episode_Reward/ang_vel_xy_l2: -0.0800
     Episode_Reward/dof_torques_l2: -0.0449
         Episode_Reward/dof_acc_l2: -0.0604
     Episode_Reward/action_rate_l2: -0.0926
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.2979
Metrics/base_velocity/error_vel_xy: 0.8052
Metrics/base_velocity/error_vel_yaw: 0.3872
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 5112000
                    Iteration time: 1.09s
                      Time elapsed: 00:02:35
                               ETA: 00:24:52

################################################################################
                     [1m Learning iteration 142/1500 [0m                      

                       Computation: 32825 steps/s (collection: 1.044s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0156
               Mean surrogate loss: -0.0073
                 Mean entropy loss: 13.1507
                       Mean reward: 8.53
               Mean episode length: 733.83
Episode_Reward/track_lin_vel_xy_exp: 0.4005
Episode_Reward/track_ang_vel_z_exp: 0.3873
       Episode_Reward/lin_vel_z_l2: -0.0286
      Episode_Reward/ang_vel_xy_l2: -0.0913
     Episode_Reward/dof_torques_l2: -0.0522
         Episode_Reward/dof_acc_l2: -0.0684
     Episode_Reward/action_rate_l2: -0.1071
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.2931
Metrics/base_velocity/error_vel_xy: 0.8480
Metrics/base_velocity/error_vel_yaw: 0.4536
      Episode_Termination/time_out: 2.0833
  Episode_Termination/base_contact: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 5148000
                    Iteration time: 1.10s
                      Time elapsed: 00:02:37
                               ETA: 00:24:51

################################################################################
                     [1m Learning iteration 143/1500 [0m                      

                       Computation: 33168 steps/s (collection: 1.034s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0161
               Mean surrogate loss: -0.0068
                 Mean entropy loss: 13.1312
                       Mean reward: 8.18
               Mean episode length: 760.31
Episode_Reward/track_lin_vel_xy_exp: 0.2829
Episode_Reward/track_ang_vel_z_exp: 0.3887
       Episode_Reward/lin_vel_z_l2: -0.0262
      Episode_Reward/ang_vel_xy_l2: -0.0879
     Episode_Reward/dof_torques_l2: -0.0539
         Episode_Reward/dof_acc_l2: -0.0581
     Episode_Reward/action_rate_l2: -0.1046
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.2907
Metrics/base_velocity/error_vel_xy: 1.0011
Metrics/base_velocity/error_vel_yaw: 0.4369
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 5184000
                    Iteration time: 1.09s
                      Time elapsed: 00:02:38
                               ETA: 00:24:50

################################################################################
                     [1m Learning iteration 144/1500 [0m                      

                       Computation: 32747 steps/s (collection: 1.050s, learning 0.049s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0122
               Mean surrogate loss: -0.0085
                 Mean entropy loss: 13.1135
                       Mean reward: 7.90
               Mean episode length: 780.14
Episode_Reward/track_lin_vel_xy_exp: 0.3824
Episode_Reward/track_ang_vel_z_exp: 0.4207
       Episode_Reward/lin_vel_z_l2: -0.0304
      Episode_Reward/ang_vel_xy_l2: -0.1007
     Episode_Reward/dof_torques_l2: -0.0573
         Episode_Reward/dof_acc_l2: -0.0763
     Episode_Reward/action_rate_l2: -0.1171
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.2866
Metrics/base_velocity/error_vel_xy: 1.0030
Metrics/base_velocity/error_vel_yaw: 0.5149
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 5220000
                    Iteration time: 1.10s
                      Time elapsed: 00:02:39
                               ETA: 00:24:49

################################################################################
                     [1m Learning iteration 145/1500 [0m                      

                       Computation: 33057 steps/s (collection: 1.037s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0155
               Mean surrogate loss: -0.0076
                 Mean entropy loss: 13.1022
                       Mean reward: 8.58
               Mean episode length: 769.00
Episode_Reward/track_lin_vel_xy_exp: 0.3651
Episode_Reward/track_ang_vel_z_exp: 0.3743
       Episode_Reward/lin_vel_z_l2: -0.0276
      Episode_Reward/ang_vel_xy_l2: -0.0869
     Episode_Reward/dof_torques_l2: -0.0521
         Episode_Reward/dof_acc_l2: -0.0650
     Episode_Reward/action_rate_l2: -0.1038
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.2821
Metrics/base_velocity/error_vel_xy: 0.8822
Metrics/base_velocity/error_vel_yaw: 0.4483
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 5256000
                    Iteration time: 1.09s
                      Time elapsed: 00:02:40
                               ETA: 00:24:47

################################################################################
                     [1m Learning iteration 146/1500 [0m                      

                       Computation: 33168 steps/s (collection: 1.032s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0131
               Mean surrogate loss: -0.0072
                 Mean entropy loss: 13.1045
                       Mean reward: 8.11
               Mean episode length: 750.47
Episode_Reward/track_lin_vel_xy_exp: 0.3560
Episode_Reward/track_ang_vel_z_exp: 0.3850
       Episode_Reward/lin_vel_z_l2: -0.0281
      Episode_Reward/ang_vel_xy_l2: -0.0913
     Episode_Reward/dof_torques_l2: -0.0531
         Episode_Reward/dof_acc_l2: -0.0677
     Episode_Reward/action_rate_l2: -0.1091
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.2805
Metrics/base_velocity/error_vel_xy: 0.9491
Metrics/base_velocity/error_vel_yaw: 0.4874
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 5292000
                    Iteration time: 1.09s
                      Time elapsed: 00:02:41
                               ETA: 00:24:46

################################################################################
                     [1m Learning iteration 147/1500 [0m                      

                       Computation: 33222 steps/s (collection: 1.031s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0165
               Mean surrogate loss: -0.0073
                 Mean entropy loss: 13.1117
                       Mean reward: 9.35
               Mean episode length: 782.92
Episode_Reward/track_lin_vel_xy_exp: 0.5119
Episode_Reward/track_ang_vel_z_exp: 0.4427
       Episode_Reward/lin_vel_z_l2: -0.0306
      Episode_Reward/ang_vel_xy_l2: -0.1021
     Episode_Reward/dof_torques_l2: -0.0591
         Episode_Reward/dof_acc_l2: -0.0760
     Episode_Reward/action_rate_l2: -0.1211
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.2796
Metrics/base_velocity/error_vel_xy: 0.8920
Metrics/base_velocity/error_vel_yaw: 0.4885
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 5328000
                    Iteration time: 1.08s
                      Time elapsed: 00:02:42
                               ETA: 00:24:45

################################################################################
                     [1m Learning iteration 148/1500 [0m                      

                       Computation: 33176 steps/s (collection: 1.032s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0192
               Mean surrogate loss: -0.0070
                 Mean entropy loss: 13.1100
                       Mean reward: 9.46
               Mean episode length: 781.13
Episode_Reward/track_lin_vel_xy_exp: 0.3831
Episode_Reward/track_ang_vel_z_exp: 0.3952
       Episode_Reward/lin_vel_z_l2: -0.0280
      Episode_Reward/ang_vel_xy_l2: -0.0882
     Episode_Reward/dof_torques_l2: -0.0564
         Episode_Reward/dof_acc_l2: -0.0649
     Episode_Reward/action_rate_l2: -0.1091
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.2822
Metrics/base_velocity/error_vel_xy: 0.9044
Metrics/base_velocity/error_vel_yaw: 0.4596
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 5364000
                    Iteration time: 1.09s
                      Time elapsed: 00:02:43
                               ETA: 00:24:44

################################################################################
                     [1m Learning iteration 149/1500 [0m                      

                       Computation: 32282 steps/s (collection: 1.063s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0145
               Mean surrogate loss: -0.0074
                 Mean entropy loss: 13.1082
                       Mean reward: 8.09
               Mean episode length: 768.81
Episode_Reward/track_lin_vel_xy_exp: 0.2941
Episode_Reward/track_ang_vel_z_exp: 0.4067
       Episode_Reward/lin_vel_z_l2: -0.0287
      Episode_Reward/ang_vel_xy_l2: -0.0928
     Episode_Reward/dof_torques_l2: -0.0555
         Episode_Reward/dof_acc_l2: -0.0717
     Episode_Reward/action_rate_l2: -0.1107
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.2866
Metrics/base_velocity/error_vel_xy: 1.0554
Metrics/base_velocity/error_vel_yaw: 0.4410
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 5400000
                    Iteration time: 1.12s
                      Time elapsed: 00:02:44
                               ETA: 00:24:43

################################################################################
                     [1m Learning iteration 150/1500 [0m                      

                       Computation: 32724 steps/s (collection: 1.047s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0166
               Mean surrogate loss: -0.0071
                 Mean entropy loss: 13.1061
                       Mean reward: 9.38
               Mean episode length: 793.77
Episode_Reward/track_lin_vel_xy_exp: 0.5056
Episode_Reward/track_ang_vel_z_exp: 0.4487
       Episode_Reward/lin_vel_z_l2: -0.0309
      Episode_Reward/ang_vel_xy_l2: -0.1050
     Episode_Reward/dof_torques_l2: -0.0611
         Episode_Reward/dof_acc_l2: -0.0761
     Episode_Reward/action_rate_l2: -0.1238
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.2925
Metrics/base_velocity/error_vel_xy: 0.9393
Metrics/base_velocity/error_vel_yaw: 0.5070
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 5436000
                    Iteration time: 1.10s
                      Time elapsed: 00:02:45
                               ETA: 00:24:42

################################################################################
                     [1m Learning iteration 151/1500 [0m                      

                       Computation: 33478 steps/s (collection: 1.025s, learning 0.050s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0187
               Mean surrogate loss: -0.0072
                 Mean entropy loss: 13.1044
                       Mean reward: 10.55
               Mean episode length: 813.69
Episode_Reward/track_lin_vel_xy_exp: 0.4489
Episode_Reward/track_ang_vel_z_exp: 0.4019
       Episode_Reward/lin_vel_z_l2: -0.0297
      Episode_Reward/ang_vel_xy_l2: -0.0962
     Episode_Reward/dof_torques_l2: -0.0566
         Episode_Reward/dof_acc_l2: -0.0777
     Episode_Reward/action_rate_l2: -0.1139
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.2957
Metrics/base_velocity/error_vel_xy: 0.8662
Metrics/base_velocity/error_vel_yaw: 0.4910
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 5472000
                    Iteration time: 1.08s
                      Time elapsed: 00:02:46
                               ETA: 00:24:40

################################################################################
                     [1m Learning iteration 152/1500 [0m                      

                       Computation: 33693 steps/s (collection: 1.015s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0171
               Mean surrogate loss: -0.0080
                 Mean entropy loss: 13.1321
                       Mean reward: 8.39
               Mean episode length: 748.74
Episode_Reward/track_lin_vel_xy_exp: 0.3097
Episode_Reward/track_ang_vel_z_exp: 0.3595
       Episode_Reward/lin_vel_z_l2: -0.0261
      Episode_Reward/ang_vel_xy_l2: -0.0824
     Episode_Reward/dof_torques_l2: -0.0478
         Episode_Reward/dof_acc_l2: -0.0636
     Episode_Reward/action_rate_l2: -0.0994
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.2961
Metrics/base_velocity/error_vel_xy: 0.8671
Metrics/base_velocity/error_vel_yaw: 0.4112
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 5508000
                    Iteration time: 1.07s
                      Time elapsed: 00:02:47
                               ETA: 00:24:39

################################################################################
                     [1m Learning iteration 153/1500 [0m                      

                       Computation: 33619 steps/s (collection: 1.018s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0194
               Mean surrogate loss: -0.0076
                 Mean entropy loss: 13.1374
                       Mean reward: 7.51
               Mean episode length: 728.05
Episode_Reward/track_lin_vel_xy_exp: 0.3528
Episode_Reward/track_ang_vel_z_exp: 0.3868
       Episode_Reward/lin_vel_z_l2: -0.0289
      Episode_Reward/ang_vel_xy_l2: -0.0904
     Episode_Reward/dof_torques_l2: -0.0560
         Episode_Reward/dof_acc_l2: -0.0728
     Episode_Reward/action_rate_l2: -0.1083
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.2936
Metrics/base_velocity/error_vel_xy: 0.9348
Metrics/base_velocity/error_vel_yaw: 0.4771
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 5544000
                    Iteration time: 1.07s
                      Time elapsed: 00:02:49
                               ETA: 00:24:38

################################################################################
                     [1m Learning iteration 154/1500 [0m                      

                       Computation: 33703 steps/s (collection: 1.015s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0187
               Mean surrogate loss: -0.0070
                 Mean entropy loss: 13.1387
                       Mean reward: 8.66
               Mean episode length: 786.45
Episode_Reward/track_lin_vel_xy_exp: 0.3385
Episode_Reward/track_ang_vel_z_exp: 0.3785
       Episode_Reward/lin_vel_z_l2: -0.0286
      Episode_Reward/ang_vel_xy_l2: -0.0890
     Episode_Reward/dof_torques_l2: -0.0517
         Episode_Reward/dof_acc_l2: -0.0716
     Episode_Reward/action_rate_l2: -0.1055
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.2982
Metrics/base_velocity/error_vel_xy: 0.8932
Metrics/base_velocity/error_vel_yaw: 0.4377
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5580000
                    Iteration time: 1.07s
                      Time elapsed: 00:02:50
                               ETA: 00:24:36

################################################################################
                     [1m Learning iteration 155/1500 [0m                      

                       Computation: 32903 steps/s (collection: 1.042s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0163
               Mean surrogate loss: -0.0076
                 Mean entropy loss: 13.1511
                       Mean reward: 9.26
               Mean episode length: 795.78
Episode_Reward/track_lin_vel_xy_exp: 0.4128
Episode_Reward/track_ang_vel_z_exp: 0.4336
       Episode_Reward/lin_vel_z_l2: -0.0308
      Episode_Reward/ang_vel_xy_l2: -0.1012
     Episode_Reward/dof_torques_l2: -0.0583
         Episode_Reward/dof_acc_l2: -0.0798
     Episode_Reward/action_rate_l2: -0.1189
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3045
Metrics/base_velocity/error_vel_xy: 1.0158
Metrics/base_velocity/error_vel_yaw: 0.4719
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 5616000
                    Iteration time: 1.09s
                      Time elapsed: 00:02:51
                               ETA: 00:24:35

################################################################################
                     [1m Learning iteration 156/1500 [0m                      

                       Computation: 33148 steps/s (collection: 1.033s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0111
               Mean surrogate loss: -0.0078
                 Mean entropy loss: 13.1555
                       Mean reward: 9.04
               Mean episode length: 800.59
Episode_Reward/track_lin_vel_xy_exp: 0.3633
Episode_Reward/track_ang_vel_z_exp: 0.3874
       Episode_Reward/lin_vel_z_l2: -0.0278
      Episode_Reward/ang_vel_xy_l2: -0.0903
     Episode_Reward/dof_torques_l2: -0.0550
         Episode_Reward/dof_acc_l2: -0.0689
     Episode_Reward/action_rate_l2: -0.1092
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3044
Metrics/base_velocity/error_vel_xy: 0.9426
Metrics/base_velocity/error_vel_yaw: 0.4620
      Episode_Termination/time_out: 1.0833
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 5652000
                    Iteration time: 1.09s
                      Time elapsed: 00:02:52
                               ETA: 00:24:34

################################################################################
                     [1m Learning iteration 157/1500 [0m                      

                       Computation: 33162 steps/s (collection: 1.032s, learning 0.054s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0078
                 Mean entropy loss: 13.1546
                       Mean reward: 9.14
               Mean episode length: 790.53
Episode_Reward/track_lin_vel_xy_exp: 0.4003
Episode_Reward/track_ang_vel_z_exp: 0.4391
       Episode_Reward/lin_vel_z_l2: -0.0325
      Episode_Reward/ang_vel_xy_l2: -0.1027
     Episode_Reward/dof_torques_l2: -0.0643
         Episode_Reward/dof_acc_l2: -0.0802
     Episode_Reward/action_rate_l2: -0.1236
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3078
Metrics/base_velocity/error_vel_xy: 1.0485
Metrics/base_velocity/error_vel_yaw: 0.5222
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 5688000
                    Iteration time: 1.09s
                      Time elapsed: 00:02:53
                               ETA: 00:24:33

################################################################################
                     [1m Learning iteration 158/1500 [0m                      

                       Computation: 32837 steps/s (collection: 1.045s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0086
                 Mean entropy loss: 13.1496
                       Mean reward: 8.88
               Mean episode length: 765.92
Episode_Reward/track_lin_vel_xy_exp: 0.4081
Episode_Reward/track_ang_vel_z_exp: 0.3951
       Episode_Reward/lin_vel_z_l2: -0.0298
      Episode_Reward/ang_vel_xy_l2: -0.0929
     Episode_Reward/dof_torques_l2: -0.0527
         Episode_Reward/dof_acc_l2: -0.0701
     Episode_Reward/action_rate_l2: -0.1074
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3071
Metrics/base_velocity/error_vel_xy: 0.8528
Metrics/base_velocity/error_vel_yaw: 0.4309
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 5724000
                    Iteration time: 1.10s
                      Time elapsed: 00:02:54
                               ETA: 00:24:32

################################################################################
                     [1m Learning iteration 159/1500 [0m                      

                       Computation: 33215 steps/s (collection: 1.026s, learning 0.057s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0166
               Mean surrogate loss: -0.0079
                 Mean entropy loss: 13.1377
                       Mean reward: 8.48
               Mean episode length: 755.78
Episode_Reward/track_lin_vel_xy_exp: 0.4254
Episode_Reward/track_ang_vel_z_exp: 0.4338
       Episode_Reward/lin_vel_z_l2: -0.0332
      Episode_Reward/ang_vel_xy_l2: -0.1048
     Episode_Reward/dof_torques_l2: -0.0583
         Episode_Reward/dof_acc_l2: -0.0816
     Episode_Reward/action_rate_l2: -0.1215
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3099
Metrics/base_velocity/error_vel_xy: 0.9981
Metrics/base_velocity/error_vel_yaw: 0.5020
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 5760000
                    Iteration time: 1.08s
                      Time elapsed: 00:02:55
                               ETA: 00:24:31

################################################################################
                     [1m Learning iteration 160/1500 [0m                      

                       Computation: 31856 steps/s (collection: 1.078s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0166
               Mean surrogate loss: -0.0085
                 Mean entropy loss: 13.1321
                       Mean reward: 9.39
               Mean episode length: 816.67
Episode_Reward/track_lin_vel_xy_exp: 0.4502
Episode_Reward/track_ang_vel_z_exp: 0.4299
       Episode_Reward/lin_vel_z_l2: -0.0309
      Episode_Reward/ang_vel_xy_l2: -0.1001
     Episode_Reward/dof_torques_l2: -0.0612
         Episode_Reward/dof_acc_l2: -0.0780
     Episode_Reward/action_rate_l2: -0.1196
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3115
Metrics/base_velocity/error_vel_xy: 0.9311
Metrics/base_velocity/error_vel_yaw: 0.4893
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 5796000
                    Iteration time: 1.13s
                      Time elapsed: 00:02:56
                               ETA: 00:24:30

################################################################################
                     [1m Learning iteration 161/1500 [0m                      

                       Computation: 32486 steps/s (collection: 1.057s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0167
               Mean surrogate loss: -0.0072
                 Mean entropy loss: 13.1286
                       Mean reward: 8.80
               Mean episode length: 787.40
Episode_Reward/track_lin_vel_xy_exp: 0.3698
Episode_Reward/track_ang_vel_z_exp: 0.3871
       Episode_Reward/lin_vel_z_l2: -0.0310
      Episode_Reward/ang_vel_xy_l2: -0.0939
     Episode_Reward/dof_torques_l2: -0.0537
         Episode_Reward/dof_acc_l2: -0.0778
     Episode_Reward/action_rate_l2: -0.1101
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3171
Metrics/base_velocity/error_vel_xy: 0.8979
Metrics/base_velocity/error_vel_yaw: 0.4582
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 5832000
                    Iteration time: 1.11s
                      Time elapsed: 00:02:57
                               ETA: 00:24:29

################################################################################
                     [1m Learning iteration 162/1500 [0m                      

                       Computation: 31847 steps/s (collection: 1.078s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0164
               Mean surrogate loss: -0.0096
                 Mean entropy loss: 13.1224
                       Mean reward: 8.08
               Mean episode length: 767.69
Episode_Reward/track_lin_vel_xy_exp: 0.3224
Episode_Reward/track_ang_vel_z_exp: 0.4056
       Episode_Reward/lin_vel_z_l2: -0.0271
      Episode_Reward/ang_vel_xy_l2: -0.0940
     Episode_Reward/dof_torques_l2: -0.0523
         Episode_Reward/dof_acc_l2: -0.0730
     Episode_Reward/action_rate_l2: -0.1120
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3179
Metrics/base_velocity/error_vel_xy: 1.0144
Metrics/base_velocity/error_vel_yaw: 0.4556
      Episode_Termination/time_out: 1.1667
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 5868000
                    Iteration time: 1.13s
                      Time elapsed: 00:02:58
                               ETA: 00:24:28

################################################################################
                     [1m Learning iteration 163/1500 [0m                      

                       Computation: 32506 steps/s (collection: 1.056s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0178
               Mean surrogate loss: -0.0075
                 Mean entropy loss: 13.1119
                       Mean reward: 7.70
               Mean episode length: 722.76
Episode_Reward/track_lin_vel_xy_exp: 0.4116
Episode_Reward/track_ang_vel_z_exp: 0.3874
       Episode_Reward/lin_vel_z_l2: -0.0283
      Episode_Reward/ang_vel_xy_l2: -0.0932
     Episode_Reward/dof_torques_l2: -0.0529
         Episode_Reward/dof_acc_l2: -0.0743
     Episode_Reward/action_rate_l2: -0.1087
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3187
Metrics/base_velocity/error_vel_xy: 0.8520
Metrics/base_velocity/error_vel_yaw: 0.4425
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 5904000
                    Iteration time: 1.11s
                      Time elapsed: 00:03:00
                               ETA: 00:24:27

################################################################################
                     [1m Learning iteration 164/1500 [0m                      

                       Computation: 32460 steps/s (collection: 1.057s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0135
               Mean surrogate loss: -0.0074
                 Mean entropy loss: 13.1139
                       Mean reward: 7.84
               Mean episode length: 716.29
Episode_Reward/track_lin_vel_xy_exp: 0.3591
Episode_Reward/track_ang_vel_z_exp: 0.4113
       Episode_Reward/lin_vel_z_l2: -0.0286
      Episode_Reward/ang_vel_xy_l2: -0.0939
     Episode_Reward/dof_torques_l2: -0.0515
         Episode_Reward/dof_acc_l2: -0.0782
     Episode_Reward/action_rate_l2: -0.1121
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3156
Metrics/base_velocity/error_vel_xy: 1.0198
Metrics/base_velocity/error_vel_yaw: 0.4489
      Episode_Termination/time_out: 0.9583
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 5940000
                    Iteration time: 1.11s
                      Time elapsed: 00:03:01
                               ETA: 00:24:26

################################################################################
                     [1m Learning iteration 165/1500 [0m                      

                       Computation: 32654 steps/s (collection: 1.051s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0069
                 Mean entropy loss: 13.1114
                       Mean reward: 9.01
               Mean episode length: 743.43
Episode_Reward/track_lin_vel_xy_exp: 0.4199
Episode_Reward/track_ang_vel_z_exp: 0.3021
       Episode_Reward/lin_vel_z_l2: -0.0268
      Episode_Reward/ang_vel_xy_l2: -0.0756
     Episode_Reward/dof_torques_l2: -0.0425
         Episode_Reward/dof_acc_l2: -0.0631
     Episode_Reward/action_rate_l2: -0.0881
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3130
Metrics/base_velocity/error_vel_xy: 0.5726
Metrics/base_velocity/error_vel_yaw: 0.3730
      Episode_Termination/time_out: 0.9167
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 5976000
                    Iteration time: 1.10s
                      Time elapsed: 00:03:02
                               ETA: 00:24:25

################################################################################
                     [1m Learning iteration 166/1500 [0m                      

                       Computation: 32165 steps/s (collection: 1.067s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0212
               Mean surrogate loss: -0.0085
                 Mean entropy loss: 13.1112
                       Mean reward: 9.12
               Mean episode length: 755.97
Episode_Reward/track_lin_vel_xy_exp: 0.3652
Episode_Reward/track_ang_vel_z_exp: 0.3811
       Episode_Reward/lin_vel_z_l2: -0.0316
      Episode_Reward/ang_vel_xy_l2: -0.0943
     Episode_Reward/dof_torques_l2: -0.0533
         Episode_Reward/dof_acc_l2: -0.0820
     Episode_Reward/action_rate_l2: -0.1087
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3135
Metrics/base_velocity/error_vel_xy: 0.9104
Metrics/base_velocity/error_vel_yaw: 0.4439
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 6012000
                    Iteration time: 1.12s
                      Time elapsed: 00:03:03
                               ETA: 00:24:24

################################################################################
                     [1m Learning iteration 167/1500 [0m                      

                       Computation: 32617 steps/s (collection: 1.053s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0160
               Mean surrogate loss: -0.0085
                 Mean entropy loss: 13.1164
                       Mean reward: 8.32
               Mean episode length: 752.82
Episode_Reward/track_lin_vel_xy_exp: 0.3820
Episode_Reward/track_ang_vel_z_exp: 0.4073
       Episode_Reward/lin_vel_z_l2: -0.0335
      Episode_Reward/ang_vel_xy_l2: -0.1022
     Episode_Reward/dof_torques_l2: -0.0545
         Episode_Reward/dof_acc_l2: -0.0887
     Episode_Reward/action_rate_l2: -0.1157
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3149
Metrics/base_velocity/error_vel_xy: 0.9640
Metrics/base_velocity/error_vel_yaw: 0.4652
      Episode_Termination/time_out: 0.9583
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 6048000
                    Iteration time: 1.10s
                      Time elapsed: 00:03:04
                               ETA: 00:24:23

################################################################################
                     [1m Learning iteration 168/1500 [0m                      

                       Computation: 33288 steps/s (collection: 1.032s, learning 0.050s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0117
               Mean surrogate loss: -0.0095
                 Mean entropy loss: 13.1228
                       Mean reward: 8.02
               Mean episode length: 790.13
Episode_Reward/track_lin_vel_xy_exp: 0.3571
Episode_Reward/track_ang_vel_z_exp: 0.4023
       Episode_Reward/lin_vel_z_l2: -0.0312
      Episode_Reward/ang_vel_xy_l2: -0.0972
     Episode_Reward/dof_torques_l2: -0.0558
         Episode_Reward/dof_acc_l2: -0.0831
     Episode_Reward/action_rate_l2: -0.1142
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3184
Metrics/base_velocity/error_vel_xy: 0.9422
Metrics/base_velocity/error_vel_yaw: 0.4631
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 6084000
                    Iteration time: 1.08s
                      Time elapsed: 00:03:05
                               ETA: 00:24:22

################################################################################
                     [1m Learning iteration 169/1500 [0m                      

                       Computation: 32510 steps/s (collection: 1.055s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0174
               Mean surrogate loss: -0.0081
                 Mean entropy loss: 13.1179
                       Mean reward: 8.61
               Mean episode length: 807.15
Episode_Reward/track_lin_vel_xy_exp: 0.4557
Episode_Reward/track_ang_vel_z_exp: 0.4157
       Episode_Reward/lin_vel_z_l2: -0.0299
      Episode_Reward/ang_vel_xy_l2: -0.0998
     Episode_Reward/dof_torques_l2: -0.0553
         Episode_Reward/dof_acc_l2: -0.0747
     Episode_Reward/action_rate_l2: -0.1170
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3194
Metrics/base_velocity/error_vel_xy: 0.9058
Metrics/base_velocity/error_vel_yaw: 0.4788
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 6120000
                    Iteration time: 1.11s
                      Time elapsed: 00:03:06
                               ETA: 00:24:21

################################################################################
                     [1m Learning iteration 170/1500 [0m                      

                       Computation: 32869 steps/s (collection: 1.042s, learning 0.054s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0207
               Mean surrogate loss: -0.0078
                 Mean entropy loss: 13.1103
                       Mean reward: 9.22
               Mean episode length: 794.14
Episode_Reward/track_lin_vel_xy_exp: 0.4547
Episode_Reward/track_ang_vel_z_exp: 0.4029
       Episode_Reward/lin_vel_z_l2: -0.0309
      Episode_Reward/ang_vel_xy_l2: -0.0974
     Episode_Reward/dof_torques_l2: -0.0558
         Episode_Reward/dof_acc_l2: -0.0807
     Episode_Reward/action_rate_l2: -0.1154
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3209
Metrics/base_velocity/error_vel_xy: 0.8875
Metrics/base_velocity/error_vel_yaw: 0.4742
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 6156000
                    Iteration time: 1.10s
                      Time elapsed: 00:03:07
                               ETA: 00:24:20

################################################################################
                     [1m Learning iteration 171/1500 [0m                      

                       Computation: 32743 steps/s (collection: 1.047s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0210
               Mean surrogate loss: -0.0079
                 Mean entropy loss: 13.1117
                       Mean reward: 9.42
               Mean episode length: 782.88
Episode_Reward/track_lin_vel_xy_exp: 0.3858
Episode_Reward/track_ang_vel_z_exp: 0.3958
       Episode_Reward/lin_vel_z_l2: -0.0302
      Episode_Reward/ang_vel_xy_l2: -0.0942
     Episode_Reward/dof_torques_l2: -0.0517
         Episode_Reward/dof_acc_l2: -0.0828
     Episode_Reward/action_rate_l2: -0.1125
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3226
Metrics/base_velocity/error_vel_xy: 0.9026
Metrics/base_velocity/error_vel_yaw: 0.4397
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 6192000
                    Iteration time: 1.10s
                      Time elapsed: 00:03:08
                               ETA: 00:24:18

################################################################################
                     [1m Learning iteration 172/1500 [0m                      

                       Computation: 32167 steps/s (collection: 1.067s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0242
               Mean surrogate loss: -0.0067
                 Mean entropy loss: 13.1282
                       Mean reward: 9.14
               Mean episode length: 752.43
Episode_Reward/track_lin_vel_xy_exp: 0.4364
Episode_Reward/track_ang_vel_z_exp: 0.3916
       Episode_Reward/lin_vel_z_l2: -0.0283
      Episode_Reward/ang_vel_xy_l2: -0.0928
     Episode_Reward/dof_torques_l2: -0.0519
         Episode_Reward/dof_acc_l2: -0.0774
     Episode_Reward/action_rate_l2: -0.1100
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3238
Metrics/base_velocity/error_vel_xy: 0.8207
Metrics/base_velocity/error_vel_yaw: 0.4295
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 6228000
                    Iteration time: 1.12s
                      Time elapsed: 00:03:09
                               ETA: 00:24:18

################################################################################
                     [1m Learning iteration 173/1500 [0m                      

                       Computation: 33049 steps/s (collection: 1.037s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0195
               Mean surrogate loss: -0.0082
                 Mean entropy loss: 13.1321
                       Mean reward: 10.18
               Mean episode length: 806.03
Episode_Reward/track_lin_vel_xy_exp: 0.4885
Episode_Reward/track_ang_vel_z_exp: 0.4360
       Episode_Reward/lin_vel_z_l2: -0.0346
      Episode_Reward/ang_vel_xy_l2: -0.1039
     Episode_Reward/dof_torques_l2: -0.0589
         Episode_Reward/dof_acc_l2: -0.0892
     Episode_Reward/action_rate_l2: -0.1226
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3297
Metrics/base_velocity/error_vel_xy: 0.8883
Metrics/base_velocity/error_vel_yaw: 0.4694
      Episode_Termination/time_out: 2.0417
  Episode_Termination/base_contact: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 6264000
                    Iteration time: 1.09s
                      Time elapsed: 00:03:11
                               ETA: 00:24:16

################################################################################
                     [1m Learning iteration 174/1500 [0m                      

                       Computation: 32907 steps/s (collection: 1.042s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0161
               Mean surrogate loss: -0.0096
                 Mean entropy loss: 13.1229
                       Mean reward: 9.70
               Mean episode length: 808.36
Episode_Reward/track_lin_vel_xy_exp: 0.4377
Episode_Reward/track_ang_vel_z_exp: 0.4373
       Episode_Reward/lin_vel_z_l2: -0.0299
      Episode_Reward/ang_vel_xy_l2: -0.0990
     Episode_Reward/dof_torques_l2: -0.0591
         Episode_Reward/dof_acc_l2: -0.0814
     Episode_Reward/action_rate_l2: -0.1220
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3372
Metrics/base_velocity/error_vel_xy: 0.9714
Metrics/base_velocity/error_vel_yaw: 0.4811
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 6300000
                    Iteration time: 1.09s
                      Time elapsed: 00:03:12
                               ETA: 00:24:15

################################################################################
                     [1m Learning iteration 175/1500 [0m                      

                       Computation: 32615 steps/s (collection: 1.053s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0187
               Mean surrogate loss: -0.0076
                 Mean entropy loss: 13.1199
                       Mean reward: 8.49
               Mean episode length: 754.88
Episode_Reward/track_lin_vel_xy_exp: 0.3615
Episode_Reward/track_ang_vel_z_exp: 0.3398
       Episode_Reward/lin_vel_z_l2: -0.0292
      Episode_Reward/ang_vel_xy_l2: -0.0818
     Episode_Reward/dof_torques_l2: -0.0502
         Episode_Reward/dof_acc_l2: -0.0742
     Episode_Reward/action_rate_l2: -0.0986
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3392
Metrics/base_velocity/error_vel_xy: 0.7657
Metrics/base_velocity/error_vel_yaw: 0.4051
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 6336000
                    Iteration time: 1.10s
                      Time elapsed: 00:03:13
                               ETA: 00:24:14

################################################################################
                     [1m Learning iteration 176/1500 [0m                      

                       Computation: 32462 steps/s (collection: 1.056s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0157
               Mean surrogate loss: -0.0082
                 Mean entropy loss: 13.1194
                       Mean reward: 8.59
               Mean episode length: 714.84
Episode_Reward/track_lin_vel_xy_exp: 0.4708
Episode_Reward/track_ang_vel_z_exp: 0.4147
       Episode_Reward/lin_vel_z_l2: -0.0303
      Episode_Reward/ang_vel_xy_l2: -0.0964
     Episode_Reward/dof_torques_l2: -0.0578
         Episode_Reward/dof_acc_l2: -0.0828
     Episode_Reward/action_rate_l2: -0.1183
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3389
Metrics/base_velocity/error_vel_xy: 0.8925
Metrics/base_velocity/error_vel_yaw: 0.4832
      Episode_Termination/time_out: 1.1667
  Episode_Termination/base_contact: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 6372000
                    Iteration time: 1.11s
                      Time elapsed: 00:03:14
                               ETA: 00:24:13

################################################################################
                     [1m Learning iteration 177/1500 [0m                      

                       Computation: 32540 steps/s (collection: 1.054s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0165
               Mean surrogate loss: -0.0091
                 Mean entropy loss: 13.1204
                       Mean reward: 7.81
               Mean episode length: 690.93
Episode_Reward/track_lin_vel_xy_exp: 0.2700
Episode_Reward/track_ang_vel_z_exp: 0.3481
       Episode_Reward/lin_vel_z_l2: -0.0283
      Episode_Reward/ang_vel_xy_l2: -0.0778
     Episode_Reward/dof_torques_l2: -0.0480
         Episode_Reward/dof_acc_l2: -0.0736
     Episode_Reward/action_rate_l2: -0.0997
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3405
Metrics/base_velocity/error_vel_xy: 0.8993
Metrics/base_velocity/error_vel_yaw: 0.4106
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6408000
                    Iteration time: 1.11s
                      Time elapsed: 00:03:15
                               ETA: 00:24:12

################################################################################
                     [1m Learning iteration 178/1500 [0m                      

                       Computation: 32432 steps/s (collection: 1.058s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0182
               Mean surrogate loss: -0.0078
                 Mean entropy loss: 13.1227
                       Mean reward: 8.37
               Mean episode length: 724.96
Episode_Reward/track_lin_vel_xy_exp: 0.4101
Episode_Reward/track_ang_vel_z_exp: 0.4051
       Episode_Reward/lin_vel_z_l2: -0.0285
      Episode_Reward/ang_vel_xy_l2: -0.0902
     Episode_Reward/dof_torques_l2: -0.0554
         Episode_Reward/dof_acc_l2: -0.0704
     Episode_Reward/action_rate_l2: -0.1118
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3395
Metrics/base_velocity/error_vel_xy: 0.8529
Metrics/base_velocity/error_vel_yaw: 0.4403
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 6444000
                    Iteration time: 1.11s
                      Time elapsed: 00:03:16
                               ETA: 00:24:11

################################################################################
                     [1m Learning iteration 179/1500 [0m                      

                       Computation: 31946 steps/s (collection: 1.077s, learning 0.050s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0169
               Mean surrogate loss: -0.0073
                 Mean entropy loss: 13.1222
                       Mean reward: 8.92
               Mean episode length: 755.72
Episode_Reward/track_lin_vel_xy_exp: 0.3987
Episode_Reward/track_ang_vel_z_exp: 0.3865
       Episode_Reward/lin_vel_z_l2: -0.0286
      Episode_Reward/ang_vel_xy_l2: -0.0879
     Episode_Reward/dof_torques_l2: -0.0537
         Episode_Reward/dof_acc_l2: -0.0805
     Episode_Reward/action_rate_l2: -0.1106
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3374
Metrics/base_velocity/error_vel_xy: 0.8647
Metrics/base_velocity/error_vel_yaw: 0.4470
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 6480000
                    Iteration time: 1.13s
                      Time elapsed: 00:03:17
                               ETA: 00:24:10

################################################################################
                     [1m Learning iteration 180/1500 [0m                      

                       Computation: 32496 steps/s (collection: 1.056s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0120
               Mean surrogate loss: -0.0083
                 Mean entropy loss: 13.1128
                       Mean reward: 9.11
               Mean episode length: 788.95
Episode_Reward/track_lin_vel_xy_exp: 0.5116
Episode_Reward/track_ang_vel_z_exp: 0.4375
       Episode_Reward/lin_vel_z_l2: -0.0343
      Episode_Reward/ang_vel_xy_l2: -0.1035
     Episode_Reward/dof_torques_l2: -0.0640
         Episode_Reward/dof_acc_l2: -0.0925
     Episode_Reward/action_rate_l2: -0.1261
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3418
Metrics/base_velocity/error_vel_xy: 0.9277
Metrics/base_velocity/error_vel_yaw: 0.5329
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 6516000
                    Iteration time: 1.11s
                      Time elapsed: 00:03:18
                               ETA: 00:24:09

################################################################################
                     [1m Learning iteration 181/1500 [0m                      

                       Computation: 32353 steps/s (collection: 1.060s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0189
               Mean surrogate loss: -0.0091
                 Mean entropy loss: 13.0906
                       Mean reward: 9.10
               Mean episode length: 758.75
Episode_Reward/track_lin_vel_xy_exp: 0.4384
Episode_Reward/track_ang_vel_z_exp: 0.3566
       Episode_Reward/lin_vel_z_l2: -0.0301
      Episode_Reward/ang_vel_xy_l2: -0.0866
     Episode_Reward/dof_torques_l2: -0.0498
         Episode_Reward/dof_acc_l2: -0.0799
     Episode_Reward/action_rate_l2: -0.1021
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3455
Metrics/base_velocity/error_vel_xy: 0.6982
Metrics/base_velocity/error_vel_yaw: 0.4015
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 6552000
                    Iteration time: 1.11s
                      Time elapsed: 00:03:19
                               ETA: 00:24:08

################################################################################
                     [1m Learning iteration 182/1500 [0m                      

                       Computation: 31431 steps/s (collection: 1.092s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0151
               Mean surrogate loss: -0.0067
                 Mean entropy loss: 13.0865
                       Mean reward: 8.57
               Mean episode length: 736.38
Episode_Reward/track_lin_vel_xy_exp: 0.4678
Episode_Reward/track_ang_vel_z_exp: 0.4289
       Episode_Reward/lin_vel_z_l2: -0.0345
      Episode_Reward/ang_vel_xy_l2: -0.1056
     Episode_Reward/dof_torques_l2: -0.0584
         Episode_Reward/dof_acc_l2: -0.1034
     Episode_Reward/action_rate_l2: -0.1263
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3509
Metrics/base_velocity/error_vel_xy: 0.9496
Metrics/base_velocity/error_vel_yaw: 0.4989
      Episode_Termination/time_out: 1.1250
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 6588000
                    Iteration time: 1.15s
                      Time elapsed: 00:03:21
                               ETA: 00:24:07

################################################################################
                     [1m Learning iteration 183/1500 [0m                      

                       Computation: 32061 steps/s (collection: 1.068s, learning 0.054s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0211
               Mean surrogate loss: -0.0074
                 Mean entropy loss: 13.0846
                       Mean reward: 9.45
               Mean episode length: 805.93
Episode_Reward/track_lin_vel_xy_exp: 0.4443
Episode_Reward/track_ang_vel_z_exp: 0.3961
       Episode_Reward/lin_vel_z_l2: -0.0305
      Episode_Reward/ang_vel_xy_l2: -0.0941
     Episode_Reward/dof_torques_l2: -0.0541
         Episode_Reward/dof_acc_l2: -0.0934
     Episode_Reward/action_rate_l2: -0.1146
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3572
Metrics/base_velocity/error_vel_xy: 0.8279
Metrics/base_velocity/error_vel_yaw: 0.4581
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 6624000
                    Iteration time: 1.12s
                      Time elapsed: 00:03:22
                               ETA: 00:24:07

################################################################################
                     [1m Learning iteration 184/1500 [0m                      

                       Computation: 32578 steps/s (collection: 1.052s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0152
               Mean surrogate loss: -0.0075
                 Mean entropy loss: 13.0775
                       Mean reward: 9.40
               Mean episode length: 831.88
Episode_Reward/track_lin_vel_xy_exp: 0.4555
Episode_Reward/track_ang_vel_z_exp: 0.4407
       Episode_Reward/lin_vel_z_l2: -0.0320
      Episode_Reward/ang_vel_xy_l2: -0.1066
     Episode_Reward/dof_torques_l2: -0.0658
         Episode_Reward/dof_acc_l2: -0.0911
     Episode_Reward/action_rate_l2: -0.1330
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3602
Metrics/base_velocity/error_vel_xy: 1.1038
Metrics/base_velocity/error_vel_yaw: 0.6209
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 6660000
                    Iteration time: 1.11s
                      Time elapsed: 00:03:23
                               ETA: 00:24:05

################################################################################
                     [1m Learning iteration 185/1500 [0m                      

                       Computation: 32296 steps/s (collection: 1.063s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0163
               Mean surrogate loss: -0.0088
                 Mean entropy loss: 13.0667
                       Mean reward: 10.06
               Mean episode length: 868.55
Episode_Reward/track_lin_vel_xy_exp: 0.5774
Episode_Reward/track_ang_vel_z_exp: 0.4685
       Episode_Reward/lin_vel_z_l2: -0.0337
      Episode_Reward/ang_vel_xy_l2: -0.1083
     Episode_Reward/dof_torques_l2: -0.0682
         Episode_Reward/dof_acc_l2: -0.1010
     Episode_Reward/action_rate_l2: -0.1377
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3621
Metrics/base_velocity/error_vel_xy: 1.0030
Metrics/base_velocity/error_vel_yaw: 0.5788
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 6696000
                    Iteration time: 1.11s
                      Time elapsed: 00:03:24
                               ETA: 00:24:04

################################################################################
                     [1m Learning iteration 186/1500 [0m                      

                       Computation: 31738 steps/s (collection: 1.081s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0182
               Mean surrogate loss: -0.0077
                 Mean entropy loss: 13.0668
                       Mean reward: 10.14
               Mean episode length: 812.80
Episode_Reward/track_lin_vel_xy_exp: 0.4460
Episode_Reward/track_ang_vel_z_exp: 0.3956
       Episode_Reward/lin_vel_z_l2: -0.0318
      Episode_Reward/ang_vel_xy_l2: -0.0957
     Episode_Reward/dof_torques_l2: -0.0535
         Episode_Reward/dof_acc_l2: -0.0825
     Episode_Reward/action_rate_l2: -0.1117
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3663
Metrics/base_velocity/error_vel_xy: 0.8438
Metrics/base_velocity/error_vel_yaw: 0.4455
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 6732000
                    Iteration time: 1.13s
                      Time elapsed: 00:03:25
                               ETA: 00:24:04

################################################################################
                     [1m Learning iteration 187/1500 [0m                      

                       Computation: 30453 steps/s (collection: 1.129s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0205
               Mean surrogate loss: -0.0081
                 Mean entropy loss: 13.0669
                       Mean reward: 10.01
               Mean episode length: 791.66
Episode_Reward/track_lin_vel_xy_exp: 0.4820
Episode_Reward/track_ang_vel_z_exp: 0.4313
       Episode_Reward/lin_vel_z_l2: -0.0332
      Episode_Reward/ang_vel_xy_l2: -0.1012
     Episode_Reward/dof_torques_l2: -0.0599
         Episode_Reward/dof_acc_l2: -0.0932
     Episode_Reward/action_rate_l2: -0.1235
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3733
Metrics/base_velocity/error_vel_xy: 0.8923
Metrics/base_velocity/error_vel_yaw: 0.4681
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 6768000
                    Iteration time: 1.18s
                      Time elapsed: 00:03:26
                               ETA: 00:24:03

################################################################################
                     [1m Learning iteration 188/1500 [0m                      

                       Computation: 33123 steps/s (collection: 1.034s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0123
               Mean surrogate loss: -0.0080
                 Mean entropy loss: 13.0661
                       Mean reward: 10.21
               Mean episode length: 805.67
Episode_Reward/track_lin_vel_xy_exp: 0.5257
Episode_Reward/track_ang_vel_z_exp: 0.4177
       Episode_Reward/lin_vel_z_l2: -0.0301
      Episode_Reward/ang_vel_xy_l2: -0.0966
     Episode_Reward/dof_torques_l2: -0.0556
         Episode_Reward/dof_acc_l2: -0.0882
     Episode_Reward/action_rate_l2: -0.1195
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3798
Metrics/base_velocity/error_vel_xy: 0.8152
Metrics/base_velocity/error_vel_yaw: 0.4761
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6804000
                    Iteration time: 1.09s
                      Time elapsed: 00:03:27
                               ETA: 00:24:02

################################################################################
                     [1m Learning iteration 189/1500 [0m                      

                       Computation: 33408 steps/s (collection: 1.024s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0147
               Mean surrogate loss: -0.0090
                 Mean entropy loss: 13.0514
                       Mean reward: 10.04
               Mean episode length: 781.18
Episode_Reward/track_lin_vel_xy_exp: 0.4217
Episode_Reward/track_ang_vel_z_exp: 0.4217
       Episode_Reward/lin_vel_z_l2: -0.0308
      Episode_Reward/ang_vel_xy_l2: -0.0978
     Episode_Reward/dof_torques_l2: -0.0581
         Episode_Reward/dof_acc_l2: -0.0867
     Episode_Reward/action_rate_l2: -0.1198
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3804
Metrics/base_velocity/error_vel_xy: 0.9403
Metrics/base_velocity/error_vel_yaw: 0.4807
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 6840000
                    Iteration time: 1.08s
                      Time elapsed: 00:03:28
                               ETA: 00:24:01

################################################################################
                     [1m Learning iteration 190/1500 [0m                      

                       Computation: 33435 steps/s (collection: 1.024s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0154
               Mean surrogate loss: -0.0088
                 Mean entropy loss: 13.0457
                       Mean reward: 10.01
               Mean episode length: 785.32
Episode_Reward/track_lin_vel_xy_exp: 0.4863
Episode_Reward/track_ang_vel_z_exp: 0.4297
       Episode_Reward/lin_vel_z_l2: -0.0310
      Episode_Reward/ang_vel_xy_l2: -0.0973
     Episode_Reward/dof_torques_l2: -0.0628
         Episode_Reward/dof_acc_l2: -0.0886
     Episode_Reward/action_rate_l2: -0.1221
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3839
Metrics/base_velocity/error_vel_xy: 0.8926
Metrics/base_velocity/error_vel_yaw: 0.4693
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 6876000
                    Iteration time: 1.08s
                      Time elapsed: 00:03:29
                               ETA: 00:23:59

################################################################################
                     [1m Learning iteration 191/1500 [0m                      

                       Computation: 33154 steps/s (collection: 1.032s, learning 0.054s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0194
               Mean surrogate loss: -0.0093
                 Mean entropy loss: 13.0335
                       Mean reward: 10.83
               Mean episode length: 816.60
Episode_Reward/track_lin_vel_xy_exp: 0.6034
Episode_Reward/track_ang_vel_z_exp: 0.4598
       Episode_Reward/lin_vel_z_l2: -0.0329
      Episode_Reward/ang_vel_xy_l2: -0.1075
     Episode_Reward/dof_torques_l2: -0.0642
         Episode_Reward/dof_acc_l2: -0.0989
     Episode_Reward/action_rate_l2: -0.1325
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3895
Metrics/base_velocity/error_vel_xy: 0.8480
Metrics/base_velocity/error_vel_yaw: 0.5175
      Episode_Termination/time_out: 1.9583
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 6912000
                    Iteration time: 1.09s
                      Time elapsed: 00:03:31
                               ETA: 00:23:58

################################################################################
                     [1m Learning iteration 192/1500 [0m                      

                       Computation: 31955 steps/s (collection: 1.073s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0155
               Mean surrogate loss: -0.0079
                 Mean entropy loss: 13.0532
                       Mean reward: 11.96
               Mean episode length: 873.24
Episode_Reward/track_lin_vel_xy_exp: 0.6216
Episode_Reward/track_ang_vel_z_exp: 0.4848
       Episode_Reward/lin_vel_z_l2: -0.0332
      Episode_Reward/ang_vel_xy_l2: -0.1117
     Episode_Reward/dof_torques_l2: -0.0696
         Episode_Reward/dof_acc_l2: -0.1038
     Episode_Reward/action_rate_l2: -0.1390
      Episode_Reward/feet_air_time: -0.0018
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3918
Metrics/base_velocity/error_vel_xy: 0.9248
Metrics/base_velocity/error_vel_yaw: 0.5410
      Episode_Termination/time_out: 1.9583
  Episode_Termination/base_contact: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 6948000
                    Iteration time: 1.13s
                      Time elapsed: 00:03:32
                               ETA: 00:23:57

################################################################################
                     [1m Learning iteration 193/1500 [0m                      

                       Computation: 31983 steps/s (collection: 1.073s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0174
               Mean surrogate loss: -0.0092
                 Mean entropy loss: 13.0641
                       Mean reward: 11.00
               Mean episode length: 853.34
Episode_Reward/track_lin_vel_xy_exp: 0.4665
Episode_Reward/track_ang_vel_z_exp: 0.4532
       Episode_Reward/lin_vel_z_l2: -0.0324
      Episode_Reward/ang_vel_xy_l2: -0.1078
     Episode_Reward/dof_torques_l2: -0.0607
         Episode_Reward/dof_acc_l2: -0.1044
     Episode_Reward/action_rate_l2: -0.1296
      Episode_Reward/feet_air_time: -0.0018
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.3964
Metrics/base_velocity/error_vel_xy: 1.0108
Metrics/base_velocity/error_vel_yaw: 0.5066
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 6984000
                    Iteration time: 1.13s
                      Time elapsed: 00:03:33
                               ETA: 00:23:56

################################################################################
                     [1m Learning iteration 194/1500 [0m                      

                       Computation: 30608 steps/s (collection: 1.123s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0230
               Mean surrogate loss: -0.0085
                 Mean entropy loss: 13.0557
                       Mean reward: 10.43
               Mean episode length: 827.86
Episode_Reward/track_lin_vel_xy_exp: 0.5679
Episode_Reward/track_ang_vel_z_exp: 0.4382
       Episode_Reward/lin_vel_z_l2: -0.0358
      Episode_Reward/ang_vel_xy_l2: -0.1056
     Episode_Reward/dof_torques_l2: -0.0657
         Episode_Reward/dof_acc_l2: -0.1006
     Episode_Reward/action_rate_l2: -0.1291
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4044
Metrics/base_velocity/error_vel_xy: 0.8573
Metrics/base_velocity/error_vel_yaw: 0.5096
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 7020000
                    Iteration time: 1.18s
                      Time elapsed: 00:03:34
                               ETA: 00:23:56

################################################################################
                     [1m Learning iteration 195/1500 [0m                      

                       Computation: 31995 steps/s (collection: 1.073s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0204
               Mean surrogate loss: -0.0078
                 Mean entropy loss: 13.0602
                       Mean reward: 10.28
               Mean episode length: 776.11
Episode_Reward/track_lin_vel_xy_exp: 0.5564
Episode_Reward/track_ang_vel_z_exp: 0.4140
       Episode_Reward/lin_vel_z_l2: -0.0330
      Episode_Reward/ang_vel_xy_l2: -0.0987
     Episode_Reward/dof_torques_l2: -0.0596
         Episode_Reward/dof_acc_l2: -0.1008
     Episode_Reward/action_rate_l2: -0.1210
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4129
Metrics/base_velocity/error_vel_xy: 0.7580
Metrics/base_velocity/error_vel_yaw: 0.4557
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 7056000
                    Iteration time: 1.13s
                      Time elapsed: 00:03:35
                               ETA: 00:23:55

################################################################################
                     [1m Learning iteration 196/1500 [0m                      

                       Computation: 32458 steps/s (collection: 1.058s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0206
               Mean surrogate loss: -0.0077
                 Mean entropy loss: 13.0697
                       Mean reward: 11.16
               Mean episode length: 791.62
Episode_Reward/track_lin_vel_xy_exp: 0.5893
Episode_Reward/track_ang_vel_z_exp: 0.4434
       Episode_Reward/lin_vel_z_l2: -0.0345
      Episode_Reward/ang_vel_xy_l2: -0.1100
     Episode_Reward/dof_torques_l2: -0.0621
         Episode_Reward/dof_acc_l2: -0.1077
     Episode_Reward/action_rate_l2: -0.1288
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4201
Metrics/base_velocity/error_vel_xy: 0.8026
Metrics/base_velocity/error_vel_yaw: 0.4825
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 7092000
                    Iteration time: 1.11s
                      Time elapsed: 00:03:36
                               ETA: 00:23:54

################################################################################
                     [1m Learning iteration 197/1500 [0m                      

                       Computation: 31777 steps/s (collection: 1.080s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0153
               Mean surrogate loss: -0.0095
                 Mean entropy loss: 13.0707
                       Mean reward: 11.21
               Mean episode length: 812.46
Episode_Reward/track_lin_vel_xy_exp: 0.5612
Episode_Reward/track_ang_vel_z_exp: 0.4288
       Episode_Reward/lin_vel_z_l2: -0.0352
      Episode_Reward/ang_vel_xy_l2: -0.1076
     Episode_Reward/dof_torques_l2: -0.0617
         Episode_Reward/dof_acc_l2: -0.1108
     Episode_Reward/action_rate_l2: -0.1284
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4278
Metrics/base_velocity/error_vel_xy: 0.8040
Metrics/base_velocity/error_vel_yaw: 0.5004
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 7128000
                    Iteration time: 1.13s
                      Time elapsed: 00:03:37
                               ETA: 00:23:53

################################################################################
                     [1m Learning iteration 198/1500 [0m                      

                       Computation: 32417 steps/s (collection: 1.057s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0086
                 Mean entropy loss: 13.0555
                       Mean reward: 9.81
               Mean episode length: 785.15
Episode_Reward/track_lin_vel_xy_exp: 0.4013
Episode_Reward/track_ang_vel_z_exp: 0.4131
       Episode_Reward/lin_vel_z_l2: -0.0306
      Episode_Reward/ang_vel_xy_l2: -0.0969
     Episode_Reward/dof_torques_l2: -0.0614
         Episode_Reward/dof_acc_l2: -0.0946
     Episode_Reward/action_rate_l2: -0.1211
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4292
Metrics/base_velocity/error_vel_xy: 0.9663
Metrics/base_velocity/error_vel_yaw: 0.4929
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 7164000
                    Iteration time: 1.11s
                      Time elapsed: 00:03:38
                               ETA: 00:23:52

################################################################################
                     [1m Learning iteration 199/1500 [0m                      

                       Computation: 32257 steps/s (collection: 1.063s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0196
               Mean surrogate loss: -0.0077
                 Mean entropy loss: 13.0421
                       Mean reward: 10.57
               Mean episode length: 777.96
Episode_Reward/track_lin_vel_xy_exp: 0.5629
Episode_Reward/track_ang_vel_z_exp: 0.4009
       Episode_Reward/lin_vel_z_l2: -0.0343
      Episode_Reward/ang_vel_xy_l2: -0.0981
     Episode_Reward/dof_torques_l2: -0.0551
         Episode_Reward/dof_acc_l2: -0.0983
     Episode_Reward/action_rate_l2: -0.1157
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4349
Metrics/base_velocity/error_vel_xy: 0.6937
Metrics/base_velocity/error_vel_yaw: 0.4272
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 7200000
                    Iteration time: 1.12s
                      Time elapsed: 00:03:40
                               ETA: 00:23:51

################################################################################
                     [1m Learning iteration 200/1500 [0m                      

                       Computation: 33288 steps/s (collection: 1.029s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0225
               Mean surrogate loss: -0.0076
                 Mean entropy loss: 13.0495
                       Mean reward: 11.34
               Mean episode length: 802.36
Episode_Reward/track_lin_vel_xy_exp: 0.4333
Episode_Reward/track_ang_vel_z_exp: 0.3903
       Episode_Reward/lin_vel_z_l2: -0.0299
      Episode_Reward/ang_vel_xy_l2: -0.0902
     Episode_Reward/dof_torques_l2: -0.0545
         Episode_Reward/dof_acc_l2: -0.0916
     Episode_Reward/action_rate_l2: -0.1120
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4379
Metrics/base_velocity/error_vel_xy: 0.8296
Metrics/base_velocity/error_vel_yaw: 0.4372
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 7236000
                    Iteration time: 1.08s
                      Time elapsed: 00:03:41
                               ETA: 00:23:50

################################################################################
                     [1m Learning iteration 201/1500 [0m                      

                       Computation: 32535 steps/s (collection: 1.054s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0189
               Mean surrogate loss: -0.0095
                 Mean entropy loss: 13.0379
                       Mean reward: 10.02
               Mean episode length: 795.99
Episode_Reward/track_lin_vel_xy_exp: 0.4730
Episode_Reward/track_ang_vel_z_exp: 0.3937
       Episode_Reward/lin_vel_z_l2: -0.0343
      Episode_Reward/ang_vel_xy_l2: -0.0978
     Episode_Reward/dof_torques_l2: -0.0587
         Episode_Reward/dof_acc_l2: -0.1029
     Episode_Reward/action_rate_l2: -0.1172
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4422
Metrics/base_velocity/error_vel_xy: 0.8302
Metrics/base_velocity/error_vel_yaw: 0.4598
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 7272000
                    Iteration time: 1.11s
                      Time elapsed: 00:03:42
                               ETA: 00:23:49

################################################################################
                     [1m Learning iteration 202/1500 [0m                      

                       Computation: 31582 steps/s (collection: 1.088s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0179
               Mean surrogate loss: -0.0090
                 Mean entropy loss: 13.0349
                       Mean reward: 10.21
               Mean episode length: 824.00
Episode_Reward/track_lin_vel_xy_exp: 0.4852
Episode_Reward/track_ang_vel_z_exp: 0.4444
       Episode_Reward/lin_vel_z_l2: -0.0292
      Episode_Reward/ang_vel_xy_l2: -0.0971
     Episode_Reward/dof_torques_l2: -0.0594
         Episode_Reward/dof_acc_l2: -0.0873
     Episode_Reward/action_rate_l2: -0.1236
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4490
Metrics/base_velocity/error_vel_xy: 0.9217
Metrics/base_velocity/error_vel_yaw: 0.4603
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 7308000
                    Iteration time: 1.14s
                      Time elapsed: 00:03:43
                               ETA: 00:23:48

################################################################################
                     [1m Learning iteration 203/1500 [0m                      

                       Computation: 31666 steps/s (collection: 1.086s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0196
               Mean surrogate loss: -0.0082
                 Mean entropy loss: 13.0363
                       Mean reward: 11.05
               Mean episode length: 827.18
Episode_Reward/track_lin_vel_xy_exp: 0.5591
Episode_Reward/track_ang_vel_z_exp: 0.4171
       Episode_Reward/lin_vel_z_l2: -0.0321
      Episode_Reward/ang_vel_xy_l2: -0.1013
     Episode_Reward/dof_torques_l2: -0.0589
         Episode_Reward/dof_acc_l2: -0.0993
     Episode_Reward/action_rate_l2: -0.1237
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4557
Metrics/base_velocity/error_vel_xy: 0.8167
Metrics/base_velocity/error_vel_yaw: 0.5075
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 7344000
                    Iteration time: 1.14s
                      Time elapsed: 00:03:44
                               ETA: 00:23:47

################################################################################
                     [1m Learning iteration 204/1500 [0m                      

                       Computation: 32163 steps/s (collection: 1.067s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0198
               Mean surrogate loss: -0.0092
                 Mean entropy loss: 13.0189
                       Mean reward: 8.86
               Mean episode length: 747.94
Episode_Reward/track_lin_vel_xy_exp: 0.3578
Episode_Reward/track_ang_vel_z_exp: 0.3661
       Episode_Reward/lin_vel_z_l2: -0.0313
      Episode_Reward/ang_vel_xy_l2: -0.0920
     Episode_Reward/dof_torques_l2: -0.0510
         Episode_Reward/dof_acc_l2: -0.0986
     Episode_Reward/action_rate_l2: -0.1074
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4588
Metrics/base_velocity/error_vel_xy: 0.8391
Metrics/base_velocity/error_vel_yaw: 0.4078
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 7380000
                    Iteration time: 1.12s
                      Time elapsed: 00:03:45
                               ETA: 00:23:46

################################################################################
                     [1m Learning iteration 205/1500 [0m                      

                       Computation: 33375 steps/s (collection: 1.027s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0181
               Mean surrogate loss: -0.0077
                 Mean entropy loss: 13.0139
                       Mean reward: 8.55
               Mean episode length: 729.76
Episode_Reward/track_lin_vel_xy_exp: 0.5022
Episode_Reward/track_ang_vel_z_exp: 0.4318
       Episode_Reward/lin_vel_z_l2: -0.0343
      Episode_Reward/ang_vel_xy_l2: -0.1073
     Episode_Reward/dof_torques_l2: -0.0606
         Episode_Reward/dof_acc_l2: -0.1145
     Episode_Reward/action_rate_l2: -0.1267
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4595
Metrics/base_velocity/error_vel_xy: 0.8728
Metrics/base_velocity/error_vel_yaw: 0.4852
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 7416000
                    Iteration time: 1.08s
                      Time elapsed: 00:03:46
                               ETA: 00:23:45

################################################################################
                     [1m Learning iteration 206/1500 [0m                      

                       Computation: 33625 steps/s (collection: 1.019s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0169
               Mean surrogate loss: -0.0095
                 Mean entropy loss: 13.0183
                       Mean reward: 8.67
               Mean episode length: 720.30
Episode_Reward/track_lin_vel_xy_exp: 0.4774
Episode_Reward/track_ang_vel_z_exp: 0.4141
       Episode_Reward/lin_vel_z_l2: -0.0325
      Episode_Reward/ang_vel_xy_l2: -0.0977
     Episode_Reward/dof_torques_l2: -0.0561
         Episode_Reward/dof_acc_l2: -0.0967
     Episode_Reward/action_rate_l2: -0.1188
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4619
Metrics/base_velocity/error_vel_xy: 0.8262
Metrics/base_velocity/error_vel_yaw: 0.4390
      Episode_Termination/time_out: 0.9167
  Episode_Termination/base_contact: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 7452000
                    Iteration time: 1.07s
                      Time elapsed: 00:03:47
                               ETA: 00:23:43

################################################################################
                     [1m Learning iteration 207/1500 [0m                      

                       Computation: 31388 steps/s (collection: 1.094s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0160
               Mean surrogate loss: -0.0091
                 Mean entropy loss: 13.0201
                       Mean reward: 9.68
               Mean episode length: 752.24
Episode_Reward/track_lin_vel_xy_exp: 0.5135
Episode_Reward/track_ang_vel_z_exp: 0.4278
       Episode_Reward/lin_vel_z_l2: -0.0310
      Episode_Reward/ang_vel_xy_l2: -0.1004
     Episode_Reward/dof_torques_l2: -0.0597
         Episode_Reward/dof_acc_l2: -0.0980
     Episode_Reward/action_rate_l2: -0.1237
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4628
Metrics/base_velocity/error_vel_xy: 0.8591
Metrics/base_velocity/error_vel_yaw: 0.4952
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 7488000
                    Iteration time: 1.15s
                      Time elapsed: 00:03:48
                               ETA: 00:23:43

################################################################################
                     [1m Learning iteration 208/1500 [0m                      

                       Computation: 31110 steps/s (collection: 1.103s, learning 0.054s)
             Mean action noise std: 0.72
          Mean value_function loss: 0.0195
               Mean surrogate loss: -0.0088
                 Mean entropy loss: 13.0087
                       Mean reward: 9.22
               Mean episode length: 806.01
Episode_Reward/track_lin_vel_xy_exp: 0.4591
Episode_Reward/track_ang_vel_z_exp: 0.4460
       Episode_Reward/lin_vel_z_l2: -0.0341
      Episode_Reward/ang_vel_xy_l2: -0.1085
     Episode_Reward/dof_torques_l2: -0.0617
         Episode_Reward/dof_acc_l2: -0.1071
     Episode_Reward/action_rate_l2: -0.1295
      Episode_Reward/feet_air_time: -0.0018
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4688
Metrics/base_velocity/error_vel_xy: 0.9963
Metrics/base_velocity/error_vel_yaw: 0.5135
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 7524000
                    Iteration time: 1.16s
                      Time elapsed: 00:03:50
                               ETA: 00:23:42

################################################################################
                     [1m Learning iteration 209/1500 [0m                      

                       Computation: 31936 steps/s (collection: 1.074s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0180
               Mean surrogate loss: -0.0083
                 Mean entropy loss: 13.0096
                       Mean reward: 9.12
               Mean episode length: 786.44
Episode_Reward/track_lin_vel_xy_exp: 0.5329
Episode_Reward/track_ang_vel_z_exp: 0.4135
       Episode_Reward/lin_vel_z_l2: -0.0326
      Episode_Reward/ang_vel_xy_l2: -0.1018
     Episode_Reward/dof_torques_l2: -0.0558
         Episode_Reward/dof_acc_l2: -0.1016
     Episode_Reward/action_rate_l2: -0.1171
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4725
Metrics/base_velocity/error_vel_xy: 0.7271
Metrics/base_velocity/error_vel_yaw: 0.4265
      Episode_Termination/time_out: 1.0833
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 7560000
                    Iteration time: 1.13s
                      Time elapsed: 00:03:51
                               ETA: 00:23:41

################################################################################
                     [1m Learning iteration 210/1500 [0m                      

                       Computation: 31388 steps/s (collection: 1.095s, learning 0.052s)
             Mean action noise std: 0.72
          Mean value_function loss: 0.0172
               Mean surrogate loss: -0.0097
                 Mean entropy loss: 13.0120
                       Mean reward: 10.38
               Mean episode length: 800.52
Episode_Reward/track_lin_vel_xy_exp: 0.5222
Episode_Reward/track_ang_vel_z_exp: 0.3994
       Episode_Reward/lin_vel_z_l2: -0.0301
      Episode_Reward/ang_vel_xy_l2: -0.0893
     Episode_Reward/dof_torques_l2: -0.0545
         Episode_Reward/dof_acc_l2: -0.0938
     Episode_Reward/action_rate_l2: -0.1128
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4730
Metrics/base_velocity/error_vel_xy: 0.7180
Metrics/base_velocity/error_vel_yaw: 0.4134
      Episode_Termination/time_out: 1.1667
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 7596000
                    Iteration time: 1.15s
                      Time elapsed: 00:03:52
                               ETA: 00:23:40

################################################################################
                     [1m Learning iteration 211/1500 [0m                      

                       Computation: 31965 steps/s (collection: 1.074s, learning 0.052s)
             Mean action noise std: 0.72
          Mean value_function loss: 0.0215
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 13.0045
                       Mean reward: 10.63
               Mean episode length: 818.44
Episode_Reward/track_lin_vel_xy_exp: 0.5274
Episode_Reward/track_ang_vel_z_exp: 0.4532
       Episode_Reward/lin_vel_z_l2: -0.0322
      Episode_Reward/ang_vel_xy_l2: -0.1065
     Episode_Reward/dof_torques_l2: -0.0616
         Episode_Reward/dof_acc_l2: -0.1069
     Episode_Reward/action_rate_l2: -0.1305
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4741
Metrics/base_velocity/error_vel_xy: 0.9123
Metrics/base_velocity/error_vel_yaw: 0.4892
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 7632000
                    Iteration time: 1.13s
                      Time elapsed: 00:03:53
                               ETA: 00:23:39

################################################################################
                     [1m Learning iteration 212/1500 [0m                      

                       Computation: 31462 steps/s (collection: 1.091s, learning 0.053s)
             Mean action noise std: 0.72
          Mean value_function loss: 0.0193
               Mean surrogate loss: -0.0083
                 Mean entropy loss: 13.0002
                       Mean reward: 10.79
               Mean episode length: 819.71
Episode_Reward/track_lin_vel_xy_exp: 0.5188
Episode_Reward/track_ang_vel_z_exp: 0.3951
       Episode_Reward/lin_vel_z_l2: -0.0333
      Episode_Reward/ang_vel_xy_l2: -0.0976
     Episode_Reward/dof_torques_l2: -0.0549
         Episode_Reward/dof_acc_l2: -0.1028
     Episode_Reward/action_rate_l2: -0.1152
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4780
Metrics/base_velocity/error_vel_xy: 0.7379
Metrics/base_velocity/error_vel_yaw: 0.4357
      Episode_Termination/time_out: 1.0833
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 7668000
                    Iteration time: 1.14s
                      Time elapsed: 00:03:54
                               ETA: 00:23:38

################################################################################
                     [1m Learning iteration 213/1500 [0m                      

                       Computation: 31488 steps/s (collection: 1.091s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0225
               Mean surrogate loss: -0.0086
                 Mean entropy loss: 13.0075
                       Mean reward: 10.56
               Mean episode length: 797.04
Episode_Reward/track_lin_vel_xy_exp: 0.5240
Episode_Reward/track_ang_vel_z_exp: 0.3812
       Episode_Reward/lin_vel_z_l2: -0.0325
      Episode_Reward/ang_vel_xy_l2: -0.0962
     Episode_Reward/dof_torques_l2: -0.0561
         Episode_Reward/dof_acc_l2: -0.1049
     Episode_Reward/action_rate_l2: -0.1153
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4843
Metrics/base_velocity/error_vel_xy: 0.7130
Metrics/base_velocity/error_vel_yaw: 0.4575
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 7704000
                    Iteration time: 1.14s
                      Time elapsed: 00:03:55
                               ETA: 00:23:37

################################################################################
                     [1m Learning iteration 214/1500 [0m                      

                       Computation: 32742 steps/s (collection: 1.049s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0227
               Mean surrogate loss: -0.0090
                 Mean entropy loss: 13.0372
                       Mean reward: 10.71
               Mean episode length: 820.20
Episode_Reward/track_lin_vel_xy_exp: 0.6020
Episode_Reward/track_ang_vel_z_exp: 0.4518
       Episode_Reward/lin_vel_z_l2: -0.0362
      Episode_Reward/ang_vel_xy_l2: -0.1119
     Episode_Reward/dof_torques_l2: -0.0638
         Episode_Reward/dof_acc_l2: -0.1189
     Episode_Reward/action_rate_l2: -0.1345
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4909
Metrics/base_velocity/error_vel_xy: 0.8355
Metrics/base_velocity/error_vel_yaw: 0.5174
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 7740000
                    Iteration time: 1.10s
                      Time elapsed: 00:03:56
                               ETA: 00:23:36

################################################################################
                     [1m Learning iteration 215/1500 [0m                      

                       Computation: 31780 steps/s (collection: 1.079s, learning 0.054s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0201
               Mean surrogate loss: -0.0089
                 Mean entropy loss: 13.0492
                       Mean reward: 12.02
               Mean episode length: 878.68
Episode_Reward/track_lin_vel_xy_exp: 0.6248
Episode_Reward/track_ang_vel_z_exp: 0.4781
       Episode_Reward/lin_vel_z_l2: -0.0347
      Episode_Reward/ang_vel_xy_l2: -0.1128
     Episode_Reward/dof_torques_l2: -0.0648
         Episode_Reward/dof_acc_l2: -0.1120
     Episode_Reward/action_rate_l2: -0.1375
      Episode_Reward/feet_air_time: -0.0018
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.4960
Metrics/base_velocity/error_vel_xy: 0.8780
Metrics/base_velocity/error_vel_yaw: 0.5061
      Episode_Termination/time_out: 2.6250
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 7776000
                    Iteration time: 1.13s
                      Time elapsed: 00:03:58
                               ETA: 00:23:35

################################################################################
                     [1m Learning iteration 216/1500 [0m                      

                       Computation: 31814 steps/s (collection: 1.079s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0228
               Mean surrogate loss: -0.0077
                 Mean entropy loss: 13.0611
                       Mean reward: 11.93
               Mean episode length: 862.85
Episode_Reward/track_lin_vel_xy_exp: 0.6400
Episode_Reward/track_ang_vel_z_exp: 0.4637
       Episode_Reward/lin_vel_z_l2: -0.0417
      Episode_Reward/ang_vel_xy_l2: -0.1155
     Episode_Reward/dof_torques_l2: -0.0666
         Episode_Reward/dof_acc_l2: -0.1258
     Episode_Reward/action_rate_l2: -0.1383
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.5034
Metrics/base_velocity/error_vel_xy: 0.8644
Metrics/base_velocity/error_vel_yaw: 0.5137
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 7812000
                    Iteration time: 1.13s
                      Time elapsed: 00:03:59
                               ETA: 00:23:35

################################################################################
                     [1m Learning iteration 217/1500 [0m                      

                       Computation: 32649 steps/s (collection: 1.050s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0193
               Mean surrogate loss: -0.0078
                 Mean entropy loss: 13.0755
                       Mean reward: 12.36
               Mean episode length: 856.57
Episode_Reward/track_lin_vel_xy_exp: 0.7723
Episode_Reward/track_ang_vel_z_exp: 0.4753
       Episode_Reward/lin_vel_z_l2: -0.0361
      Episode_Reward/ang_vel_xy_l2: -0.1111
     Episode_Reward/dof_torques_l2: -0.0681
         Episode_Reward/dof_acc_l2: -0.1095
     Episode_Reward/action_rate_l2: -0.1358
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.5119
Metrics/base_velocity/error_vel_xy: 0.6957
Metrics/base_velocity/error_vel_yaw: 0.4857
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 7848000
                    Iteration time: 1.10s
                      Time elapsed: 00:04:00
                               ETA: 00:23:33

################################################################################
                     [1m Learning iteration 218/1500 [0m                      

                       Computation: 32651 steps/s (collection: 1.049s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0159
               Mean surrogate loss: -0.0074
                 Mean entropy loss: 13.0901
                       Mean reward: 12.46
               Mean episode length: 849.59
Episode_Reward/track_lin_vel_xy_exp: 0.4894
Episode_Reward/track_ang_vel_z_exp: 0.3591
       Episode_Reward/lin_vel_z_l2: -0.0326
      Episode_Reward/ang_vel_xy_l2: -0.0902
     Episode_Reward/dof_torques_l2: -0.0486
         Episode_Reward/dof_acc_l2: -0.0898
     Episode_Reward/action_rate_l2: -0.1042
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.5176
Metrics/base_velocity/error_vel_xy: 0.6407
Metrics/base_velocity/error_vel_yaw: 0.3896
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 7884000
                    Iteration time: 1.10s
                      Time elapsed: 00:04:01
                               ETA: 00:23:32

################################################################################
                     [1m Learning iteration 219/1500 [0m                      

                       Computation: 32726 steps/s (collection: 1.048s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0191
               Mean surrogate loss: -0.0089
                 Mean entropy loss: 13.0775
                       Mean reward: 12.24
               Mean episode length: 855.02
Episode_Reward/track_lin_vel_xy_exp: 0.5577
Episode_Reward/track_ang_vel_z_exp: 0.4265
       Episode_Reward/lin_vel_z_l2: -0.0392
      Episode_Reward/ang_vel_xy_l2: -0.1088
     Episode_Reward/dof_torques_l2: -0.0632
         Episode_Reward/dof_acc_l2: -0.1133
     Episode_Reward/action_rate_l2: -0.1281
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.5246
Metrics/base_velocity/error_vel_xy: 0.8001
Metrics/base_velocity/error_vel_yaw: 0.4856
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 7920000
                    Iteration time: 1.10s
                      Time elapsed: 00:04:02
                               ETA: 00:23:31

################################################################################
                     [1m Learning iteration 220/1500 [0m                      

                       Computation: 32444 steps/s (collection: 1.058s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0268
               Mean surrogate loss: -0.0080
                 Mean entropy loss: 13.0776
                       Mean reward: 11.57
               Mean episode length: 828.18
Episode_Reward/track_lin_vel_xy_exp: 0.6393
Episode_Reward/track_ang_vel_z_exp: 0.4357
       Episode_Reward/lin_vel_z_l2: -0.0379
      Episode_Reward/ang_vel_xy_l2: -0.1101
     Episode_Reward/dof_torques_l2: -0.0622
         Episode_Reward/dof_acc_l2: -0.1175
     Episode_Reward/action_rate_l2: -0.1289
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.5314
Metrics/base_velocity/error_vel_xy: 0.7441
Metrics/base_velocity/error_vel_yaw: 0.4939
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 7956000
                    Iteration time: 1.11s
                      Time elapsed: 00:04:03
                               ETA: 00:23:30

################################################################################
                     [1m Learning iteration 221/1500 [0m                      

                       Computation: 31575 steps/s (collection: 1.088s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0225
               Mean surrogate loss: -0.0077
                 Mean entropy loss: 13.0744
                       Mean reward: 12.10
               Mean episode length: 826.99
Episode_Reward/track_lin_vel_xy_exp: 0.6032
Episode_Reward/track_ang_vel_z_exp: 0.4379
       Episode_Reward/lin_vel_z_l2: -0.0349
      Episode_Reward/ang_vel_xy_l2: -0.1056
     Episode_Reward/dof_torques_l2: -0.0638
         Episode_Reward/dof_acc_l2: -0.1118
     Episode_Reward/action_rate_l2: -0.1271
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.5397
Metrics/base_velocity/error_vel_xy: 0.7720
Metrics/base_velocity/error_vel_yaw: 0.4586
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 7992000
                    Iteration time: 1.14s
                      Time elapsed: 00:04:04
                               ETA: 00:23:29

################################################################################
                     [1m Learning iteration 222/1500 [0m                      

                       Computation: 32492 steps/s (collection: 1.054s, learning 0.054s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0190
               Mean surrogate loss: -0.0087
                 Mean entropy loss: 13.0668
                       Mean reward: 12.22
               Mean episode length: 853.40
Episode_Reward/track_lin_vel_xy_exp: 0.6454
Episode_Reward/track_ang_vel_z_exp: 0.4534
       Episode_Reward/lin_vel_z_l2: -0.0368
      Episode_Reward/ang_vel_xy_l2: -0.1090
     Episode_Reward/dof_torques_l2: -0.0642
         Episode_Reward/dof_acc_l2: -0.1131
     Episode_Reward/action_rate_l2: -0.1324
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.5453
Metrics/base_velocity/error_vel_xy: 0.7823
Metrics/base_velocity/error_vel_yaw: 0.4999
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 8028000
                    Iteration time: 1.11s
                      Time elapsed: 00:04:05
                               ETA: 00:23:28

################################################################################
                     [1m Learning iteration 223/1500 [0m                      

                       Computation: 33027 steps/s (collection: 1.037s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0186
               Mean surrogate loss: -0.0075
                 Mean entropy loss: 13.0631
                       Mean reward: 11.11
               Mean episode length: 816.89
Episode_Reward/track_lin_vel_xy_exp: 0.5697
Episode_Reward/track_ang_vel_z_exp: 0.4454
       Episode_Reward/lin_vel_z_l2: -0.0455
      Episode_Reward/ang_vel_xy_l2: -0.1195
     Episode_Reward/dof_torques_l2: -0.0642
         Episode_Reward/dof_acc_l2: -0.1418
     Episode_Reward/action_rate_l2: -0.1370
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.5562
Metrics/base_velocity/error_vel_xy: 0.8516
Metrics/base_velocity/error_vel_yaw: 0.5164
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 8064000
                    Iteration time: 1.09s
                      Time elapsed: 00:04:06
                               ETA: 00:23:27

################################################################################
                     [1m Learning iteration 224/1500 [0m                      

                       Computation: 32969 steps/s (collection: 1.039s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0170
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 13.0454
                       Mean reward: 10.58
               Mean episode length: 785.77
Episode_Reward/track_lin_vel_xy_exp: 0.5512
Episode_Reward/track_ang_vel_z_exp: 0.3685
       Episode_Reward/lin_vel_z_l2: -0.0356
      Episode_Reward/ang_vel_xy_l2: -0.0968
     Episode_Reward/dof_torques_l2: -0.0514
         Episode_Reward/dof_acc_l2: -0.1070
     Episode_Reward/action_rate_l2: -0.1121
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.5606
Metrics/base_velocity/error_vel_xy: 0.6324
Metrics/base_velocity/error_vel_yaw: 0.4312
      Episode_Termination/time_out: 1.0417
  Episode_Termination/base_contact: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 8100000
                    Iteration time: 1.09s
                      Time elapsed: 00:04:07
                               ETA: 00:23:26

################################################################################
                     [1m Learning iteration 225/1500 [0m                      

                       Computation: 32857 steps/s (collection: 1.046s, learning 0.049s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0226
               Mean surrogate loss: -0.0084
                 Mean entropy loss: 13.0169
                       Mean reward: 10.35
               Mean episode length: 793.13
Episode_Reward/track_lin_vel_xy_exp: 0.5509
Episode_Reward/track_ang_vel_z_exp: 0.4480
       Episode_Reward/lin_vel_z_l2: -0.0413
      Episode_Reward/ang_vel_xy_l2: -0.1213
     Episode_Reward/dof_torques_l2: -0.0635
         Episode_Reward/dof_acc_l2: -0.1371
     Episode_Reward/action_rate_l2: -0.1365
      Episode_Reward/feet_air_time: -0.0018
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.5629
Metrics/base_velocity/error_vel_xy: 0.9033
Metrics/base_velocity/error_vel_yaw: 0.5212
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 8136000
                    Iteration time: 1.10s
                      Time elapsed: 00:04:09
                               ETA: 00:23:25

################################################################################
                     [1m Learning iteration 226/1500 [0m                      

                       Computation: 33799 steps/s (collection: 1.013s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0185
               Mean surrogate loss: -0.0090
                 Mean entropy loss: 13.0273
                       Mean reward: 11.40
               Mean episode length: 833.11
Episode_Reward/track_lin_vel_xy_exp: 0.7351
Episode_Reward/track_ang_vel_z_exp: 0.4792
       Episode_Reward/lin_vel_z_l2: -0.0352
      Episode_Reward/ang_vel_xy_l2: -0.1141
     Episode_Reward/dof_torques_l2: -0.0695
         Episode_Reward/dof_acc_l2: -0.1158
     Episode_Reward/action_rate_l2: -0.1379
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.5696
Metrics/base_velocity/error_vel_xy: 0.7488
Metrics/base_velocity/error_vel_yaw: 0.4892
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 8172000
                    Iteration time: 1.07s
                      Time elapsed: 00:04:10
                               ETA: 00:23:23

################################################################################
                     [1m Learning iteration 227/1500 [0m                      

                       Computation: 32647 steps/s (collection: 1.051s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0221
               Mean surrogate loss: -0.0084
                 Mean entropy loss: 13.0314
                       Mean reward: 12.21
               Mean episode length: 856.41
Episode_Reward/track_lin_vel_xy_exp: 0.7078
Episode_Reward/track_ang_vel_z_exp: 0.4563
       Episode_Reward/lin_vel_z_l2: -0.0390
      Episode_Reward/ang_vel_xy_l2: -0.1161
     Episode_Reward/dof_torques_l2: -0.0653
         Episode_Reward/dof_acc_l2: -0.1324
     Episode_Reward/action_rate_l2: -0.1367
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.5786
Metrics/base_velocity/error_vel_xy: 0.7662
Metrics/base_velocity/error_vel_yaw: 0.5021
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 8208000
                    Iteration time: 1.10s
                      Time elapsed: 00:04:11
                               ETA: 00:23:22

################################################################################
                     [1m Learning iteration 228/1500 [0m                      

                       Computation: 33059 steps/s (collection: 1.038s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0236
               Mean surrogate loss: -0.0084
                 Mean entropy loss: 13.0307
                       Mean reward: 12.04
               Mean episode length: 826.18
Episode_Reward/track_lin_vel_xy_exp: 0.6386
Episode_Reward/track_ang_vel_z_exp: 0.4582
       Episode_Reward/lin_vel_z_l2: -0.0376
      Episode_Reward/ang_vel_xy_l2: -0.1114
     Episode_Reward/dof_torques_l2: -0.0648
         Episode_Reward/dof_acc_l2: -0.1192
     Episode_Reward/action_rate_l2: -0.1348
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.5877
Metrics/base_velocity/error_vel_xy: 0.8108
Metrics/base_velocity/error_vel_yaw: 0.5050
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 8244000
                    Iteration time: 1.09s
                      Time elapsed: 00:04:12
                               ETA: 00:23:21

################################################################################
                     [1m Learning iteration 229/1500 [0m                      

                       Computation: 33175 steps/s (collection: 1.032s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0218
               Mean surrogate loss: -0.0092
                 Mean entropy loss: 13.0365
                       Mean reward: 12.67
               Mean episode length: 848.97
Episode_Reward/track_lin_vel_xy_exp: 0.6901
Episode_Reward/track_ang_vel_z_exp: 0.4592
       Episode_Reward/lin_vel_z_l2: -0.0374
      Episode_Reward/ang_vel_xy_l2: -0.1125
     Episode_Reward/dof_torques_l2: -0.0664
         Episode_Reward/dof_acc_l2: -0.1187
     Episode_Reward/action_rate_l2: -0.1354
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.5941
Metrics/base_velocity/error_vel_xy: 0.7612
Metrics/base_velocity/error_vel_yaw: 0.5034
      Episode_Termination/time_out: 2.0833
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 8280000
                    Iteration time: 1.09s
                      Time elapsed: 00:04:13
                               ETA: 00:23:20

################################################################################
                     [1m Learning iteration 230/1500 [0m                      

                       Computation: 31893 steps/s (collection: 1.076s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0240
               Mean surrogate loss: -0.0086
                 Mean entropy loss: 13.0346
                       Mean reward: 12.79
               Mean episode length: 840.94
Episode_Reward/track_lin_vel_xy_exp: 0.5461
Episode_Reward/track_ang_vel_z_exp: 0.3916
       Episode_Reward/lin_vel_z_l2: -0.0336
      Episode_Reward/ang_vel_xy_l2: -0.0975
     Episode_Reward/dof_torques_l2: -0.0564
         Episode_Reward/dof_acc_l2: -0.1025
     Episode_Reward/action_rate_l2: -0.1148
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6027
Metrics/base_velocity/error_vel_xy: 0.6845
Metrics/base_velocity/error_vel_yaw: 0.4281
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 8316000
                    Iteration time: 1.13s
                      Time elapsed: 00:04:14
                               ETA: 00:23:19

################################################################################
                     [1m Learning iteration 231/1500 [0m                      

                       Computation: 33083 steps/s (collection: 1.037s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0202
               Mean surrogate loss: -0.0078
                 Mean entropy loss: 13.0185
                       Mean reward: 12.16
               Mean episode length: 835.73
Episode_Reward/track_lin_vel_xy_exp: 0.5663
Episode_Reward/track_ang_vel_z_exp: 0.4240
       Episode_Reward/lin_vel_z_l2: -0.0323
      Episode_Reward/ang_vel_xy_l2: -0.0974
     Episode_Reward/dof_torques_l2: -0.0612
         Episode_Reward/dof_acc_l2: -0.1005
     Episode_Reward/action_rate_l2: -0.1225
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6043
Metrics/base_velocity/error_vel_xy: 0.7590
Metrics/base_velocity/error_vel_yaw: 0.4486
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 8352000
                    Iteration time: 1.09s
                      Time elapsed: 00:04:15
                               ETA: 00:23:18

################################################################################
                     [1m Learning iteration 232/1500 [0m                      

                       Computation: 30746 steps/s (collection: 1.112s, learning 0.059s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0208
               Mean surrogate loss: -0.0083
                 Mean entropy loss: 13.0076
                       Mean reward: 12.50
               Mean episode length: 867.34
Episode_Reward/track_lin_vel_xy_exp: 0.6420
Episode_Reward/track_ang_vel_z_exp: 0.4799
       Episode_Reward/lin_vel_z_l2: -0.0373
      Episode_Reward/ang_vel_xy_l2: -0.1183
     Episode_Reward/dof_torques_l2: -0.0729
         Episode_Reward/dof_acc_l2: -0.1293
     Episode_Reward/action_rate_l2: -0.1435
      Episode_Reward/feet_air_time: -0.0018
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6077
Metrics/base_velocity/error_vel_xy: 0.8966
Metrics/base_velocity/error_vel_yaw: 0.5455
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 8388000
                    Iteration time: 1.17s
                      Time elapsed: 00:04:16
                               ETA: 00:23:17

################################################################################
                     [1m Learning iteration 233/1500 [0m                      

                       Computation: 32790 steps/s (collection: 1.046s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0237
               Mean surrogate loss: -0.0090
                 Mean entropy loss: 13.0150
                       Mean reward: 13.13
               Mean episode length: 892.50
Episode_Reward/track_lin_vel_xy_exp: 0.7769
Episode_Reward/track_ang_vel_z_exp: 0.4812
       Episode_Reward/lin_vel_z_l2: -0.0427
      Episode_Reward/ang_vel_xy_l2: -0.1189
     Episode_Reward/dof_torques_l2: -0.0719
         Episode_Reward/dof_acc_l2: -0.1360
     Episode_Reward/action_rate_l2: -0.1445
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6151
Metrics/base_velocity/error_vel_xy: 0.7449
Metrics/base_velocity/error_vel_yaw: 0.5319
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 8424000
                    Iteration time: 1.10s
                      Time elapsed: 00:04:17
                               ETA: 00:23:16

################################################################################
                     [1m Learning iteration 234/1500 [0m                      

                       Computation: 32182 steps/s (collection: 1.066s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0210
               Mean surrogate loss: -0.0094
                 Mean entropy loss: 13.0289
                       Mean reward: 13.57
               Mean episode length: 873.66
Episode_Reward/track_lin_vel_xy_exp: 0.6501
Episode_Reward/track_ang_vel_z_exp: 0.4152
       Episode_Reward/lin_vel_z_l2: -0.0372
      Episode_Reward/ang_vel_xy_l2: -0.1067
     Episode_Reward/dof_torques_l2: -0.0605
         Episode_Reward/dof_acc_l2: -0.1179
     Episode_Reward/action_rate_l2: -0.1228
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6255
Metrics/base_velocity/error_vel_xy: 0.6729
Metrics/base_velocity/error_vel_yaw: 0.4438
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 8460000
                    Iteration time: 1.12s
                      Time elapsed: 00:04:19
                               ETA: 00:23:15

################################################################################
                     [1m Learning iteration 235/1500 [0m                      

                       Computation: 31159 steps/s (collection: 1.101s, learning 0.055s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0209
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 13.0300
                       Mean reward: 13.25
               Mean episode length: 846.70
Episode_Reward/track_lin_vel_xy_exp: 0.6485
Episode_Reward/track_ang_vel_z_exp: 0.4273
       Episode_Reward/lin_vel_z_l2: -0.0381
      Episode_Reward/ang_vel_xy_l2: -0.1097
     Episode_Reward/dof_torques_l2: -0.0648
         Episode_Reward/dof_acc_l2: -0.1274
     Episode_Reward/action_rate_l2: -0.1311
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6343
Metrics/base_velocity/error_vel_xy: 0.7058
Metrics/base_velocity/error_vel_yaw: 0.4799
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 8496000
                    Iteration time: 1.16s
                      Time elapsed: 00:04:20
                               ETA: 00:23:14

################################################################################
                     [1m Learning iteration 236/1500 [0m                      

                       Computation: 33000 steps/s (collection: 1.038s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0259
               Mean surrogate loss: -0.0089
                 Mean entropy loss: 13.0422
                       Mean reward: 12.82
               Mean episode length: 854.57
Episode_Reward/track_lin_vel_xy_exp: 0.7428
Episode_Reward/track_ang_vel_z_exp: 0.4757
       Episode_Reward/lin_vel_z_l2: -0.0370
      Episode_Reward/ang_vel_xy_l2: -0.1157
     Episode_Reward/dof_torques_l2: -0.0704
         Episode_Reward/dof_acc_l2: -0.1210
     Episode_Reward/action_rate_l2: -0.1392
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6400
Metrics/base_velocity/error_vel_xy: 0.7212
Metrics/base_velocity/error_vel_yaw: 0.5046
      Episode_Termination/time_out: 2.1250
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 8532000
                    Iteration time: 1.09s
                      Time elapsed: 00:04:21
                               ETA: 00:23:13

################################################################################
                     [1m Learning iteration 237/1500 [0m                      

                       Computation: 32144 steps/s (collection: 1.067s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0216
               Mean surrogate loss: -0.0097
                 Mean entropy loss: 13.0425
                       Mean reward: 13.32
               Mean episode length: 864.97
Episode_Reward/track_lin_vel_xy_exp: 0.6881
Episode_Reward/track_ang_vel_z_exp: 0.4337
       Episode_Reward/lin_vel_z_l2: -0.0433
      Episode_Reward/ang_vel_xy_l2: -0.1176
     Episode_Reward/dof_torques_l2: -0.0673
         Episode_Reward/dof_acc_l2: -0.1411
     Episode_Reward/action_rate_l2: -0.1352
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6452
Metrics/base_velocity/error_vel_xy: 0.7433
Metrics/base_velocity/error_vel_yaw: 0.5216
      Episode_Termination/time_out: 1.9583
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 8568000
                    Iteration time: 1.12s
                      Time elapsed: 00:04:22
                               ETA: 00:23:12

################################################################################
                     [1m Learning iteration 238/1500 [0m                      

                       Computation: 32561 steps/s (collection: 1.053s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0197
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 13.0512
                       Mean reward: 12.53
               Mean episode length: 840.71
Episode_Reward/track_lin_vel_xy_exp: 0.5898
Episode_Reward/track_ang_vel_z_exp: 0.4226
       Episode_Reward/lin_vel_z_l2: -0.0350
      Episode_Reward/ang_vel_xy_l2: -0.1071
     Episode_Reward/dof_torques_l2: -0.0605
         Episode_Reward/dof_acc_l2: -0.1131
     Episode_Reward/action_rate_l2: -0.1240
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6507
Metrics/base_velocity/error_vel_xy: 0.7600
Metrics/base_velocity/error_vel_yaw: 0.4704
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 8604000
                    Iteration time: 1.11s
                      Time elapsed: 00:04:23
                               ETA: 00:23:11

################################################################################
                     [1m Learning iteration 239/1500 [0m                      

                       Computation: 33849 steps/s (collection: 1.011s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0197
               Mean surrogate loss: -0.0098
                 Mean entropy loss: 13.0498
                       Mean reward: 12.50
               Mean episode length: 836.93
Episode_Reward/track_lin_vel_xy_exp: 0.6007
Episode_Reward/track_ang_vel_z_exp: 0.4449
       Episode_Reward/lin_vel_z_l2: -0.0344
      Episode_Reward/ang_vel_xy_l2: -0.1065
     Episode_Reward/dof_torques_l2: -0.0668
         Episode_Reward/dof_acc_l2: -0.1175
     Episode_Reward/action_rate_l2: -0.1319
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6573
Metrics/base_velocity/error_vel_xy: 0.8152
Metrics/base_velocity/error_vel_yaw: 0.4942
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 8640000
                    Iteration time: 1.06s
                      Time elapsed: 00:04:24
                               ETA: 00:23:10

################################################################################
                     [1m Learning iteration 240/1500 [0m                      

                       Computation: 33013 steps/s (collection: 1.039s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0218
               Mean surrogate loss: -0.0082
                 Mean entropy loss: 13.0582
                       Mean reward: 13.41
               Mean episode length: 866.33
Episode_Reward/track_lin_vel_xy_exp: 0.8059
Episode_Reward/track_ang_vel_z_exp: 0.4907
       Episode_Reward/lin_vel_z_l2: -0.0369
      Episode_Reward/ang_vel_xy_l2: -0.1173
     Episode_Reward/dof_torques_l2: -0.0744
         Episode_Reward/dof_acc_l2: -0.1303
     Episode_Reward/action_rate_l2: -0.1449
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6644
Metrics/base_velocity/error_vel_xy: 0.7166
Metrics/base_velocity/error_vel_yaw: 0.5352
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 8676000
                    Iteration time: 1.09s
                      Time elapsed: 00:04:25
                               ETA: 00:23:08

################################################################################
                     [1m Learning iteration 241/1500 [0m                      

                       Computation: 32682 steps/s (collection: 1.050s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0238
               Mean surrogate loss: -0.0095
                 Mean entropy loss: 13.0641
                       Mean reward: 13.89
               Mean episode length: 862.80
Episode_Reward/track_lin_vel_xy_exp: 0.7066
Episode_Reward/track_ang_vel_z_exp: 0.4732
       Episode_Reward/lin_vel_z_l2: -0.0391
      Episode_Reward/ang_vel_xy_l2: -0.1154
     Episode_Reward/dof_torques_l2: -0.0691
         Episode_Reward/dof_acc_l2: -0.1317
     Episode_Reward/action_rate_l2: -0.1404
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6719
Metrics/base_velocity/error_vel_xy: 0.7977
Metrics/base_velocity/error_vel_yaw: 0.4970
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 8712000
                    Iteration time: 1.10s
                      Time elapsed: 00:04:26
                               ETA: 00:23:07

################################################################################
                     [1m Learning iteration 242/1500 [0m                      

                       Computation: 31067 steps/s (collection: 1.105s, learning 0.054s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0204
               Mean surrogate loss: -0.0086
                 Mean entropy loss: 13.0619
                       Mean reward: 13.42
               Mean episode length: 864.35
Episode_Reward/track_lin_vel_xy_exp: 0.6946
Episode_Reward/track_ang_vel_z_exp: 0.4638
       Episode_Reward/lin_vel_z_l2: -0.0404
      Episode_Reward/ang_vel_xy_l2: -0.1194
     Episode_Reward/dof_torques_l2: -0.0682
         Episode_Reward/dof_acc_l2: -0.1386
     Episode_Reward/action_rate_l2: -0.1418
      Episode_Reward/feet_air_time: -0.0018
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6817
Metrics/base_velocity/error_vel_xy: 0.7910
Metrics/base_velocity/error_vel_yaw: 0.5310
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 8748000
                    Iteration time: 1.16s
                      Time elapsed: 00:04:27
                               ETA: 00:23:06

################################################################################
                     [1m Learning iteration 243/1500 [0m                      

                       Computation: 32506 steps/s (collection: 1.055s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0211
               Mean surrogate loss: -0.0092
                 Mean entropy loss: 13.0539
                       Mean reward: 12.74
               Mean episode length: 865.60
Episode_Reward/track_lin_vel_xy_exp: 0.7276
Episode_Reward/track_ang_vel_z_exp: 0.4547
       Episode_Reward/lin_vel_z_l2: -0.0426
      Episode_Reward/ang_vel_xy_l2: -0.1186
     Episode_Reward/dof_torques_l2: -0.0740
         Episode_Reward/dof_acc_l2: -0.1332
     Episode_Reward/action_rate_l2: -0.1411
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6869
Metrics/base_velocity/error_vel_xy: 0.7426
Metrics/base_velocity/error_vel_yaw: 0.5379
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 8784000
                    Iteration time: 1.11s
                      Time elapsed: 00:04:29
                               ETA: 00:23:05

################################################################################
                     [1m Learning iteration 244/1500 [0m                      

                       Computation: 30589 steps/s (collection: 1.126s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0260
               Mean surrogate loss: -0.0095
                 Mean entropy loss: 13.0577
                       Mean reward: 11.84
               Mean episode length: 825.36
Episode_Reward/track_lin_vel_xy_exp: 0.5952
Episode_Reward/track_ang_vel_z_exp: 0.4271
       Episode_Reward/lin_vel_z_l2: -0.0376
      Episode_Reward/ang_vel_xy_l2: -0.1090
     Episode_Reward/dof_torques_l2: -0.0656
         Episode_Reward/dof_acc_l2: -0.1285
     Episode_Reward/action_rate_l2: -0.1328
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6878
Metrics/base_velocity/error_vel_xy: 0.7711
Metrics/base_velocity/error_vel_yaw: 0.4980
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 8820000
                    Iteration time: 1.18s
                      Time elapsed: 00:04:30
                               ETA: 00:23:05

################################################################################
                     [1m Learning iteration 245/1500 [0m                      

                       Computation: 32260 steps/s (collection: 1.065s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0208
               Mean surrogate loss: -0.0094
                 Mean entropy loss: 13.0608
                       Mean reward: 10.24
               Mean episode length: 770.32
Episode_Reward/track_lin_vel_xy_exp: 0.5531
Episode_Reward/track_ang_vel_z_exp: 0.3961
       Episode_Reward/lin_vel_z_l2: -0.0373
      Episode_Reward/ang_vel_xy_l2: -0.1077
     Episode_Reward/dof_torques_l2: -0.0576
         Episode_Reward/dof_acc_l2: -0.1194
     Episode_Reward/action_rate_l2: -0.1225
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6851
Metrics/base_velocity/error_vel_xy: 0.7530
Metrics/base_velocity/error_vel_yaw: 0.4806
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 8856000
                    Iteration time: 1.12s
                      Time elapsed: 00:04:31
                               ETA: 00:23:04

################################################################################
                     [1m Learning iteration 246/1500 [0m                      

                       Computation: 31513 steps/s (collection: 1.091s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0234
               Mean surrogate loss: -0.0087
                 Mean entropy loss: 13.0632
                       Mean reward: 10.41
               Mean episode length: 778.84
Episode_Reward/track_lin_vel_xy_exp: 0.5058
Episode_Reward/track_ang_vel_z_exp: 0.3773
       Episode_Reward/lin_vel_z_l2: -0.0313
      Episode_Reward/ang_vel_xy_l2: -0.0941
     Episode_Reward/dof_torques_l2: -0.0567
         Episode_Reward/dof_acc_l2: -0.1074
     Episode_Reward/action_rate_l2: -0.1154
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6854
Metrics/base_velocity/error_vel_xy: 0.7463
Metrics/base_velocity/error_vel_yaw: 0.4524
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 8892000
                    Iteration time: 1.14s
                      Time elapsed: 00:04:32
                               ETA: 00:23:03

################################################################################
                     [1m Learning iteration 247/1500 [0m                      

                       Computation: 31184 steps/s (collection: 1.103s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0224
               Mean surrogate loss: -0.0087
                 Mean entropy loss: 13.0654
                       Mean reward: 10.81
               Mean episode length: 768.78
Episode_Reward/track_lin_vel_xy_exp: 0.6051
Episode_Reward/track_ang_vel_z_exp: 0.3851
       Episode_Reward/lin_vel_z_l2: -0.0375
      Episode_Reward/ang_vel_xy_l2: -0.1040
     Episode_Reward/dof_torques_l2: -0.0595
         Episode_Reward/dof_acc_l2: -0.1295
     Episode_Reward/action_rate_l2: -0.1208
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6916
Metrics/base_velocity/error_vel_xy: 0.6338
Metrics/base_velocity/error_vel_yaw: 0.4531
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 8928000
                    Iteration time: 1.15s
                      Time elapsed: 00:04:33
                               ETA: 00:23:02

################################################################################
                     [1m Learning iteration 248/1500 [0m                      

                       Computation: 31572 steps/s (collection: 1.081s, learning 0.059s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0206
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 13.0538
                       Mean reward: 11.35
               Mean episode length: 757.17
Episode_Reward/track_lin_vel_xy_exp: 0.7048
Episode_Reward/track_ang_vel_z_exp: 0.4249
       Episode_Reward/lin_vel_z_l2: -0.0392
      Episode_Reward/ang_vel_xy_l2: -0.1135
     Episode_Reward/dof_torques_l2: -0.0678
         Episode_Reward/dof_acc_l2: -0.1363
     Episode_Reward/action_rate_l2: -0.1344
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.6939
Metrics/base_velocity/error_vel_xy: 0.6722
Metrics/base_velocity/error_vel_yaw: 0.5086
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 8964000
                    Iteration time: 1.14s
                      Time elapsed: 00:04:34
                               ETA: 00:23:01

################################################################################
                     [1m Learning iteration 249/1500 [0m                      

                       Computation: 32735 steps/s (collection: 1.048s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0254
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 13.0493
                       Mean reward: 12.88
               Mean episode length: 822.84
Episode_Reward/track_lin_vel_xy_exp: 0.7534
Episode_Reward/track_ang_vel_z_exp: 0.4800
       Episode_Reward/lin_vel_z_l2: -0.0363
      Episode_Reward/ang_vel_xy_l2: -0.1155
     Episode_Reward/dof_torques_l2: -0.0686
         Episode_Reward/dof_acc_l2: -0.1368
     Episode_Reward/action_rate_l2: -0.1414
      Episode_Reward/feet_air_time: -0.0018
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7016
Metrics/base_velocity/error_vel_xy: 0.7500
Metrics/base_velocity/error_vel_yaw: 0.5001
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 9000000
                    Iteration time: 1.10s
                      Time elapsed: 00:04:35
                               ETA: 00:23:00

################################################################################
                     [1m Learning iteration 250/1500 [0m                      

                       Computation: 33860 steps/s (collection: 1.010s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0232
               Mean surrogate loss: -0.0084
                 Mean entropy loss: 13.0443
                       Mean reward: 12.89
               Mean episode length: 827.45
Episode_Reward/track_lin_vel_xy_exp: 0.7406
Episode_Reward/track_ang_vel_z_exp: 0.4677
       Episode_Reward/lin_vel_z_l2: -0.0385
      Episode_Reward/ang_vel_xy_l2: -0.1245
     Episode_Reward/dof_torques_l2: -0.0680
         Episode_Reward/dof_acc_l2: -0.1405
     Episode_Reward/action_rate_l2: -0.1431
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7077
Metrics/base_velocity/error_vel_xy: 0.7698
Metrics/base_velocity/error_vel_yaw: 0.5317
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 9036000
                    Iteration time: 1.06s
                      Time elapsed: 00:04:36
                               ETA: 00:22:59

################################################################################
                     [1m Learning iteration 251/1500 [0m                      

                       Computation: 31280 steps/s (collection: 1.096s, learning 0.055s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0272
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 13.0299
                       Mean reward: 12.54
               Mean episode length: 822.70
Episode_Reward/track_lin_vel_xy_exp: 0.6964
Episode_Reward/track_ang_vel_z_exp: 0.4103
       Episode_Reward/lin_vel_z_l2: -0.0362
      Episode_Reward/ang_vel_xy_l2: -0.1093
     Episode_Reward/dof_torques_l2: -0.0663
         Episode_Reward/dof_acc_l2: -0.1332
     Episode_Reward/action_rate_l2: -0.1286
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7102
Metrics/base_velocity/error_vel_xy: 0.6064
Metrics/base_velocity/error_vel_yaw: 0.4660
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 9072000
                    Iteration time: 1.15s
                      Time elapsed: 00:04:38
                               ETA: 00:22:58

################################################################################
                     [1m Learning iteration 252/1500 [0m                      

                       Computation: 30573 steps/s (collection: 1.124s, learning 0.054s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0206
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 13.0250
                       Mean reward: 12.45
               Mean episode length: 809.75
Episode_Reward/track_lin_vel_xy_exp: 0.7160
Episode_Reward/track_ang_vel_z_exp: 0.4561
       Episode_Reward/lin_vel_z_l2: -0.0396
      Episode_Reward/ang_vel_xy_l2: -0.1228
     Episode_Reward/dof_torques_l2: -0.0692
         Episode_Reward/dof_acc_l2: -0.1440
     Episode_Reward/action_rate_l2: -0.1403
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7148
Metrics/base_velocity/error_vel_xy: 0.7342
Metrics/base_velocity/error_vel_yaw: 0.5059
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 9108000
                    Iteration time: 1.18s
                      Time elapsed: 00:04:39
                               ETA: 00:22:57

################################################################################
                     [1m Learning iteration 253/1500 [0m                      

                       Computation: 32659 steps/s (collection: 1.050s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0211
               Mean surrogate loss: -0.0087
                 Mean entropy loss: 13.0194
                       Mean reward: 12.49
               Mean episode length: 805.12
Episode_Reward/track_lin_vel_xy_exp: 0.6999
Episode_Reward/track_ang_vel_z_exp: 0.4245
       Episode_Reward/lin_vel_z_l2: -0.0371
      Episode_Reward/ang_vel_xy_l2: -0.1068
     Episode_Reward/dof_torques_l2: -0.0640
         Episode_Reward/dof_acc_l2: -0.1301
     Episode_Reward/action_rate_l2: -0.1306
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7208
Metrics/base_velocity/error_vel_xy: 0.6480
Metrics/base_velocity/error_vel_yaw: 0.4886
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 9144000
                    Iteration time: 1.10s
                      Time elapsed: 00:04:40
                               ETA: 00:22:56

################################################################################
                     [1m Learning iteration 254/1500 [0m                      

                       Computation: 32322 steps/s (collection: 1.062s, learning 0.052s)
             Mean action noise std: 0.72
          Mean value_function loss: 0.0197
               Mean surrogate loss: -0.0090
                 Mean entropy loss: 13.0066
                       Mean reward: 13.13
               Mean episode length: 837.40
Episode_Reward/track_lin_vel_xy_exp: 0.7335
Episode_Reward/track_ang_vel_z_exp: 0.4591
       Episode_Reward/lin_vel_z_l2: -0.0413
      Episode_Reward/ang_vel_xy_l2: -0.1210
     Episode_Reward/dof_torques_l2: -0.0676
         Episode_Reward/dof_acc_l2: -0.1485
     Episode_Reward/action_rate_l2: -0.1428
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7293
Metrics/base_velocity/error_vel_xy: 0.7185
Metrics/base_velocity/error_vel_yaw: 0.5223
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 9180000
                    Iteration time: 1.11s
                      Time elapsed: 00:04:41
                               ETA: 00:22:55

################################################################################
                     [1m Learning iteration 255/1500 [0m                      

                       Computation: 31476 steps/s (collection: 1.091s, learning 0.052s)
             Mean action noise std: 0.72
          Mean value_function loss: 0.0268
               Mean surrogate loss: -0.0101
                 Mean entropy loss: 12.9904
                       Mean reward: 12.83
               Mean episode length: 852.87
Episode_Reward/track_lin_vel_xy_exp: 0.6140
Episode_Reward/track_ang_vel_z_exp: 0.4118
       Episode_Reward/lin_vel_z_l2: -0.0406
      Episode_Reward/ang_vel_xy_l2: -0.1239
     Episode_Reward/dof_torques_l2: -0.0655
         Episode_Reward/dof_acc_l2: -0.1460
     Episode_Reward/action_rate_l2: -0.1364
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7341
Metrics/base_velocity/error_vel_xy: 0.7812
Metrics/base_velocity/error_vel_yaw: 0.5397
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 9216000
                    Iteration time: 1.14s
                      Time elapsed: 00:04:42
                               ETA: 00:22:54

################################################################################
                     [1m Learning iteration 256/1500 [0m                      

                       Computation: 31335 steps/s (collection: 1.097s, learning 0.052s)
             Mean action noise std: 0.72
          Mean value_function loss: 0.0220
               Mean surrogate loss: -0.0094
                 Mean entropy loss: 12.9945
                       Mean reward: 12.83
               Mean episode length: 864.19
Episode_Reward/track_lin_vel_xy_exp: 0.7731
Episode_Reward/track_ang_vel_z_exp: 0.4749
       Episode_Reward/lin_vel_z_l2: -0.0420
      Episode_Reward/ang_vel_xy_l2: -0.1193
     Episode_Reward/dof_torques_l2: -0.0702
         Episode_Reward/dof_acc_l2: -0.1530
     Episode_Reward/action_rate_l2: -0.1465
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7405
Metrics/base_velocity/error_vel_xy: 0.7246
Metrics/base_velocity/error_vel_yaw: 0.5220
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 9252000
                    Iteration time: 1.15s
                      Time elapsed: 00:04:43
                               ETA: 00:22:53

################################################################################
                     [1m Learning iteration 257/1500 [0m                      

                       Computation: 31020 steps/s (collection: 1.107s, learning 0.054s)
             Mean action noise std: 0.72
          Mean value_function loss: 0.0219
               Mean surrogate loss: -0.0087
                 Mean entropy loss: 12.9975
                       Mean reward: 12.71
               Mean episode length: 840.06
Episode_Reward/track_lin_vel_xy_exp: 0.5701
Episode_Reward/track_ang_vel_z_exp: 0.3988
       Episode_Reward/lin_vel_z_l2: -0.0359
      Episode_Reward/ang_vel_xy_l2: -0.1055
     Episode_Reward/dof_torques_l2: -0.0594
         Episode_Reward/dof_acc_l2: -0.1169
     Episode_Reward/action_rate_l2: -0.1211
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7437
Metrics/base_velocity/error_vel_xy: 0.7036
Metrics/base_velocity/error_vel_yaw: 0.4531
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 9288000
                    Iteration time: 1.16s
                      Time elapsed: 00:04:44
                               ETA: 00:22:52

################################################################################
                     [1m Learning iteration 258/1500 [0m                      

                       Computation: 30968 steps/s (collection: 1.108s, learning 0.055s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0206
               Mean surrogate loss: -0.0092
                 Mean entropy loss: 13.0093
                       Mean reward: 12.70
               Mean episode length: 825.15
Episode_Reward/track_lin_vel_xy_exp: 0.6536
Episode_Reward/track_ang_vel_z_exp: 0.4150
       Episode_Reward/lin_vel_z_l2: -0.0409
      Episode_Reward/ang_vel_xy_l2: -0.1084
     Episode_Reward/dof_torques_l2: -0.0642
         Episode_Reward/dof_acc_l2: -0.1315
     Episode_Reward/action_rate_l2: -0.1285
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7506
Metrics/base_velocity/error_vel_xy: 0.6661
Metrics/base_velocity/error_vel_yaw: 0.4547
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 9324000
                    Iteration time: 1.16s
                      Time elapsed: 00:04:46
                               ETA: 00:22:51

################################################################################
                     [1m Learning iteration 259/1500 [0m                      

                       Computation: 32547 steps/s (collection: 1.055s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0221
               Mean surrogate loss: -0.0085
                 Mean entropy loss: 13.0177
                       Mean reward: 12.56
               Mean episode length: 804.30
Episode_Reward/track_lin_vel_xy_exp: 0.7471
Episode_Reward/track_ang_vel_z_exp: 0.4237
       Episode_Reward/lin_vel_z_l2: -0.0447
      Episode_Reward/ang_vel_xy_l2: -0.1178
     Episode_Reward/dof_torques_l2: -0.0663
         Episode_Reward/dof_acc_l2: -0.1523
     Episode_Reward/action_rate_l2: -0.1340
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7582
Metrics/base_velocity/error_vel_xy: 0.5826
Metrics/base_velocity/error_vel_yaw: 0.4718
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 9360000
                    Iteration time: 1.11s
                      Time elapsed: 00:04:47
                               ETA: 00:22:50

################################################################################
                     [1m Learning iteration 260/1500 [0m                      

                       Computation: 33530 steps/s (collection: 1.024s, learning 0.050s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0232
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 13.0275
                       Mean reward: 13.16
               Mean episode length: 807.33
Episode_Reward/track_lin_vel_xy_exp: 0.8202
Episode_Reward/track_ang_vel_z_exp: 0.4547
       Episode_Reward/lin_vel_z_l2: -0.0364
      Episode_Reward/ang_vel_xy_l2: -0.1128
     Episode_Reward/dof_torques_l2: -0.0665
         Episode_Reward/dof_acc_l2: -0.1312
     Episode_Reward/action_rate_l2: -0.1356
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7669
Metrics/base_velocity/error_vel_xy: 0.5737
Metrics/base_velocity/error_vel_yaw: 0.4798
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 9396000
                    Iteration time: 1.07s
                      Time elapsed: 00:04:48
                               ETA: 00:22:49

################################################################################
                     [1m Learning iteration 261/1500 [0m                      

                       Computation: 31884 steps/s (collection: 1.077s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0250
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 13.0233
                       Mean reward: 12.94
               Mean episode length: 829.29
Episode_Reward/track_lin_vel_xy_exp: 0.6975
Episode_Reward/track_ang_vel_z_exp: 0.4272
       Episode_Reward/lin_vel_z_l2: -0.0452
      Episode_Reward/ang_vel_xy_l2: -0.1218
     Episode_Reward/dof_torques_l2: -0.0686
         Episode_Reward/dof_acc_l2: -0.1558
     Episode_Reward/action_rate_l2: -0.1410
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7759
Metrics/base_velocity/error_vel_xy: 0.7064
Metrics/base_velocity/error_vel_yaw: 0.5305
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 9432000
                    Iteration time: 1.13s
                      Time elapsed: 00:04:49
                               ETA: 00:22:48

################################################################################
                     [1m Learning iteration 262/1500 [0m                      

                       Computation: 31591 steps/s (collection: 1.082s, learning 0.058s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0226
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 13.0196
                       Mean reward: 11.88
               Mean episode length: 830.56
Episode_Reward/track_lin_vel_xy_exp: 0.5824
Episode_Reward/track_ang_vel_z_exp: 0.4586
       Episode_Reward/lin_vel_z_l2: -0.0380
      Episode_Reward/ang_vel_xy_l2: -0.1193
     Episode_Reward/dof_torques_l2: -0.0700
         Episode_Reward/dof_acc_l2: -0.1423
     Episode_Reward/action_rate_l2: -0.1434
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7872
Metrics/base_velocity/error_vel_xy: 0.9275
Metrics/base_velocity/error_vel_yaw: 0.5232
      Episode_Termination/time_out: 1.9167
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 9468000
                    Iteration time: 1.14s
                      Time elapsed: 00:04:50
                               ETA: 00:22:47

################################################################################
                     [1m Learning iteration 263/1500 [0m                      

                       Computation: 32602 steps/s (collection: 1.051s, learning 0.054s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0211
               Mean surrogate loss: -0.0095
                 Mean entropy loss: 13.0178
                       Mean reward: 12.05
               Mean episode length: 840.10
Episode_Reward/track_lin_vel_xy_exp: 0.7208
Episode_Reward/track_ang_vel_z_exp: 0.4432
       Episode_Reward/lin_vel_z_l2: -0.0414
      Episode_Reward/ang_vel_xy_l2: -0.1179
     Episode_Reward/dof_torques_l2: -0.0739
         Episode_Reward/dof_acc_l2: -0.1471
     Episode_Reward/action_rate_l2: -0.1408
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.7954
Metrics/base_velocity/error_vel_xy: 0.7206
Metrics/base_velocity/error_vel_yaw: 0.5166
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 9504000
                    Iteration time: 1.10s
                      Time elapsed: 00:04:51
                               ETA: 00:22:46

################################################################################
                     [1m Learning iteration 264/1500 [0m                      

                       Computation: 33384 steps/s (collection: 1.025s, learning 0.054s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0223
               Mean surrogate loss: -0.0088
                 Mean entropy loss: 13.0149
                       Mean reward: 12.94
               Mean episode length: 861.55
Episode_Reward/track_lin_vel_xy_exp: 0.7906
Episode_Reward/track_ang_vel_z_exp: 0.4648
       Episode_Reward/lin_vel_z_l2: -0.0389
      Episode_Reward/ang_vel_xy_l2: -0.1215
     Episode_Reward/dof_torques_l2: -0.0738
         Episode_Reward/dof_acc_l2: -0.1461
     Episode_Reward/action_rate_l2: -0.1450
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.8049
Metrics/base_velocity/error_vel_xy: 0.6886
Metrics/base_velocity/error_vel_yaw: 0.5300
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 9540000
                    Iteration time: 1.08s
                      Time elapsed: 00:04:52
                               ETA: 00:22:45

################################################################################
                     [1m Learning iteration 265/1500 [0m                      

                       Computation: 33757 steps/s (collection: 1.015s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0232
               Mean surrogate loss: -0.0088
                 Mean entropy loss: 13.0165
                       Mean reward: 13.62
               Mean episode length: 874.61
Episode_Reward/track_lin_vel_xy_exp: 0.7249
Episode_Reward/track_ang_vel_z_exp: 0.4590
       Episode_Reward/lin_vel_z_l2: -0.0433
      Episode_Reward/ang_vel_xy_l2: -0.1244
     Episode_Reward/dof_torques_l2: -0.0672
         Episode_Reward/dof_acc_l2: -0.1430
     Episode_Reward/action_rate_l2: -0.1420
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.8102
Metrics/base_velocity/error_vel_xy: 0.7285
Metrics/base_velocity/error_vel_yaw: 0.5113
      Episode_Termination/time_out: 1.9583
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 9576000
                    Iteration time: 1.07s
                      Time elapsed: 00:04:53
                               ETA: 00:22:43

################################################################################
                     [1m Learning iteration 266/1500 [0m                      

                       Computation: 33239 steps/s (collection: 1.031s, learning 0.052s)
             Mean action noise std: 0.72
          Mean value_function loss: 0.0240
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 13.0009
                       Mean reward: 13.81
               Mean episode length: 864.16
Episode_Reward/track_lin_vel_xy_exp: 0.7307
Episode_Reward/track_ang_vel_z_exp: 0.4486
       Episode_Reward/lin_vel_z_l2: -0.0383
      Episode_Reward/ang_vel_xy_l2: -0.1137
     Episode_Reward/dof_torques_l2: -0.0668
         Episode_Reward/dof_acc_l2: -0.1322
     Episode_Reward/action_rate_l2: -0.1361
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.8121
Metrics/base_velocity/error_vel_xy: 0.6769
Metrics/base_velocity/error_vel_yaw: 0.4901
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 9612000
                    Iteration time: 1.08s
                      Time elapsed: 00:04:54
                               ETA: 00:22:42

################################################################################
                     [1m Learning iteration 267/1500 [0m                      

                       Computation: 32107 steps/s (collection: 1.067s, learning 0.054s)
             Mean action noise std: 0.72
          Mean value_function loss: 0.0207
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 12.9927
                       Mean reward: 14.13
               Mean episode length: 866.51
Episode_Reward/track_lin_vel_xy_exp: 0.7559
Episode_Reward/track_ang_vel_z_exp: 0.4580
       Episode_Reward/lin_vel_z_l2: -0.0415
      Episode_Reward/ang_vel_xy_l2: -0.1247
     Episode_Reward/dof_torques_l2: -0.0685
         Episode_Reward/dof_acc_l2: -0.1571
     Episode_Reward/action_rate_l2: -0.1425
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.8194
Metrics/base_velocity/error_vel_xy: 0.7040
Metrics/base_velocity/error_vel_yaw: 0.5041
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 9648000
                    Iteration time: 1.12s
                      Time elapsed: 00:04:55
                               ETA: 00:22:41

################################################################################
                     [1m Learning iteration 268/1500 [0m                      

                       Computation: 33059 steps/s (collection: 1.038s, learning 0.051s)
             Mean action noise std: 0.72
          Mean value_function loss: 0.0192
               Mean surrogate loss: -0.0088
                 Mean entropy loss: 12.9981
                       Mean reward: 12.38
               Mean episode length: 805.63
Episode_Reward/track_lin_vel_xy_exp: 0.6050
Episode_Reward/track_ang_vel_z_exp: 0.3864
       Episode_Reward/lin_vel_z_l2: -0.0364
      Episode_Reward/ang_vel_xy_l2: -0.1078
     Episode_Reward/dof_torques_l2: -0.0641
         Episode_Reward/dof_acc_l2: -0.1345
     Episode_Reward/action_rate_l2: -0.1244
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.8264
Metrics/base_velocity/error_vel_xy: 0.6409
Metrics/base_velocity/error_vel_yaw: 0.4586
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 9684000
                    Iteration time: 1.09s
                      Time elapsed: 00:04:57
                               ETA: 00:22:40

################################################################################
                     [1m Learning iteration 269/1500 [0m                      

                       Computation: 30815 steps/s (collection: 1.112s, learning 0.057s)
             Mean action noise std: 0.72
          Mean value_function loss: 0.0267
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 12.9983
                       Mean reward: 13.10
               Mean episode length: 836.51
Episode_Reward/track_lin_vel_xy_exp: 0.7329
Episode_Reward/track_ang_vel_z_exp: 0.4569
       Episode_Reward/lin_vel_z_l2: -0.0407
      Episode_Reward/ang_vel_xy_l2: -0.1177
     Episode_Reward/dof_torques_l2: -0.0727
         Episode_Reward/dof_acc_l2: -0.1522
     Episode_Reward/action_rate_l2: -0.1432
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.8347
Metrics/base_velocity/error_vel_xy: 0.7158
Metrics/base_velocity/error_vel_yaw: 0.5061
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 9720000
                    Iteration time: 1.17s
                      Time elapsed: 00:04:58
                               ETA: 00:22:39

################################################################################
                     [1m Learning iteration 270/1500 [0m                      

                       Computation: 32748 steps/s (collection: 1.048s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0267
               Mean surrogate loss: -0.0088
                 Mean entropy loss: 13.0169
                       Mean reward: 13.33
               Mean episode length: 847.63
Episode_Reward/track_lin_vel_xy_exp: 0.6445
Episode_Reward/track_ang_vel_z_exp: 0.4418
       Episode_Reward/lin_vel_z_l2: -0.0353
      Episode_Reward/ang_vel_xy_l2: -0.1075
     Episode_Reward/dof_torques_l2: -0.0676
         Episode_Reward/dof_acc_l2: -0.1303
     Episode_Reward/action_rate_l2: -0.1356
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.8456
Metrics/base_velocity/error_vel_xy: 0.7638
Metrics/base_velocity/error_vel_yaw: 0.5074
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 9756000
                    Iteration time: 1.10s
                      Time elapsed: 00:04:59
                               ETA: 00:22:38

################################################################################
                     [1m Learning iteration 271/1500 [0m                      

                       Computation: 30440 steps/s (collection: 1.130s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0237
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 13.0258
                       Mean reward: 13.03
               Mean episode length: 834.44
Episode_Reward/track_lin_vel_xy_exp: 0.7639
Episode_Reward/track_ang_vel_z_exp: 0.4691
       Episode_Reward/lin_vel_z_l2: -0.0406
      Episode_Reward/ang_vel_xy_l2: -0.1212
     Episode_Reward/dof_torques_l2: -0.0725
         Episode_Reward/dof_acc_l2: -0.1554
     Episode_Reward/action_rate_l2: -0.1458
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.8468
Metrics/base_velocity/error_vel_xy: 0.6984
Metrics/base_velocity/error_vel_yaw: 0.5130
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 9792000
                    Iteration time: 1.18s
                      Time elapsed: 00:05:00
                               ETA: 00:22:37

################################################################################
                     [1m Learning iteration 272/1500 [0m                      

                       Computation: 30655 steps/s (collection: 1.120s, learning 0.055s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0284
               Mean surrogate loss: -0.0096
                 Mean entropy loss: 13.0319
                       Mean reward: 12.06
               Mean episode length: 774.09
Episode_Reward/track_lin_vel_xy_exp: 0.6334
Episode_Reward/track_ang_vel_z_exp: 0.3792
       Episode_Reward/lin_vel_z_l2: -0.0374
      Episode_Reward/ang_vel_xy_l2: -0.1057
     Episode_Reward/dof_torques_l2: -0.0592
         Episode_Reward/dof_acc_l2: -0.1283
     Episode_Reward/action_rate_l2: -0.1205
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.8519
Metrics/base_velocity/error_vel_xy: 0.5980
Metrics/base_velocity/error_vel_yaw: 0.4463
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 9828000
                    Iteration time: 1.17s
                      Time elapsed: 00:05:01
                               ETA: 00:22:37

################################################################################
                     [1m Learning iteration 273/1500 [0m                      

                       Computation: 30828 steps/s (collection: 1.115s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0262
               Mean surrogate loss: -0.0101
                 Mean entropy loss: 13.0322
                       Mean reward: 11.11
               Mean episode length: 747.16
Episode_Reward/track_lin_vel_xy_exp: 0.7069
Episode_Reward/track_ang_vel_z_exp: 0.4170
       Episode_Reward/lin_vel_z_l2: -0.0388
      Episode_Reward/ang_vel_xy_l2: -0.1095
     Episode_Reward/dof_torques_l2: -0.0703
         Episode_Reward/dof_acc_l2: -0.1395
     Episode_Reward/action_rate_l2: -0.1331
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.8593
Metrics/base_velocity/error_vel_xy: 0.6473
Metrics/base_velocity/error_vel_yaw: 0.4887
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 9864000
                    Iteration time: 1.17s
                      Time elapsed: 00:05:02
                               ETA: 00:22:36

################################################################################
                     [1m Learning iteration 274/1500 [0m                      

                       Computation: 30607 steps/s (collection: 1.122s, learning 0.054s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0253
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 13.0359
                       Mean reward: 12.59
               Mean episode length: 792.90
Episode_Reward/track_lin_vel_xy_exp: 0.6916
Episode_Reward/track_ang_vel_z_exp: 0.4069
       Episode_Reward/lin_vel_z_l2: -0.0395
      Episode_Reward/ang_vel_xy_l2: -0.1051
     Episode_Reward/dof_torques_l2: -0.0640
         Episode_Reward/dof_acc_l2: -0.1307
     Episode_Reward/action_rate_l2: -0.1270
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.8656
Metrics/base_velocity/error_vel_xy: 0.5782
Metrics/base_velocity/error_vel_yaw: 0.4344
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 9900000
                    Iteration time: 1.18s
                      Time elapsed: 00:05:04
                               ETA: 00:22:35

################################################################################
                     [1m Learning iteration 275/1500 [0m                      

                       Computation: 30780 steps/s (collection: 1.117s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0194
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 13.0240
                       Mean reward: 12.53
               Mean episode length: 801.55
Episode_Reward/track_lin_vel_xy_exp: 0.7390
Episode_Reward/track_ang_vel_z_exp: 0.4246
       Episode_Reward/lin_vel_z_l2: -0.0388
      Episode_Reward/ang_vel_xy_l2: -0.1181
     Episode_Reward/dof_torques_l2: -0.0709
         Episode_Reward/dof_acc_l2: -0.1471
     Episode_Reward/action_rate_l2: -0.1369
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.8738
Metrics/base_velocity/error_vel_xy: 0.6529
Metrics/base_velocity/error_vel_yaw: 0.5282
      Episode_Termination/time_out: 1.0417
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 9936000
                    Iteration time: 1.17s
                      Time elapsed: 00:05:05
                               ETA: 00:22:34

################################################################################
                     [1m Learning iteration 276/1500 [0m                      

                       Computation: 33269 steps/s (collection: 1.031s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0255
               Mean surrogate loss: -0.0089
                 Mean entropy loss: 13.0201
                       Mean reward: 13.05
               Mean episode length: 827.97
Episode_Reward/track_lin_vel_xy_exp: 0.7505
Episode_Reward/track_ang_vel_z_exp: 0.4519
       Episode_Reward/lin_vel_z_l2: -0.0384
      Episode_Reward/ang_vel_xy_l2: -0.1203
     Episode_Reward/dof_torques_l2: -0.0721
         Episode_Reward/dof_acc_l2: -0.1527
     Episode_Reward/action_rate_l2: -0.1415
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.8820
Metrics/base_velocity/error_vel_xy: 0.7046
Metrics/base_velocity/error_vel_yaw: 0.5044
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 9972000
                    Iteration time: 1.08s
                      Time elapsed: 00:05:06
                               ETA: 00:22:33

################################################################################
                     [1m Learning iteration 277/1500 [0m                      

                       Computation: 31235 steps/s (collection: 1.100s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0242
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 13.0159
                       Mean reward: 12.05
               Mean episode length: 819.05
Episode_Reward/track_lin_vel_xy_exp: 0.6981
Episode_Reward/track_ang_vel_z_exp: 0.4111
       Episode_Reward/lin_vel_z_l2: -0.0401
      Episode_Reward/ang_vel_xy_l2: -0.1099
     Episode_Reward/dof_torques_l2: -0.0711
         Episode_Reward/dof_acc_l2: -0.1477
     Episode_Reward/action_rate_l2: -0.1352
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.8942
Metrics/base_velocity/error_vel_xy: 0.6400
Metrics/base_velocity/error_vel_yaw: 0.4985
      Episode_Termination/time_out: 2.0000
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 10008000
                    Iteration time: 1.15s
                      Time elapsed: 00:05:07
                               ETA: 00:22:32

################################################################################
                     [1m Learning iteration 278/1500 [0m                      

                       Computation: 31027 steps/s (collection: 1.107s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0231
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 13.0077
                       Mean reward: 12.34
               Mean episode length: 793.85
Episode_Reward/track_lin_vel_xy_exp: 0.7652
Episode_Reward/track_ang_vel_z_exp: 0.4398
       Episode_Reward/lin_vel_z_l2: -0.0386
      Episode_Reward/ang_vel_xy_l2: -0.1097
     Episode_Reward/dof_torques_l2: -0.0655
         Episode_Reward/dof_acc_l2: -0.1308
     Episode_Reward/action_rate_l2: -0.1319
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.8997
Metrics/base_velocity/error_vel_xy: 0.5820
Metrics/base_velocity/error_vel_yaw: 0.4634
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 10044000
                    Iteration time: 1.16s
                      Time elapsed: 00:05:08
                               ETA: 00:22:31

################################################################################
                     [1m Learning iteration 279/1500 [0m                      

                       Computation: 32842 steps/s (collection: 1.044s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0232
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 13.0087
                       Mean reward: 12.80
               Mean episode length: 784.89
Episode_Reward/track_lin_vel_xy_exp: 0.7247
Episode_Reward/track_ang_vel_z_exp: 0.4034
       Episode_Reward/lin_vel_z_l2: -0.0416
      Episode_Reward/ang_vel_xy_l2: -0.1131
     Episode_Reward/dof_torques_l2: -0.0663
         Episode_Reward/dof_acc_l2: -0.1518
     Episode_Reward/action_rate_l2: -0.1317
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9110
Metrics/base_velocity/error_vel_xy: 0.5660
Metrics/base_velocity/error_vel_yaw: 0.4801
      Episode_Termination/time_out: 2.4583
  Episode_Termination/base_contact: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 10080000
                    Iteration time: 1.10s
                      Time elapsed: 00:05:09
                               ETA: 00:22:30

################################################################################
                     [1m Learning iteration 280/1500 [0m                      

                       Computation: 33158 steps/s (collection: 1.035s, learning 0.050s)
             Mean action noise std: 0.72
          Mean value_function loss: 0.0218
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 13.0009
                       Mean reward: 12.47
               Mean episode length: 805.30
Episode_Reward/track_lin_vel_xy_exp: 0.7141
Episode_Reward/track_ang_vel_z_exp: 0.4085
       Episode_Reward/lin_vel_z_l2: -0.0424
      Episode_Reward/ang_vel_xy_l2: -0.1205
     Episode_Reward/dof_torques_l2: -0.0696
         Episode_Reward/dof_acc_l2: -0.1594
     Episode_Reward/action_rate_l2: -0.1350
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9227
Metrics/base_velocity/error_vel_xy: 0.6316
Metrics/base_velocity/error_vel_yaw: 0.5180
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 10116000
                    Iteration time: 1.09s
                      Time elapsed: 00:05:10
                               ETA: 00:22:29

################################################################################
                     [1m Learning iteration 281/1500 [0m                      

                       Computation: 33598 steps/s (collection: 1.019s, learning 0.053s)
             Mean action noise std: 0.72
          Mean value_function loss: 0.0252
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 12.9968
                       Mean reward: 12.13
               Mean episode length: 767.40
Episode_Reward/track_lin_vel_xy_exp: 0.5929
Episode_Reward/track_ang_vel_z_exp: 0.3410
       Episode_Reward/lin_vel_z_l2: -0.0367
      Episode_Reward/ang_vel_xy_l2: -0.0923
     Episode_Reward/dof_torques_l2: -0.0560
         Episode_Reward/dof_acc_l2: -0.1213
     Episode_Reward/action_rate_l2: -0.1100
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9338
Metrics/base_velocity/error_vel_xy: 0.4924
Metrics/base_velocity/error_vel_yaw: 0.3866
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 10152000
                    Iteration time: 1.07s
                      Time elapsed: 00:05:11
                               ETA: 00:22:28

################################################################################
                     [1m Learning iteration 282/1500 [0m                      

                       Computation: 32941 steps/s (collection: 1.042s, learning 0.051s)
             Mean action noise std: 0.72
          Mean value_function loss: 0.0254
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 12.9939
                       Mean reward: 11.49
               Mean episode length: 725.59
Episode_Reward/track_lin_vel_xy_exp: 0.5889
Episode_Reward/track_ang_vel_z_exp: 0.3501
       Episode_Reward/lin_vel_z_l2: -0.0328
      Episode_Reward/ang_vel_xy_l2: -0.0946
     Episode_Reward/dof_torques_l2: -0.0653
         Episode_Reward/dof_acc_l2: -0.1200
     Episode_Reward/action_rate_l2: -0.1162
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9419
Metrics/base_velocity/error_vel_xy: 0.5962
Metrics/base_velocity/error_vel_yaw: 0.4915
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 10188000
                    Iteration time: 1.09s
                      Time elapsed: 00:05:12
                               ETA: 00:22:26

################################################################################
                     [1m Learning iteration 283/1500 [0m                      

                       Computation: 31616 steps/s (collection: 1.086s, learning 0.053s)
             Mean action noise std: 0.72
          Mean value_function loss: 0.0253
               Mean surrogate loss: -0.0096
                 Mean entropy loss: 12.9925
                       Mean reward: 11.62
               Mean episode length: 768.94
Episode_Reward/track_lin_vel_xy_exp: 0.7497
Episode_Reward/track_ang_vel_z_exp: 0.4435
       Episode_Reward/lin_vel_z_l2: -0.0485
      Episode_Reward/ang_vel_xy_l2: -0.1281
     Episode_Reward/dof_torques_l2: -0.0722
         Episode_Reward/dof_acc_l2: -0.1797
     Episode_Reward/action_rate_l2: -0.1469
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9514
Metrics/base_velocity/error_vel_xy: 0.6967
Metrics/base_velocity/error_vel_yaw: 0.5374
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 10224000
                    Iteration time: 1.14s
                      Time elapsed: 00:05:14
                               ETA: 00:22:25

################################################################################
                     [1m Learning iteration 284/1500 [0m                      

                       Computation: 32644 steps/s (collection: 1.052s, learning 0.051s)
             Mean action noise std: 0.72
          Mean value_function loss: 0.0194
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 12.9885
                       Mean reward: 12.91
               Mean episode length: 824.34
Episode_Reward/track_lin_vel_xy_exp: 0.7013
Episode_Reward/track_ang_vel_z_exp: 0.4469
       Episode_Reward/lin_vel_z_l2: -0.0461
      Episode_Reward/ang_vel_xy_l2: -0.1247
     Episode_Reward/dof_torques_l2: -0.0739
         Episode_Reward/dof_acc_l2: -0.1638
     Episode_Reward/action_rate_l2: -0.1487
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9647
Metrics/base_velocity/error_vel_xy: 0.7819
Metrics/base_velocity/error_vel_yaw: 0.5571
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 10260000
                    Iteration time: 1.10s
                      Time elapsed: 00:05:15
                               ETA: 00:22:24

################################################################################
                     [1m Learning iteration 285/1500 [0m                      

                       Computation: 31885 steps/s (collection: 1.077s, learning 0.052s)
             Mean action noise std: 0.72
          Mean value_function loss: 0.0256
               Mean surrogate loss: -0.0091
                 Mean entropy loss: 12.9755
                       Mean reward: 12.65
               Mean episode length: 798.77
Episode_Reward/track_lin_vel_xy_exp: 0.7240
Episode_Reward/track_ang_vel_z_exp: 0.3834
       Episode_Reward/lin_vel_z_l2: -0.0386
      Episode_Reward/ang_vel_xy_l2: -0.1043
     Episode_Reward/dof_torques_l2: -0.0693
         Episode_Reward/dof_acc_l2: -0.1443
     Episode_Reward/action_rate_l2: -0.1267
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9734
Metrics/base_velocity/error_vel_xy: 0.5033
Metrics/base_velocity/error_vel_yaw: 0.4420
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 10296000
                    Iteration time: 1.13s
                      Time elapsed: 00:05:16
                               ETA: 00:22:23

################################################################################
                     [1m Learning iteration 286/1500 [0m                      

                       Computation: 31881 steps/s (collection: 1.076s, learning 0.053s)
             Mean action noise std: 0.72
          Mean value_function loss: 0.0271
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 12.9752
                       Mean reward: 12.40
               Mean episode length: 796.82
Episode_Reward/track_lin_vel_xy_exp: 0.6802
Episode_Reward/track_ang_vel_z_exp: 0.4234
       Episode_Reward/lin_vel_z_l2: -0.0437
      Episode_Reward/ang_vel_xy_l2: -0.1177
     Episode_Reward/dof_torques_l2: -0.0713
         Episode_Reward/dof_acc_l2: -0.1549
     Episode_Reward/action_rate_l2: -0.1411
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9805
Metrics/base_velocity/error_vel_xy: 0.7101
Metrics/base_velocity/error_vel_yaw: 0.5096
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 10332000
                    Iteration time: 1.13s
                      Time elapsed: 00:05:17
                               ETA: 00:22:22

################################################################################
                     [1m Learning iteration 287/1500 [0m                      

                       Computation: 32768 steps/s (collection: 1.045s, learning 0.054s)
             Mean action noise std: 0.72
          Mean value_function loss: 0.0268
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 12.9771
                       Mean reward: 12.30
               Mean episode length: 805.91
Episode_Reward/track_lin_vel_xy_exp: 0.7515
Episode_Reward/track_ang_vel_z_exp: 0.4095
       Episode_Reward/lin_vel_z_l2: -0.0415
      Episode_Reward/ang_vel_xy_l2: -0.1127
     Episode_Reward/dof_torques_l2: -0.0652
         Episode_Reward/dof_acc_l2: -0.1498
     Episode_Reward/action_rate_l2: -0.1311
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9891
Metrics/base_velocity/error_vel_xy: 0.5537
Metrics/base_velocity/error_vel_yaw: 0.4832
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 10368000
                    Iteration time: 1.10s
                      Time elapsed: 00:05:18
                               ETA: 00:22:21

################################################################################
                     [1m Learning iteration 288/1500 [0m                      

                       Computation: 33100 steps/s (collection: 1.034s, learning 0.053s)
             Mean action noise std: 0.72
          Mean value_function loss: 0.0266
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 12.9858
                       Mean reward: 13.60
               Mean episode length: 850.30
Episode_Reward/track_lin_vel_xy_exp: 0.7619
Episode_Reward/track_ang_vel_z_exp: 0.4290
       Episode_Reward/lin_vel_z_l2: -0.0406
      Episode_Reward/ang_vel_xy_l2: -0.1194
     Episode_Reward/dof_torques_l2: -0.0724
         Episode_Reward/dof_acc_l2: -0.1509
     Episode_Reward/action_rate_l2: -0.1379
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 0.9979
Metrics/base_velocity/error_vel_xy: 0.6164
Metrics/base_velocity/error_vel_yaw: 0.5003
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 10404000
                    Iteration time: 1.09s
                      Time elapsed: 00:05:19
                               ETA: 00:22:20

################################################################################
                     [1m Learning iteration 289/1500 [0m                      

                       Computation: 33371 steps/s (collection: 1.026s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0276
               Mean surrogate loss: -0.0095
                 Mean entropy loss: 13.0018
                       Mean reward: 13.99
               Mean episode length: 838.12
Episode_Reward/track_lin_vel_xy_exp: 0.8148
Episode_Reward/track_ang_vel_z_exp: 0.4358
       Episode_Reward/lin_vel_z_l2: -0.0428
      Episode_Reward/ang_vel_xy_l2: -0.1169
     Episode_Reward/dof_torques_l2: -0.0764
         Episode_Reward/dof_acc_l2: -0.1626
     Episode_Reward/action_rate_l2: -0.1433
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0016
Metrics/base_velocity/error_vel_xy: 0.5908
Metrics/base_velocity/error_vel_yaw: 0.5220
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 10440000
                    Iteration time: 1.08s
                      Time elapsed: 00:05:20
                               ETA: 00:22:19

################################################################################
                     [1m Learning iteration 290/1500 [0m                      

                       Computation: 32100 steps/s (collection: 1.068s, learning 0.054s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0240
               Mean surrogate loss: -0.0079
                 Mean entropy loss: 13.0232
                       Mean reward: 12.05
               Mean episode length: 778.51
Episode_Reward/track_lin_vel_xy_exp: 0.6887
Episode_Reward/track_ang_vel_z_exp: 0.3730
       Episode_Reward/lin_vel_z_l2: -0.0428
      Episode_Reward/ang_vel_xy_l2: -0.1120
     Episode_Reward/dof_torques_l2: -0.0694
         Episode_Reward/dof_acc_l2: -0.1526
     Episode_Reward/action_rate_l2: -0.1290
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0086
Metrics/base_velocity/error_vel_xy: 0.5655
Metrics/base_velocity/error_vel_yaw: 0.5161
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 10476000
                    Iteration time: 1.12s
                      Time elapsed: 00:05:21
                               ETA: 00:22:18

################################################################################
                     [1m Learning iteration 291/1500 [0m                      

                       Computation: 33018 steps/s (collection: 1.037s, learning 0.054s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0270
               Mean surrogate loss: -0.0101
                 Mean entropy loss: 13.0263
                       Mean reward: 11.85
               Mean episode length: 783.77
Episode_Reward/track_lin_vel_xy_exp: 0.7933
Episode_Reward/track_ang_vel_z_exp: 0.4469
       Episode_Reward/lin_vel_z_l2: -0.0409
      Episode_Reward/ang_vel_xy_l2: -0.1200
     Episode_Reward/dof_torques_l2: -0.0751
         Episode_Reward/dof_acc_l2: -0.1638
     Episode_Reward/action_rate_l2: -0.1466
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0153
Metrics/base_velocity/error_vel_xy: 0.6534
Metrics/base_velocity/error_vel_yaw: 0.5230
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 10512000
                    Iteration time: 1.09s
                      Time elapsed: 00:05:22
                               ETA: 00:22:17

################################################################################
                     [1m Learning iteration 292/1500 [0m                      

                       Computation: 33156 steps/s (collection: 1.034s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0219
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 13.0310
                       Mean reward: 11.80
               Mean episode length: 780.71
Episode_Reward/track_lin_vel_xy_exp: 0.6675
Episode_Reward/track_ang_vel_z_exp: 0.3729
       Episode_Reward/lin_vel_z_l2: -0.0390
      Episode_Reward/ang_vel_xy_l2: -0.1034
     Episode_Reward/dof_torques_l2: -0.0582
         Episode_Reward/dof_acc_l2: -0.1254
     Episode_Reward/action_rate_l2: -0.1189
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0200
Metrics/base_velocity/error_vel_xy: 0.5376
Metrics/base_velocity/error_vel_yaw: 0.4511
      Episode_Termination/time_out: 0.8750
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 10548000
                    Iteration time: 1.09s
                      Time elapsed: 00:05:24
                               ETA: 00:22:15

################################################################################
                     [1m Learning iteration 293/1500 [0m                      

                       Computation: 33623 steps/s (collection: 1.020s, learning 0.050s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0244
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 13.0302
                       Mean reward: 11.62
               Mean episode length: 816.01
Episode_Reward/track_lin_vel_xy_exp: 0.6475
Episode_Reward/track_ang_vel_z_exp: 0.4004
       Episode_Reward/lin_vel_z_l2: -0.0421
      Episode_Reward/ang_vel_xy_l2: -0.1147
     Episode_Reward/dof_torques_l2: -0.0708
         Episode_Reward/dof_acc_l2: -0.1558
     Episode_Reward/action_rate_l2: -0.1361
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0210
Metrics/base_velocity/error_vel_xy: 0.7039
Metrics/base_velocity/error_vel_yaw: 0.5316
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 10584000
                    Iteration time: 1.07s
                      Time elapsed: 00:05:25
                               ETA: 00:22:14

################################################################################
                     [1m Learning iteration 294/1500 [0m                      

                       Computation: 33431 steps/s (collection: 1.026s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0295
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 13.0381
                       Mean reward: 11.95
               Mean episode length: 805.14
Episode_Reward/track_lin_vel_xy_exp: 0.7654
Episode_Reward/track_ang_vel_z_exp: 0.4107
       Episode_Reward/lin_vel_z_l2: -0.0415
      Episode_Reward/ang_vel_xy_l2: -0.1148
     Episode_Reward/dof_torques_l2: -0.0695
         Episode_Reward/dof_acc_l2: -0.1578
     Episode_Reward/action_rate_l2: -0.1370
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0291
Metrics/base_velocity/error_vel_xy: 0.5869
Metrics/base_velocity/error_vel_yaw: 0.5150
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 10620000
                    Iteration time: 1.08s
                      Time elapsed: 00:05:26
                               ETA: 00:22:13

################################################################################
                     [1m Learning iteration 295/1500 [0m                      

                       Computation: 34197 steps/s (collection: 1.003s, learning 0.050s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0264
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 13.0477
                       Mean reward: 11.68
               Mean episode length: 759.18
Episode_Reward/track_lin_vel_xy_exp: 0.6695
Episode_Reward/track_ang_vel_z_exp: 0.4060
       Episode_Reward/lin_vel_z_l2: -0.0392
      Episode_Reward/ang_vel_xy_l2: -0.1100
     Episode_Reward/dof_torques_l2: -0.0690
         Episode_Reward/dof_acc_l2: -0.1506
     Episode_Reward/action_rate_l2: -0.1337
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0332
Metrics/base_velocity/error_vel_xy: 0.6494
Metrics/base_velocity/error_vel_yaw: 0.4949
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 10656000
                    Iteration time: 1.05s
                      Time elapsed: 00:05:27
                               ETA: 00:22:12

################################################################################
                     [1m Learning iteration 296/1500 [0m                      

                       Computation: 33520 steps/s (collection: 1.022s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0214
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 13.0470
                       Mean reward: 11.71
               Mean episode length: 764.35
Episode_Reward/track_lin_vel_xy_exp: 0.7146
Episode_Reward/track_ang_vel_z_exp: 0.4393
       Episode_Reward/lin_vel_z_l2: -0.0417
      Episode_Reward/ang_vel_xy_l2: -0.1178
     Episode_Reward/dof_torques_l2: -0.0691
         Episode_Reward/dof_acc_l2: -0.1517
     Episode_Reward/action_rate_l2: -0.1398
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0372
Metrics/base_velocity/error_vel_xy: 0.6746
Metrics/base_velocity/error_vel_yaw: 0.4927
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 10692000
                    Iteration time: 1.07s
                      Time elapsed: 00:05:28
                               ETA: 00:22:10

################################################################################
                     [1m Learning iteration 297/1500 [0m                      

                       Computation: 31998 steps/s (collection: 1.072s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0218
               Mean surrogate loss: -0.0105
                 Mean entropy loss: 13.0572
                       Mean reward: 12.24
               Mean episode length: 806.59
Episode_Reward/track_lin_vel_xy_exp: 0.7030
Episode_Reward/track_ang_vel_z_exp: 0.3957
       Episode_Reward/lin_vel_z_l2: -0.0427
      Episode_Reward/ang_vel_xy_l2: -0.1137
     Episode_Reward/dof_torques_l2: -0.0692
         Episode_Reward/dof_acc_l2: -0.1556
     Episode_Reward/action_rate_l2: -0.1344
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0422
Metrics/base_velocity/error_vel_xy: 0.6154
Metrics/base_velocity/error_vel_yaw: 0.5293
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 10728000
                    Iteration time: 1.13s
                      Time elapsed: 00:05:29
                               ETA: 00:22:09

################################################################################
                     [1m Learning iteration 298/1500 [0m                      

                       Computation: 33290 steps/s (collection: 1.029s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0254
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 13.0728
                       Mean reward: 13.32
               Mean episode length: 825.45
Episode_Reward/track_lin_vel_xy_exp: 0.7858
Episode_Reward/track_ang_vel_z_exp: 0.4348
       Episode_Reward/lin_vel_z_l2: -0.0419
      Episode_Reward/ang_vel_xy_l2: -0.1197
     Episode_Reward/dof_torques_l2: -0.0768
         Episode_Reward/dof_acc_l2: -0.1577
     Episode_Reward/action_rate_l2: -0.1443
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0464
Metrics/base_velocity/error_vel_xy: 0.6461
Metrics/base_velocity/error_vel_yaw: 0.5589
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 10764000
                    Iteration time: 1.08s
                      Time elapsed: 00:05:30
                               ETA: 00:22:08

################################################################################
                     [1m Learning iteration 299/1500 [0m                      

                       Computation: 33803 steps/s (collection: 1.011s, learning 0.054s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0299
               Mean surrogate loss: -0.0101
                 Mean entropy loss: 13.0940
                       Mean reward: 13.18
               Mean episode length: 811.76
Episode_Reward/track_lin_vel_xy_exp: 0.7072
Episode_Reward/track_ang_vel_z_exp: 0.3861
       Episode_Reward/lin_vel_z_l2: -0.0428
      Episode_Reward/ang_vel_xy_l2: -0.1107
     Episode_Reward/dof_torques_l2: -0.0653
         Episode_Reward/dof_acc_l2: -0.1569
     Episode_Reward/action_rate_l2: -0.1300
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0596
Metrics/base_velocity/error_vel_xy: 0.5494
Metrics/base_velocity/error_vel_yaw: 0.4808
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 10800000
                    Iteration time: 1.06s
                      Time elapsed: 00:05:31
                               ETA: 00:22:07

################################################################################
                     [1m Learning iteration 300/1500 [0m                      

                       Computation: 33525 steps/s (collection: 1.022s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0247
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 13.1106
                       Mean reward: 13.34
               Mean episode length: 822.47
Episode_Reward/track_lin_vel_xy_exp: 0.7604
Episode_Reward/track_ang_vel_z_exp: 0.4111
       Episode_Reward/lin_vel_z_l2: -0.0476
      Episode_Reward/ang_vel_xy_l2: -0.1138
     Episode_Reward/dof_torques_l2: -0.0735
         Episode_Reward/dof_acc_l2: -0.1638
     Episode_Reward/action_rate_l2: -0.1397
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0699
Metrics/base_velocity/error_vel_xy: 0.5960
Metrics/base_velocity/error_vel_yaw: 0.5187
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 10836000
                    Iteration time: 1.07s
                      Time elapsed: 00:05:32
                               ETA: 00:22:06

################################################################################
                     [1m Learning iteration 301/1500 [0m                      

                       Computation: 33570 steps/s (collection: 1.019s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0200
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 13.1125
                       Mean reward: 12.96
               Mean episode length: 792.80
Episode_Reward/track_lin_vel_xy_exp: 0.6373
Episode_Reward/track_ang_vel_z_exp: 0.3736
       Episode_Reward/lin_vel_z_l2: -0.0409
      Episode_Reward/ang_vel_xy_l2: -0.1116
     Episode_Reward/dof_torques_l2: -0.0670
         Episode_Reward/dof_acc_l2: -0.1440
     Episode_Reward/action_rate_l2: -0.1248
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0724
Metrics/base_velocity/error_vel_xy: 0.6100
Metrics/base_velocity/error_vel_yaw: 0.4723
      Episode_Termination/time_out: 1.0000
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 10872000
                    Iteration time: 1.07s
                      Time elapsed: 00:05:33
                               ETA: 00:22:04

################################################################################
                     [1m Learning iteration 302/1500 [0m                      

                       Computation: 32019 steps/s (collection: 1.073s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0230
               Mean surrogate loss: -0.0094
                 Mean entropy loss: 13.1115
                       Mean reward: 12.41
               Mean episode length: 761.33
Episode_Reward/track_lin_vel_xy_exp: 0.6617
Episode_Reward/track_ang_vel_z_exp: 0.3510
       Episode_Reward/lin_vel_z_l2: -0.0373
      Episode_Reward/ang_vel_xy_l2: -0.1059
     Episode_Reward/dof_torques_l2: -0.0633
         Episode_Reward/dof_acc_l2: -0.1372
     Episode_Reward/action_rate_l2: -0.1210
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0730
Metrics/base_velocity/error_vel_xy: 0.5305
Metrics/base_velocity/error_vel_yaw: 0.5015
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 10908000
                    Iteration time: 1.12s
                      Time elapsed: 00:05:34
                               ETA: 00:22:03

################################################################################
                     [1m Learning iteration 303/1500 [0m                      

                       Computation: 32334 steps/s (collection: 1.060s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0251
               Mean surrogate loss: -0.0092
                 Mean entropy loss: 13.1032
                       Mean reward: 11.73
               Mean episode length: 757.55
Episode_Reward/track_lin_vel_xy_exp: 0.6311
Episode_Reward/track_ang_vel_z_exp: 0.3596
       Episode_Reward/lin_vel_z_l2: -0.0354
      Episode_Reward/ang_vel_xy_l2: -0.0958
     Episode_Reward/dof_torques_l2: -0.0625
         Episode_Reward/dof_acc_l2: -0.1306
     Episode_Reward/action_rate_l2: -0.1184
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0771
Metrics/base_velocity/error_vel_xy: 0.5382
Metrics/base_velocity/error_vel_yaw: 0.4294
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 10944000
                    Iteration time: 1.11s
                      Time elapsed: 00:05:35
                               ETA: 00:22:02

################################################################################
                     [1m Learning iteration 304/1500 [0m                      

                       Computation: 32923 steps/s (collection: 1.041s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0232
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 13.1058
                       Mean reward: 12.27
               Mean episode length: 793.98
Episode_Reward/track_lin_vel_xy_exp: 0.7420
Episode_Reward/track_ang_vel_z_exp: 0.4272
       Episode_Reward/lin_vel_z_l2: -0.0432
      Episode_Reward/ang_vel_xy_l2: -0.1163
     Episode_Reward/dof_torques_l2: -0.0705
         Episode_Reward/dof_acc_l2: -0.1651
     Episode_Reward/action_rate_l2: -0.1419
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0837
Metrics/base_velocity/error_vel_xy: 0.6367
Metrics/base_velocity/error_vel_yaw: 0.5134
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 10980000
                    Iteration time: 1.09s
                      Time elapsed: 00:05:37
                               ETA: 00:22:01

################################################################################
                     [1m Learning iteration 305/1500 [0m                      

                       Computation: 32825 steps/s (collection: 1.043s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0242
               Mean surrogate loss: -0.0101
                 Mean entropy loss: 13.1041
                       Mean reward: 12.25
               Mean episode length: 802.13
Episode_Reward/track_lin_vel_xy_exp: 0.6976
Episode_Reward/track_ang_vel_z_exp: 0.4069
       Episode_Reward/lin_vel_z_l2: -0.0437
      Episode_Reward/ang_vel_xy_l2: -0.1158
     Episode_Reward/dof_torques_l2: -0.0748
         Episode_Reward/dof_acc_l2: -0.1575
     Episode_Reward/action_rate_l2: -0.1361
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.0917
Metrics/base_velocity/error_vel_xy: 0.6145
Metrics/base_velocity/error_vel_yaw: 0.4883
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 11016000
                    Iteration time: 1.10s
                      Time elapsed: 00:05:38
                               ETA: 00:22:00

################################################################################
                     [1m Learning iteration 306/1500 [0m                      

                       Computation: 32873 steps/s (collection: 1.044s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0239
               Mean surrogate loss: -0.0101
                 Mean entropy loss: 13.1055
                       Mean reward: 13.51
               Mean episode length: 837.80
Episode_Reward/track_lin_vel_xy_exp: 0.9270
Episode_Reward/track_ang_vel_z_exp: 0.4741
       Episode_Reward/lin_vel_z_l2: -0.0470
      Episode_Reward/ang_vel_xy_l2: -0.1216
     Episode_Reward/dof_torques_l2: -0.0778
         Episode_Reward/dof_acc_l2: -0.1747
     Episode_Reward/action_rate_l2: -0.1543
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.1007
Metrics/base_velocity/error_vel_xy: 0.5598
Metrics/base_velocity/error_vel_yaw: 0.5251
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 11052000
                    Iteration time: 1.10s
                      Time elapsed: 00:05:39
                               ETA: 00:21:59

################################################################################
                     [1m Learning iteration 307/1500 [0m                      

                       Computation: 32170 steps/s (collection: 1.066s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0237
               Mean surrogate loss: -0.0089
                 Mean entropy loss: 13.1151
                       Mean reward: 14.26
               Mean episode length: 875.99
Episode_Reward/track_lin_vel_xy_exp: 0.8994
Episode_Reward/track_ang_vel_z_exp: 0.4647
       Episode_Reward/lin_vel_z_l2: -0.0532
      Episode_Reward/ang_vel_xy_l2: -0.1305
     Episode_Reward/dof_torques_l2: -0.0817
         Episode_Reward/dof_acc_l2: -0.1969
     Episode_Reward/action_rate_l2: -0.1601
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.1131
Metrics/base_velocity/error_vel_xy: 0.5899
Metrics/base_velocity/error_vel_yaw: 0.5431
      Episode_Termination/time_out: 2.0833
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 11088000
                    Iteration time: 1.12s
                      Time elapsed: 00:05:40
                               ETA: 00:21:58

################################################################################
                     [1m Learning iteration 308/1500 [0m                      

                       Computation: 33494 steps/s (collection: 1.024s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0272
               Mean surrogate loss: -0.0101
                 Mean entropy loss: 13.1308
                       Mean reward: 14.05
               Mean episode length: 831.37
Episode_Reward/track_lin_vel_xy_exp: 0.7956
Episode_Reward/track_ang_vel_z_exp: 0.4141
       Episode_Reward/lin_vel_z_l2: -0.0424
      Episode_Reward/ang_vel_xy_l2: -0.1149
     Episode_Reward/dof_torques_l2: -0.0678
         Episode_Reward/dof_acc_l2: -0.1535
     Episode_Reward/action_rate_l2: -0.1363
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.1256
Metrics/base_velocity/error_vel_xy: 0.5278
Metrics/base_velocity/error_vel_yaw: 0.5091
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 11124000
                    Iteration time: 1.07s
                      Time elapsed: 00:05:41
                               ETA: 00:21:57

################################################################################
                     [1m Learning iteration 309/1500 [0m                      

                       Computation: 32241 steps/s (collection: 1.061s, learning 0.055s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0224
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 13.1243
                       Mean reward: 13.36
               Mean episode length: 806.58
Episode_Reward/track_lin_vel_xy_exp: 0.8088
Episode_Reward/track_ang_vel_z_exp: 0.4097
       Episode_Reward/lin_vel_z_l2: -0.0477
      Episode_Reward/ang_vel_xy_l2: -0.1170
     Episode_Reward/dof_torques_l2: -0.0731
         Episode_Reward/dof_acc_l2: -0.1786
     Episode_Reward/action_rate_l2: -0.1429
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.1280
Metrics/base_velocity/error_vel_xy: 0.5025
Metrics/base_velocity/error_vel_yaw: 0.4731
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 11160000
                    Iteration time: 1.12s
                      Time elapsed: 00:05:42
                               ETA: 00:21:55

################################################################################
                     [1m Learning iteration 310/1500 [0m                      

                       Computation: 31562 steps/s (collection: 1.086s, learning 0.055s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0230
               Mean surrogate loss: -0.0095
                 Mean entropy loss: 13.1002
                       Mean reward: 14.02
               Mean episode length: 834.45
Episode_Reward/track_lin_vel_xy_exp: 0.9351
Episode_Reward/track_ang_vel_z_exp: 0.4826
       Episode_Reward/lin_vel_z_l2: -0.0501
      Episode_Reward/ang_vel_xy_l2: -0.1333
     Episode_Reward/dof_torques_l2: -0.0848
         Episode_Reward/dof_acc_l2: -0.1912
     Episode_Reward/action_rate_l2: -0.1611
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.1378
Metrics/base_velocity/error_vel_xy: 0.5759
Metrics/base_velocity/error_vel_yaw: 0.5356
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 11196000
                    Iteration time: 1.14s
                      Time elapsed: 00:05:43
                               ETA: 00:21:55

################################################################################
                     [1m Learning iteration 311/1500 [0m                      

                       Computation: 31957 steps/s (collection: 1.076s, learning 0.050s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0242
               Mean surrogate loss: -0.0097
                 Mean entropy loss: 13.0857
                       Mean reward: 14.08
               Mean episode length: 843.04
Episode_Reward/track_lin_vel_xy_exp: 0.7703
Episode_Reward/track_ang_vel_z_exp: 0.4226
       Episode_Reward/lin_vel_z_l2: -0.0423
      Episode_Reward/ang_vel_xy_l2: -0.1152
     Episode_Reward/dof_torques_l2: -0.0764
         Episode_Reward/dof_acc_l2: -0.1613
     Episode_Reward/action_rate_l2: -0.1422
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.1505
Metrics/base_velocity/error_vel_xy: 0.5973
Metrics/base_velocity/error_vel_yaw: 0.5105
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 11232000
                    Iteration time: 1.13s
                      Time elapsed: 00:05:44
                               ETA: 00:21:53

################################################################################
                     [1m Learning iteration 312/1500 [0m                      

                       Computation: 31061 steps/s (collection: 1.106s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0228
               Mean surrogate loss: -0.0096
                 Mean entropy loss: 13.0864
                       Mean reward: 14.47
               Mean episode length: 844.97
Episode_Reward/track_lin_vel_xy_exp: 0.9039
Episode_Reward/track_ang_vel_z_exp: 0.4631
       Episode_Reward/lin_vel_z_l2: -0.0506
      Episode_Reward/ang_vel_xy_l2: -0.1257
     Episode_Reward/dof_torques_l2: -0.0812
         Episode_Reward/dof_acc_l2: -0.1792
     Episode_Reward/action_rate_l2: -0.1539
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.1624
Metrics/base_velocity/error_vel_xy: 0.5638
Metrics/base_velocity/error_vel_yaw: 0.5296
      Episode_Termination/time_out: 2.0000
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 11268000
                    Iteration time: 1.16s
                      Time elapsed: 00:05:45
                               ETA: 00:21:53

################################################################################
                     [1m Learning iteration 313/1500 [0m                      

                       Computation: 32765 steps/s (collection: 1.047s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0233
               Mean surrogate loss: -0.0093
                 Mean entropy loss: 13.0865
                       Mean reward: 14.70
               Mean episode length: 871.68
Episode_Reward/track_lin_vel_xy_exp: 0.8514
Episode_Reward/track_ang_vel_z_exp: 0.4531
       Episode_Reward/lin_vel_z_l2: -0.0472
      Episode_Reward/ang_vel_xy_l2: -0.1282
     Episode_Reward/dof_torques_l2: -0.0814
         Episode_Reward/dof_acc_l2: -0.1806
     Episode_Reward/action_rate_l2: -0.1537
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.1750
Metrics/base_velocity/error_vel_xy: 0.6296
Metrics/base_velocity/error_vel_yaw: 0.5550
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 11304000
                    Iteration time: 1.10s
                      Time elapsed: 00:05:47
                               ETA: 00:21:51

################################################################################
                     [1m Learning iteration 314/1500 [0m                      

                       Computation: 32917 steps/s (collection: 1.038s, learning 0.055s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0247
               Mean surrogate loss: -0.0105
                 Mean entropy loss: 13.0808
                       Mean reward: 14.64
               Mean episode length: 900.50
Episode_Reward/track_lin_vel_xy_exp: 0.9239
Episode_Reward/track_ang_vel_z_exp: 0.4608
       Episode_Reward/lin_vel_z_l2: -0.0500
      Episode_Reward/ang_vel_xy_l2: -0.1319
     Episode_Reward/dof_torques_l2: -0.0810
         Episode_Reward/dof_acc_l2: -0.1907
     Episode_Reward/action_rate_l2: -0.1579
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.1914
Metrics/base_velocity/error_vel_xy: 0.5673
Metrics/base_velocity/error_vel_yaw: 0.5530
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 11340000
                    Iteration time: 1.09s
                      Time elapsed: 00:05:48
                               ETA: 00:21:50

################################################################################
                     [1m Learning iteration 315/1500 [0m                      

                       Computation: 33031 steps/s (collection: 1.037s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0298
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 13.0770
                       Mean reward: 13.89
               Mean episode length: 894.16
Episode_Reward/track_lin_vel_xy_exp: 0.7426
Episode_Reward/track_ang_vel_z_exp: 0.4323
       Episode_Reward/lin_vel_z_l2: -0.0461
      Episode_Reward/ang_vel_xy_l2: -0.1238
     Episode_Reward/dof_torques_l2: -0.0841
         Episode_Reward/dof_acc_l2: -0.1703
     Episode_Reward/action_rate_l2: -0.1500
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.1992
Metrics/base_velocity/error_vel_xy: 0.7124
Metrics/base_velocity/error_vel_yaw: 0.5785
      Episode_Termination/time_out: 2.0833
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 11376000
                    Iteration time: 1.09s
                      Time elapsed: 00:05:49
                               ETA: 00:21:49

################################################################################
                     [1m Learning iteration 316/1500 [0m                      

                       Computation: 33490 steps/s (collection: 1.021s, learning 0.054s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0216
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 13.0812
                       Mean reward: 14.48
               Mean episode length: 901.98
Episode_Reward/track_lin_vel_xy_exp: 0.9983
Episode_Reward/track_ang_vel_z_exp: 0.4836
       Episode_Reward/lin_vel_z_l2: -0.0558
      Episode_Reward/ang_vel_xy_l2: -0.1394
     Episode_Reward/dof_torques_l2: -0.0866
         Episode_Reward/dof_acc_l2: -0.2032
     Episode_Reward/action_rate_l2: -0.1676
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.2160
Metrics/base_velocity/error_vel_xy: 0.5717
Metrics/base_velocity/error_vel_yaw: 0.5907
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 11412000
                    Iteration time: 1.07s
                      Time elapsed: 00:05:50
                               ETA: 00:21:48

################################################################################
                     [1m Learning iteration 317/1500 [0m                      

                       Computation: 32431 steps/s (collection: 1.057s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0227
               Mean surrogate loss: -0.0083
                 Mean entropy loss: 13.0814
                       Mean reward: 15.00
               Mean episode length: 905.53
Episode_Reward/track_lin_vel_xy_exp: 0.8386
Episode_Reward/track_ang_vel_z_exp: 0.4348
       Episode_Reward/lin_vel_z_l2: -0.0555
      Episode_Reward/ang_vel_xy_l2: -0.1280
     Episode_Reward/dof_torques_l2: -0.0796
         Episode_Reward/dof_acc_l2: -0.1764
     Episode_Reward/action_rate_l2: -0.1507
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.2240
Metrics/base_velocity/error_vel_xy: 0.5934
Metrics/base_velocity/error_vel_yaw: 0.5655
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 11448000
                    Iteration time: 1.11s
                      Time elapsed: 00:05:51
                               ETA: 00:21:47

################################################################################
                     [1m Learning iteration 318/1500 [0m                      

                       Computation: 33262 steps/s (collection: 1.029s, learning 0.054s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0217
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 13.0879
                       Mean reward: 14.46
               Mean episode length: 878.73
Episode_Reward/track_lin_vel_xy_exp: 0.8624
Episode_Reward/track_ang_vel_z_exp: 0.4696
       Episode_Reward/lin_vel_z_l2: -0.0529
      Episode_Reward/ang_vel_xy_l2: -0.1314
     Episode_Reward/dof_torques_l2: -0.0822
         Episode_Reward/dof_acc_l2: -0.1952
     Episode_Reward/action_rate_l2: -0.1618
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.2324
Metrics/base_velocity/error_vel_xy: 0.6528
Metrics/base_velocity/error_vel_yaw: 0.5644
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 11484000
                    Iteration time: 1.08s
                      Time elapsed: 00:05:52
                               ETA: 00:21:46

################################################################################
                     [1m Learning iteration 319/1500 [0m                      

                       Computation: 33662 steps/s (collection: 1.017s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0243
               Mean surrogate loss: -0.0087
                 Mean entropy loss: 13.0822
                       Mean reward: 13.90
               Mean episode length: 852.22
Episode_Reward/track_lin_vel_xy_exp: 0.8981
Episode_Reward/track_ang_vel_z_exp: 0.4503
       Episode_Reward/lin_vel_z_l2: -0.0503
      Episode_Reward/ang_vel_xy_l2: -0.1245
     Episode_Reward/dof_torques_l2: -0.0767
         Episode_Reward/dof_acc_l2: -0.1857
     Episode_Reward/action_rate_l2: -0.1515
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.2425
Metrics/base_velocity/error_vel_xy: 0.5497
Metrics/base_velocity/error_vel_yaw: 0.5351
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 11520000
                    Iteration time: 1.07s
                      Time elapsed: 00:05:53
                               ETA: 00:21:44

################################################################################
                     [1m Learning iteration 320/1500 [0m                      

                       Computation: 32620 steps/s (collection: 1.050s, learning 0.054s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0230
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 13.0797
                       Mean reward: 15.63
               Mean episode length: 896.38
Episode_Reward/track_lin_vel_xy_exp: 1.0773
Episode_Reward/track_ang_vel_z_exp: 0.5102
       Episode_Reward/lin_vel_z_l2: -0.0578
      Episode_Reward/ang_vel_xy_l2: -0.1345
     Episode_Reward/dof_torques_l2: -0.0863
         Episode_Reward/dof_acc_l2: -0.2022
     Episode_Reward/action_rate_l2: -0.1672
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.2505
Metrics/base_velocity/error_vel_xy: 0.4954
Metrics/base_velocity/error_vel_yaw: 0.5329
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 11556000
                    Iteration time: 1.10s
                      Time elapsed: 00:05:54
                               ETA: 00:21:43

################################################################################
                     [1m Learning iteration 321/1500 [0m                      

                       Computation: 33745 steps/s (collection: 1.015s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0245
               Mean surrogate loss: -0.0088
                 Mean entropy loss: 13.0822
                       Mean reward: 17.35
               Mean episode length: 920.33
Episode_Reward/track_lin_vel_xy_exp: 0.9812
Episode_Reward/track_ang_vel_z_exp: 0.4740
       Episode_Reward/lin_vel_z_l2: -0.0514
      Episode_Reward/ang_vel_xy_l2: -0.1360
     Episode_Reward/dof_torques_l2: -0.0827
         Episode_Reward/dof_acc_l2: -0.2039
     Episode_Reward/action_rate_l2: -0.1617
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.2646
Metrics/base_velocity/error_vel_xy: 0.5457
Metrics/base_velocity/error_vel_yaw: 0.5510
      Episode_Termination/time_out: 2.2917
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 11592000
                    Iteration time: 1.07s
                      Time elapsed: 00:05:55
                               ETA: 00:21:42

################################################################################
                     [1m Learning iteration 322/1500 [0m                      

                       Computation: 32468 steps/s (collection: 1.056s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0200
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 13.0796
                       Mean reward: 17.75
               Mean episode length: 937.29
Episode_Reward/track_lin_vel_xy_exp: 0.8296
Episode_Reward/track_ang_vel_z_exp: 0.4720
       Episode_Reward/lin_vel_z_l2: -0.0458
      Episode_Reward/ang_vel_xy_l2: -0.1252
     Episode_Reward/dof_torques_l2: -0.0794
         Episode_Reward/dof_acc_l2: -0.1828
     Episode_Reward/action_rate_l2: -0.1559
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.2791
Metrics/base_velocity/error_vel_xy: 0.6452
Metrics/base_velocity/error_vel_yaw: 0.5180
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 11628000
                    Iteration time: 1.11s
                      Time elapsed: 00:05:56
                               ETA: 00:21:41

################################################################################
                     [1m Learning iteration 323/1500 [0m                      

                       Computation: 32878 steps/s (collection: 1.044s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0276
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 13.0746
                       Mean reward: 16.26
               Mean episode length: 931.52
Episode_Reward/track_lin_vel_xy_exp: 0.9299
Episode_Reward/track_ang_vel_z_exp: 0.4767
       Episode_Reward/lin_vel_z_l2: -0.0472
      Episode_Reward/ang_vel_xy_l2: -0.1261
     Episode_Reward/dof_torques_l2: -0.0840
         Episode_Reward/dof_acc_l2: -0.1782
     Episode_Reward/action_rate_l2: -0.1567
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.2851
Metrics/base_velocity/error_vel_xy: 0.6095
Metrics/base_velocity/error_vel_yaw: 0.5665
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 11664000
                    Iteration time: 1.09s
                      Time elapsed: 00:05:57
                               ETA: 00:21:40

################################################################################
                     [1m Learning iteration 324/1500 [0m                      

                       Computation: 33847 steps/s (collection: 1.010s, learning 0.054s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0263
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 13.0846
                       Mean reward: 14.24
               Mean episode length: 862.32
Episode_Reward/track_lin_vel_xy_exp: 0.7731
Episode_Reward/track_ang_vel_z_exp: 0.4104
       Episode_Reward/lin_vel_z_l2: -0.0440
      Episode_Reward/ang_vel_xy_l2: -0.1166
     Episode_Reward/dof_torques_l2: -0.0794
         Episode_Reward/dof_acc_l2: -0.1669
     Episode_Reward/action_rate_l2: -0.1436
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.2921
Metrics/base_velocity/error_vel_xy: 0.5884
Metrics/base_velocity/error_vel_yaw: 0.5328
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 11700000
                    Iteration time: 1.06s
                      Time elapsed: 00:05:59
                               ETA: 00:21:39

################################################################################
                     [1m Learning iteration 325/1500 [0m                      

                       Computation: 33842 steps/s (collection: 1.012s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0259
               Mean surrogate loss: -0.0086
                 Mean entropy loss: 13.0981
                       Mean reward: 13.56
               Mean episode length: 820.15
Episode_Reward/track_lin_vel_xy_exp: 0.7613
Episode_Reward/track_ang_vel_z_exp: 0.4034
       Episode_Reward/lin_vel_z_l2: -0.0454
      Episode_Reward/ang_vel_xy_l2: -0.1130
     Episode_Reward/dof_torques_l2: -0.0739
         Episode_Reward/dof_acc_l2: -0.1655
     Episode_Reward/action_rate_l2: -0.1411
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.2991
Metrics/base_velocity/error_vel_xy: 0.5679
Metrics/base_velocity/error_vel_yaw: 0.5273
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 11736000
                    Iteration time: 1.06s
                      Time elapsed: 00:06:00
                               ETA: 00:21:37

################################################################################
                     [1m Learning iteration 326/1500 [0m                      

                       Computation: 33995 steps/s (collection: 1.006s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0233
               Mean surrogate loss: -0.0098
                 Mean entropy loss: 13.1135
                       Mean reward: 13.62
               Mean episode length: 817.68
Episode_Reward/track_lin_vel_xy_exp: 0.8418
Episode_Reward/track_ang_vel_z_exp: 0.4215
       Episode_Reward/lin_vel_z_l2: -0.0429
      Episode_Reward/ang_vel_xy_l2: -0.1128
     Episode_Reward/dof_torques_l2: -0.0757
         Episode_Reward/dof_acc_l2: -0.1639
     Episode_Reward/action_rate_l2: -0.1409
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.3151
Metrics/base_velocity/error_vel_xy: 0.4886
Metrics/base_velocity/error_vel_yaw: 0.4773
      Episode_Termination/time_out: 1.0833
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 11772000
                    Iteration time: 1.06s
                      Time elapsed: 00:06:01
                               ETA: 00:21:36

################################################################################
                     [1m Learning iteration 327/1500 [0m                      

                       Computation: 33460 steps/s (collection: 1.024s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0242
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 13.1190
                       Mean reward: 15.05
               Mean episode length: 848.14
Episode_Reward/track_lin_vel_xy_exp: 0.8864
Episode_Reward/track_ang_vel_z_exp: 0.4363
       Episode_Reward/lin_vel_z_l2: -0.0470
      Episode_Reward/ang_vel_xy_l2: -0.1201
     Episode_Reward/dof_torques_l2: -0.0803
         Episode_Reward/dof_acc_l2: -0.1800
     Episode_Reward/action_rate_l2: -0.1501
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.3218
Metrics/base_velocity/error_vel_xy: 0.5274
Metrics/base_velocity/error_vel_yaw: 0.5188
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 11808000
                    Iteration time: 1.08s
                      Time elapsed: 00:06:02
                               ETA: 00:21:35

################################################################################
                     [1m Learning iteration 328/1500 [0m                      

                       Computation: 33665 steps/s (collection: 1.017s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0259
               Mean surrogate loss: -0.0085
                 Mean entropy loss: 13.1396
                       Mean reward: 15.34
               Mean episode length: 856.95
Episode_Reward/track_lin_vel_xy_exp: 0.9596
Episode_Reward/track_ang_vel_z_exp: 0.4595
       Episode_Reward/lin_vel_z_l2: -0.0500
      Episode_Reward/ang_vel_xy_l2: -0.1263
     Episode_Reward/dof_torques_l2: -0.0801
         Episode_Reward/dof_acc_l2: -0.1838
     Episode_Reward/action_rate_l2: -0.1555
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.3344
Metrics/base_velocity/error_vel_xy: 0.4925
Metrics/base_velocity/error_vel_yaw: 0.5283
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 11844000
                    Iteration time: 1.07s
                      Time elapsed: 00:06:03
                               ETA: 00:21:34

################################################################################
                     [1m Learning iteration 329/1500 [0m                      

                       Computation: 33787 steps/s (collection: 1.012s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0245
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 13.1399
                       Mean reward: 15.00
               Mean episode length: 872.41
Episode_Reward/track_lin_vel_xy_exp: 0.8440
Episode_Reward/track_ang_vel_z_exp: 0.4469
       Episode_Reward/lin_vel_z_l2: -0.0506
      Episode_Reward/ang_vel_xy_l2: -0.1228
     Episode_Reward/dof_torques_l2: -0.0794
         Episode_Reward/dof_acc_l2: -0.1907
     Episode_Reward/action_rate_l2: -0.1556
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.3436
Metrics/base_velocity/error_vel_xy: 0.5976
Metrics/base_velocity/error_vel_yaw: 0.5374
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 11880000
                    Iteration time: 1.07s
                      Time elapsed: 00:06:04
                               ETA: 00:21:32

################################################################################
                     [1m Learning iteration 330/1500 [0m                      

                       Computation: 34053 steps/s (collection: 1.005s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0223
               Mean surrogate loss: -0.0096
                 Mean entropy loss: 13.1298
                       Mean reward: 14.40
               Mean episode length: 881.10
Episode_Reward/track_lin_vel_xy_exp: 0.9272
Episode_Reward/track_ang_vel_z_exp: 0.4876
       Episode_Reward/lin_vel_z_l2: -0.0565
      Episode_Reward/ang_vel_xy_l2: -0.1374
     Episode_Reward/dof_torques_l2: -0.0888
         Episode_Reward/dof_acc_l2: -0.2055
     Episode_Reward/action_rate_l2: -0.1685
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.3577
Metrics/base_velocity/error_vel_xy: 0.6639
Metrics/base_velocity/error_vel_yaw: 0.5969
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 11916000
                    Iteration time: 1.06s
                      Time elapsed: 00:06:05
                               ETA: 00:21:31

################################################################################
                     [1m Learning iteration 331/1500 [0m                      

                       Computation: 33487 steps/s (collection: 1.022s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0242
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 13.1198
                       Mean reward: 15.16
               Mean episode length: 895.77
Episode_Reward/track_lin_vel_xy_exp: 0.9470
Episode_Reward/track_ang_vel_z_exp: 0.4793
       Episode_Reward/lin_vel_z_l2: -0.0526
      Episode_Reward/ang_vel_xy_l2: -0.1277
     Episode_Reward/dof_torques_l2: -0.0861
         Episode_Reward/dof_acc_l2: -0.1885
     Episode_Reward/action_rate_l2: -0.1633
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.3727
Metrics/base_velocity/error_vel_xy: 0.5669
Metrics/base_velocity/error_vel_yaw: 0.5441
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 11952000
                    Iteration time: 1.08s
                      Time elapsed: 00:06:06
                               ETA: 00:21:30

################################################################################
                     [1m Learning iteration 332/1500 [0m                      

                       Computation: 33200 steps/s (collection: 1.030s, learning 0.054s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0241
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 13.1087
                       Mean reward: 14.47
               Mean episode length: 871.09
Episode_Reward/track_lin_vel_xy_exp: 0.9049
Episode_Reward/track_ang_vel_z_exp: 0.4580
       Episode_Reward/lin_vel_z_l2: -0.0549
      Episode_Reward/ang_vel_xy_l2: -0.1292
     Episode_Reward/dof_torques_l2: -0.0829
         Episode_Reward/dof_acc_l2: -0.2003
     Episode_Reward/action_rate_l2: -0.1597
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.3827
Metrics/base_velocity/error_vel_xy: 0.5737
Metrics/base_velocity/error_vel_yaw: 0.5309
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 11988000
                    Iteration time: 1.08s
                      Time elapsed: 00:06:07
                               ETA: 00:21:29

################################################################################
                     [1m Learning iteration 333/1500 [0m                      

                       Computation: 31750 steps/s (collection: 1.082s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0232
               Mean surrogate loss: -0.0093
                 Mean entropy loss: 13.1188
                       Mean reward: 14.35
               Mean episode length: 854.26
Episode_Reward/track_lin_vel_xy_exp: 0.9054
Episode_Reward/track_ang_vel_z_exp: 0.4639
       Episode_Reward/lin_vel_z_l2: -0.0518
      Episode_Reward/ang_vel_xy_l2: -0.1317
     Episode_Reward/dof_torques_l2: -0.0808
         Episode_Reward/dof_acc_l2: -0.1994
     Episode_Reward/action_rate_l2: -0.1590
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.3933
Metrics/base_velocity/error_vel_xy: 0.5740
Metrics/base_velocity/error_vel_yaw: 0.5277
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 12024000
                    Iteration time: 1.13s
                      Time elapsed: 00:06:08
                               ETA: 00:21:28

################################################################################
                     [1m Learning iteration 334/1500 [0m                      

                       Computation: 31573 steps/s (collection: 1.078s, learning 0.062s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0216
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 13.1288
                       Mean reward: 14.22
               Mean episode length: 872.27
Episode_Reward/track_lin_vel_xy_exp: 0.8841
Episode_Reward/track_ang_vel_z_exp: 0.4782
       Episode_Reward/lin_vel_z_l2: -0.0546
      Episode_Reward/ang_vel_xy_l2: -0.1408
     Episode_Reward/dof_torques_l2: -0.0851
         Episode_Reward/dof_acc_l2: -0.2086
     Episode_Reward/action_rate_l2: -0.1697
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.4013
Metrics/base_velocity/error_vel_xy: 0.7098
Metrics/base_velocity/error_vel_yaw: 0.6022
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 12060000
                    Iteration time: 1.14s
                      Time elapsed: 00:06:09
                               ETA: 00:21:27

################################################################################
                     [1m Learning iteration 335/1500 [0m                      

                       Computation: 30644 steps/s (collection: 1.120s, learning 0.054s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0258
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 13.1247
                       Mean reward: 15.08
               Mean episode length: 894.58
Episode_Reward/track_lin_vel_xy_exp: 0.8801
Episode_Reward/track_ang_vel_z_exp: 0.4620
       Episode_Reward/lin_vel_z_l2: -0.0469
      Episode_Reward/ang_vel_xy_l2: -0.1254
     Episode_Reward/dof_torques_l2: -0.0813
         Episode_Reward/dof_acc_l2: -0.1931
     Episode_Reward/action_rate_l2: -0.1576
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.4099
Metrics/base_velocity/error_vel_xy: 0.6142
Metrics/base_velocity/error_vel_yaw: 0.5607
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 12096000
                    Iteration time: 1.17s
                      Time elapsed: 00:06:11
                               ETA: 00:21:26

################################################################################
                     [1m Learning iteration 336/1500 [0m                      

                       Computation: 32476 steps/s (collection: 1.056s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0255
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 13.1215
                       Mean reward: 13.95
               Mean episode length: 865.35
Episode_Reward/track_lin_vel_xy_exp: 0.8159
Episode_Reward/track_ang_vel_z_exp: 0.4345
       Episode_Reward/lin_vel_z_l2: -0.0512
      Episode_Reward/ang_vel_xy_l2: -0.1240
     Episode_Reward/dof_torques_l2: -0.0750
         Episode_Reward/dof_acc_l2: -0.1887
     Episode_Reward/action_rate_l2: -0.1501
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.4160
Metrics/base_velocity/error_vel_xy: 0.6036
Metrics/base_velocity/error_vel_yaw: 0.5113
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 12132000
                    Iteration time: 1.11s
                      Time elapsed: 00:06:12
                               ETA: 00:21:25

################################################################################
                     [1m Learning iteration 337/1500 [0m                      

                       Computation: 33280 steps/s (collection: 1.030s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0256
               Mean surrogate loss: -0.0092
                 Mean entropy loss: 13.1205
                       Mean reward: 14.57
               Mean episode length: 887.93
Episode_Reward/track_lin_vel_xy_exp: 0.9672
Episode_Reward/track_ang_vel_z_exp: 0.4964
       Episode_Reward/lin_vel_z_l2: -0.0633
      Episode_Reward/ang_vel_xy_l2: -0.1491
     Episode_Reward/dof_torques_l2: -0.0917
         Episode_Reward/dof_acc_l2: -0.2130
     Episode_Reward/action_rate_l2: -0.1770
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.4262
Metrics/base_velocity/error_vel_xy: 0.6434
Metrics/base_velocity/error_vel_yaw: 0.6287
      Episode_Termination/time_out: 2.0000
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 12168000
                    Iteration time: 1.08s
                      Time elapsed: 00:06:13
                               ETA: 00:21:24

################################################################################
                     [1m Learning iteration 338/1500 [0m                      

                       Computation: 33668 steps/s (collection: 1.016s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0237
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 13.1109
                       Mean reward: 14.38
               Mean episode length: 873.96
Episode_Reward/track_lin_vel_xy_exp: 0.8237
Episode_Reward/track_ang_vel_z_exp: 0.4314
       Episode_Reward/lin_vel_z_l2: -0.0453
      Episode_Reward/ang_vel_xy_l2: -0.1210
     Episode_Reward/dof_torques_l2: -0.0802
         Episode_Reward/dof_acc_l2: -0.1803
     Episode_Reward/action_rate_l2: -0.1519
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.4398
Metrics/base_velocity/error_vel_xy: 0.5922
Metrics/base_velocity/error_vel_yaw: 0.5425
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 12204000
                    Iteration time: 1.07s
                      Time elapsed: 00:06:14
                               ETA: 00:21:22

################################################################################
                     [1m Learning iteration 339/1500 [0m                      

                       Computation: 33598 steps/s (collection: 1.021s, learning 0.050s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0267
               Mean surrogate loss: -0.0098
                 Mean entropy loss: 13.1032
                       Mean reward: 14.52
               Mean episode length: 879.00
Episode_Reward/track_lin_vel_xy_exp: 0.9107
Episode_Reward/track_ang_vel_z_exp: 0.4680
       Episode_Reward/lin_vel_z_l2: -0.0479
      Episode_Reward/ang_vel_xy_l2: -0.1305
     Episode_Reward/dof_torques_l2: -0.0858
         Episode_Reward/dof_acc_l2: -0.1904
     Episode_Reward/action_rate_l2: -0.1618
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.4503
Metrics/base_velocity/error_vel_xy: 0.5888
Metrics/base_velocity/error_vel_yaw: 0.5570
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 12240000
                    Iteration time: 1.07s
                      Time elapsed: 00:06:15
                               ETA: 00:21:21

################################################################################
                     [1m Learning iteration 340/1500 [0m                      

                       Computation: 34055 steps/s (collection: 1.007s, learning 0.050s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0242
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 13.1119
                       Mean reward: 14.88
               Mean episode length: 877.38
Episode_Reward/track_lin_vel_xy_exp: 0.9678
Episode_Reward/track_ang_vel_z_exp: 0.4730
       Episode_Reward/lin_vel_z_l2: -0.0489
      Episode_Reward/ang_vel_xy_l2: -0.1273
     Episode_Reward/dof_torques_l2: -0.0809
         Episode_Reward/dof_acc_l2: -0.1810
     Episode_Reward/action_rate_l2: -0.1569
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.4598
Metrics/base_velocity/error_vel_xy: 0.5062
Metrics/base_velocity/error_vel_yaw: 0.5157
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 12276000
                    Iteration time: 1.06s
                      Time elapsed: 00:06:16
                               ETA: 00:21:20

################################################################################
                     [1m Learning iteration 341/1500 [0m                      

                       Computation: 34021 steps/s (collection: 1.005s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0249
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 13.1190
                       Mean reward: 13.68
               Mean episode length: 835.59
Episode_Reward/track_lin_vel_xy_exp: 0.8696
Episode_Reward/track_ang_vel_z_exp: 0.4406
       Episode_Reward/lin_vel_z_l2: -0.0519
      Episode_Reward/ang_vel_xy_l2: -0.1278
     Episode_Reward/dof_torques_l2: -0.0775
         Episode_Reward/dof_acc_l2: -0.1912
     Episode_Reward/action_rate_l2: -0.1552
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.4711
Metrics/base_velocity/error_vel_xy: 0.5612
Metrics/base_velocity/error_vel_yaw: 0.5326
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 12312000
                    Iteration time: 1.06s
                      Time elapsed: 00:06:17
                               ETA: 00:21:19

################################################################################
                     [1m Learning iteration 342/1500 [0m                      

                       Computation: 33629 steps/s (collection: 1.018s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0282
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 13.1337
                       Mean reward: 14.62
               Mean episode length: 849.70
Episode_Reward/track_lin_vel_xy_exp: 0.9394
Episode_Reward/track_ang_vel_z_exp: 0.4444
       Episode_Reward/lin_vel_z_l2: -0.0484
      Episode_Reward/ang_vel_xy_l2: -0.1247
     Episode_Reward/dof_torques_l2: -0.0808
         Episode_Reward/dof_acc_l2: -0.1864
     Episode_Reward/action_rate_l2: -0.1537
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.4818
Metrics/base_velocity/error_vel_xy: 0.4613
Metrics/base_velocity/error_vel_yaw: 0.5106
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 12348000
                    Iteration time: 1.07s
                      Time elapsed: 00:06:18
                               ETA: 00:21:17

################################################################################
                     [1m Learning iteration 343/1500 [0m                      

                       Computation: 33764 steps/s (collection: 1.013s, learning 0.054s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0232
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 13.1473
                       Mean reward: 14.18
               Mean episode length: 810.23
Episode_Reward/track_lin_vel_xy_exp: 0.8554
Episode_Reward/track_ang_vel_z_exp: 0.4279
       Episode_Reward/lin_vel_z_l2: -0.0498
      Episode_Reward/ang_vel_xy_l2: -0.1213
     Episode_Reward/dof_torques_l2: -0.0706
         Episode_Reward/dof_acc_l2: -0.1715
     Episode_Reward/action_rate_l2: -0.1436
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.4907
Metrics/base_velocity/error_vel_xy: 0.4819
Metrics/base_velocity/error_vel_yaw: 0.4760
      Episode_Termination/time_out: 1.1667
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 12384000
                    Iteration time: 1.07s
                      Time elapsed: 00:06:19
                               ETA: 00:21:16

################################################################################
                     [1m Learning iteration 344/1500 [0m                      

                       Computation: 34032 steps/s (collection: 1.006s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0262
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 13.1508
                       Mean reward: 14.14
               Mean episode length: 825.69
Episode_Reward/track_lin_vel_xy_exp: 0.8936
Episode_Reward/track_ang_vel_z_exp: 0.4368
       Episode_Reward/lin_vel_z_l2: -0.0489
      Episode_Reward/ang_vel_xy_l2: -0.1204
     Episode_Reward/dof_torques_l2: -0.0784
         Episode_Reward/dof_acc_l2: -0.1756
     Episode_Reward/action_rate_l2: -0.1508
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.4978
Metrics/base_velocity/error_vel_xy: 0.5034
Metrics/base_velocity/error_vel_yaw: 0.5240
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 12420000
                    Iteration time: 1.06s
                      Time elapsed: 00:06:20
                               ETA: 00:21:15

################################################################################
                     [1m Learning iteration 345/1500 [0m                      

                       Computation: 33995 steps/s (collection: 1.006s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0271
               Mean surrogate loss: -0.0098
                 Mean entropy loss: 13.1455
                       Mean reward: 14.92
               Mean episode length: 858.27
Episode_Reward/track_lin_vel_xy_exp: 0.9145
Episode_Reward/track_ang_vel_z_exp: 0.4484
       Episode_Reward/lin_vel_z_l2: -0.0457
      Episode_Reward/ang_vel_xy_l2: -0.1248
     Episode_Reward/dof_torques_l2: -0.0837
         Episode_Reward/dof_acc_l2: -0.1862
     Episode_Reward/action_rate_l2: -0.1562
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.5090
Metrics/base_velocity/error_vel_xy: 0.5415
Metrics/base_velocity/error_vel_yaw: 0.5538
      Episode_Termination/time_out: 2.0000
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 12456000
                    Iteration time: 1.06s
                      Time elapsed: 00:06:21
                               ETA: 00:21:14

################################################################################
                     [1m Learning iteration 346/1500 [0m                      

                       Computation: 33618 steps/s (collection: 1.018s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0248
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 13.1441
                       Mean reward: 16.01
               Mean episode length: 897.59
Episode_Reward/track_lin_vel_xy_exp: 1.0645
Episode_Reward/track_ang_vel_z_exp: 0.4923
       Episode_Reward/lin_vel_z_l2: -0.0485
      Episode_Reward/ang_vel_xy_l2: -0.1317
     Episode_Reward/dof_torques_l2: -0.0877
         Episode_Reward/dof_acc_l2: -0.1994
     Episode_Reward/action_rate_l2: -0.1656
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.5230
Metrics/base_velocity/error_vel_xy: 0.4677
Metrics/base_velocity/error_vel_yaw: 0.5310
      Episode_Termination/time_out: 1.9167
  Episode_Termination/base_contact: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 12492000
                    Iteration time: 1.07s
                      Time elapsed: 00:06:22
                               ETA: 00:21:12

################################################################################
                     [1m Learning iteration 347/1500 [0m                      

                       Computation: 31768 steps/s (collection: 1.082s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0244
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 13.1466
                       Mean reward: 16.14
               Mean episode length: 887.45
Episode_Reward/track_lin_vel_xy_exp: 0.9475
Episode_Reward/track_ang_vel_z_exp: 0.4736
       Episode_Reward/lin_vel_z_l2: -0.0478
      Episode_Reward/ang_vel_xy_l2: -0.1249
     Episode_Reward/dof_torques_l2: -0.0853
         Episode_Reward/dof_acc_l2: -0.1920
     Episode_Reward/action_rate_l2: -0.1617
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.5318
Metrics/base_velocity/error_vel_xy: 0.5551
Metrics/base_velocity/error_vel_yaw: 0.5422
      Episode_Termination/time_out: 1.9583
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 12528000
                    Iteration time: 1.13s
                      Time elapsed: 00:06:23
                               ETA: 00:21:11

################################################################################
                     [1m Learning iteration 348/1500 [0m                      

                       Computation: 31200 steps/s (collection: 1.099s, learning 0.055s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0232
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 13.1537
                       Mean reward: 16.27
               Mean episode length: 885.58
Episode_Reward/track_lin_vel_xy_exp: 0.9564
Episode_Reward/track_ang_vel_z_exp: 0.4643
       Episode_Reward/lin_vel_z_l2: -0.0567
      Episode_Reward/ang_vel_xy_l2: -0.1312
     Episode_Reward/dof_torques_l2: -0.0846
         Episode_Reward/dof_acc_l2: -0.2146
     Episode_Reward/action_rate_l2: -0.1647
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.5465
Metrics/base_velocity/error_vel_xy: 0.5234
Metrics/base_velocity/error_vel_yaw: 0.5397
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 12564000
                    Iteration time: 1.15s
                      Time elapsed: 00:06:25
                               ETA: 00:21:11

################################################################################
                     [1m Learning iteration 349/1500 [0m                      

                       Computation: 32405 steps/s (collection: 1.059s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0234
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 13.1483
                       Mean reward: 15.43
               Mean episode length: 877.98
Episode_Reward/track_lin_vel_xy_exp: 0.7689
Episode_Reward/track_ang_vel_z_exp: 0.4075
       Episode_Reward/lin_vel_z_l2: -0.0507
      Episode_Reward/ang_vel_xy_l2: -0.1201
     Episode_Reward/dof_torques_l2: -0.0735
         Episode_Reward/dof_acc_l2: -0.1843
     Episode_Reward/action_rate_l2: -0.1479
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.5598
Metrics/base_velocity/error_vel_xy: 0.6084
Metrics/base_velocity/error_vel_yaw: 0.5175
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 12600000
                    Iteration time: 1.11s
                      Time elapsed: 00:06:26
                               ETA: 00:21:09

################################################################################
                     [1m Learning iteration 350/1500 [0m                      

                       Computation: 33184 steps/s (collection: 1.033s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0235
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 13.1389
                       Mean reward: 14.08
               Mean episode length: 858.11
Episode_Reward/track_lin_vel_xy_exp: 0.7939
Episode_Reward/track_ang_vel_z_exp: 0.4020
       Episode_Reward/lin_vel_z_l2: -0.0467
      Episode_Reward/ang_vel_xy_l2: -0.1209
     Episode_Reward/dof_torques_l2: -0.0746
         Episode_Reward/dof_acc_l2: -0.1872
     Episode_Reward/action_rate_l2: -0.1456
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.5717
Metrics/base_velocity/error_vel_xy: 0.5426
Metrics/base_velocity/error_vel_yaw: 0.5147
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 12636000
                    Iteration time: 1.08s
                      Time elapsed: 00:06:27
                               ETA: 00:21:08

################################################################################
                     [1m Learning iteration 351/1500 [0m                      

                       Computation: 31565 steps/s (collection: 1.088s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0218
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 13.1325
                       Mean reward: 13.92
               Mean episode length: 855.47
Episode_Reward/track_lin_vel_xy_exp: 0.9470
Episode_Reward/track_ang_vel_z_exp: 0.4643
       Episode_Reward/lin_vel_z_l2: -0.0565
      Episode_Reward/ang_vel_xy_l2: -0.1361
     Episode_Reward/dof_torques_l2: -0.0891
         Episode_Reward/dof_acc_l2: -0.2256
     Episode_Reward/action_rate_l2: -0.1697
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.5839
Metrics/base_velocity/error_vel_xy: 0.5715
Metrics/base_velocity/error_vel_yaw: 0.5715
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 12672000
                    Iteration time: 1.14s
                      Time elapsed: 00:06:28
                               ETA: 00:21:07

################################################################################
                     [1m Learning iteration 352/1500 [0m                      

                       Computation: 30839 steps/s (collection: 1.112s, learning 0.055s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0264
               Mean surrogate loss: -0.0105
                 Mean entropy loss: 13.1260
                       Mean reward: 14.90
               Mean episode length: 877.00
Episode_Reward/track_lin_vel_xy_exp: 0.9316
Episode_Reward/track_ang_vel_z_exp: 0.4701
       Episode_Reward/lin_vel_z_l2: -0.0475
      Episode_Reward/ang_vel_xy_l2: -0.1307
     Episode_Reward/dof_torques_l2: -0.0835
         Episode_Reward/dof_acc_l2: -0.1892
     Episode_Reward/action_rate_l2: -0.1612
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.5958
Metrics/base_velocity/error_vel_xy: 0.5722
Metrics/base_velocity/error_vel_yaw: 0.5568
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 12708000
                    Iteration time: 1.17s
                      Time elapsed: 00:06:29
                               ETA: 00:21:06

################################################################################
                     [1m Learning iteration 353/1500 [0m                      

                       Computation: 30913 steps/s (collection: 1.112s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0274
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 13.1300
                       Mean reward: 15.53
               Mean episode length: 868.72
Episode_Reward/track_lin_vel_xy_exp: 1.0111
Episode_Reward/track_ang_vel_z_exp: 0.4849
       Episode_Reward/lin_vel_z_l2: -0.0537
      Episode_Reward/ang_vel_xy_l2: -0.1329
     Episode_Reward/dof_torques_l2: -0.0848
         Episode_Reward/dof_acc_l2: -0.1997
     Episode_Reward/action_rate_l2: -0.1646
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.6089
Metrics/base_velocity/error_vel_xy: 0.4948
Metrics/base_velocity/error_vel_yaw: 0.5184
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 12744000
                    Iteration time: 1.16s
                      Time elapsed: 00:06:30
                               ETA: 00:21:06

################################################################################
                     [1m Learning iteration 354/1500 [0m                      

                       Computation: 33100 steps/s (collection: 1.035s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0242
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 13.1412
                       Mean reward: 15.65
               Mean episode length: 873.07
Episode_Reward/track_lin_vel_xy_exp: 0.9810
Episode_Reward/track_ang_vel_z_exp: 0.4851
       Episode_Reward/lin_vel_z_l2: -0.0476
      Episode_Reward/ang_vel_xy_l2: -0.1317
     Episode_Reward/dof_torques_l2: -0.0851
         Episode_Reward/dof_acc_l2: -0.1876
     Episode_Reward/action_rate_l2: -0.1624
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.6215
Metrics/base_velocity/error_vel_xy: 0.5387
Metrics/base_velocity/error_vel_yaw: 0.5364
      Episode_Termination/time_out: 1.9583
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 12780000
                    Iteration time: 1.09s
                      Time elapsed: 00:06:31
                               ETA: 00:21:04

################################################################################
                     [1m Learning iteration 355/1500 [0m                      

                       Computation: 32041 steps/s (collection: 1.071s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0258
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 13.1377
                       Mean reward: 15.12
               Mean episode length: 880.86
Episode_Reward/track_lin_vel_xy_exp: 0.9464
Episode_Reward/track_ang_vel_z_exp: 0.4724
       Episode_Reward/lin_vel_z_l2: -0.0501
      Episode_Reward/ang_vel_xy_l2: -0.1322
     Episode_Reward/dof_torques_l2: -0.0853
         Episode_Reward/dof_acc_l2: -0.1999
     Episode_Reward/action_rate_l2: -0.1646
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.6341
Metrics/base_velocity/error_vel_xy: 0.5673
Metrics/base_velocity/error_vel_yaw: 0.5524
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 12816000
                    Iteration time: 1.12s
                      Time elapsed: 00:06:32
                               ETA: 00:21:03

################################################################################
                     [1m Learning iteration 356/1500 [0m                      

                       Computation: 32142 steps/s (collection: 1.068s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0276
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 13.1490
                       Mean reward: 14.46
               Mean episode length: 857.62
Episode_Reward/track_lin_vel_xy_exp: 0.7699
Episode_Reward/track_ang_vel_z_exp: 0.3890
       Episode_Reward/lin_vel_z_l2: -0.0480
      Episode_Reward/ang_vel_xy_l2: -0.1176
     Episode_Reward/dof_torques_l2: -0.0737
         Episode_Reward/dof_acc_l2: -0.1768
     Episode_Reward/action_rate_l2: -0.1406
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.6495
Metrics/base_velocity/error_vel_xy: 0.5247
Metrics/base_velocity/error_vel_yaw: 0.5113
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 12852000
                    Iteration time: 1.12s
                      Time elapsed: 00:06:34
                               ETA: 00:21:02

################################################################################
                     [1m Learning iteration 357/1500 [0m                      

                       Computation: 31909 steps/s (collection: 1.075s, learning 0.054s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0292
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 13.1559
                       Mean reward: 14.56
               Mean episode length: 858.91
Episode_Reward/track_lin_vel_xy_exp: 0.9431
Episode_Reward/track_ang_vel_z_exp: 0.4725
       Episode_Reward/lin_vel_z_l2: -0.0515
      Episode_Reward/ang_vel_xy_l2: -0.1303
     Episode_Reward/dof_torques_l2: -0.0871
         Episode_Reward/dof_acc_l2: -0.1922
     Episode_Reward/action_rate_l2: -0.1633
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.6593
Metrics/base_velocity/error_vel_xy: 0.5556
Metrics/base_velocity/error_vel_yaw: 0.5364
      Episode_Termination/time_out: 2.2917
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 12888000
                    Iteration time: 1.13s
                      Time elapsed: 00:06:35
                               ETA: 00:21:01

################################################################################
                     [1m Learning iteration 358/1500 [0m                      

                       Computation: 32558 steps/s (collection: 1.053s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0208
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 13.1316
                       Mean reward: 16.10
               Mean episode length: 894.99
Episode_Reward/track_lin_vel_xy_exp: 1.0681
Episode_Reward/track_ang_vel_z_exp: 0.5109
       Episode_Reward/lin_vel_z_l2: -0.0544
      Episode_Reward/ang_vel_xy_l2: -0.1444
     Episode_Reward/dof_torques_l2: -0.0960
         Episode_Reward/dof_acc_l2: -0.2148
     Episode_Reward/action_rate_l2: -0.1790
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.6705
Metrics/base_velocity/error_vel_xy: 0.5962
Metrics/base_velocity/error_vel_yaw: 0.5908
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 12924000
                    Iteration time: 1.11s
                      Time elapsed: 00:06:36
                               ETA: 00:21:00

################################################################################
                     [1m Learning iteration 359/1500 [0m                      

                       Computation: 32694 steps/s (collection: 1.045s, learning 0.056s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0254
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 13.1056
                       Mean reward: 15.97
               Mean episode length: 893.36
Episode_Reward/track_lin_vel_xy_exp: 0.7856
Episode_Reward/track_ang_vel_z_exp: 0.4096
       Episode_Reward/lin_vel_z_l2: -0.0437
      Episode_Reward/ang_vel_xy_l2: -0.1169
     Episode_Reward/dof_torques_l2: -0.0759
         Episode_Reward/dof_acc_l2: -0.1721
     Episode_Reward/action_rate_l2: -0.1424
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.6745
Metrics/base_velocity/error_vel_xy: 0.5509
Metrics/base_velocity/error_vel_yaw: 0.4883
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 12960000
                    Iteration time: 1.10s
                      Time elapsed: 00:06:37
                               ETA: 00:20:59

################################################################################
                     [1m Learning iteration 360/1500 [0m                      

                       Computation: 33120 steps/s (collection: 1.034s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0244
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 13.0882
                       Mean reward: 14.82
               Mean episode length: 846.78
Episode_Reward/track_lin_vel_xy_exp: 0.9165
Episode_Reward/track_ang_vel_z_exp: 0.4440
       Episode_Reward/lin_vel_z_l2: -0.0492
      Episode_Reward/ang_vel_xy_l2: -0.1222
     Episode_Reward/dof_torques_l2: -0.0806
         Episode_Reward/dof_acc_l2: -0.1854
     Episode_Reward/action_rate_l2: -0.1549
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.6817
Metrics/base_velocity/error_vel_xy: 0.4964
Metrics/base_velocity/error_vel_yaw: 0.5207
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 12996000
                    Iteration time: 1.09s
                      Time elapsed: 00:06:38
                               ETA: 00:20:58

################################################################################
                     [1m Learning iteration 361/1500 [0m                      

                       Computation: 34084 steps/s (collection: 1.004s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0245
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 13.0846
                       Mean reward: 15.50
               Mean episode length: 888.22
Episode_Reward/track_lin_vel_xy_exp: 1.0264
Episode_Reward/track_ang_vel_z_exp: 0.5042
       Episode_Reward/lin_vel_z_l2: -0.0515
      Episode_Reward/ang_vel_xy_l2: -0.1408
     Episode_Reward/dof_torques_l2: -0.0921
         Episode_Reward/dof_acc_l2: -0.2022
     Episode_Reward/action_rate_l2: -0.1752
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.6884
Metrics/base_velocity/error_vel_xy: 0.6165
Metrics/base_velocity/error_vel_yaw: 0.5988
      Episode_Termination/time_out: 2.0000
  Episode_Termination/base_contact: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 13032000
                    Iteration time: 1.06s
                      Time elapsed: 00:06:39
                               ETA: 00:20:57

################################################################################
                     [1m Learning iteration 362/1500 [0m                      

                       Computation: 31247 steps/s (collection: 1.101s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0293
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 13.0924
                       Mean reward: 15.92
               Mean episode length: 888.83
Episode_Reward/track_lin_vel_xy_exp: 0.9640
Episode_Reward/track_ang_vel_z_exp: 0.4601
       Episode_Reward/lin_vel_z_l2: -0.0544
      Episode_Reward/ang_vel_xy_l2: -0.1324
     Episode_Reward/dof_torques_l2: -0.0837
         Episode_Reward/dof_acc_l2: -0.2166
     Episode_Reward/action_rate_l2: -0.1655
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.7005
Metrics/base_velocity/error_vel_xy: 0.5000
Metrics/base_velocity/error_vel_yaw: 0.5342
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 13068000
                    Iteration time: 1.15s
                      Time elapsed: 00:06:40
                               ETA: 00:20:56

################################################################################
                     [1m Learning iteration 363/1500 [0m                      

                       Computation: 29745 steps/s (collection: 1.158s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0253
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 13.0975
                       Mean reward: 15.11
               Mean episode length: 858.25
Episode_Reward/track_lin_vel_xy_exp: 0.8482
Episode_Reward/track_ang_vel_z_exp: 0.4261
       Episode_Reward/lin_vel_z_l2: -0.0522
      Episode_Reward/ang_vel_xy_l2: -0.1273
     Episode_Reward/dof_torques_l2: -0.0766
         Episode_Reward/dof_acc_l2: -0.1971
     Episode_Reward/action_rate_l2: -0.1553
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.7139
Metrics/base_velocity/error_vel_xy: 0.5413
Metrics/base_velocity/error_vel_yaw: 0.5288
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 13104000
                    Iteration time: 1.21s
                      Time elapsed: 00:06:41
                               ETA: 00:20:55

################################################################################
                     [1m Learning iteration 364/1500 [0m                      

                       Computation: 32757 steps/s (collection: 1.046s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0284
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 13.0954
                       Mean reward: 14.05
               Mean episode length: 809.44
Episode_Reward/track_lin_vel_xy_exp: 0.7385
Episode_Reward/track_ang_vel_z_exp: 0.3621
       Episode_Reward/lin_vel_z_l2: -0.0444
      Episode_Reward/ang_vel_xy_l2: -0.1090
     Episode_Reward/dof_torques_l2: -0.0639
         Episode_Reward/dof_acc_l2: -0.1515
     Episode_Reward/action_rate_l2: -0.1256
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.7175
Metrics/base_velocity/error_vel_xy: 0.4272
Metrics/base_velocity/error_vel_yaw: 0.4403
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 13140000
                    Iteration time: 1.10s
                      Time elapsed: 00:06:43
                               ETA: 00:20:54

################################################################################
                     [1m Learning iteration 365/1500 [0m                      

                       Computation: 32153 steps/s (collection: 1.069s, learning 0.050s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0256
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 13.1039
                       Mean reward: 14.15
               Mean episode length: 819.48
Episode_Reward/track_lin_vel_xy_exp: 0.8710
Episode_Reward/track_ang_vel_z_exp: 0.4383
       Episode_Reward/lin_vel_z_l2: -0.0471
      Episode_Reward/ang_vel_xy_l2: -0.1198
     Episode_Reward/dof_torques_l2: -0.0832
         Episode_Reward/dof_acc_l2: -0.1746
     Episode_Reward/action_rate_l2: -0.1541
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.7272
Metrics/base_velocity/error_vel_xy: 0.5405
Metrics/base_velocity/error_vel_yaw: 0.5041
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 13176000
                    Iteration time: 1.12s
                      Time elapsed: 00:06:44
                               ETA: 00:20:53

################################################################################
                     [1m Learning iteration 366/1500 [0m                      

                       Computation: 31270 steps/s (collection: 1.098s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0273
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 13.1092
                       Mean reward: 15.02
               Mean episode length: 867.95
Episode_Reward/track_lin_vel_xy_exp: 0.9214
Episode_Reward/track_ang_vel_z_exp: 0.4626
       Episode_Reward/lin_vel_z_l2: -0.0484
      Episode_Reward/ang_vel_xy_l2: -0.1300
     Episode_Reward/dof_torques_l2: -0.0867
         Episode_Reward/dof_acc_l2: -0.1885
     Episode_Reward/action_rate_l2: -0.1622
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.7388
Metrics/base_velocity/error_vel_xy: 0.5639
Metrics/base_velocity/error_vel_yaw: 0.5418
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 13212000
                    Iteration time: 1.15s
                      Time elapsed: 00:06:45
                               ETA: 00:20:52

################################################################################
                     [1m Learning iteration 367/1500 [0m                      

                       Computation: 33016 steps/s (collection: 1.037s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0305
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 13.1245
                       Mean reward: 15.74
               Mean episode length: 897.43
Episode_Reward/track_lin_vel_xy_exp: 0.9459
Episode_Reward/track_ang_vel_z_exp: 0.4494
       Episode_Reward/lin_vel_z_l2: -0.0470
      Episode_Reward/ang_vel_xy_l2: -0.1265
     Episode_Reward/dof_torques_l2: -0.0784
         Episode_Reward/dof_acc_l2: -0.1814
     Episode_Reward/action_rate_l2: -0.1520
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.7527
Metrics/base_velocity/error_vel_xy: 0.4648
Metrics/base_velocity/error_vel_yaw: 0.5005
      Episode_Termination/time_out: 2.0417
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 13248000
                    Iteration time: 1.09s
                      Time elapsed: 00:06:46
                               ETA: 00:20:51

################################################################################
                     [1m Learning iteration 368/1500 [0m                      

                       Computation: 32259 steps/s (collection: 1.066s, learning 0.050s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0246
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 13.1281
                       Mean reward: 16.43
               Mean episode length: 900.54
Episode_Reward/track_lin_vel_xy_exp: 0.8360
Episode_Reward/track_ang_vel_z_exp: 0.4149
       Episode_Reward/lin_vel_z_l2: -0.0467
      Episode_Reward/ang_vel_xy_l2: -0.1141
     Episode_Reward/dof_torques_l2: -0.0784
         Episode_Reward/dof_acc_l2: -0.1649
     Episode_Reward/action_rate_l2: -0.1449
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.7634
Metrics/base_velocity/error_vel_xy: 0.4860
Metrics/base_velocity/error_vel_yaw: 0.4669
      Episode_Termination/time_out: 1.1667
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 13284000
                    Iteration time: 1.12s
                      Time elapsed: 00:06:47
                               ETA: 00:20:50

################################################################################
                     [1m Learning iteration 369/1500 [0m                      

                       Computation: 32313 steps/s (collection: 1.062s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0261
               Mean surrogate loss: -0.0093
                 Mean entropy loss: 13.1250
                       Mean reward: 16.68
               Mean episode length: 910.59
Episode_Reward/track_lin_vel_xy_exp: 0.9858
Episode_Reward/track_ang_vel_z_exp: 0.4766
       Episode_Reward/lin_vel_z_l2: -0.0484
      Episode_Reward/ang_vel_xy_l2: -0.1296
     Episode_Reward/dof_torques_l2: -0.0879
         Episode_Reward/dof_acc_l2: -0.1951
     Episode_Reward/action_rate_l2: -0.1655
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.7713
Metrics/base_velocity/error_vel_xy: 0.5475
Metrics/base_velocity/error_vel_yaw: 0.5687
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 13320000
                    Iteration time: 1.11s
                      Time elapsed: 00:06:48
                               ETA: 00:20:48

################################################################################
                     [1m Learning iteration 370/1500 [0m                      

                       Computation: 32961 steps/s (collection: 1.041s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0254
               Mean surrogate loss: -0.0093
                 Mean entropy loss: 13.1210
                       Mean reward: 14.89
               Mean episode length: 900.67
Episode_Reward/track_lin_vel_xy_exp: 0.8947
Episode_Reward/track_ang_vel_z_exp: 0.4683
       Episode_Reward/lin_vel_z_l2: -0.0529
      Episode_Reward/ang_vel_xy_l2: -0.1360
     Episode_Reward/dof_torques_l2: -0.0867
         Episode_Reward/dof_acc_l2: -0.2105
     Episode_Reward/action_rate_l2: -0.1674
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.7824
Metrics/base_velocity/error_vel_xy: 0.6555
Metrics/base_velocity/error_vel_yaw: 0.5924
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 13356000
                    Iteration time: 1.09s
                      Time elapsed: 00:06:49
                               ETA: 00:20:47

################################################################################
                     [1m Learning iteration 371/1500 [0m                      

                       Computation: 33951 steps/s (collection: 1.011s, learning 0.050s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0278
               Mean surrogate loss: -0.0105
                 Mean entropy loss: 13.1321
                       Mean reward: 15.07
               Mean episode length: 902.42
Episode_Reward/track_lin_vel_xy_exp: 0.9517
Episode_Reward/track_ang_vel_z_exp: 0.4680
       Episode_Reward/lin_vel_z_l2: -0.0539
      Episode_Reward/ang_vel_xy_l2: -0.1388
     Episode_Reward/dof_torques_l2: -0.0861
         Episode_Reward/dof_acc_l2: -0.2120
     Episode_Reward/action_rate_l2: -0.1675
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.7925
Metrics/base_velocity/error_vel_xy: 0.5659
Metrics/base_velocity/error_vel_yaw: 0.5722
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 13392000
                    Iteration time: 1.06s
                      Time elapsed: 00:06:50
                               ETA: 00:20:46

################################################################################
                     [1m Learning iteration 372/1500 [0m                      

                       Computation: 33784 steps/s (collection: 1.014s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0271
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 13.1432
                       Mean reward: 15.68
               Mean episode length: 888.24
Episode_Reward/track_lin_vel_xy_exp: 0.9794
Episode_Reward/track_ang_vel_z_exp: 0.4617
       Episode_Reward/lin_vel_z_l2: -0.0420
      Episode_Reward/ang_vel_xy_l2: -0.1240
     Episode_Reward/dof_torques_l2: -0.0803
         Episode_Reward/dof_acc_l2: -0.1781
     Episode_Reward/action_rate_l2: -0.1549
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.8039
Metrics/base_velocity/error_vel_xy: 0.4487
Metrics/base_velocity/error_vel_yaw: 0.4871
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 13428000
                    Iteration time: 1.07s
                      Time elapsed: 00:06:51
                               ETA: 00:20:45

################################################################################
                     [1m Learning iteration 373/1500 [0m                      

                       Computation: 33124 steps/s (collection: 1.034s, learning 0.053s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0280
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 13.1390
                       Mean reward: 16.29
               Mean episode length: 902.19
Episode_Reward/track_lin_vel_xy_exp: 0.8808
Episode_Reward/track_ang_vel_z_exp: 0.4582
       Episode_Reward/lin_vel_z_l2: -0.0504
      Episode_Reward/ang_vel_xy_l2: -0.1285
     Episode_Reward/dof_torques_l2: -0.0856
         Episode_Reward/dof_acc_l2: -0.1872
     Episode_Reward/action_rate_l2: -0.1585
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.8167
Metrics/base_velocity/error_vel_xy: 0.5989
Metrics/base_velocity/error_vel_yaw: 0.5293
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 13464000
                    Iteration time: 1.09s
                      Time elapsed: 00:06:52
                               ETA: 00:20:44

################################################################################
                     [1m Learning iteration 374/1500 [0m                      

                       Computation: 31174 steps/s (collection: 1.090s, learning 0.065s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0264
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 13.1359
                       Mean reward: 15.91
               Mean episode length: 882.10
Episode_Reward/track_lin_vel_xy_exp: 0.8575
Episode_Reward/track_ang_vel_z_exp: 0.4454
       Episode_Reward/lin_vel_z_l2: -0.0462
      Episode_Reward/ang_vel_xy_l2: -0.1230
     Episode_Reward/dof_torques_l2: -0.0834
         Episode_Reward/dof_acc_l2: -0.1864
     Episode_Reward/action_rate_l2: -0.1554
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.8243
Metrics/base_velocity/error_vel_xy: 0.5821
Metrics/base_velocity/error_vel_yaw: 0.5174
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 13500000
                    Iteration time: 1.15s
                      Time elapsed: 00:06:54
                               ETA: 00:20:43

################################################################################
                     [1m Learning iteration 375/1500 [0m                      

                       Computation: 31019 steps/s (collection: 1.108s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0268
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 13.1402
                       Mean reward: 15.63
               Mean episode length: 884.70
Episode_Reward/track_lin_vel_xy_exp: 1.0682
Episode_Reward/track_ang_vel_z_exp: 0.5084
       Episode_Reward/lin_vel_z_l2: -0.0478
      Episode_Reward/ang_vel_xy_l2: -0.1402
     Episode_Reward/dof_torques_l2: -0.0812
         Episode_Reward/dof_acc_l2: -0.2074
     Episode_Reward/action_rate_l2: -0.1681
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.8323
Metrics/base_velocity/error_vel_xy: 0.5129
Metrics/base_velocity/error_vel_yaw: 0.5443
      Episode_Termination/time_out: 1.1667
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 13536000
                    Iteration time: 1.16s
                      Time elapsed: 00:06:55
                               ETA: 00:20:42

################################################################################
                     [1m Learning iteration 376/1500 [0m                      

                       Computation: 31126 steps/s (collection: 1.105s, learning 0.051s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0229
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 13.1423
                       Mean reward: 15.79
               Mean episode length: 868.10
Episode_Reward/track_lin_vel_xy_exp: 0.9834
Episode_Reward/track_ang_vel_z_exp: 0.4750
       Episode_Reward/lin_vel_z_l2: -0.0472
      Episode_Reward/ang_vel_xy_l2: -0.1313
     Episode_Reward/dof_torques_l2: -0.0846
         Episode_Reward/dof_acc_l2: -0.1903
     Episode_Reward/action_rate_l2: -0.1617
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.8393
Metrics/base_velocity/error_vel_xy: 0.5172
Metrics/base_velocity/error_vel_yaw: 0.5252
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 13572000
                    Iteration time: 1.16s
                      Time elapsed: 00:06:56
                               ETA: 00:20:41

################################################################################
                     [1m Learning iteration 377/1500 [0m                      

                       Computation: 32069 steps/s (collection: 1.069s, learning 0.054s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0269
               Mean surrogate loss: -0.0096
                 Mean entropy loss: 13.1551
                       Mean reward: 16.48
               Mean episode length: 891.37
Episode_Reward/track_lin_vel_xy_exp: 1.0076
Episode_Reward/track_ang_vel_z_exp: 0.4682
       Episode_Reward/lin_vel_z_l2: -0.0489
      Episode_Reward/ang_vel_xy_l2: -0.1378
     Episode_Reward/dof_torques_l2: -0.0854
         Episode_Reward/dof_acc_l2: -0.2060
     Episode_Reward/action_rate_l2: -0.1662
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.8473
Metrics/base_velocity/error_vel_xy: 0.5276
Metrics/base_velocity/error_vel_yaw: 0.6009
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 13608000
                    Iteration time: 1.12s
                      Time elapsed: 00:06:57
                               ETA: 00:20:40

################################################################################
                     [1m Learning iteration 378/1500 [0m                      

                       Computation: 32620 steps/s (collection: 1.052s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0261
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 13.1712
                       Mean reward: 16.58
               Mean episode length: 916.47
Episode_Reward/track_lin_vel_xy_exp: 0.8921
Episode_Reward/track_ang_vel_z_exp: 0.4451
       Episode_Reward/lin_vel_z_l2: -0.0452
      Episode_Reward/ang_vel_xy_l2: -0.1254
     Episode_Reward/dof_torques_l2: -0.0828
         Episode_Reward/dof_acc_l2: -0.1779
     Episode_Reward/action_rate_l2: -0.1549
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.8641
Metrics/base_velocity/error_vel_xy: 0.5708
Metrics/base_velocity/error_vel_yaw: 0.5493
      Episode_Termination/time_out: 2.0833
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 13644000
                    Iteration time: 1.10s
                      Time elapsed: 00:06:58
                               ETA: 00:20:39

################################################################################
                     [1m Learning iteration 379/1500 [0m                      

                       Computation: 33217 steps/s (collection: 1.032s, learning 0.052s)
             Mean action noise std: 0.73
          Mean value_function loss: 0.0227
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 13.1649
                       Mean reward: 14.67
               Mean episode length: 850.21
Episode_Reward/track_lin_vel_xy_exp: 0.8419
Episode_Reward/track_ang_vel_z_exp: 0.4119
       Episode_Reward/lin_vel_z_l2: -0.0473
      Episode_Reward/ang_vel_xy_l2: -0.1197
     Episode_Reward/dof_torques_l2: -0.0774
         Episode_Reward/dof_acc_l2: -0.1754
     Episode_Reward/action_rate_l2: -0.1475
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.8737
Metrics/base_velocity/error_vel_xy: 0.4895
Metrics/base_velocity/error_vel_yaw: 0.4916
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 13680000
                    Iteration time: 1.08s
                      Time elapsed: 00:06:59
                               ETA: 00:20:38

################################################################################
                     [1m Learning iteration 380/1500 [0m                      

                       Computation: 32740 steps/s (collection: 1.048s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0263
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 13.1588
                       Mean reward: 14.84
               Mean episode length: 867.01
Episode_Reward/track_lin_vel_xy_exp: 0.9721
Episode_Reward/track_ang_vel_z_exp: 0.4896
       Episode_Reward/lin_vel_z_l2: -0.0531
      Episode_Reward/ang_vel_xy_l2: -0.1425
     Episode_Reward/dof_torques_l2: -0.0890
         Episode_Reward/dof_acc_l2: -0.2154
     Episode_Reward/action_rate_l2: -0.1771
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.8816
Metrics/base_velocity/error_vel_xy: 0.6369
Metrics/base_velocity/error_vel_yaw: 0.6115
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 13716000
                    Iteration time: 1.10s
                      Time elapsed: 00:07:00
                               ETA: 00:20:36

################################################################################
                     [1m Learning iteration 381/1500 [0m                      

                       Computation: 32952 steps/s (collection: 1.040s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0282
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 13.1714
                       Mean reward: 16.07
               Mean episode length: 890.80
Episode_Reward/track_lin_vel_xy_exp: 0.9720
Episode_Reward/track_ang_vel_z_exp: 0.4607
       Episode_Reward/lin_vel_z_l2: -0.0492
      Episode_Reward/ang_vel_xy_l2: -0.1281
     Episode_Reward/dof_torques_l2: -0.0868
         Episode_Reward/dof_acc_l2: -0.1875
     Episode_Reward/action_rate_l2: -0.1593
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.8977
Metrics/base_velocity/error_vel_xy: 0.5063
Metrics/base_velocity/error_vel_yaw: 0.5428
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 13752000
                    Iteration time: 1.09s
                      Time elapsed: 00:07:01
                               ETA: 00:20:35

################################################################################
                     [1m Learning iteration 382/1500 [0m                      

                       Computation: 32747 steps/s (collection: 1.047s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0235
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 13.1717
                       Mean reward: 16.07
               Mean episode length: 891.57
Episode_Reward/track_lin_vel_xy_exp: 1.0577
Episode_Reward/track_ang_vel_z_exp: 0.4947
       Episode_Reward/lin_vel_z_l2: -0.0542
      Episode_Reward/ang_vel_xy_l2: -0.1409
     Episode_Reward/dof_torques_l2: -0.0872
         Episode_Reward/dof_acc_l2: -0.2016
     Episode_Reward/action_rate_l2: -0.1710
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.9113
Metrics/base_velocity/error_vel_xy: 0.4894
Metrics/base_velocity/error_vel_yaw: 0.5556
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 13788000
                    Iteration time: 1.10s
                      Time elapsed: 00:07:02
                               ETA: 00:20:34

################################################################################
                     [1m Learning iteration 383/1500 [0m                      

                       Computation: 32400 steps/s (collection: 1.059s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0263
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 13.1765
                       Mean reward: 17.16
               Mean episode length: 886.73
Episode_Reward/track_lin_vel_xy_exp: 1.0060
Episode_Reward/track_ang_vel_z_exp: 0.4556
       Episode_Reward/lin_vel_z_l2: -0.0450
      Episode_Reward/ang_vel_xy_l2: -0.1241
     Episode_Reward/dof_torques_l2: -0.0780
         Episode_Reward/dof_acc_l2: -0.1806
     Episode_Reward/action_rate_l2: -0.1528
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.9226
Metrics/base_velocity/error_vel_xy: 0.3967
Metrics/base_velocity/error_vel_yaw: 0.4880
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 13824000
                    Iteration time: 1.11s
                      Time elapsed: 00:07:04
                               ETA: 00:20:33

################################################################################
                     [1m Learning iteration 384/1500 [0m                      

                       Computation: 31222 steps/s (collection: 1.101s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0226
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 13.1760
                       Mean reward: 16.59
               Mean episode length: 883.79
Episode_Reward/track_lin_vel_xy_exp: 0.8379
Episode_Reward/track_ang_vel_z_exp: 0.4469
       Episode_Reward/lin_vel_z_l2: -0.0460
      Episode_Reward/ang_vel_xy_l2: -0.1299
     Episode_Reward/dof_torques_l2: -0.0844
         Episode_Reward/dof_acc_l2: -0.1849
     Episode_Reward/action_rate_l2: -0.1580
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.9339
Metrics/base_velocity/error_vel_xy: 0.6445
Metrics/base_velocity/error_vel_yaw: 0.5591
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 13860000
                    Iteration time: 1.15s
                      Time elapsed: 00:07:05
                               ETA: 00:20:32

################################################################################
                     [1m Learning iteration 385/1500 [0m                      

                       Computation: 31897 steps/s (collection: 1.075s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0257
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 13.1751
                       Mean reward: 15.95
               Mean episode length: 877.75
Episode_Reward/track_lin_vel_xy_exp: 0.9944
Episode_Reward/track_ang_vel_z_exp: 0.4781
       Episode_Reward/lin_vel_z_l2: -0.0506
      Episode_Reward/ang_vel_xy_l2: -0.1379
     Episode_Reward/dof_torques_l2: -0.0849
         Episode_Reward/dof_acc_l2: -0.2056
     Episode_Reward/action_rate_l2: -0.1662
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.9399
Metrics/base_velocity/error_vel_xy: 0.5032
Metrics/base_velocity/error_vel_yaw: 0.5248
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 13896000
                    Iteration time: 1.13s
                      Time elapsed: 00:07:06
                               ETA: 00:20:31

################################################################################
                     [1m Learning iteration 386/1500 [0m                      

                       Computation: 31386 steps/s (collection: 1.093s, learning 0.054s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0250
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 13.1828
                       Mean reward: 15.64
               Mean episode length: 876.76
Episode_Reward/track_lin_vel_xy_exp: 1.0541
Episode_Reward/track_ang_vel_z_exp: 0.5043
       Episode_Reward/lin_vel_z_l2: -0.0494
      Episode_Reward/ang_vel_xy_l2: -0.1376
     Episode_Reward/dof_torques_l2: -0.0873
         Episode_Reward/dof_acc_l2: -0.1990
     Episode_Reward/action_rate_l2: -0.1703
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.9517
Metrics/base_velocity/error_vel_xy: 0.5076
Metrics/base_velocity/error_vel_yaw: 0.5353
      Episode_Termination/time_out: 2.2083
  Episode_Termination/base_contact: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 13932000
                    Iteration time: 1.15s
                      Time elapsed: 00:07:07
                               ETA: 00:20:30

################################################################################
                     [1m Learning iteration 387/1500 [0m                      

                       Computation: 31931 steps/s (collection: 1.072s, learning 0.055s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0251
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 13.1883
                       Mean reward: 17.26
               Mean episode length: 915.57
Episode_Reward/track_lin_vel_xy_exp: 1.0259
Episode_Reward/track_ang_vel_z_exp: 0.4873
       Episode_Reward/lin_vel_z_l2: -0.0494
      Episode_Reward/ang_vel_xy_l2: -0.1356
     Episode_Reward/dof_torques_l2: -0.0856
         Episode_Reward/dof_acc_l2: -0.1994
     Episode_Reward/action_rate_l2: -0.1663
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.9637
Metrics/base_velocity/error_vel_xy: 0.4981
Metrics/base_velocity/error_vel_yaw: 0.5243
      Episode_Termination/time_out: 2.1667
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 13968000
                    Iteration time: 1.13s
                      Time elapsed: 00:07:08
                               ETA: 00:20:29

################################################################################
                     [1m Learning iteration 388/1500 [0m                      

                       Computation: 32145 steps/s (collection: 1.068s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0245
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 13.1795
                       Mean reward: 16.23
               Mean episode length: 865.66
Episode_Reward/track_lin_vel_xy_exp: 0.8440
Episode_Reward/track_ang_vel_z_exp: 0.4115
       Episode_Reward/lin_vel_z_l2: -0.0445
      Episode_Reward/ang_vel_xy_l2: -0.1162
     Episode_Reward/dof_torques_l2: -0.0781
         Episode_Reward/dof_acc_l2: -0.1651
     Episode_Reward/action_rate_l2: -0.1429
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.9698
Metrics/base_velocity/error_vel_xy: 0.4813
Metrics/base_velocity/error_vel_yaw: 0.5026
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 14004000
                    Iteration time: 1.12s
                      Time elapsed: 00:07:09
                               ETA: 00:20:28

################################################################################
                     [1m Learning iteration 389/1500 [0m                      

                       Computation: 33302 steps/s (collection: 1.028s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0288
               Mean surrogate loss: -0.0105
                 Mean entropy loss: 13.1820
                       Mean reward: 15.70
               Mean episode length: 866.76
Episode_Reward/track_lin_vel_xy_exp: 0.9354
Episode_Reward/track_ang_vel_z_exp: 0.4718
       Episode_Reward/lin_vel_z_l2: -0.0523
      Episode_Reward/ang_vel_xy_l2: -0.1384
     Episode_Reward/dof_torques_l2: -0.0839
         Episode_Reward/dof_acc_l2: -0.2092
     Episode_Reward/action_rate_l2: -0.1655
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.9796
Metrics/base_velocity/error_vel_xy: 0.5596
Metrics/base_velocity/error_vel_yaw: 0.5410
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 14040000
                    Iteration time: 1.08s
                      Time elapsed: 00:07:10
                               ETA: 00:20:27

################################################################################
                     [1m Learning iteration 390/1500 [0m                      

                       Computation: 32956 steps/s (collection: 1.038s, learning 0.054s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0254
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 13.1818
                       Mean reward: 15.21
               Mean episode length: 881.05
Episode_Reward/track_lin_vel_xy_exp: 0.9223
Episode_Reward/track_ang_vel_z_exp: 0.4536
       Episode_Reward/lin_vel_z_l2: -0.0552
      Episode_Reward/ang_vel_xy_l2: -0.1355
     Episode_Reward/dof_torques_l2: -0.0866
         Episode_Reward/dof_acc_l2: -0.2225
     Episode_Reward/action_rate_l2: -0.1680
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.9883
Metrics/base_velocity/error_vel_xy: 0.5743
Metrics/base_velocity/error_vel_yaw: 0.5700
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 14076000
                    Iteration time: 1.09s
                      Time elapsed: 00:07:11
                               ETA: 00:20:26

################################################################################
                     [1m Learning iteration 391/1500 [0m                      

                       Computation: 33810 steps/s (collection: 1.011s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0292
               Mean surrogate loss: -0.0089
                 Mean entropy loss: 13.1746
                       Mean reward: 15.71
               Mean episode length: 901.74
Episode_Reward/track_lin_vel_xy_exp: 0.9659
Episode_Reward/track_ang_vel_z_exp: 0.4747
       Episode_Reward/lin_vel_z_l2: -0.0484
      Episode_Reward/ang_vel_xy_l2: -0.1337
     Episode_Reward/dof_torques_l2: -0.0808
         Episode_Reward/dof_acc_l2: -0.1882
     Episode_Reward/action_rate_l2: -0.1612
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 1.9974
Metrics/base_velocity/error_vel_xy: 0.5259
Metrics/base_velocity/error_vel_yaw: 0.5330
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 14112000
                    Iteration time: 1.06s
                      Time elapsed: 00:07:13
                               ETA: 00:20:24

################################################################################
                     [1m Learning iteration 392/1500 [0m                      

                       Computation: 33409 steps/s (collection: 1.025s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0281
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 13.1762
                       Mean reward: 15.67
               Mean episode length: 898.76
Episode_Reward/track_lin_vel_xy_exp: 0.9982
Episode_Reward/track_ang_vel_z_exp: 0.5000
       Episode_Reward/lin_vel_z_l2: -0.0567
      Episode_Reward/ang_vel_xy_l2: -0.1389
     Episode_Reward/dof_torques_l2: -0.0918
         Episode_Reward/dof_acc_l2: -0.2051
     Episode_Reward/action_rate_l2: -0.1752
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.0063
Metrics/base_velocity/error_vel_xy: 0.5802
Metrics/base_velocity/error_vel_yaw: 0.5554
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 14148000
                    Iteration time: 1.08s
                      Time elapsed: 00:07:14
                               ETA: 00:20:23

################################################################################
                     [1m Learning iteration 393/1500 [0m                      

                       Computation: 33094 steps/s (collection: 1.033s, learning 0.055s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0290
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 13.1803
                       Mean reward: 14.72
               Mean episode length: 872.28
Episode_Reward/track_lin_vel_xy_exp: 0.8525
Episode_Reward/track_ang_vel_z_exp: 0.4226
       Episode_Reward/lin_vel_z_l2: -0.0468
      Episode_Reward/ang_vel_xy_l2: -0.1205
     Episode_Reward/dof_torques_l2: -0.0762
         Episode_Reward/dof_acc_l2: -0.1824
     Episode_Reward/action_rate_l2: -0.1478
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.0160
Metrics/base_velocity/error_vel_xy: 0.4958
Metrics/base_velocity/error_vel_yaw: 0.4673
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 14184000
                    Iteration time: 1.09s
                      Time elapsed: 00:07:15
                               ETA: 00:20:22

################################################################################
                     [1m Learning iteration 394/1500 [0m                      

                       Computation: 31390 steps/s (collection: 1.095s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0278
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 13.1872
                       Mean reward: 14.16
               Mean episode length: 835.02
Episode_Reward/track_lin_vel_xy_exp: 0.8373
Episode_Reward/track_ang_vel_z_exp: 0.4254
       Episode_Reward/lin_vel_z_l2: -0.0484
      Episode_Reward/ang_vel_xy_l2: -0.1229
     Episode_Reward/dof_torques_l2: -0.0777
         Episode_Reward/dof_acc_l2: -0.1802
     Episode_Reward/action_rate_l2: -0.1512
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.0271
Metrics/base_velocity/error_vel_xy: 0.5490
Metrics/base_velocity/error_vel_yaw: 0.5204
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 14220000
                    Iteration time: 1.15s
                      Time elapsed: 00:07:16
                               ETA: 00:20:21

################################################################################
                     [1m Learning iteration 395/1500 [0m                      

                       Computation: 32219 steps/s (collection: 1.066s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0276
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 13.1947
                       Mean reward: 15.29
               Mean episode length: 873.70
Episode_Reward/track_lin_vel_xy_exp: 1.0880
Episode_Reward/track_ang_vel_z_exp: 0.5025
       Episode_Reward/lin_vel_z_l2: -0.0553
      Episode_Reward/ang_vel_xy_l2: -0.1410
     Episode_Reward/dof_torques_l2: -0.0920
         Episode_Reward/dof_acc_l2: -0.2208
     Episode_Reward/action_rate_l2: -0.1764
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.0403
Metrics/base_velocity/error_vel_xy: 0.4784
Metrics/base_velocity/error_vel_yaw: 0.5517
      Episode_Termination/time_out: 2.0833
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 14256000
                    Iteration time: 1.12s
                      Time elapsed: 00:07:17
                               ETA: 00:20:20

################################################################################
                     [1m Learning iteration 396/1500 [0m                      

                       Computation: 31335 steps/s (collection: 1.097s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0223
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 13.1925
                       Mean reward: 15.82
               Mean episode length: 908.98
Episode_Reward/track_lin_vel_xy_exp: 0.9180
Episode_Reward/track_ang_vel_z_exp: 0.4576
       Episode_Reward/lin_vel_z_l2: -0.0494
      Episode_Reward/ang_vel_xy_l2: -0.1328
     Episode_Reward/dof_torques_l2: -0.0893
         Episode_Reward/dof_acc_l2: -0.1997
     Episode_Reward/action_rate_l2: -0.1649
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.0604
Metrics/base_velocity/error_vel_xy: 0.5996
Metrics/base_velocity/error_vel_yaw: 0.5785
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 14292000
                    Iteration time: 1.15s
                      Time elapsed: 00:07:18
                               ETA: 00:20:19

################################################################################
                     [1m Learning iteration 397/1500 [0m                      

                       Computation: 32656 steps/s (collection: 1.049s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0271
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 13.1918
                       Mean reward: 15.29
               Mean episode length: 890.91
Episode_Reward/track_lin_vel_xy_exp: 0.8783
Episode_Reward/track_ang_vel_z_exp: 0.4153
       Episode_Reward/lin_vel_z_l2: -0.0527
      Episode_Reward/ang_vel_xy_l2: -0.1278
     Episode_Reward/dof_torques_l2: -0.0737
         Episode_Reward/dof_acc_l2: -0.1831
     Episode_Reward/action_rate_l2: -0.1462
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.0717
Metrics/base_velocity/error_vel_xy: 0.4538
Metrics/base_velocity/error_vel_yaw: 0.4920
      Episode_Termination/time_out: 2.0417
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 14328000
                    Iteration time: 1.10s
                      Time elapsed: 00:07:19
                               ETA: 00:20:18

################################################################################
                     [1m Learning iteration 398/1500 [0m                      

                       Computation: 31849 steps/s (collection: 1.076s, learning 0.054s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0284
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 13.1861
                       Mean reward: 14.32
               Mean episode length: 879.34
Episode_Reward/track_lin_vel_xy_exp: 0.9694
Episode_Reward/track_ang_vel_z_exp: 0.4907
       Episode_Reward/lin_vel_z_l2: -0.0507
      Episode_Reward/ang_vel_xy_l2: -0.1383
     Episode_Reward/dof_torques_l2: -0.0936
         Episode_Reward/dof_acc_l2: -0.2046
     Episode_Reward/action_rate_l2: -0.1739
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.0784
Metrics/base_velocity/error_vel_xy: 0.6352
Metrics/base_velocity/error_vel_yaw: 0.5776
      Episode_Termination/time_out: 2.0417
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 14364000
                    Iteration time: 1.13s
                      Time elapsed: 00:07:20
                               ETA: 00:20:17

################################################################################
                     [1m Learning iteration 399/1500 [0m                      

                       Computation: 32399 steps/s (collection: 1.059s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0304
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 13.1869
                       Mean reward: 15.25
               Mean episode length: 890.88
Episode_Reward/track_lin_vel_xy_exp: 0.9584
Episode_Reward/track_ang_vel_z_exp: 0.4745
       Episode_Reward/lin_vel_z_l2: -0.0450
      Episode_Reward/ang_vel_xy_l2: -0.1278
     Episode_Reward/dof_torques_l2: -0.0842
         Episode_Reward/dof_acc_l2: -0.1823
     Episode_Reward/action_rate_l2: -0.1606
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.0901
Metrics/base_velocity/error_vel_xy: 0.5293
Metrics/base_velocity/error_vel_yaw: 0.5242
      Episode_Termination/time_out: 1.9583
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 14400000
                    Iteration time: 1.11s
                      Time elapsed: 00:07:21
                               ETA: 00:20:16

################################################################################
                     [1m Learning iteration 400/1500 [0m                      

                       Computation: 32002 steps/s (collection: 1.069s, learning 0.056s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0273
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 13.1869
                       Mean reward: 16.50
               Mean episode length: 906.21
Episode_Reward/track_lin_vel_xy_exp: 0.9222
Episode_Reward/track_ang_vel_z_exp: 0.4333
       Episode_Reward/lin_vel_z_l2: -0.0468
      Episode_Reward/ang_vel_xy_l2: -0.1219
     Episode_Reward/dof_torques_l2: -0.0785
         Episode_Reward/dof_acc_l2: -0.1806
     Episode_Reward/action_rate_l2: -0.1510
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.0978
Metrics/base_velocity/error_vel_xy: 0.4595
Metrics/base_velocity/error_vel_yaw: 0.5037
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 14436000
                    Iteration time: 1.12s
                      Time elapsed: 00:07:23
                               ETA: 00:20:15

################################################################################
                     [1m Learning iteration 401/1500 [0m                      

                       Computation: 32009 steps/s (collection: 1.071s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0281
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 13.1785
                       Mean reward: 15.38
               Mean episode length: 851.22
Episode_Reward/track_lin_vel_xy_exp: 0.8278
Episode_Reward/track_ang_vel_z_exp: 0.4141
       Episode_Reward/lin_vel_z_l2: -0.0444
      Episode_Reward/ang_vel_xy_l2: -0.1164
     Episode_Reward/dof_torques_l2: -0.0728
         Episode_Reward/dof_acc_l2: -0.1667
     Episode_Reward/action_rate_l2: -0.1419
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.1091
Metrics/base_velocity/error_vel_xy: 0.4863
Metrics/base_velocity/error_vel_yaw: 0.4699
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 14472000
                    Iteration time: 1.12s
                      Time elapsed: 00:07:24
                               ETA: 00:20:14

################################################################################
                     [1m Learning iteration 402/1500 [0m                      

                       Computation: 32551 steps/s (collection: 1.055s, learning 0.051s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0267
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 13.1691
                       Mean reward: 15.90
               Mean episode length: 873.40
Episode_Reward/track_lin_vel_xy_exp: 0.9779
Episode_Reward/track_ang_vel_z_exp: 0.4746
       Episode_Reward/lin_vel_z_l2: -0.0475
      Episode_Reward/ang_vel_xy_l2: -0.1326
     Episode_Reward/dof_torques_l2: -0.0836
         Episode_Reward/dof_acc_l2: -0.1872
     Episode_Reward/action_rate_l2: -0.1610
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.1208
Metrics/base_velocity/error_vel_xy: 0.5269
Metrics/base_velocity/error_vel_yaw: 0.5397
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 14508000
                    Iteration time: 1.11s
                      Time elapsed: 00:07:25
                               ETA: 00:20:13

################################################################################
                     [1m Learning iteration 403/1500 [0m                      

                       Computation: 33023 steps/s (collection: 1.039s, learning 0.051s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0276
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 13.1688
                       Mean reward: 16.20
               Mean episode length: 848.68
Episode_Reward/track_lin_vel_xy_exp: 0.8897
Episode_Reward/track_ang_vel_z_exp: 0.4154
       Episode_Reward/lin_vel_z_l2: -0.0478
      Episode_Reward/ang_vel_xy_l2: -0.1131
     Episode_Reward/dof_torques_l2: -0.0735
         Episode_Reward/dof_acc_l2: -0.1746
     Episode_Reward/action_rate_l2: -0.1435
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.1287
Metrics/base_velocity/error_vel_xy: 0.4078
Metrics/base_velocity/error_vel_yaw: 0.4603
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 14544000
                    Iteration time: 1.09s
                      Time elapsed: 00:07:26
                               ETA: 00:20:12

################################################################################
                     [1m Learning iteration 404/1500 [0m                      

                       Computation: 32493 steps/s (collection: 1.058s, learning 0.049s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0356
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 13.1897
                       Mean reward: 16.97
               Mean episode length: 855.53
Episode_Reward/track_lin_vel_xy_exp: 1.1201
Episode_Reward/track_ang_vel_z_exp: 0.5108
       Episode_Reward/lin_vel_z_l2: -0.0493
      Episode_Reward/ang_vel_xy_l2: -0.1355
     Episode_Reward/dof_torques_l2: -0.0813
         Episode_Reward/dof_acc_l2: -0.1906
     Episode_Reward/action_rate_l2: -0.1666
      Episode_Reward/feet_air_time: -0.0017
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.1345
Metrics/base_velocity/error_vel_xy: 0.4362
Metrics/base_velocity/error_vel_yaw: 0.5130
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 14580000
                    Iteration time: 1.11s
                      Time elapsed: 00:07:27
                               ETA: 00:20:10

################################################################################
                     [1m Learning iteration 405/1500 [0m                      

                       Computation: 33083 steps/s (collection: 1.037s, learning 0.051s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0254
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 13.2156
                       Mean reward: 16.65
               Mean episode length: 860.91
Episode_Reward/track_lin_vel_xy_exp: 0.8894
Episode_Reward/track_ang_vel_z_exp: 0.4346
       Episode_Reward/lin_vel_z_l2: -0.0494
      Episode_Reward/ang_vel_xy_l2: -0.1324
     Episode_Reward/dof_torques_l2: -0.0821
         Episode_Reward/dof_acc_l2: -0.1796
     Episode_Reward/action_rate_l2: -0.1569
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.1521
Metrics/base_velocity/error_vel_xy: 0.5658
Metrics/base_velocity/error_vel_yaw: 0.5934
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 14616000
                    Iteration time: 1.09s
                      Time elapsed: 00:07:28
                               ETA: 00:20:09

################################################################################
                     [1m Learning iteration 406/1500 [0m                      

                       Computation: 33756 steps/s (collection: 1.013s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0289
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 13.2254
                       Mean reward: 15.94
               Mean episode length: 866.33
Episode_Reward/track_lin_vel_xy_exp: 0.8942
Episode_Reward/track_ang_vel_z_exp: 0.4402
       Episode_Reward/lin_vel_z_l2: -0.0480
      Episode_Reward/ang_vel_xy_l2: -0.1274
     Episode_Reward/dof_torques_l2: -0.0789
         Episode_Reward/dof_acc_l2: -0.1872
     Episode_Reward/action_rate_l2: -0.1529
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.1593
Metrics/base_velocity/error_vel_xy: 0.5124
Metrics/base_velocity/error_vel_yaw: 0.5098
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 14652000
                    Iteration time: 1.07s
                      Time elapsed: 00:07:29
                               ETA: 00:20:08

################################################################################
                     [1m Learning iteration 407/1500 [0m                      

                       Computation: 32662 steps/s (collection: 1.050s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0307
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 13.2378
                       Mean reward: 14.74
               Mean episode length: 840.86
Episode_Reward/track_lin_vel_xy_exp: 0.8252
Episode_Reward/track_ang_vel_z_exp: 0.4268
       Episode_Reward/lin_vel_z_l2: -0.0492
      Episode_Reward/ang_vel_xy_l2: -0.1223
     Episode_Reward/dof_torques_l2: -0.0827
         Episode_Reward/dof_acc_l2: -0.1739
     Episode_Reward/action_rate_l2: -0.1506
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.1626
Metrics/base_velocity/error_vel_xy: 0.5607
Metrics/base_velocity/error_vel_yaw: 0.5257
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 14688000
                    Iteration time: 1.10s
                      Time elapsed: 00:07:30
                               ETA: 00:20:07

################################################################################
                     [1m Learning iteration 408/1500 [0m                      

                       Computation: 32349 steps/s (collection: 1.060s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0279
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 13.2392
                       Mean reward: 14.40
               Mean episode length: 836.06
Episode_Reward/track_lin_vel_xy_exp: 0.8356
Episode_Reward/track_ang_vel_z_exp: 0.4289
       Episode_Reward/lin_vel_z_l2: -0.0494
      Episode_Reward/ang_vel_xy_l2: -0.1206
     Episode_Reward/dof_torques_l2: -0.0814
         Episode_Reward/dof_acc_l2: -0.1778
     Episode_Reward/action_rate_l2: -0.1527
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.1676
Metrics/base_velocity/error_vel_xy: 0.5670
Metrics/base_velocity/error_vel_yaw: 0.5219
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 14724000
                    Iteration time: 1.11s
                      Time elapsed: 00:07:31
                               ETA: 00:20:06

################################################################################
                     [1m Learning iteration 409/1500 [0m                      

                       Computation: 33555 steps/s (collection: 1.018s, learning 0.055s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0289
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 13.2406
                       Mean reward: 15.54
               Mean episode length: 860.89
Episode_Reward/track_lin_vel_xy_exp: 0.8814
Episode_Reward/track_ang_vel_z_exp: 0.4430
       Episode_Reward/lin_vel_z_l2: -0.0564
      Episode_Reward/ang_vel_xy_l2: -0.1316
     Episode_Reward/dof_torques_l2: -0.0876
         Episode_Reward/dof_acc_l2: -0.1913
     Episode_Reward/action_rate_l2: -0.1622
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.1761
Metrics/base_velocity/error_vel_xy: 0.5961
Metrics/base_velocity/error_vel_yaw: 0.5874
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 14760000
                    Iteration time: 1.07s
                      Time elapsed: 00:07:32
                               ETA: 00:20:05

################################################################################
                     [1m Learning iteration 410/1500 [0m                      

                       Computation: 31624 steps/s (collection: 1.086s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0281
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 13.2476
                       Mean reward: 14.27
               Mean episode length: 839.26
Episode_Reward/track_lin_vel_xy_exp: 0.9215
Episode_Reward/track_ang_vel_z_exp: 0.4546
       Episode_Reward/lin_vel_z_l2: -0.0495
      Episode_Reward/ang_vel_xy_l2: -0.1273
     Episode_Reward/dof_torques_l2: -0.0875
         Episode_Reward/dof_acc_l2: -0.1835
     Episode_Reward/action_rate_l2: -0.1604
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.1861
Metrics/base_velocity/error_vel_xy: 0.5444
Metrics/base_velocity/error_vel_yaw: 0.5305
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 14796000
                    Iteration time: 1.14s
                      Time elapsed: 00:07:34
                               ETA: 00:20:04

################################################################################
                     [1m Learning iteration 411/1500 [0m                      

                       Computation: 32736 steps/s (collection: 1.047s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0256
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 13.2539
                       Mean reward: 13.01
               Mean episode length: 795.48
Episode_Reward/track_lin_vel_xy_exp: 0.7814
Episode_Reward/track_ang_vel_z_exp: 0.3884
       Episode_Reward/lin_vel_z_l2: -0.0479
      Episode_Reward/ang_vel_xy_l2: -0.1119
     Episode_Reward/dof_torques_l2: -0.0721
         Episode_Reward/dof_acc_l2: -0.1749
     Episode_Reward/action_rate_l2: -0.1394
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.1923
Metrics/base_velocity/error_vel_xy: 0.4799
Metrics/base_velocity/error_vel_yaw: 0.4735
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 14832000
                    Iteration time: 1.10s
                      Time elapsed: 00:07:35
                               ETA: 00:20:03

################################################################################
                     [1m Learning iteration 412/1500 [0m                      

                       Computation: 33831 steps/s (collection: 1.014s, learning 0.050s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0305
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 13.2443
                       Mean reward: 13.05
               Mean episode length: 794.24
Episode_Reward/track_lin_vel_xy_exp: 0.9342
Episode_Reward/track_ang_vel_z_exp: 0.4663
       Episode_Reward/lin_vel_z_l2: -0.0505
      Episode_Reward/ang_vel_xy_l2: -0.1343
     Episode_Reward/dof_torques_l2: -0.0846
         Episode_Reward/dof_acc_l2: -0.1942
     Episode_Reward/action_rate_l2: -0.1644
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.1989
Metrics/base_velocity/error_vel_xy: 0.5850
Metrics/base_velocity/error_vel_yaw: 0.5685
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 14868000
                    Iteration time: 1.06s
                      Time elapsed: 00:07:36
                               ETA: 00:20:01

################################################################################
                     [1m Learning iteration 413/1500 [0m                      

                       Computation: 34006 steps/s (collection: 1.008s, learning 0.050s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0262
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 13.2457
                       Mean reward: 13.79
               Mean episode length: 836.17
Episode_Reward/track_lin_vel_xy_exp: 0.7853
Episode_Reward/track_ang_vel_z_exp: 0.3890
       Episode_Reward/lin_vel_z_l2: -0.0570
      Episode_Reward/ang_vel_xy_l2: -0.1215
     Episode_Reward/dof_torques_l2: -0.0725
         Episode_Reward/dof_acc_l2: -0.1849
     Episode_Reward/action_rate_l2: -0.1433
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.2086
Metrics/base_velocity/error_vel_xy: 0.5148
Metrics/base_velocity/error_vel_yaw: 0.4875
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 14904000
                    Iteration time: 1.06s
                      Time elapsed: 00:07:37
                               ETA: 00:20:00

################################################################################
                     [1m Learning iteration 414/1500 [0m                      

                       Computation: 33034 steps/s (collection: 1.037s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0288
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 13.2458
                       Mean reward: 14.25
               Mean episode length: 831.04
Episode_Reward/track_lin_vel_xy_exp: 0.9208
Episode_Reward/track_ang_vel_z_exp: 0.4412
       Episode_Reward/lin_vel_z_l2: -0.0548
      Episode_Reward/ang_vel_xy_l2: -0.1300
     Episode_Reward/dof_torques_l2: -0.0789
         Episode_Reward/dof_acc_l2: -0.1982
     Episode_Reward/action_rate_l2: -0.1584
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.2181
Metrics/base_velocity/error_vel_xy: 0.4842
Metrics/base_velocity/error_vel_yaw: 0.5017
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 14940000
                    Iteration time: 1.09s
                      Time elapsed: 00:07:38
                               ETA: 00:19:59

################################################################################
                     [1m Learning iteration 415/1500 [0m                      

                       Computation: 32282 steps/s (collection: 1.061s, learning 0.054s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0244
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 13.2474
                       Mean reward: 14.46
               Mean episode length: 827.95
Episode_Reward/track_lin_vel_xy_exp: 0.9194
Episode_Reward/track_ang_vel_z_exp: 0.4402
       Episode_Reward/lin_vel_z_l2: -0.0492
      Episode_Reward/ang_vel_xy_l2: -0.1222
     Episode_Reward/dof_torques_l2: -0.0802
         Episode_Reward/dof_acc_l2: -0.1830
     Episode_Reward/action_rate_l2: -0.1553
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.2271
Metrics/base_velocity/error_vel_xy: 0.4713
Metrics/base_velocity/error_vel_yaw: 0.4997
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 14976000
                    Iteration time: 1.12s
                      Time elapsed: 00:07:39
                               ETA: 00:19:58

################################################################################
                     [1m Learning iteration 416/1500 [0m                      

                       Computation: 34261 steps/s (collection: 0.999s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0318
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 13.2531
                       Mean reward: 15.25
               Mean episode length: 831.53
Episode_Reward/track_lin_vel_xy_exp: 0.8577
Episode_Reward/track_ang_vel_z_exp: 0.4209
       Episode_Reward/lin_vel_z_l2: -0.0472
      Episode_Reward/ang_vel_xy_l2: -0.1173
     Episode_Reward/dof_torques_l2: -0.0771
         Episode_Reward/dof_acc_l2: -0.1783
     Episode_Reward/action_rate_l2: -0.1475
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.2334
Metrics/base_velocity/error_vel_xy: 0.4923
Metrics/base_velocity/error_vel_yaw: 0.4875
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 15012000
                    Iteration time: 1.05s
                      Time elapsed: 00:07:40
                               ETA: 00:19:57

################################################################################
                     [1m Learning iteration 417/1500 [0m                      

                       Computation: 34525 steps/s (collection: 0.989s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0267
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 13.2698
                       Mean reward: 13.32
               Mean episode length: 763.42
Episode_Reward/track_lin_vel_xy_exp: 0.8643
Episode_Reward/track_ang_vel_z_exp: 0.4172
       Episode_Reward/lin_vel_z_l2: -0.0475
      Episode_Reward/ang_vel_xy_l2: -0.1216
     Episode_Reward/dof_torques_l2: -0.0716
         Episode_Reward/dof_acc_l2: -0.1967
     Episode_Reward/action_rate_l2: -0.1477
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.2404
Metrics/base_velocity/error_vel_xy: 0.4882
Metrics/base_velocity/error_vel_yaw: 0.5008
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 15048000
                    Iteration time: 1.04s
                      Time elapsed: 00:07:41
                               ETA: 00:19:55

################################################################################
                     [1m Learning iteration 418/1500 [0m                      

                       Computation: 33804 steps/s (collection: 1.014s, learning 0.051s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0286
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 13.2959
                       Mean reward: 13.04
               Mean episode length: 780.17
Episode_Reward/track_lin_vel_xy_exp: 0.7852
Episode_Reward/track_ang_vel_z_exp: 0.3972
       Episode_Reward/lin_vel_z_l2: -0.0513
      Episode_Reward/ang_vel_xy_l2: -0.1162
     Episode_Reward/dof_torques_l2: -0.0772
         Episode_Reward/dof_acc_l2: -0.1816
     Episode_Reward/action_rate_l2: -0.1468
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.2441
Metrics/base_velocity/error_vel_xy: 0.5359
Metrics/base_velocity/error_vel_yaw: 0.5179
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 15084000
                    Iteration time: 1.06s
                      Time elapsed: 00:07:42
                               ETA: 00:19:54

################################################################################
                     [1m Learning iteration 419/1500 [0m                      

                       Computation: 32658 steps/s (collection: 1.050s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0277
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 13.3011
                       Mean reward: 14.01
               Mean episode length: 816.18
Episode_Reward/track_lin_vel_xy_exp: 0.9521
Episode_Reward/track_ang_vel_z_exp: 0.4465
       Episode_Reward/lin_vel_z_l2: -0.0495
      Episode_Reward/ang_vel_xy_l2: -0.1291
     Episode_Reward/dof_torques_l2: -0.0771
         Episode_Reward/dof_acc_l2: -0.1979
     Episode_Reward/action_rate_l2: -0.1582
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.2573
Metrics/base_velocity/error_vel_xy: 0.4521
Metrics/base_velocity/error_vel_yaw: 0.5001
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 15120000
                    Iteration time: 1.10s
                      Time elapsed: 00:07:43
                               ETA: 00:19:53

################################################################################
                     [1m Learning iteration 420/1500 [0m                      

                       Computation: 31787 steps/s (collection: 1.081s, learning 0.051s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0260
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 13.3011
                       Mean reward: 15.29
               Mean episode length: 858.78
Episode_Reward/track_lin_vel_xy_exp: 0.9322
Episode_Reward/track_ang_vel_z_exp: 0.4658
       Episode_Reward/lin_vel_z_l2: -0.0488
      Episode_Reward/ang_vel_xy_l2: -0.1263
     Episode_Reward/dof_torques_l2: -0.0853
         Episode_Reward/dof_acc_l2: -0.1876
     Episode_Reward/action_rate_l2: -0.1630
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.2728
Metrics/base_velocity/error_vel_xy: 0.5500
Metrics/base_velocity/error_vel_yaw: 0.5112
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 15156000
                    Iteration time: 1.13s
                      Time elapsed: 00:07:44
                               ETA: 00:19:52

################################################################################
                     [1m Learning iteration 421/1500 [0m                      

                       Computation: 32739 steps/s (collection: 1.048s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0268
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 13.2952
                       Mean reward: 15.26
               Mean episode length: 857.02
Episode_Reward/track_lin_vel_xy_exp: 0.8506
Episode_Reward/track_ang_vel_z_exp: 0.4341
       Episode_Reward/lin_vel_z_l2: -0.0509
      Episode_Reward/ang_vel_xy_l2: -0.1255
     Episode_Reward/dof_torques_l2: -0.0776
         Episode_Reward/dof_acc_l2: -0.1840
     Episode_Reward/action_rate_l2: -0.1537
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.2825
Metrics/base_velocity/error_vel_xy: 0.5611
Metrics/base_velocity/error_vel_yaw: 0.5188
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 15192000
                    Iteration time: 1.10s
                      Time elapsed: 00:07:45
                               ETA: 00:19:51

################################################################################
                     [1m Learning iteration 422/1500 [0m                      

                       Computation: 34005 steps/s (collection: 1.006s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0271
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 13.2932
                       Mean reward: 13.64
               Mean episode length: 845.85
Episode_Reward/track_lin_vel_xy_exp: 0.8817
Episode_Reward/track_ang_vel_z_exp: 0.4391
       Episode_Reward/lin_vel_z_l2: -0.0490
      Episode_Reward/ang_vel_xy_l2: -0.1260
     Episode_Reward/dof_torques_l2: -0.0824
         Episode_Reward/dof_acc_l2: -0.1941
     Episode_Reward/action_rate_l2: -0.1585
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.2880
Metrics/base_velocity/error_vel_xy: 0.5281
Metrics/base_velocity/error_vel_yaw: 0.5282
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 15228000
                    Iteration time: 1.06s
                      Time elapsed: 00:07:47
                               ETA: 00:19:50

################################################################################
                     [1m Learning iteration 423/1500 [0m                      

                       Computation: 33271 steps/s (collection: 1.029s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0237
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 13.2888
                       Mean reward: 14.94
               Mean episode length: 877.13
Episode_Reward/track_lin_vel_xy_exp: 1.0215
Episode_Reward/track_ang_vel_z_exp: 0.4897
       Episode_Reward/lin_vel_z_l2: -0.0531
      Episode_Reward/ang_vel_xy_l2: -0.1341
     Episode_Reward/dof_torques_l2: -0.0884
         Episode_Reward/dof_acc_l2: -0.2004
     Episode_Reward/action_rate_l2: -0.1713
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.2993
Metrics/base_velocity/error_vel_xy: 0.5163
Metrics/base_velocity/error_vel_yaw: 0.5415
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 15264000
                    Iteration time: 1.08s
                      Time elapsed: 00:07:48
                               ETA: 00:19:49

################################################################################
                     [1m Learning iteration 424/1500 [0m                      

                       Computation: 31860 steps/s (collection: 1.075s, learning 0.055s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0253
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 13.2869
                       Mean reward: 16.34
               Mean episode length: 923.42
Episode_Reward/track_lin_vel_xy_exp: 1.0935
Episode_Reward/track_ang_vel_z_exp: 0.5096
       Episode_Reward/lin_vel_z_l2: -0.0605
      Episode_Reward/ang_vel_xy_l2: -0.1431
     Episode_Reward/dof_torques_l2: -0.0889
         Episode_Reward/dof_acc_l2: -0.2282
     Episode_Reward/action_rate_l2: -0.1800
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.3159
Metrics/base_velocity/error_vel_xy: 0.4983
Metrics/base_velocity/error_vel_yaw: 0.5536
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 15300000
                    Iteration time: 1.13s
                      Time elapsed: 00:07:49
                               ETA: 00:19:48

################################################################################
                     [1m Learning iteration 425/1500 [0m                      

                       Computation: 32456 steps/s (collection: 1.060s, learning 0.050s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0271
               Mean surrogate loss: -0.0094
                 Mean entropy loss: 13.2854
                       Mean reward: 16.39
               Mean episode length: 923.84
Episode_Reward/track_lin_vel_xy_exp: 0.8634
Episode_Reward/track_ang_vel_z_exp: 0.4143
       Episode_Reward/lin_vel_z_l2: -0.0491
      Episode_Reward/ang_vel_xy_l2: -0.1178
     Episode_Reward/dof_torques_l2: -0.0734
         Episode_Reward/dof_acc_l2: -0.1764
     Episode_Reward/action_rate_l2: -0.1459
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.3283
Metrics/base_velocity/error_vel_xy: 0.4485
Metrics/base_velocity/error_vel_yaw: 0.4492
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 15336000
                    Iteration time: 1.11s
                      Time elapsed: 00:07:50
                               ETA: 00:19:46

################################################################################
                     [1m Learning iteration 426/1500 [0m                      

                       Computation: 31959 steps/s (collection: 1.074s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0253
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 13.2972
                       Mean reward: 16.41
               Mean episode length: 904.37
Episode_Reward/track_lin_vel_xy_exp: 0.9876
Episode_Reward/track_ang_vel_z_exp: 0.4781
       Episode_Reward/lin_vel_z_l2: -0.0502
      Episode_Reward/ang_vel_xy_l2: -0.1325
     Episode_Reward/dof_torques_l2: -0.0859
         Episode_Reward/dof_acc_l2: -0.2048
     Episode_Reward/action_rate_l2: -0.1656
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.3372
Metrics/base_velocity/error_vel_xy: 0.5117
Metrics/base_velocity/error_vel_yaw: 0.5272
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 15372000
                    Iteration time: 1.13s
                      Time elapsed: 00:07:51
                               ETA: 00:19:45

################################################################################
                     [1m Learning iteration 427/1500 [0m                      

                       Computation: 33642 steps/s (collection: 1.019s, learning 0.051s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0269
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 13.3161
                       Mean reward: 16.82
               Mean episode length: 894.51
Episode_Reward/track_lin_vel_xy_exp: 1.0112
Episode_Reward/track_ang_vel_z_exp: 0.5038
       Episode_Reward/lin_vel_z_l2: -0.0561
      Episode_Reward/ang_vel_xy_l2: -0.1396
     Episode_Reward/dof_torques_l2: -0.0877
         Episode_Reward/dof_acc_l2: -0.2009
     Episode_Reward/action_rate_l2: -0.1729
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.3471
Metrics/base_velocity/error_vel_xy: 0.5619
Metrics/base_velocity/error_vel_yaw: 0.5437
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 15408000
                    Iteration time: 1.07s
                      Time elapsed: 00:07:52
                               ETA: 00:19:44

################################################################################
                     [1m Learning iteration 428/1500 [0m                      

                       Computation: 32503 steps/s (collection: 1.054s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0300
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 13.3276
                       Mean reward: 16.64
               Mean episode length: 911.18
Episode_Reward/track_lin_vel_xy_exp: 1.0004
Episode_Reward/track_ang_vel_z_exp: 0.4830
       Episode_Reward/lin_vel_z_l2: -0.0556
      Episode_Reward/ang_vel_xy_l2: -0.1362
     Episode_Reward/dof_torques_l2: -0.0865
         Episode_Reward/dof_acc_l2: -0.2136
     Episode_Reward/action_rate_l2: -0.1733
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.3590
Metrics/base_velocity/error_vel_xy: 0.5533
Metrics/base_velocity/error_vel_yaw: 0.5679
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 15444000
                    Iteration time: 1.11s
                      Time elapsed: 00:07:53
                               ETA: 00:19:43

################################################################################
                     [1m Learning iteration 429/1500 [0m                      

                       Computation: 33260 steps/s (collection: 1.032s, learning 0.051s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0256
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 13.3391
                       Mean reward: 16.00
               Mean episode length: 885.98
Episode_Reward/track_lin_vel_xy_exp: 0.9063
Episode_Reward/track_ang_vel_z_exp: 0.4380
       Episode_Reward/lin_vel_z_l2: -0.0517
      Episode_Reward/ang_vel_xy_l2: -0.1219
     Episode_Reward/dof_torques_l2: -0.0827
         Episode_Reward/dof_acc_l2: -0.1840
     Episode_Reward/action_rate_l2: -0.1556
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.3661
Metrics/base_velocity/error_vel_xy: 0.4820
Metrics/base_velocity/error_vel_yaw: 0.4984
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 15480000
                    Iteration time: 1.08s
                      Time elapsed: 00:07:54
                               ETA: 00:19:42

################################################################################
                     [1m Learning iteration 430/1500 [0m                      

                       Computation: 32283 steps/s (collection: 1.061s, learning 0.054s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0287
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 13.3326
                       Mean reward: 16.32
               Mean episode length: 885.75
Episode_Reward/track_lin_vel_xy_exp: 0.9404
Episode_Reward/track_ang_vel_z_exp: 0.4506
       Episode_Reward/lin_vel_z_l2: -0.0583
      Episode_Reward/ang_vel_xy_l2: -0.1328
     Episode_Reward/dof_torques_l2: -0.0806
         Episode_Reward/dof_acc_l2: -0.2064
     Episode_Reward/action_rate_l2: -0.1627
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.3789
Metrics/base_velocity/error_vel_xy: 0.4886
Metrics/base_velocity/error_vel_yaw: 0.5225
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 15516000
                    Iteration time: 1.12s
                      Time elapsed: 00:07:55
                               ETA: 00:19:41

################################################################################
                     [1m Learning iteration 431/1500 [0m                      

                       Computation: 30749 steps/s (collection: 1.117s, learning 0.054s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0263
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 13.3354
                       Mean reward: 16.06
               Mean episode length: 863.24
Episode_Reward/track_lin_vel_xy_exp: 0.9854
Episode_Reward/track_ang_vel_z_exp: 0.4641
       Episode_Reward/lin_vel_z_l2: -0.0515
      Episode_Reward/ang_vel_xy_l2: -0.1277
     Episode_Reward/dof_torques_l2: -0.0834
         Episode_Reward/dof_acc_l2: -0.1947
     Episode_Reward/action_rate_l2: -0.1624
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.3904
Metrics/base_velocity/error_vel_xy: 0.4783
Metrics/base_velocity/error_vel_yaw: 0.5094
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 15552000
                    Iteration time: 1.17s
                      Time elapsed: 00:07:57
                               ETA: 00:19:40

################################################################################
                     [1m Learning iteration 432/1500 [0m                      

                       Computation: 31319 steps/s (collection: 1.095s, learning 0.054s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0316
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 13.3388
                       Mean reward: 15.05
               Mean episode length: 834.30
Episode_Reward/track_lin_vel_xy_exp: 0.9207
Episode_Reward/track_ang_vel_z_exp: 0.4373
       Episode_Reward/lin_vel_z_l2: -0.0536
      Episode_Reward/ang_vel_xy_l2: -0.1242
     Episode_Reward/dof_torques_l2: -0.0795
         Episode_Reward/dof_acc_l2: -0.1862
     Episode_Reward/action_rate_l2: -0.1569
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.3972
Metrics/base_velocity/error_vel_xy: 0.4668
Metrics/base_velocity/error_vel_yaw: 0.5128
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 15588000
                    Iteration time: 1.15s
                      Time elapsed: 00:07:58
                               ETA: 00:19:39

################################################################################
                     [1m Learning iteration 433/1500 [0m                      

                       Computation: 32112 steps/s (collection: 1.070s, learning 0.051s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0262
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 13.3378
                       Mean reward: 14.35
               Mean episode length: 802.52
Episode_Reward/track_lin_vel_xy_exp: 0.9169
Episode_Reward/track_ang_vel_z_exp: 0.4341
       Episode_Reward/lin_vel_z_l2: -0.0554
      Episode_Reward/ang_vel_xy_l2: -0.1238
     Episode_Reward/dof_torques_l2: -0.0790
         Episode_Reward/dof_acc_l2: -0.1884
     Episode_Reward/action_rate_l2: -0.1554
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.4060
Metrics/base_velocity/error_vel_xy: 0.4938
Metrics/base_velocity/error_vel_yaw: 0.5366
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 15624000
                    Iteration time: 1.12s
                      Time elapsed: 00:07:59
                               ETA: 00:19:38

################################################################################
                     [1m Learning iteration 434/1500 [0m                      

                       Computation: 33990 steps/s (collection: 1.008s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0236
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 13.3254
                       Mean reward: 14.86
               Mean episode length: 829.40
Episode_Reward/track_lin_vel_xy_exp: 1.0139
Episode_Reward/track_ang_vel_z_exp: 0.4756
       Episode_Reward/lin_vel_z_l2: -0.0505
      Episode_Reward/ang_vel_xy_l2: -0.1326
     Episode_Reward/dof_torques_l2: -0.0857
         Episode_Reward/dof_acc_l2: -0.1941
     Episode_Reward/action_rate_l2: -0.1658
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.4152
Metrics/base_velocity/error_vel_xy: 0.4801
Metrics/base_velocity/error_vel_yaw: 0.5259
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 15660000
                    Iteration time: 1.06s
                      Time elapsed: 00:08:00
                               ETA: 00:19:37

################################################################################
                     [1m Learning iteration 435/1500 [0m                      

                       Computation: 32353 steps/s (collection: 1.061s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0256
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 13.3105
                       Mean reward: 14.02
               Mean episode length: 798.53
Episode_Reward/track_lin_vel_xy_exp: 0.8406
Episode_Reward/track_ang_vel_z_exp: 0.4085
       Episode_Reward/lin_vel_z_l2: -0.0527
      Episode_Reward/ang_vel_xy_l2: -0.1216
     Episode_Reward/dof_torques_l2: -0.0789
         Episode_Reward/dof_acc_l2: -0.1871
     Episode_Reward/action_rate_l2: -0.1523
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.4292
Metrics/base_velocity/error_vel_xy: 0.5056
Metrics/base_velocity/error_vel_yaw: 0.5222
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 15696000
                    Iteration time: 1.11s
                      Time elapsed: 00:08:01
                               ETA: 00:19:36

################################################################################
                     [1m Learning iteration 436/1500 [0m                      

                       Computation: 32077 steps/s (collection: 1.070s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0240
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 13.3135
                       Mean reward: 14.81
               Mean episode length: 820.35
Episode_Reward/track_lin_vel_xy_exp: 1.0360
Episode_Reward/track_ang_vel_z_exp: 0.4673
       Episode_Reward/lin_vel_z_l2: -0.0535
      Episode_Reward/ang_vel_xy_l2: -0.1297
     Episode_Reward/dof_torques_l2: -0.0850
         Episode_Reward/dof_acc_l2: -0.2148
     Episode_Reward/action_rate_l2: -0.1670
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.4431
Metrics/base_velocity/error_vel_xy: 0.4156
Metrics/base_velocity/error_vel_yaw: 0.5086
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 15732000
                    Iteration time: 1.12s
                      Time elapsed: 00:08:02
                               ETA: 00:19:35

################################################################################
                     [1m Learning iteration 437/1500 [0m                      

                       Computation: 30611 steps/s (collection: 1.120s, learning 0.056s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0291
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 13.3049
                       Mean reward: 16.55
               Mean episode length: 891.04
Episode_Reward/track_lin_vel_xy_exp: 1.0579
Episode_Reward/track_ang_vel_z_exp: 0.4966
       Episode_Reward/lin_vel_z_l2: -0.0522
      Episode_Reward/ang_vel_xy_l2: -0.1369
     Episode_Reward/dof_torques_l2: -0.0879
         Episode_Reward/dof_acc_l2: -0.2034
     Episode_Reward/action_rate_l2: -0.1761
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.4519
Metrics/base_velocity/error_vel_xy: 0.4961
Metrics/base_velocity/error_vel_yaw: 0.5469
      Episode_Termination/time_out: 1.9583
  Episode_Termination/base_contact: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 15768000
                    Iteration time: 1.18s
                      Time elapsed: 00:08:03
                               ETA: 00:19:34

################################################################################
                     [1m Learning iteration 438/1500 [0m                      

                       Computation: 30699 steps/s (collection: 1.120s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0284
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 13.3085
                       Mean reward: 17.44
               Mean episode length: 925.83
Episode_Reward/track_lin_vel_xy_exp: 1.0135
Episode_Reward/track_ang_vel_z_exp: 0.4874
       Episode_Reward/lin_vel_z_l2: -0.0575
      Episode_Reward/ang_vel_xy_l2: -0.1354
     Episode_Reward/dof_torques_l2: -0.0877
         Episode_Reward/dof_acc_l2: -0.2131
     Episode_Reward/action_rate_l2: -0.1738
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.4639
Metrics/base_velocity/error_vel_xy: 0.5431
Metrics/base_velocity/error_vel_yaw: 0.5479
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 15804000
                    Iteration time: 1.17s
                      Time elapsed: 00:08:04
                               ETA: 00:19:33

################################################################################
                     [1m Learning iteration 439/1500 [0m                      

                       Computation: 30816 steps/s (collection: 1.115s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0275
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 13.3047
                       Mean reward: 15.43
               Mean episode length: 867.39
Episode_Reward/track_lin_vel_xy_exp: 0.8913
Episode_Reward/track_ang_vel_z_exp: 0.4508
       Episode_Reward/lin_vel_z_l2: -0.0535
      Episode_Reward/ang_vel_xy_l2: -0.1310
     Episode_Reward/dof_torques_l2: -0.0810
         Episode_Reward/dof_acc_l2: -0.1986
     Episode_Reward/action_rate_l2: -0.1645
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.4719
Metrics/base_velocity/error_vel_xy: 0.5917
Metrics/base_velocity/error_vel_yaw: 0.5405
      Episode_Termination/time_out: 2.0833
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 15840000
                    Iteration time: 1.17s
                      Time elapsed: 00:08:06
                               ETA: 00:19:32

################################################################################
                     [1m Learning iteration 440/1500 [0m                      

                       Computation: 31966 steps/s (collection: 1.075s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0275
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 13.3027
                       Mean reward: 16.46
               Mean episode length: 896.59
Episode_Reward/track_lin_vel_xy_exp: 1.0861
Episode_Reward/track_ang_vel_z_exp: 0.5153
       Episode_Reward/lin_vel_z_l2: -0.0510
      Episode_Reward/ang_vel_xy_l2: -0.1392
     Episode_Reward/dof_torques_l2: -0.0941
         Episode_Reward/dof_acc_l2: -0.2059
     Episode_Reward/action_rate_l2: -0.1801
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.4800
Metrics/base_velocity/error_vel_xy: 0.5313
Metrics/base_velocity/error_vel_yaw: 0.5626
      Episode_Termination/time_out: 2.0000
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 15876000
                    Iteration time: 1.13s
                      Time elapsed: 00:08:07
                               ETA: 00:19:31

################################################################################
                     [1m Learning iteration 441/1500 [0m                      

                       Computation: 33325 steps/s (collection: 1.027s, learning 0.054s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0244
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 13.3007
                       Mean reward: 16.12
               Mean episode length: 890.04
Episode_Reward/track_lin_vel_xy_exp: 0.9613
Episode_Reward/track_ang_vel_z_exp: 0.4533
       Episode_Reward/lin_vel_z_l2: -0.0596
      Episode_Reward/ang_vel_xy_l2: -0.1313
     Episode_Reward/dof_torques_l2: -0.0829
         Episode_Reward/dof_acc_l2: -0.2091
     Episode_Reward/action_rate_l2: -0.1658
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.4950
Metrics/base_velocity/error_vel_xy: 0.4650
Metrics/base_velocity/error_vel_yaw: 0.5074
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 15912000
                    Iteration time: 1.08s
                      Time elapsed: 00:08:08
                               ETA: 00:19:29

################################################################################
                     [1m Learning iteration 442/1500 [0m                      

                       Computation: 31027 steps/s (collection: 1.108s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0265
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 13.2908
                       Mean reward: 16.32
               Mean episode length: 864.89
Episode_Reward/track_lin_vel_xy_exp: 0.9798
Episode_Reward/track_ang_vel_z_exp: 0.4599
       Episode_Reward/lin_vel_z_l2: -0.0468
      Episode_Reward/ang_vel_xy_l2: -0.1240
     Episode_Reward/dof_torques_l2: -0.0808
         Episode_Reward/dof_acc_l2: -0.1904
     Episode_Reward/action_rate_l2: -0.1598
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5020
Metrics/base_velocity/error_vel_xy: 0.4596
Metrics/base_velocity/error_vel_yaw: 0.5026
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 15948000
                    Iteration time: 1.16s
                      Time elapsed: 00:08:09
                               ETA: 00:19:29

################################################################################
                     [1m Learning iteration 443/1500 [0m                      

                       Computation: 31314 steps/s (collection: 1.097s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0273
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 13.2978
                       Mean reward: 16.32
               Mean episode length: 883.22
Episode_Reward/track_lin_vel_xy_exp: 0.9177
Episode_Reward/track_ang_vel_z_exp: 0.4538
       Episode_Reward/lin_vel_z_l2: -0.0489
      Episode_Reward/ang_vel_xy_l2: -0.1271
     Episode_Reward/dof_torques_l2: -0.0792
         Episode_Reward/dof_acc_l2: -0.1838
     Episode_Reward/action_rate_l2: -0.1591
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5106
Metrics/base_velocity/error_vel_xy: 0.5232
Metrics/base_velocity/error_vel_yaw: 0.5201
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 15984000
                    Iteration time: 1.15s
                      Time elapsed: 00:08:10
                               ETA: 00:19:28

################################################################################
                     [1m Learning iteration 444/1500 [0m                      

                       Computation: 31761 steps/s (collection: 1.079s, learning 0.054s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0258
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 13.2915
                       Mean reward: 16.56
               Mean episode length: 889.54
Episode_Reward/track_lin_vel_xy_exp: 0.9440
Episode_Reward/track_ang_vel_z_exp: 0.4502
       Episode_Reward/lin_vel_z_l2: -0.0476
      Episode_Reward/ang_vel_xy_l2: -0.1266
     Episode_Reward/dof_torques_l2: -0.0813
         Episode_Reward/dof_acc_l2: -0.1832
     Episode_Reward/action_rate_l2: -0.1607
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5242
Metrics/base_velocity/error_vel_xy: 0.4765
Metrics/base_velocity/error_vel_yaw: 0.5024
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 16020000
                    Iteration time: 1.13s
                      Time elapsed: 00:08:11
                               ETA: 00:19:26

################################################################################
                     [1m Learning iteration 445/1500 [0m                      

                       Computation: 32468 steps/s (collection: 1.057s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0256
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 13.2787
                       Mean reward: 16.72
               Mean episode length: 885.84
Episode_Reward/track_lin_vel_xy_exp: 0.9733
Episode_Reward/track_ang_vel_z_exp: 0.4435
       Episode_Reward/lin_vel_z_l2: -0.0411
      Episode_Reward/ang_vel_xy_l2: -0.1163
     Episode_Reward/dof_torques_l2: -0.0731
         Episode_Reward/dof_acc_l2: -0.1660
     Episode_Reward/action_rate_l2: -0.1514
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5321
Metrics/base_velocity/error_vel_xy: 0.3864
Metrics/base_velocity/error_vel_yaw: 0.4664
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 16056000
                    Iteration time: 1.11s
                      Time elapsed: 00:08:12
                               ETA: 00:19:25

################################################################################
                     [1m Learning iteration 446/1500 [0m                      

                       Computation: 32427 steps/s (collection: 1.058s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0277
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 13.2809
                       Mean reward: 17.05
               Mean episode length: 874.28
Episode_Reward/track_lin_vel_xy_exp: 1.0301
Episode_Reward/track_ang_vel_z_exp: 0.4821
       Episode_Reward/lin_vel_z_l2: -0.0522
      Episode_Reward/ang_vel_xy_l2: -0.1313
     Episode_Reward/dof_torques_l2: -0.0840
         Episode_Reward/dof_acc_l2: -0.2010
     Episode_Reward/action_rate_l2: -0.1687
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5419
Metrics/base_velocity/error_vel_xy: 0.4651
Metrics/base_velocity/error_vel_yaw: 0.5084
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 16092000
                    Iteration time: 1.11s
                      Time elapsed: 00:08:13
                               ETA: 00:19:24

################################################################################
                     [1m Learning iteration 447/1500 [0m                      

                       Computation: 32970 steps/s (collection: 1.039s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0262
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 13.2898
                       Mean reward: 17.08
               Mean episode length: 874.55
Episode_Reward/track_lin_vel_xy_exp: 0.9148
Episode_Reward/track_ang_vel_z_exp: 0.4237
       Episode_Reward/lin_vel_z_l2: -0.0462
      Episode_Reward/ang_vel_xy_l2: -0.1160
     Episode_Reward/dof_torques_l2: -0.0710
         Episode_Reward/dof_acc_l2: -0.1767
     Episode_Reward/action_rate_l2: -0.1473
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5530
Metrics/base_velocity/error_vel_xy: 0.3864
Metrics/base_velocity/error_vel_yaw: 0.4433
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 16128000
                    Iteration time: 1.09s
                      Time elapsed: 00:08:15
                               ETA: 00:19:23

################################################################################
                     [1m Learning iteration 448/1500 [0m                      

                       Computation: 34113 steps/s (collection: 1.004s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0264
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 13.2781
                       Mean reward: 16.81
               Mean episode length: 871.00
Episode_Reward/track_lin_vel_xy_exp: 1.0039
Episode_Reward/track_ang_vel_z_exp: 0.4789
       Episode_Reward/lin_vel_z_l2: -0.0462
      Episode_Reward/ang_vel_xy_l2: -0.1313
     Episode_Reward/dof_torques_l2: -0.0845
         Episode_Reward/dof_acc_l2: -0.2014
     Episode_Reward/action_rate_l2: -0.1692
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5612
Metrics/base_velocity/error_vel_xy: 0.5063
Metrics/base_velocity/error_vel_yaw: 0.5398
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 16164000
                    Iteration time: 1.06s
                      Time elapsed: 00:08:16
                               ETA: 00:19:22

################################################################################
                     [1m Learning iteration 449/1500 [0m                      

                       Computation: 33668 steps/s (collection: 1.016s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0283
               Mean surrogate loss: -0.0096
                 Mean entropy loss: 13.2848
                       Mean reward: 16.63
               Mean episode length: 884.79
Episode_Reward/track_lin_vel_xy_exp: 0.9548
Episode_Reward/track_ang_vel_z_exp: 0.4339
       Episode_Reward/lin_vel_z_l2: -0.0499
      Episode_Reward/ang_vel_xy_l2: -0.1187
     Episode_Reward/dof_torques_l2: -0.0778
         Episode_Reward/dof_acc_l2: -0.1782
     Episode_Reward/action_rate_l2: -0.1526
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5710
Metrics/base_velocity/error_vel_xy: 0.3970
Metrics/base_velocity/error_vel_yaw: 0.4765
      Episode_Termination/time_out: 1.9167
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 16200000
                    Iteration time: 1.07s
                      Time elapsed: 00:08:17
                               ETA: 00:19:21

################################################################################
                     [1m Learning iteration 450/1500 [0m                      

                       Computation: 34048 steps/s (collection: 1.004s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0265
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 13.2888
                       Mean reward: 16.38
               Mean episode length: 871.74
Episode_Reward/track_lin_vel_xy_exp: 0.9524
Episode_Reward/track_ang_vel_z_exp: 0.4494
       Episode_Reward/lin_vel_z_l2: -0.0498
      Episode_Reward/ang_vel_xy_l2: -0.1257
     Episode_Reward/dof_torques_l2: -0.0836
         Episode_Reward/dof_acc_l2: -0.2069
     Episode_Reward/action_rate_l2: -0.1634
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5823
Metrics/base_velocity/error_vel_xy: 0.4609
Metrics/base_velocity/error_vel_yaw: 0.5058
      Episode_Termination/time_out: 1.9167
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 16236000
                    Iteration time: 1.06s
                      Time elapsed: 00:08:18
                               ETA: 00:19:20

################################################################################
                     [1m Learning iteration 451/1500 [0m                      

                       Computation: 33494 steps/s (collection: 1.021s, learning 0.054s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0258
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 13.2918
                       Mean reward: 16.19
               Mean episode length: 883.99
Episode_Reward/track_lin_vel_xy_exp: 1.0072
Episode_Reward/track_ang_vel_z_exp: 0.4824
       Episode_Reward/lin_vel_z_l2: -0.0508
      Episode_Reward/ang_vel_xy_l2: -0.1362
     Episode_Reward/dof_torques_l2: -0.0913
         Episode_Reward/dof_acc_l2: -0.2013
     Episode_Reward/action_rate_l2: -0.1769
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.5946
Metrics/base_velocity/error_vel_xy: 0.5677
Metrics/base_velocity/error_vel_yaw: 0.5910
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 16272000
                    Iteration time: 1.07s
                      Time elapsed: 00:08:19
                               ETA: 00:19:18

################################################################################
                     [1m Learning iteration 452/1500 [0m                      

                       Computation: 33651 steps/s (collection: 1.017s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0248
               Mean surrogate loss: -0.0095
                 Mean entropy loss: 13.2830
                       Mean reward: 15.91
               Mean episode length: 896.95
Episode_Reward/track_lin_vel_xy_exp: 0.9815
Episode_Reward/track_ang_vel_z_exp: 0.4692
       Episode_Reward/lin_vel_z_l2: -0.0525
      Episode_Reward/ang_vel_xy_l2: -0.1301
     Episode_Reward/dof_torques_l2: -0.0876
         Episode_Reward/dof_acc_l2: -0.1995
     Episode_Reward/action_rate_l2: -0.1683
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6058
Metrics/base_velocity/error_vel_xy: 0.5048
Metrics/base_velocity/error_vel_yaw: 0.5176
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 16308000
                    Iteration time: 1.07s
                      Time elapsed: 00:08:20
                               ETA: 00:19:17

################################################################################
                     [1m Learning iteration 453/1500 [0m                      

                       Computation: 33876 steps/s (collection: 1.010s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0294
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 13.2837
                       Mean reward: 15.60
               Mean episode length: 889.00
Episode_Reward/track_lin_vel_xy_exp: 0.9796
Episode_Reward/track_ang_vel_z_exp: 0.4703
       Episode_Reward/lin_vel_z_l2: -0.0496
      Episode_Reward/ang_vel_xy_l2: -0.1287
     Episode_Reward/dof_torques_l2: -0.0840
         Episode_Reward/dof_acc_l2: -0.1845
     Episode_Reward/action_rate_l2: -0.1648
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6185
Metrics/base_velocity/error_vel_xy: 0.5063
Metrics/base_velocity/error_vel_yaw: 0.5386
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 16344000
                    Iteration time: 1.06s
                      Time elapsed: 00:08:21
                               ETA: 00:19:16

################################################################################
                     [1m Learning iteration 454/1500 [0m                      

                       Computation: 33270 steps/s (collection: 1.030s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0243
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 13.2886
                       Mean reward: 16.62
               Mean episode length: 897.68
Episode_Reward/track_lin_vel_xy_exp: 1.0580
Episode_Reward/track_ang_vel_z_exp: 0.4840
       Episode_Reward/lin_vel_z_l2: -0.0491
      Episode_Reward/ang_vel_xy_l2: -0.1312
     Episode_Reward/dof_torques_l2: -0.0857
         Episode_Reward/dof_acc_l2: -0.1963
     Episode_Reward/action_rate_l2: -0.1694
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6298
Metrics/base_velocity/error_vel_xy: 0.4444
Metrics/base_velocity/error_vel_yaw: 0.5316
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 16380000
                    Iteration time: 1.08s
                      Time elapsed: 00:08:22
                               ETA: 00:19:15

################################################################################
                     [1m Learning iteration 455/1500 [0m                      

                       Computation: 33998 steps/s (collection: 1.006s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0253
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 13.2778
                       Mean reward: 16.93
               Mean episode length: 887.81
Episode_Reward/track_lin_vel_xy_exp: 0.9905
Episode_Reward/track_ang_vel_z_exp: 0.4451
       Episode_Reward/lin_vel_z_l2: -0.0462
      Episode_Reward/ang_vel_xy_l2: -0.1193
     Episode_Reward/dof_torques_l2: -0.0751
         Episode_Reward/dof_acc_l2: -0.1852
     Episode_Reward/action_rate_l2: -0.1545
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6395
Metrics/base_velocity/error_vel_xy: 0.3743
Metrics/base_velocity/error_vel_yaw: 0.4531
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 16416000
                    Iteration time: 1.06s
                      Time elapsed: 00:08:23
                               ETA: 00:19:14

################################################################################
                     [1m Learning iteration 456/1500 [0m                      

                       Computation: 33911 steps/s (collection: 1.008s, learning 0.054s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0274
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 13.2719
                       Mean reward: 17.53
               Mean episode length: 893.10
Episode_Reward/track_lin_vel_xy_exp: 1.0319
Episode_Reward/track_ang_vel_z_exp: 0.4790
       Episode_Reward/lin_vel_z_l2: -0.0459
      Episode_Reward/ang_vel_xy_l2: -0.1297
     Episode_Reward/dof_torques_l2: -0.0884
         Episode_Reward/dof_acc_l2: -0.1955
     Episode_Reward/action_rate_l2: -0.1709
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6485
Metrics/base_velocity/error_vel_xy: 0.4740
Metrics/base_velocity/error_vel_yaw: 0.5373
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 16452000
                    Iteration time: 1.06s
                      Time elapsed: 00:08:24
                               ETA: 00:19:12

################################################################################
                     [1m Learning iteration 457/1500 [0m                      

                       Computation: 34254 steps/s (collection: 0.999s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0231
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 13.2735
                       Mean reward: 17.71
               Mean episode length: 891.55
Episode_Reward/track_lin_vel_xy_exp: 1.0798
Episode_Reward/track_ang_vel_z_exp: 0.4998
       Episode_Reward/lin_vel_z_l2: -0.0537
      Episode_Reward/ang_vel_xy_l2: -0.1340
     Episode_Reward/dof_torques_l2: -0.0883
         Episode_Reward/dof_acc_l2: -0.2113
     Episode_Reward/action_rate_l2: -0.1766
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6627
Metrics/base_velocity/error_vel_xy: 0.4704
Metrics/base_velocity/error_vel_yaw: 0.5205
      Episode_Termination/time_out: 1.9167
  Episode_Termination/base_contact: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 16488000
                    Iteration time: 1.05s
                      Time elapsed: 00:08:25
                               ETA: 00:19:11

################################################################################
                     [1m Learning iteration 458/1500 [0m                      

                       Computation: 32584 steps/s (collection: 1.051s, learning 0.054s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0279
               Mean surrogate loss: -0.0101
                 Mean entropy loss: 13.2790
                       Mean reward: 17.51
               Mean episode length: 904.01
Episode_Reward/track_lin_vel_xy_exp: 1.1183
Episode_Reward/track_ang_vel_z_exp: 0.5072
       Episode_Reward/lin_vel_z_l2: -0.0472
      Episode_Reward/ang_vel_xy_l2: -0.1347
     Episode_Reward/dof_torques_l2: -0.0844
         Episode_Reward/dof_acc_l2: -0.1967
     Episode_Reward/action_rate_l2: -0.1725
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6749
Metrics/base_velocity/error_vel_xy: 0.4360
Metrics/base_velocity/error_vel_yaw: 0.5288
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 16524000
                    Iteration time: 1.10s
                      Time elapsed: 00:08:26
                               ETA: 00:19:10

################################################################################
                     [1m Learning iteration 459/1500 [0m                      

                       Computation: 33015 steps/s (collection: 1.039s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0302
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 13.2833
                       Mean reward: 16.22
               Mean episode length: 887.94
Episode_Reward/track_lin_vel_xy_exp: 0.9444
Episode_Reward/track_ang_vel_z_exp: 0.4710
       Episode_Reward/lin_vel_z_l2: -0.0549
      Episode_Reward/ang_vel_xy_l2: -0.1391
     Episode_Reward/dof_torques_l2: -0.0860
         Episode_Reward/dof_acc_l2: -0.2259
     Episode_Reward/action_rate_l2: -0.1767
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6834
Metrics/base_velocity/error_vel_xy: 0.5866
Metrics/base_velocity/error_vel_yaw: 0.5729
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 16560000
                    Iteration time: 1.09s
                      Time elapsed: 00:08:27
                               ETA: 00:19:09

################################################################################
                     [1m Learning iteration 460/1500 [0m                      

                       Computation: 31429 steps/s (collection: 1.090s, learning 0.056s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0252
               Mean surrogate loss: -0.0105
                 Mean entropy loss: 13.2852
                       Mean reward: 15.47
               Mean episode length: 881.70
Episode_Reward/track_lin_vel_xy_exp: 0.9143
Episode_Reward/track_ang_vel_z_exp: 0.4328
       Episode_Reward/lin_vel_z_l2: -0.0455
      Episode_Reward/ang_vel_xy_l2: -0.1189
     Episode_Reward/dof_torques_l2: -0.0779
         Episode_Reward/dof_acc_l2: -0.1737
     Episode_Reward/action_rate_l2: -0.1545
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.6980
Metrics/base_velocity/error_vel_xy: 0.4742
Metrics/base_velocity/error_vel_yaw: 0.5215
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 16596000
                    Iteration time: 1.15s
                      Time elapsed: 00:08:29
                               ETA: 00:19:08

################################################################################
                     [1m Learning iteration 461/1500 [0m                      

                       Computation: 31407 steps/s (collection: 1.093s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0298
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 13.2810
                       Mean reward: 16.23
               Mean episode length: 858.05
Episode_Reward/track_lin_vel_xy_exp: 1.0301
Episode_Reward/track_ang_vel_z_exp: 0.4704
       Episode_Reward/lin_vel_z_l2: -0.0522
      Episode_Reward/ang_vel_xy_l2: -0.1315
     Episode_Reward/dof_torques_l2: -0.0833
         Episode_Reward/dof_acc_l2: -0.2009
     Episode_Reward/action_rate_l2: -0.1678
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7068
Metrics/base_velocity/error_vel_xy: 0.4373
Metrics/base_velocity/error_vel_yaw: 0.5232
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 16632000
                    Iteration time: 1.15s
                      Time elapsed: 00:08:30
                               ETA: 00:19:07

################################################################################
                     [1m Learning iteration 462/1500 [0m                      

                       Computation: 31122 steps/s (collection: 1.104s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0280
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 13.2752
                       Mean reward: 16.66
               Mean episode length: 877.59
Episode_Reward/track_lin_vel_xy_exp: 0.9521
Episode_Reward/track_ang_vel_z_exp: 0.4665
       Episode_Reward/lin_vel_z_l2: -0.0471
      Episode_Reward/ang_vel_xy_l2: -0.1289
     Episode_Reward/dof_torques_l2: -0.0850
         Episode_Reward/dof_acc_l2: -0.1953
     Episode_Reward/action_rate_l2: -0.1666
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7143
Metrics/base_velocity/error_vel_xy: 0.5286
Metrics/base_velocity/error_vel_yaw: 0.5306
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 16668000
                    Iteration time: 1.16s
                      Time elapsed: 00:08:31
                               ETA: 00:19:06

################################################################################
                     [1m Learning iteration 463/1500 [0m                      

                       Computation: 32365 steps/s (collection: 1.060s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0302
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 13.2824
                       Mean reward: 15.63
               Mean episode length: 869.33
Episode_Reward/track_lin_vel_xy_exp: 0.8952
Episode_Reward/track_ang_vel_z_exp: 0.4309
       Episode_Reward/lin_vel_z_l2: -0.0511
      Episode_Reward/ang_vel_xy_l2: -0.1244
     Episode_Reward/dof_torques_l2: -0.0778
         Episode_Reward/dof_acc_l2: -0.1901
     Episode_Reward/action_rate_l2: -0.1563
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7233
Metrics/base_velocity/error_vel_xy: 0.4816
Metrics/base_velocity/error_vel_yaw: 0.4909
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 16704000
                    Iteration time: 1.11s
                      Time elapsed: 00:08:32
                               ETA: 00:19:05

################################################################################
                     [1m Learning iteration 464/1500 [0m                      

                       Computation: 32872 steps/s (collection: 1.041s, learning 0.054s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0251
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 13.2854
                       Mean reward: 14.47
               Mean episode length: 823.92
Episode_Reward/track_lin_vel_xy_exp: 0.8456
Episode_Reward/track_ang_vel_z_exp: 0.4249
       Episode_Reward/lin_vel_z_l2: -0.0460
      Episode_Reward/ang_vel_xy_l2: -0.1141
     Episode_Reward/dof_torques_l2: -0.0798
         Episode_Reward/dof_acc_l2: -0.1728
     Episode_Reward/action_rate_l2: -0.1499
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7327
Metrics/base_velocity/error_vel_xy: 0.4790
Metrics/base_velocity/error_vel_yaw: 0.4608
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 16740000
                    Iteration time: 1.10s
                      Time elapsed: 00:08:33
                               ETA: 00:19:04

################################################################################
                     [1m Learning iteration 465/1500 [0m                      

                       Computation: 33443 steps/s (collection: 1.024s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0258
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 13.2789
                       Mean reward: 14.99
               Mean episode length: 830.93
Episode_Reward/track_lin_vel_xy_exp: 1.0108
Episode_Reward/track_ang_vel_z_exp: 0.4589
       Episode_Reward/lin_vel_z_l2: -0.0483
      Episode_Reward/ang_vel_xy_l2: -0.1245
     Episode_Reward/dof_torques_l2: -0.0816
         Episode_Reward/dof_acc_l2: -0.1871
     Episode_Reward/action_rate_l2: -0.1627
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7389
Metrics/base_velocity/error_vel_xy: 0.4208
Metrics/base_velocity/error_vel_yaw: 0.5180
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 16776000
                    Iteration time: 1.08s
                      Time elapsed: 00:08:34
                               ETA: 00:19:03

################################################################################
                     [1m Learning iteration 466/1500 [0m                      

                       Computation: 33155 steps/s (collection: 1.035s, learning 0.051s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0236
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 13.2729
                       Mean reward: 16.51
               Mean episode length: 884.20
Episode_Reward/track_lin_vel_xy_exp: 1.0505
Episode_Reward/track_ang_vel_z_exp: 0.4775
       Episode_Reward/lin_vel_z_l2: -0.0497
      Episode_Reward/ang_vel_xy_l2: -0.1333
     Episode_Reward/dof_torques_l2: -0.0825
         Episode_Reward/dof_acc_l2: -0.1998
     Episode_Reward/action_rate_l2: -0.1678
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7501
Metrics/base_velocity/error_vel_xy: 0.4416
Metrics/base_velocity/error_vel_yaw: 0.5119
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 16812000
                    Iteration time: 1.09s
                      Time elapsed: 00:08:35
                               ETA: 00:19:01

################################################################################
                     [1m Learning iteration 467/1500 [0m                      

                       Computation: 33072 steps/s (collection: 1.036s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0277
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 13.2655
                       Mean reward: 16.95
               Mean episode length: 893.00
Episode_Reward/track_lin_vel_xy_exp: 1.0218
Episode_Reward/track_ang_vel_z_exp: 0.4872
       Episode_Reward/lin_vel_z_l2: -0.0491
      Episode_Reward/ang_vel_xy_l2: -0.1313
     Episode_Reward/dof_torques_l2: -0.0927
         Episode_Reward/dof_acc_l2: -0.2002
     Episode_Reward/action_rate_l2: -0.1751
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7663
Metrics/base_velocity/error_vel_xy: 0.5277
Metrics/base_velocity/error_vel_yaw: 0.5518
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 16848000
                    Iteration time: 1.09s
                      Time elapsed: 00:08:36
                               ETA: 00:19:00

################################################################################
                     [1m Learning iteration 468/1500 [0m                      

                       Computation: 34535 steps/s (collection: 0.989s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0265
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 13.2696
                       Mean reward: 16.14
               Mean episode length: 890.35
Episode_Reward/track_lin_vel_xy_exp: 0.8915
Episode_Reward/track_ang_vel_z_exp: 0.4530
       Episode_Reward/lin_vel_z_l2: -0.0580
      Episode_Reward/ang_vel_xy_l2: -0.1285
     Episode_Reward/dof_torques_l2: -0.0826
         Episode_Reward/dof_acc_l2: -0.2017
     Episode_Reward/action_rate_l2: -0.1650
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7782
Metrics/base_velocity/error_vel_xy: 0.5492
Metrics/base_velocity/error_vel_yaw: 0.5090
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 16884000
                    Iteration time: 1.04s
                      Time elapsed: 00:08:37
                               ETA: 00:18:59

################################################################################
                     [1m Learning iteration 469/1500 [0m                      

                       Computation: 34704 steps/s (collection: 0.984s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0247
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 13.2702
                       Mean reward: 15.47
               Mean episode length: 871.70
Episode_Reward/track_lin_vel_xy_exp: 0.9276
Episode_Reward/track_ang_vel_z_exp: 0.4434
       Episode_Reward/lin_vel_z_l2: -0.0493
      Episode_Reward/ang_vel_xy_l2: -0.1288
     Episode_Reward/dof_torques_l2: -0.0820
         Episode_Reward/dof_acc_l2: -0.1897
     Episode_Reward/action_rate_l2: -0.1618
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7829
Metrics/base_velocity/error_vel_xy: 0.5177
Metrics/base_velocity/error_vel_yaw: 0.5567
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 16920000
                    Iteration time: 1.04s
                      Time elapsed: 00:08:38
                               ETA: 00:18:58

################################################################################
                     [1m Learning iteration 470/1500 [0m                      

                       Computation: 33532 steps/s (collection: 1.018s, learning 0.055s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0284
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 13.2659
                       Mean reward: 15.68
               Mean episode length: 894.77
Episode_Reward/track_lin_vel_xy_exp: 0.9247
Episode_Reward/track_ang_vel_z_exp: 0.4647
       Episode_Reward/lin_vel_z_l2: -0.0482
      Episode_Reward/ang_vel_xy_l2: -0.1339
     Episode_Reward/dof_torques_l2: -0.0863
         Episode_Reward/dof_acc_l2: -0.2016
     Episode_Reward/action_rate_l2: -0.1678
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.7933
Metrics/base_velocity/error_vel_xy: 0.5851
Metrics/base_velocity/error_vel_yaw: 0.5562
      Episode_Termination/time_out: 1.9583
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 16956000
                    Iteration time: 1.07s
                      Time elapsed: 00:08:39
                               ETA: 00:18:57

################################################################################
                     [1m Learning iteration 471/1500 [0m                      

                       Computation: 34007 steps/s (collection: 1.008s, learning 0.050s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0267
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 13.2602
                       Mean reward: 16.20
               Mean episode length: 901.42
Episode_Reward/track_lin_vel_xy_exp: 1.0066
Episode_Reward/track_ang_vel_z_exp: 0.4791
       Episode_Reward/lin_vel_z_l2: -0.0511
      Episode_Reward/ang_vel_xy_l2: -0.1296
     Episode_Reward/dof_torques_l2: -0.0901
         Episode_Reward/dof_acc_l2: -0.1941
     Episode_Reward/action_rate_l2: -0.1696
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.8053
Metrics/base_velocity/error_vel_xy: 0.5287
Metrics/base_velocity/error_vel_yaw: 0.5307
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 16992000
                    Iteration time: 1.06s
                      Time elapsed: 00:08:41
                               ETA: 00:18:55

################################################################################
                     [1m Learning iteration 472/1500 [0m                      

                       Computation: 32797 steps/s (collection: 1.046s, learning 0.051s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0276
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 13.2625
                       Mean reward: 16.58
               Mean episode length: 892.47
Episode_Reward/track_lin_vel_xy_exp: 0.9349
Episode_Reward/track_ang_vel_z_exp: 0.4565
       Episode_Reward/lin_vel_z_l2: -0.0523
      Episode_Reward/ang_vel_xy_l2: -0.1313
     Episode_Reward/dof_torques_l2: -0.0835
         Episode_Reward/dof_acc_l2: -0.1856
     Episode_Reward/action_rate_l2: -0.1619
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.8213
Metrics/base_velocity/error_vel_xy: 0.5325
Metrics/base_velocity/error_vel_yaw: 0.5344
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 17028000
                    Iteration time: 1.10s
                      Time elapsed: 00:08:42
                               ETA: 00:18:54

################################################################################
                     [1m Learning iteration 473/1500 [0m                      

                       Computation: 32254 steps/s (collection: 1.064s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0265
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 13.2592
                       Mean reward: 16.08
               Mean episode length: 877.07
Episode_Reward/track_lin_vel_xy_exp: 0.9844
Episode_Reward/track_ang_vel_z_exp: 0.4717
       Episode_Reward/lin_vel_z_l2: -0.0541
      Episode_Reward/ang_vel_xy_l2: -0.1308
     Episode_Reward/dof_torques_l2: -0.0841
         Episode_Reward/dof_acc_l2: -0.1984
     Episode_Reward/action_rate_l2: -0.1676
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.8254
Metrics/base_velocity/error_vel_xy: 0.5080
Metrics/base_velocity/error_vel_yaw: 0.5285
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 17064000
                    Iteration time: 1.12s
                      Time elapsed: 00:08:43
                               ETA: 00:18:53

################################################################################
                     [1m Learning iteration 474/1500 [0m                      

                       Computation: 30855 steps/s (collection: 1.114s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0279
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 13.2522
                       Mean reward: 14.70
               Mean episode length: 859.58
Episode_Reward/track_lin_vel_xy_exp: 0.8879
Episode_Reward/track_ang_vel_z_exp: 0.4427
       Episode_Reward/lin_vel_z_l2: -0.0576
      Episode_Reward/ang_vel_xy_l2: -0.1329
     Episode_Reward/dof_torques_l2: -0.0834
         Episode_Reward/dof_acc_l2: -0.2023
     Episode_Reward/action_rate_l2: -0.1653
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.8386
Metrics/base_velocity/error_vel_xy: 0.5654
Metrics/base_velocity/error_vel_yaw: 0.5543
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 17100000
                    Iteration time: 1.17s
                      Time elapsed: 00:08:44
                               ETA: 00:18:52

################################################################################
                     [1m Learning iteration 475/1500 [0m                      

                       Computation: 31800 steps/s (collection: 1.080s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0268
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 13.2475
                       Mean reward: 14.89
               Mean episode length: 861.39
Episode_Reward/track_lin_vel_xy_exp: 0.9732
Episode_Reward/track_ang_vel_z_exp: 0.4494
       Episode_Reward/lin_vel_z_l2: -0.0579
      Episode_Reward/ang_vel_xy_l2: -0.1279
     Episode_Reward/dof_torques_l2: -0.0761
         Episode_Reward/dof_acc_l2: -0.2001
     Episode_Reward/action_rate_l2: -0.1586
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.8494
Metrics/base_velocity/error_vel_xy: 0.4121
Metrics/base_velocity/error_vel_yaw: 0.4662
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 17136000
                    Iteration time: 1.13s
                      Time elapsed: 00:08:45
                               ETA: 00:18:51

################################################################################
                     [1m Learning iteration 476/1500 [0m                      

                       Computation: 32022 steps/s (collection: 1.072s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0310
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 13.2463
                       Mean reward: 14.79
               Mean episode length: 864.43
Episode_Reward/track_lin_vel_xy_exp: 0.8658
Episode_Reward/track_ang_vel_z_exp: 0.4560
       Episode_Reward/lin_vel_z_l2: -0.0513
      Episode_Reward/ang_vel_xy_l2: -0.1292
     Episode_Reward/dof_torques_l2: -0.0900
         Episode_Reward/dof_acc_l2: -0.1957
     Episode_Reward/action_rate_l2: -0.1674
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.8539
Metrics/base_velocity/error_vel_xy: 0.6376
Metrics/base_velocity/error_vel_yaw: 0.5625
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 17172000
                    Iteration time: 1.12s
                      Time elapsed: 00:08:46
                               ETA: 00:18:50

################################################################################
                     [1m Learning iteration 477/1500 [0m                      

                       Computation: 31317 steps/s (collection: 1.096s, learning 0.054s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0276
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 13.2441
                       Mean reward: 14.08
               Mean episode length: 844.54
Episode_Reward/track_lin_vel_xy_exp: 0.8585
Episode_Reward/track_ang_vel_z_exp: 0.4330
       Episode_Reward/lin_vel_z_l2: -0.0503
      Episode_Reward/ang_vel_xy_l2: -0.1209
     Episode_Reward/dof_torques_l2: -0.0798
         Episode_Reward/dof_acc_l2: -0.1837
     Episode_Reward/action_rate_l2: -0.1542
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.8610
Metrics/base_velocity/error_vel_xy: 0.5288
Metrics/base_velocity/error_vel_yaw: 0.4908
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 17208000
                    Iteration time: 1.15s
                      Time elapsed: 00:08:47
                               ETA: 00:18:49

################################################################################
                     [1m Learning iteration 478/1500 [0m                      

                       Computation: 31603 steps/s (collection: 1.086s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0238
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 13.2445
                       Mean reward: 14.07
               Mean episode length: 822.65
Episode_Reward/track_lin_vel_xy_exp: 0.7950
Episode_Reward/track_ang_vel_z_exp: 0.3989
       Episode_Reward/lin_vel_z_l2: -0.0491
      Episode_Reward/ang_vel_xy_l2: -0.1126
     Episode_Reward/dof_torques_l2: -0.0762
         Episode_Reward/dof_acc_l2: -0.1733
     Episode_Reward/action_rate_l2: -0.1442
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.8671
Metrics/base_velocity/error_vel_xy: 0.5001
Metrics/base_velocity/error_vel_yaw: 0.4745
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 17244000
                    Iteration time: 1.14s
                      Time elapsed: 00:08:48
                               ETA: 00:18:48

################################################################################
                     [1m Learning iteration 479/1500 [0m                      

                       Computation: 32130 steps/s (collection: 1.067s, learning 0.054s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0236
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 13.2392
                       Mean reward: 14.63
               Mean episode length: 838.70
Episode_Reward/track_lin_vel_xy_exp: 1.0018
Episode_Reward/track_ang_vel_z_exp: 0.4733
       Episode_Reward/lin_vel_z_l2: -0.0544
      Episode_Reward/ang_vel_xy_l2: -0.1340
     Episode_Reward/dof_torques_l2: -0.0828
         Episode_Reward/dof_acc_l2: -0.2025
     Episode_Reward/action_rate_l2: -0.1688
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.8751
Metrics/base_velocity/error_vel_xy: 0.4865
Metrics/base_velocity/error_vel_yaw: 0.5408
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 17280000
                    Iteration time: 1.12s
                      Time elapsed: 00:08:50
                               ETA: 00:18:47

################################################################################
                     [1m Learning iteration 480/1500 [0m                      

                       Computation: 33254 steps/s (collection: 1.029s, learning 0.054s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0257
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 13.2357
                       Mean reward: 17.01
               Mean episode length: 917.55
Episode_Reward/track_lin_vel_xy_exp: 1.0279
Episode_Reward/track_ang_vel_z_exp: 0.4949
       Episode_Reward/lin_vel_z_l2: -0.0575
      Episode_Reward/ang_vel_xy_l2: -0.1360
     Episode_Reward/dof_torques_l2: -0.0914
         Episode_Reward/dof_acc_l2: -0.2108
     Episode_Reward/action_rate_l2: -0.1770
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.8854
Metrics/base_velocity/error_vel_xy: 0.5259
Metrics/base_velocity/error_vel_yaw: 0.5480
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 17316000
                    Iteration time: 1.08s
                      Time elapsed: 00:08:51
                               ETA: 00:18:46

################################################################################
                     [1m Learning iteration 481/1500 [0m                      

                       Computation: 33141 steps/s (collection: 1.032s, learning 0.054s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0246
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 13.2246
                       Mean reward: 16.65
               Mean episode length: 918.21
Episode_Reward/track_lin_vel_xy_exp: 0.9674
Episode_Reward/track_ang_vel_z_exp: 0.4660
       Episode_Reward/lin_vel_z_l2: -0.0578
      Episode_Reward/ang_vel_xy_l2: -0.1377
     Episode_Reward/dof_torques_l2: -0.0875
         Episode_Reward/dof_acc_l2: -0.2248
     Episode_Reward/action_rate_l2: -0.1732
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.8970
Metrics/base_velocity/error_vel_xy: 0.5206
Metrics/base_velocity/error_vel_yaw: 0.5496
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 17352000
                    Iteration time: 1.09s
                      Time elapsed: 00:08:52
                               ETA: 00:18:45

################################################################################
                     [1m Learning iteration 482/1500 [0m                      

                       Computation: 31666 steps/s (collection: 1.081s, learning 0.055s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0261
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 13.2141
                       Mean reward: 16.15
               Mean episode length: 889.32
Episode_Reward/track_lin_vel_xy_exp: 0.9600
Episode_Reward/track_ang_vel_z_exp: 0.4526
       Episode_Reward/lin_vel_z_l2: -0.0532
      Episode_Reward/ang_vel_xy_l2: -0.1256
     Episode_Reward/dof_torques_l2: -0.0851
         Episode_Reward/dof_acc_l2: -0.1910
     Episode_Reward/action_rate_l2: -0.1647
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9064
Metrics/base_velocity/error_vel_xy: 0.4695
Metrics/base_velocity/error_vel_yaw: 0.5107
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 17388000
                    Iteration time: 1.14s
                      Time elapsed: 00:08:53
                               ETA: 00:18:44

################################################################################
                     [1m Learning iteration 483/1500 [0m                      

                       Computation: 31217 steps/s (collection: 1.101s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0273
               Mean surrogate loss: -0.0094
                 Mean entropy loss: 13.2077
                       Mean reward: 15.59
               Mean episode length: 880.94
Episode_Reward/track_lin_vel_xy_exp: 0.8379
Episode_Reward/track_ang_vel_z_exp: 0.4193
       Episode_Reward/lin_vel_z_l2: -0.0507
      Episode_Reward/ang_vel_xy_l2: -0.1202
     Episode_Reward/dof_torques_l2: -0.0792
         Episode_Reward/dof_acc_l2: -0.1864
     Episode_Reward/action_rate_l2: -0.1544
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9178
Metrics/base_velocity/error_vel_xy: 0.5272
Metrics/base_velocity/error_vel_yaw: 0.5124
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 17424000
                    Iteration time: 1.15s
                      Time elapsed: 00:08:54
                               ETA: 00:18:43

################################################################################
                     [1m Learning iteration 484/1500 [0m                      

                       Computation: 32382 steps/s (collection: 1.056s, learning 0.056s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0294
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 13.2025
                       Mean reward: 14.95
               Mean episode length: 877.57
Episode_Reward/track_lin_vel_xy_exp: 0.9640
Episode_Reward/track_ang_vel_z_exp: 0.4649
       Episode_Reward/lin_vel_z_l2: -0.0565
      Episode_Reward/ang_vel_xy_l2: -0.1351
     Episode_Reward/dof_torques_l2: -0.0832
         Episode_Reward/dof_acc_l2: -0.2037
     Episode_Reward/action_rate_l2: -0.1688
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9257
Metrics/base_velocity/error_vel_xy: 0.5300
Metrics/base_velocity/error_vel_yaw: 0.5439
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 17460000
                    Iteration time: 1.11s
                      Time elapsed: 00:08:55
                               ETA: 00:18:42

################################################################################
                     [1m Learning iteration 485/1500 [0m                      

                       Computation: 31588 steps/s (collection: 1.087s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0268
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 13.2062
                       Mean reward: 15.85
               Mean episode length: 911.67
Episode_Reward/track_lin_vel_xy_exp: 1.1230
Episode_Reward/track_ang_vel_z_exp: 0.5182
       Episode_Reward/lin_vel_z_l2: -0.0658
      Episode_Reward/ang_vel_xy_l2: -0.1444
     Episode_Reward/dof_torques_l2: -0.0942
         Episode_Reward/dof_acc_l2: -0.2370
     Episode_Reward/action_rate_l2: -0.1889
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9369
Metrics/base_velocity/error_vel_xy: 0.5069
Metrics/base_velocity/error_vel_yaw: 0.5727
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 17496000
                    Iteration time: 1.14s
                      Time elapsed: 00:08:56
                               ETA: 00:18:41

################################################################################
                     [1m Learning iteration 486/1500 [0m                      

                       Computation: 32395 steps/s (collection: 1.059s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0248
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 13.2003
                       Mean reward: 16.74
               Mean episode length: 923.12
Episode_Reward/track_lin_vel_xy_exp: 0.9974
Episode_Reward/track_ang_vel_z_exp: 0.4653
       Episode_Reward/lin_vel_z_l2: -0.0511
      Episode_Reward/ang_vel_xy_l2: -0.1258
     Episode_Reward/dof_torques_l2: -0.0844
         Episode_Reward/dof_acc_l2: -0.1883
     Episode_Reward/action_rate_l2: -0.1631
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9517
Metrics/base_velocity/error_vel_xy: 0.4656
Metrics/base_velocity/error_vel_yaw: 0.5152
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 17532000
                    Iteration time: 1.11s
                      Time elapsed: 00:08:57
                               ETA: 00:18:39

################################################################################
                     [1m Learning iteration 487/1500 [0m                      

                       Computation: 33558 steps/s (collection: 1.020s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0269
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 13.1917
                       Mean reward: 16.18
               Mean episode length: 885.77
Episode_Reward/track_lin_vel_xy_exp: 0.9245
Episode_Reward/track_ang_vel_z_exp: 0.4466
       Episode_Reward/lin_vel_z_l2: -0.0538
      Episode_Reward/ang_vel_xy_l2: -0.1254
     Episode_Reward/dof_torques_l2: -0.0818
         Episode_Reward/dof_acc_l2: -0.1933
     Episode_Reward/action_rate_l2: -0.1612
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9605
Metrics/base_velocity/error_vel_xy: 0.5022
Metrics/base_velocity/error_vel_yaw: 0.5242
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 17568000
                    Iteration time: 1.07s
                      Time elapsed: 00:08:58
                               ETA: 00:18:38

################################################################################
                     [1m Learning iteration 488/1500 [0m                      

                       Computation: 33007 steps/s (collection: 1.037s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0243
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 13.1927
                       Mean reward: 16.58
               Mean episode length: 851.46
Episode_Reward/track_lin_vel_xy_exp: 1.1148
Episode_Reward/track_ang_vel_z_exp: 0.4997
       Episode_Reward/lin_vel_z_l2: -0.0512
      Episode_Reward/ang_vel_xy_l2: -0.1311
     Episode_Reward/dof_torques_l2: -0.0839
         Episode_Reward/dof_acc_l2: -0.1953
     Episode_Reward/action_rate_l2: -0.1693
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9753
Metrics/base_velocity/error_vel_xy: 0.3860
Metrics/base_velocity/error_vel_yaw: 0.4879
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 17604000
                    Iteration time: 1.09s
                      Time elapsed: 00:09:00
                               ETA: 00:18:37

################################################################################
                     [1m Learning iteration 489/1500 [0m                      

                       Computation: 32400 steps/s (collection: 1.058s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0233
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 13.1918
                       Mean reward: 17.52
               Mean episode length: 871.57
Episode_Reward/track_lin_vel_xy_exp: 0.9880
Episode_Reward/track_ang_vel_z_exp: 0.4606
       Episode_Reward/lin_vel_z_l2: -0.0549
      Episode_Reward/ang_vel_xy_l2: -0.1282
     Episode_Reward/dof_torques_l2: -0.0836
         Episode_Reward/dof_acc_l2: -0.1912
     Episode_Reward/action_rate_l2: -0.1620
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9876
Metrics/base_velocity/error_vel_xy: 0.4715
Metrics/base_velocity/error_vel_yaw: 0.5184
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 17640000
                    Iteration time: 1.11s
                      Time elapsed: 00:09:01
                               ETA: 00:18:36

################################################################################
                     [1m Learning iteration 490/1500 [0m                      

                       Computation: 33498 steps/s (collection: 1.023s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0266
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 13.1936
                       Mean reward: 17.67
               Mean episode length: 893.44
Episode_Reward/track_lin_vel_xy_exp: 1.0706
Episode_Reward/track_ang_vel_z_exp: 0.4913
       Episode_Reward/lin_vel_z_l2: -0.0499
      Episode_Reward/ang_vel_xy_l2: -0.1324
     Episode_Reward/dof_torques_l2: -0.0841
         Episode_Reward/dof_acc_l2: -0.1905
     Episode_Reward/action_rate_l2: -0.1664
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 2.9978
Metrics/base_velocity/error_vel_xy: 0.4457
Metrics/base_velocity/error_vel_yaw: 0.5224
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 17676000
                    Iteration time: 1.07s
                      Time elapsed: 00:09:02
                               ETA: 00:18:35

################################################################################
                     [1m Learning iteration 491/1500 [0m                      

                       Computation: 33403 steps/s (collection: 1.023s, learning 0.055s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0328
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 13.2033
                       Mean reward: 17.05
               Mean episode length: 892.35
Episode_Reward/track_lin_vel_xy_exp: 0.8941
Episode_Reward/track_ang_vel_z_exp: 0.4299
       Episode_Reward/lin_vel_z_l2: -0.0471
      Episode_Reward/ang_vel_xy_l2: -0.1178
     Episode_Reward/dof_torques_l2: -0.0791
         Episode_Reward/dof_acc_l2: -0.1688
     Episode_Reward/action_rate_l2: -0.1493
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0087
Metrics/base_velocity/error_vel_xy: 0.4574
Metrics/base_velocity/error_vel_yaw: 0.4765
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 17712000
                    Iteration time: 1.08s
                      Time elapsed: 00:09:03
                               ETA: 00:18:34

################################################################################
                     [1m Learning iteration 492/1500 [0m                      

                       Computation: 31825 steps/s (collection: 1.080s, learning 0.051s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0294
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 13.2090
                       Mean reward: 15.91
               Mean episode length: 851.47
Episode_Reward/track_lin_vel_xy_exp: 0.9628
Episode_Reward/track_ang_vel_z_exp: 0.4397
       Episode_Reward/lin_vel_z_l2: -0.0432
      Episode_Reward/ang_vel_xy_l2: -0.1170
     Episode_Reward/dof_torques_l2: -0.0790
         Episode_Reward/dof_acc_l2: -0.1670
     Episode_Reward/action_rate_l2: -0.1513
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0177
Metrics/base_velocity/error_vel_xy: 0.4016
Metrics/base_velocity/error_vel_yaw: 0.4713
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 17748000
                    Iteration time: 1.13s
                      Time elapsed: 00:09:04
                               ETA: 00:18:33

################################################################################
                     [1m Learning iteration 493/1500 [0m                      

                       Computation: 31631 steps/s (collection: 1.087s, learning 0.051s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0237
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 13.2160
                       Mean reward: 15.49
               Mean episode length: 859.36
Episode_Reward/track_lin_vel_xy_exp: 0.9181
Episode_Reward/track_ang_vel_z_exp: 0.4358
       Episode_Reward/lin_vel_z_l2: -0.0516
      Episode_Reward/ang_vel_xy_l2: -0.1215
     Episode_Reward/dof_torques_l2: -0.0786
         Episode_Reward/dof_acc_l2: -0.1745
     Episode_Reward/action_rate_l2: -0.1526
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0279
Metrics/base_velocity/error_vel_xy: 0.4472
Metrics/base_velocity/error_vel_yaw: 0.4818
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 17784000
                    Iteration time: 1.14s
                      Time elapsed: 00:09:05
                               ETA: 00:18:32

################################################################################
                     [1m Learning iteration 494/1500 [0m                      

                       Computation: 33909 steps/s (collection: 1.010s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0260
               Mean surrogate loss: -0.0093
                 Mean entropy loss: 13.2158
                       Mean reward: 16.34
               Mean episode length: 872.58
Episode_Reward/track_lin_vel_xy_exp: 1.0013
Episode_Reward/track_ang_vel_z_exp: 0.4645
       Episode_Reward/lin_vel_z_l2: -0.0492
      Episode_Reward/ang_vel_xy_l2: -0.1251
     Episode_Reward/dof_torques_l2: -0.0862
         Episode_Reward/dof_acc_l2: -0.1890
     Episode_Reward/action_rate_l2: -0.1632
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0373
Metrics/base_velocity/error_vel_xy: 0.4651
Metrics/base_velocity/error_vel_yaw: 0.5079
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 17820000
                    Iteration time: 1.06s
                      Time elapsed: 00:09:06
                               ETA: 00:18:30

################################################################################
                     [1m Learning iteration 495/1500 [0m                      

                       Computation: 33432 steps/s (collection: 1.023s, learning 0.054s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0224
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 13.2129
                       Mean reward: 16.06
               Mean episode length: 863.95
Episode_Reward/track_lin_vel_xy_exp: 0.9296
Episode_Reward/track_ang_vel_z_exp: 0.4369
       Episode_Reward/lin_vel_z_l2: -0.0543
      Episode_Reward/ang_vel_xy_l2: -0.1241
     Episode_Reward/dof_torques_l2: -0.0822
         Episode_Reward/dof_acc_l2: -0.1902
     Episode_Reward/action_rate_l2: -0.1579
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0448
Metrics/base_velocity/error_vel_xy: 0.4658
Metrics/base_velocity/error_vel_yaw: 0.5014
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 17856000
                    Iteration time: 1.08s
                      Time elapsed: 00:09:07
                               ETA: 00:18:29

################################################################################
                     [1m Learning iteration 496/1500 [0m                      

                       Computation: 33228 steps/s (collection: 1.021s, learning 0.062s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0228
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 13.2078
                       Mean reward: 16.15
               Mean episode length: 873.25
Episode_Reward/track_lin_vel_xy_exp: 1.0228
Episode_Reward/track_ang_vel_z_exp: 0.4908
       Episode_Reward/lin_vel_z_l2: -0.0512
      Episode_Reward/ang_vel_xy_l2: -0.1345
     Episode_Reward/dof_torques_l2: -0.0872
         Episode_Reward/dof_acc_l2: -0.1999
     Episode_Reward/action_rate_l2: -0.1700
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0565
Metrics/base_velocity/error_vel_xy: 0.5021
Metrics/base_velocity/error_vel_yaw: 0.5266
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 17892000
                    Iteration time: 1.08s
                      Time elapsed: 00:09:08
                               ETA: 00:18:28

################################################################################
                     [1m Learning iteration 497/1500 [0m                      

                       Computation: 32044 steps/s (collection: 1.068s, learning 0.055s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0241
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 13.2083
                       Mean reward: 17.00
               Mean episode length: 895.41
Episode_Reward/track_lin_vel_xy_exp: 1.1037
Episode_Reward/track_ang_vel_z_exp: 0.5004
       Episode_Reward/lin_vel_z_l2: -0.0543
      Episode_Reward/ang_vel_xy_l2: -0.1383
     Episode_Reward/dof_torques_l2: -0.0884
         Episode_Reward/dof_acc_l2: -0.2030
     Episode_Reward/action_rate_l2: -0.1759
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0694
Metrics/base_velocity/error_vel_xy: 0.4765
Metrics/base_velocity/error_vel_yaw: 0.5637
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 17928000
                    Iteration time: 1.12s
                      Time elapsed: 00:09:09
                               ETA: 00:18:27

################################################################################
                     [1m Learning iteration 498/1500 [0m                      

                       Computation: 31221 steps/s (collection: 1.099s, learning 0.054s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0291
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 13.2064
                       Mean reward: 16.91
               Mean episode length: 904.05
Episode_Reward/track_lin_vel_xy_exp: 1.0459
Episode_Reward/track_ang_vel_z_exp: 0.4789
       Episode_Reward/lin_vel_z_l2: -0.0620
      Episode_Reward/ang_vel_xy_l2: -0.1316
     Episode_Reward/dof_torques_l2: -0.0854
         Episode_Reward/dof_acc_l2: -0.2135
     Episode_Reward/action_rate_l2: -0.1696
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0831
Metrics/base_velocity/error_vel_xy: 0.4467
Metrics/base_velocity/error_vel_yaw: 0.5208
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 17964000
                    Iteration time: 1.15s
                      Time elapsed: 00:09:11
                               ETA: 00:18:26

################################################################################
                     [1m Learning iteration 499/1500 [0m                      

                       Computation: 31479 steps/s (collection: 1.090s, learning 0.054s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0230
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 13.2155
                       Mean reward: 17.70
               Mean episode length: 898.08
Episode_Reward/track_lin_vel_xy_exp: 0.9414
Episode_Reward/track_ang_vel_z_exp: 0.4273
       Episode_Reward/lin_vel_z_l2: -0.0527
      Episode_Reward/ang_vel_xy_l2: -0.1255
     Episode_Reward/dof_torques_l2: -0.0723
         Episode_Reward/dof_acc_l2: -0.1757
     Episode_Reward/action_rate_l2: -0.1475
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.0896
Metrics/base_velocity/error_vel_xy: 0.3893
Metrics/base_velocity/error_vel_yaw: 0.4738
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 18000000
                    Iteration time: 1.14s
                      Time elapsed: 00:09:12
                               ETA: 00:18:25

################################################################################
                     [1m Learning iteration 500/1500 [0m                      

                       Computation: 31649 steps/s (collection: 1.082s, learning 0.055s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0256
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 13.2193
                       Mean reward: 14.93
               Mean episode length: 818.34
Episode_Reward/track_lin_vel_xy_exp: 0.7413
Episode_Reward/track_ang_vel_z_exp: 0.3631
       Episode_Reward/lin_vel_z_l2: -0.0441
      Episode_Reward/ang_vel_xy_l2: -0.1081
     Episode_Reward/dof_torques_l2: -0.0656
         Episode_Reward/dof_acc_l2: -0.1539
     Episode_Reward/action_rate_l2: -0.1289
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1007
Metrics/base_velocity/error_vel_xy: 0.4433
Metrics/base_velocity/error_vel_yaw: 0.4438
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18036000
                    Iteration time: 1.14s
                      Time elapsed: 00:09:13
                               ETA: 00:18:24

################################################################################
                     [1m Learning iteration 501/1500 [0m                      

                       Computation: 32152 steps/s (collection: 1.067s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0301
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 13.2218
                       Mean reward: 14.56
               Mean episode length: 816.31
Episode_Reward/track_lin_vel_xy_exp: 0.9070
Episode_Reward/track_ang_vel_z_exp: 0.4500
       Episode_Reward/lin_vel_z_l2: -0.0524
      Episode_Reward/ang_vel_xy_l2: -0.1296
     Episode_Reward/dof_torques_l2: -0.0818
         Episode_Reward/dof_acc_l2: -0.1760
     Episode_Reward/action_rate_l2: -0.1584
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1058
Metrics/base_velocity/error_vel_xy: 0.5389
Metrics/base_velocity/error_vel_yaw: 0.5311
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 18072000
                    Iteration time: 1.12s
                      Time elapsed: 00:09:14
                               ETA: 00:18:23

################################################################################
                     [1m Learning iteration 502/1500 [0m                      

                       Computation: 31711 steps/s (collection: 1.082s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0232
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 13.2313
                       Mean reward: 15.37
               Mean episode length: 864.90
Episode_Reward/track_lin_vel_xy_exp: 1.0005
Episode_Reward/track_ang_vel_z_exp: 0.4868
       Episode_Reward/lin_vel_z_l2: -0.0529
      Episode_Reward/ang_vel_xy_l2: -0.1373
     Episode_Reward/dof_torques_l2: -0.0909
         Episode_Reward/dof_acc_l2: -0.2056
     Episode_Reward/action_rate_l2: -0.1738
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1151
Metrics/base_velocity/error_vel_xy: 0.5604
Metrics/base_velocity/error_vel_yaw: 0.5765
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 18108000
                    Iteration time: 1.14s
                      Time elapsed: 00:09:15
                               ETA: 00:18:22

################################################################################
                     [1m Learning iteration 503/1500 [0m                      

                       Computation: 32814 steps/s (collection: 1.041s, learning 0.056s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0275
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 13.2356
                       Mean reward: 17.51
               Mean episode length: 929.57
Episode_Reward/track_lin_vel_xy_exp: 1.0631
Episode_Reward/track_ang_vel_z_exp: 0.4775
       Episode_Reward/lin_vel_z_l2: -0.0504
      Episode_Reward/ang_vel_xy_l2: -0.1266
     Episode_Reward/dof_torques_l2: -0.0836
         Episode_Reward/dof_acc_l2: -0.1922
     Episode_Reward/action_rate_l2: -0.1653
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1263
Metrics/base_velocity/error_vel_xy: 0.4183
Metrics/base_velocity/error_vel_yaw: 0.5154
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 18144000
                    Iteration time: 1.10s
                      Time elapsed: 00:09:16
                               ETA: 00:18:21

################################################################################
                     [1m Learning iteration 504/1500 [0m                      

                       Computation: 32319 steps/s (collection: 1.064s, learning 0.050s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0250
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 13.2492
                       Mean reward: 16.67
               Mean episode length: 860.05
Episode_Reward/track_lin_vel_xy_exp: 0.9016
Episode_Reward/track_ang_vel_z_exp: 0.4221
       Episode_Reward/lin_vel_z_l2: -0.0543
      Episode_Reward/ang_vel_xy_l2: -0.1212
     Episode_Reward/dof_torques_l2: -0.0782
         Episode_Reward/dof_acc_l2: -0.1758
     Episode_Reward/action_rate_l2: -0.1491
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1379
Metrics/base_velocity/error_vel_xy: 0.4334
Metrics/base_velocity/error_vel_yaw: 0.4788
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 18180000
                    Iteration time: 1.11s
                      Time elapsed: 00:09:17
                               ETA: 00:18:20

################################################################################
                     [1m Learning iteration 505/1500 [0m                      

                       Computation: 32358 steps/s (collection: 1.060s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0286
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 13.2508
                       Mean reward: 16.39
               Mean episode length: 851.45
Episode_Reward/track_lin_vel_xy_exp: 1.0767
Episode_Reward/track_ang_vel_z_exp: 0.4866
       Episode_Reward/lin_vel_z_l2: -0.0501
      Episode_Reward/ang_vel_xy_l2: -0.1341
     Episode_Reward/dof_torques_l2: -0.0848
         Episode_Reward/dof_acc_l2: -0.1895
     Episode_Reward/action_rate_l2: -0.1663
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1434
Metrics/base_velocity/error_vel_xy: 0.4398
Metrics/base_velocity/error_vel_yaw: 0.5335
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 18216000
                    Iteration time: 1.11s
                      Time elapsed: 00:09:18
                               ETA: 00:18:19

################################################################################
                     [1m Learning iteration 506/1500 [0m                      

                       Computation: 32130 steps/s (collection: 1.067s, learning 0.054s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0264
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 13.2530
                       Mean reward: 17.14
               Mean episode length: 856.44
Episode_Reward/track_lin_vel_xy_exp: 1.0042
Episode_Reward/track_ang_vel_z_exp: 0.4612
       Episode_Reward/lin_vel_z_l2: -0.0500
      Episode_Reward/ang_vel_xy_l2: -0.1205
     Episode_Reward/dof_torques_l2: -0.0837
         Episode_Reward/dof_acc_l2: -0.1743
     Episode_Reward/action_rate_l2: -0.1558
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1499
Metrics/base_velocity/error_vel_xy: 0.4093
Metrics/base_velocity/error_vel_yaw: 0.4802
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 18252000
                    Iteration time: 1.12s
                      Time elapsed: 00:09:20
                               ETA: 00:18:18

################################################################################
                     [1m Learning iteration 507/1500 [0m                      

                       Computation: 33889 steps/s (collection: 1.011s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0255
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 13.2548
                       Mean reward: 17.38
               Mean episode length: 864.01
Episode_Reward/track_lin_vel_xy_exp: 1.0741
Episode_Reward/track_ang_vel_z_exp: 0.4986
       Episode_Reward/lin_vel_z_l2: -0.0524
      Episode_Reward/ang_vel_xy_l2: -0.1352
     Episode_Reward/dof_torques_l2: -0.0873
         Episode_Reward/dof_acc_l2: -0.2020
     Episode_Reward/action_rate_l2: -0.1704
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1567
Metrics/base_velocity/error_vel_xy: 0.4662
Metrics/base_velocity/error_vel_yaw: 0.5215
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 18288000
                    Iteration time: 1.06s
                      Time elapsed: 00:09:21
                               ETA: 00:18:16

################################################################################
                     [1m Learning iteration 508/1500 [0m                      

                       Computation: 34351 steps/s (collection: 0.997s, learning 0.051s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0295
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 13.2619
                       Mean reward: 17.63
               Mean episode length: 878.71
Episode_Reward/track_lin_vel_xy_exp: 1.0221
Episode_Reward/track_ang_vel_z_exp: 0.4619
       Episode_Reward/lin_vel_z_l2: -0.0572
      Episode_Reward/ang_vel_xy_l2: -0.1288
     Episode_Reward/dof_torques_l2: -0.0815
         Episode_Reward/dof_acc_l2: -0.2037
     Episode_Reward/action_rate_l2: -0.1631
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1702
Metrics/base_velocity/error_vel_xy: 0.4256
Metrics/base_velocity/error_vel_yaw: 0.5036
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 18324000
                    Iteration time: 1.05s
                      Time elapsed: 00:09:22
                               ETA: 00:18:15

################################################################################
                     [1m Learning iteration 509/1500 [0m                      

                       Computation: 32907 steps/s (collection: 1.040s, learning 0.054s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0242
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 13.2734
                       Mean reward: 17.34
               Mean episode length: 893.23
Episode_Reward/track_lin_vel_xy_exp: 1.0958
Episode_Reward/track_ang_vel_z_exp: 0.5121
       Episode_Reward/lin_vel_z_l2: -0.0559
      Episode_Reward/ang_vel_xy_l2: -0.1391
     Episode_Reward/dof_torques_l2: -0.0922
         Episode_Reward/dof_acc_l2: -0.2156
     Episode_Reward/action_rate_l2: -0.1779
      Episode_Reward/feet_air_time: -0.0016
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1851
Metrics/base_velocity/error_vel_xy: 0.4967
Metrics/base_velocity/error_vel_yaw: 0.5463
      Episode_Termination/time_out: 2.0833
  Episode_Termination/base_contact: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 18360000
                    Iteration time: 1.09s
                      Time elapsed: 00:09:23
                               ETA: 00:18:14

################################################################################
                     [1m Learning iteration 510/1500 [0m                      

                       Computation: 32012 steps/s (collection: 1.073s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0262
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 13.2796
                       Mean reward: 16.38
               Mean episode length: 874.10
Episode_Reward/track_lin_vel_xy_exp: 0.9004
Episode_Reward/track_ang_vel_z_exp: 0.4372
       Episode_Reward/lin_vel_z_l2: -0.0481
      Episode_Reward/ang_vel_xy_l2: -0.1247
     Episode_Reward/dof_torques_l2: -0.0834
         Episode_Reward/dof_acc_l2: -0.1864
     Episode_Reward/action_rate_l2: -0.1588
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.1949
Metrics/base_velocity/error_vel_xy: 0.4995
Metrics/base_velocity/error_vel_yaw: 0.5207
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 18396000
                    Iteration time: 1.12s
                      Time elapsed: 00:09:24
                               ETA: 00:18:13

################################################################################
                     [1m Learning iteration 511/1500 [0m                      

                       Computation: 32842 steps/s (collection: 1.043s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0311
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 13.2863
                       Mean reward: 16.61
               Mean episode length: 892.36
Episode_Reward/track_lin_vel_xy_exp: 1.0393
Episode_Reward/track_ang_vel_z_exp: 0.4827
       Episode_Reward/lin_vel_z_l2: -0.0539
      Episode_Reward/ang_vel_xy_l2: -0.1342
     Episode_Reward/dof_torques_l2: -0.0875
         Episode_Reward/dof_acc_l2: -0.1955
     Episode_Reward/action_rate_l2: -0.1686
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2039
Metrics/base_velocity/error_vel_xy: 0.4921
Metrics/base_velocity/error_vel_yaw: 0.5643
      Episode_Termination/time_out: 1.9167
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 18432000
                    Iteration time: 1.10s
                      Time elapsed: 00:09:25
                               ETA: 00:18:12

################################################################################
                     [1m Learning iteration 512/1500 [0m                      

                       Computation: 32091 steps/s (collection: 1.070s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0305
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 13.2861
                       Mean reward: 16.19
               Mean episode length: 858.47
Episode_Reward/track_lin_vel_xy_exp: 0.8793
Episode_Reward/track_ang_vel_z_exp: 0.4181
       Episode_Reward/lin_vel_z_l2: -0.0472
      Episode_Reward/ang_vel_xy_l2: -0.1171
     Episode_Reward/dof_torques_l2: -0.0769
         Episode_Reward/dof_acc_l2: -0.1697
     Episode_Reward/action_rate_l2: -0.1464
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2060
Metrics/base_velocity/error_vel_xy: 0.4587
Metrics/base_velocity/error_vel_yaw: 0.4947
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 18468000
                    Iteration time: 1.12s
                      Time elapsed: 00:09:26
                               ETA: 00:18:11

################################################################################
                     [1m Learning iteration 513/1500 [0m                      

                       Computation: 31406 steps/s (collection: 1.092s, learning 0.054s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0269
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 13.2813
                       Mean reward: 15.56
               Mean episode length: 848.33
Episode_Reward/track_lin_vel_xy_exp: 0.9722
Episode_Reward/track_ang_vel_z_exp: 0.4526
       Episode_Reward/lin_vel_z_l2: -0.0477
      Episode_Reward/ang_vel_xy_l2: -0.1250
     Episode_Reward/dof_torques_l2: -0.0855
         Episode_Reward/dof_acc_l2: -0.1754
     Episode_Reward/action_rate_l2: -0.1595
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2140
Metrics/base_velocity/error_vel_xy: 0.4796
Metrics/base_velocity/error_vel_yaw: 0.5500
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 18504000
                    Iteration time: 1.15s
                      Time elapsed: 00:09:27
                               ETA: 00:18:10

################################################################################
                     [1m Learning iteration 514/1500 [0m                      

                       Computation: 30533 steps/s (collection: 1.125s, learning 0.054s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0281
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 13.2891
                       Mean reward: 16.56
               Mean episode length: 861.29
Episode_Reward/track_lin_vel_xy_exp: 0.9837
Episode_Reward/track_ang_vel_z_exp: 0.4602
       Episode_Reward/lin_vel_z_l2: -0.0481
      Episode_Reward/ang_vel_xy_l2: -0.1222
     Episode_Reward/dof_torques_l2: -0.0800
         Episode_Reward/dof_acc_l2: -0.1725
     Episode_Reward/action_rate_l2: -0.1555
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2237
Metrics/base_velocity/error_vel_xy: 0.4477
Metrics/base_velocity/error_vel_yaw: 0.4823
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 18540000
                    Iteration time: 1.18s
                      Time elapsed: 00:09:28
                               ETA: 00:18:09

################################################################################
                     [1m Learning iteration 515/1500 [0m                      

                       Computation: 31189 steps/s (collection: 1.104s, learning 0.051s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0291
               Mean surrogate loss: -0.0084
                 Mean entropy loss: 13.2952
                       Mean reward: 16.26
               Mean episode length: 843.60
Episode_Reward/track_lin_vel_xy_exp: 1.0349
Episode_Reward/track_ang_vel_z_exp: 0.4659
       Episode_Reward/lin_vel_z_l2: -0.0536
      Episode_Reward/ang_vel_xy_l2: -0.1255
     Episode_Reward/dof_torques_l2: -0.0812
         Episode_Reward/dof_acc_l2: -0.1885
     Episode_Reward/action_rate_l2: -0.1618
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2346
Metrics/base_velocity/error_vel_xy: 0.4058
Metrics/base_velocity/error_vel_yaw: 0.4971
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 18576000
                    Iteration time: 1.15s
                      Time elapsed: 00:09:30
                               ETA: 00:18:08

################################################################################
                     [1m Learning iteration 516/1500 [0m                      

                       Computation: 30081 steps/s (collection: 1.143s, learning 0.054s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0243
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 13.2997
                       Mean reward: 16.16
               Mean episode length: 849.71
Episode_Reward/track_lin_vel_xy_exp: 1.0132
Episode_Reward/track_ang_vel_z_exp: 0.4623
       Episode_Reward/lin_vel_z_l2: -0.0573
      Episode_Reward/ang_vel_xy_l2: -0.1290
     Episode_Reward/dof_torques_l2: -0.0838
         Episode_Reward/dof_acc_l2: -0.1974
     Episode_Reward/action_rate_l2: -0.1642
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2389
Metrics/base_velocity/error_vel_xy: 0.4345
Metrics/base_velocity/error_vel_yaw: 0.5075
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 18612000
                    Iteration time: 1.20s
                      Time elapsed: 00:09:31
                               ETA: 00:18:07

################################################################################
                     [1m Learning iteration 517/1500 [0m                      

                       Computation: 31428 steps/s (collection: 1.091s, learning 0.054s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0238
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 13.3056
                       Mean reward: 16.00
               Mean episode length: 838.65
Episode_Reward/track_lin_vel_xy_exp: 0.9710
Episode_Reward/track_ang_vel_z_exp: 0.4529
       Episode_Reward/lin_vel_z_l2: -0.0462
      Episode_Reward/ang_vel_xy_l2: -0.1257
     Episode_Reward/dof_torques_l2: -0.0769
         Episode_Reward/dof_acc_l2: -0.1781
     Episode_Reward/action_rate_l2: -0.1550
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2432
Metrics/base_velocity/error_vel_xy: 0.4515
Metrics/base_velocity/error_vel_yaw: 0.5146
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 18648000
                    Iteration time: 1.15s
                      Time elapsed: 00:09:32
                               ETA: 00:18:06

################################################################################
                     [1m Learning iteration 518/1500 [0m                      

                       Computation: 31465 steps/s (collection: 1.091s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0233
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 13.3017
                       Mean reward: 14.97
               Mean episode length: 825.54
Episode_Reward/track_lin_vel_xy_exp: 0.8591
Episode_Reward/track_ang_vel_z_exp: 0.4339
       Episode_Reward/lin_vel_z_l2: -0.0537
      Episode_Reward/ang_vel_xy_l2: -0.1252
     Episode_Reward/dof_torques_l2: -0.0840
         Episode_Reward/dof_acc_l2: -0.1858
     Episode_Reward/action_rate_l2: -0.1588
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2505
Metrics/base_velocity/error_vel_xy: 0.5649
Metrics/base_velocity/error_vel_yaw: 0.5460
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 18684000
                    Iteration time: 1.14s
                      Time elapsed: 00:09:33
                               ETA: 00:18:05

################################################################################
                     [1m Learning iteration 519/1500 [0m                      

                       Computation: 32526 steps/s (collection: 1.055s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0273
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 13.2868
                       Mean reward: 15.29
               Mean episode length: 881.11
Episode_Reward/track_lin_vel_xy_exp: 1.0076
Episode_Reward/track_ang_vel_z_exp: 0.4945
       Episode_Reward/lin_vel_z_l2: -0.0503
      Episode_Reward/ang_vel_xy_l2: -0.1334
     Episode_Reward/dof_torques_l2: -0.0931
         Episode_Reward/dof_acc_l2: -0.2045
     Episode_Reward/action_rate_l2: -0.1741
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2569
Metrics/base_velocity/error_vel_xy: 0.5521
Metrics/base_velocity/error_vel_yaw: 0.5498
      Episode_Termination/time_out: 2.0833
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 18720000
                    Iteration time: 1.11s
                      Time elapsed: 00:09:34
                               ETA: 00:18:04

################################################################################
                     [1m Learning iteration 520/1500 [0m                      

                       Computation: 32366 steps/s (collection: 1.060s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0293
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 13.2833
                       Mean reward: 15.90
               Mean episode length: 881.65
Episode_Reward/track_lin_vel_xy_exp: 0.8820
Episode_Reward/track_ang_vel_z_exp: 0.4106
       Episode_Reward/lin_vel_z_l2: -0.0479
      Episode_Reward/ang_vel_xy_l2: -0.1148
     Episode_Reward/dof_torques_l2: -0.0763
         Episode_Reward/dof_acc_l2: -0.1789
     Episode_Reward/action_rate_l2: -0.1470
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2676
Metrics/base_velocity/error_vel_xy: 0.4153
Metrics/base_velocity/error_vel_yaw: 0.4644
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 18756000
                    Iteration time: 1.11s
                      Time elapsed: 00:09:35
                               ETA: 00:18:03

################################################################################
                     [1m Learning iteration 521/1500 [0m                      

                       Computation: 31944 steps/s (collection: 1.076s, learning 0.051s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0283
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 13.2909
                       Mean reward: 16.66
               Mean episode length: 863.64
Episode_Reward/track_lin_vel_xy_exp: 1.0511
Episode_Reward/track_ang_vel_z_exp: 0.4785
       Episode_Reward/lin_vel_z_l2: -0.0542
      Episode_Reward/ang_vel_xy_l2: -0.1289
     Episode_Reward/dof_torques_l2: -0.0860
         Episode_Reward/dof_acc_l2: -0.2000
     Episode_Reward/action_rate_l2: -0.1677
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2825
Metrics/base_velocity/error_vel_xy: 0.4385
Metrics/base_velocity/error_vel_yaw: 0.5157
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 18792000
                    Iteration time: 1.13s
                      Time elapsed: 00:09:36
                               ETA: 00:18:02

################################################################################
                     [1m Learning iteration 522/1500 [0m                      

                       Computation: 32612 steps/s (collection: 1.053s, learning 0.051s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0323
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 13.2956
                       Mean reward: 17.80
               Mean episode length: 905.68
Episode_Reward/track_lin_vel_xy_exp: 1.0824
Episode_Reward/track_ang_vel_z_exp: 0.5000
       Episode_Reward/lin_vel_z_l2: -0.0560
      Episode_Reward/ang_vel_xy_l2: -0.1375
     Episode_Reward/dof_torques_l2: -0.0881
         Episode_Reward/dof_acc_l2: -0.2111
     Episode_Reward/action_rate_l2: -0.1759
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.2952
Metrics/base_velocity/error_vel_xy: 0.4768
Metrics/base_velocity/error_vel_yaw: 0.5533
      Episode_Termination/time_out: 2.1250
  Episode_Termination/base_contact: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 18828000
                    Iteration time: 1.10s
                      Time elapsed: 00:09:38
                               ETA: 00:18:00

################################################################################
                     [1m Learning iteration 523/1500 [0m                      

                       Computation: 31377 steps/s (collection: 1.092s, learning 0.055s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0294
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 13.3020
                       Mean reward: 17.06
               Mean episode length: 886.94
Episode_Reward/track_lin_vel_xy_exp: 0.9062
Episode_Reward/track_ang_vel_z_exp: 0.4260
       Episode_Reward/lin_vel_z_l2: -0.0500
      Episode_Reward/ang_vel_xy_l2: -0.1183
     Episode_Reward/dof_torques_l2: -0.0817
         Episode_Reward/dof_acc_l2: -0.1815
     Episode_Reward/action_rate_l2: -0.1523
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3085
Metrics/base_velocity/error_vel_xy: 0.4244
Metrics/base_velocity/error_vel_yaw: 0.4658
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 18864000
                    Iteration time: 1.15s
                      Time elapsed: 00:09:39
                               ETA: 00:17:59

################################################################################
                     [1m Learning iteration 524/1500 [0m                      

                       Computation: 30607 steps/s (collection: 1.120s, learning 0.057s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0269
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 13.3105
                       Mean reward: 16.17
               Mean episode length: 842.73
Episode_Reward/track_lin_vel_xy_exp: 0.9478
Episode_Reward/track_ang_vel_z_exp: 0.4462
       Episode_Reward/lin_vel_z_l2: -0.0556
      Episode_Reward/ang_vel_xy_l2: -0.1278
     Episode_Reward/dof_torques_l2: -0.0806
         Episode_Reward/dof_acc_l2: -0.2011
     Episode_Reward/action_rate_l2: -0.1599
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3174
Metrics/base_velocity/error_vel_xy: 0.4395
Metrics/base_velocity/error_vel_yaw: 0.4806
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 18900000
                    Iteration time: 1.18s
                      Time elapsed: 00:09:40
                               ETA: 00:17:58

################################################################################
                     [1m Learning iteration 525/1500 [0m                      

                       Computation: 32367 steps/s (collection: 1.058s, learning 0.054s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0311
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 13.3131
                       Mean reward: 15.12
               Mean episode length: 814.09
Episode_Reward/track_lin_vel_xy_exp: 0.8981
Episode_Reward/track_ang_vel_z_exp: 0.4279
       Episode_Reward/lin_vel_z_l2: -0.0462
      Episode_Reward/ang_vel_xy_l2: -0.1217
     Episode_Reward/dof_torques_l2: -0.0829
         Episode_Reward/dof_acc_l2: -0.1802
     Episode_Reward/action_rate_l2: -0.1554
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3222
Metrics/base_velocity/error_vel_xy: 0.4816
Metrics/base_velocity/error_vel_yaw: 0.5160
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 18936000
                    Iteration time: 1.11s
                      Time elapsed: 00:09:41
                               ETA: 00:17:57

################################################################################
                     [1m Learning iteration 526/1500 [0m                      

                       Computation: 30702 steps/s (collection: 1.120s, learning 0.052s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0307
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 13.3270
                       Mean reward: 15.19
               Mean episode length: 837.60
Episode_Reward/track_lin_vel_xy_exp: 0.9928
Episode_Reward/track_ang_vel_z_exp: 0.4627
       Episode_Reward/lin_vel_z_l2: -0.0556
      Episode_Reward/ang_vel_xy_l2: -0.1329
     Episode_Reward/dof_torques_l2: -0.0824
         Episode_Reward/dof_acc_l2: -0.1992
     Episode_Reward/action_rate_l2: -0.1655
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3316
Metrics/base_velocity/error_vel_xy: 0.4702
Metrics/base_velocity/error_vel_yaw: 0.5285
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 18972000
                    Iteration time: 1.17s
                      Time elapsed: 00:09:42
                               ETA: 00:17:56

################################################################################
                     [1m Learning iteration 527/1500 [0m                      

                       Computation: 32927 steps/s (collection: 1.040s, learning 0.054s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0261
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 13.3215
                       Mean reward: 15.99
               Mean episode length: 847.49
Episode_Reward/track_lin_vel_xy_exp: 0.8670
Episode_Reward/track_ang_vel_z_exp: 0.4034
       Episode_Reward/lin_vel_z_l2: -0.0456
      Episode_Reward/ang_vel_xy_l2: -0.1144
     Episode_Reward/dof_torques_l2: -0.0731
         Episode_Reward/dof_acc_l2: -0.1686
     Episode_Reward/action_rate_l2: -0.1439
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3408
Metrics/base_velocity/error_vel_xy: 0.4278
Metrics/base_velocity/error_vel_yaw: 0.4827
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 19008000
                    Iteration time: 1.09s
                      Time elapsed: 00:09:43
                               ETA: 00:17:55

################################################################################
                     [1m Learning iteration 528/1500 [0m                      

                       Computation: 32978 steps/s (collection: 1.041s, learning 0.051s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0267
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 13.3167
                       Mean reward: 16.28
               Mean episode length: 832.01
Episode_Reward/track_lin_vel_xy_exp: 0.9644
Episode_Reward/track_ang_vel_z_exp: 0.4371
       Episode_Reward/lin_vel_z_l2: -0.0449
      Episode_Reward/ang_vel_xy_l2: -0.1155
     Episode_Reward/dof_torques_l2: -0.0784
         Episode_Reward/dof_acc_l2: -0.1660
     Episode_Reward/action_rate_l2: -0.1499
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3477
Metrics/base_velocity/error_vel_xy: 0.4028
Metrics/base_velocity/error_vel_yaw: 0.4732
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 19044000
                    Iteration time: 1.09s
                      Time elapsed: 00:09:44
                               ETA: 00:17:54

################################################################################
                     [1m Learning iteration 529/1500 [0m                      

                       Computation: 30830 steps/s (collection: 1.113s, learning 0.054s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0262
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 13.3124
                       Mean reward: 16.58
               Mean episode length: 828.84
Episode_Reward/track_lin_vel_xy_exp: 0.8888
Episode_Reward/track_ang_vel_z_exp: 0.4291
       Episode_Reward/lin_vel_z_l2: -0.0494
      Episode_Reward/ang_vel_xy_l2: -0.1226
     Episode_Reward/dof_torques_l2: -0.0828
         Episode_Reward/dof_acc_l2: -0.1812
     Episode_Reward/action_rate_l2: -0.1553
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3629
Metrics/base_velocity/error_vel_xy: 0.5079
Metrics/base_velocity/error_vel_yaw: 0.5165
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 19080000
                    Iteration time: 1.17s
                      Time elapsed: 00:09:45
                               ETA: 00:17:53

################################################################################
                     [1m Learning iteration 530/1500 [0m                      

                       Computation: 32766 steps/s (collection: 1.046s, learning 0.053s)
             Mean action noise std: 0.74
          Mean value_function loss: 0.0255
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 13.3217
                       Mean reward: 15.45
               Mean episode length: 837.49
Episode_Reward/track_lin_vel_xy_exp: 0.9629
Episode_Reward/track_ang_vel_z_exp: 0.4485
       Episode_Reward/lin_vel_z_l2: -0.0498
      Episode_Reward/ang_vel_xy_l2: -0.1231
     Episode_Reward/dof_torques_l2: -0.0803
         Episode_Reward/dof_acc_l2: -0.1847
     Episode_Reward/action_rate_l2: -0.1589
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3712
Metrics/base_velocity/error_vel_xy: 0.4446
Metrics/base_velocity/error_vel_yaw: 0.4949
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 19116000
                    Iteration time: 1.10s
                      Time elapsed: 00:09:47
                               ETA: 00:17:52

################################################################################
                     [1m Learning iteration 531/1500 [0m                      

                       Computation: 32514 steps/s (collection: 1.055s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0249
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 13.3363
                       Mean reward: 15.61
               Mean episode length: 838.94
Episode_Reward/track_lin_vel_xy_exp: 0.9842
Episode_Reward/track_ang_vel_z_exp: 0.4550
       Episode_Reward/lin_vel_z_l2: -0.0500
      Episode_Reward/ang_vel_xy_l2: -0.1270
     Episode_Reward/dof_torques_l2: -0.0828
         Episode_Reward/dof_acc_l2: -0.1918
     Episode_Reward/action_rate_l2: -0.1624
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3800
Metrics/base_velocity/error_vel_xy: 0.4432
Metrics/base_velocity/error_vel_yaw: 0.5144
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 19152000
                    Iteration time: 1.11s
                      Time elapsed: 00:09:48
                               ETA: 00:17:51

################################################################################
                     [1m Learning iteration 532/1500 [0m                      

                       Computation: 31960 steps/s (collection: 1.074s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0286
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 13.3397
                       Mean reward: 15.61
               Mean episode length: 821.45
Episode_Reward/track_lin_vel_xy_exp: 0.8755
Episode_Reward/track_ang_vel_z_exp: 0.4013
       Episode_Reward/lin_vel_z_l2: -0.0432
      Episode_Reward/ang_vel_xy_l2: -0.1125
     Episode_Reward/dof_torques_l2: -0.0741
         Episode_Reward/dof_acc_l2: -0.1667
     Episode_Reward/action_rate_l2: -0.1439
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.3890
Metrics/base_velocity/error_vel_xy: 0.4063
Metrics/base_velocity/error_vel_yaw: 0.4730
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 19188000
                    Iteration time: 1.13s
                      Time elapsed: 00:09:49
                               ETA: 00:17:50

################################################################################
                     [1m Learning iteration 533/1500 [0m                      

                       Computation: 32280 steps/s (collection: 1.062s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0279
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 13.3488
                       Mean reward: 14.26
               Mean episode length: 795.80
Episode_Reward/track_lin_vel_xy_exp: 0.7849
Episode_Reward/track_ang_vel_z_exp: 0.3875
       Episode_Reward/lin_vel_z_l2: -0.0443
      Episode_Reward/ang_vel_xy_l2: -0.1174
     Episode_Reward/dof_torques_l2: -0.0802
         Episode_Reward/dof_acc_l2: -0.1719
     Episode_Reward/action_rate_l2: -0.1475
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4021
Metrics/base_velocity/error_vel_xy: 0.5054
Metrics/base_velocity/error_vel_yaw: 0.5228
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 19224000
                    Iteration time: 1.12s
                      Time elapsed: 00:09:50
                               ETA: 00:17:49

################################################################################
                     [1m Learning iteration 534/1500 [0m                      

                       Computation: 34273 steps/s (collection: 0.998s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0242
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 13.3506
                       Mean reward: 15.54
               Mean episode length: 860.83
Episode_Reward/track_lin_vel_xy_exp: 1.0351
Episode_Reward/track_ang_vel_z_exp: 0.4854
       Episode_Reward/lin_vel_z_l2: -0.0532
      Episode_Reward/ang_vel_xy_l2: -0.1370
     Episode_Reward/dof_torques_l2: -0.0910
         Episode_Reward/dof_acc_l2: -0.1942
     Episode_Reward/action_rate_l2: -0.1728
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4068
Metrics/base_velocity/error_vel_xy: 0.5099
Metrics/base_velocity/error_vel_yaw: 0.5694
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 19260000
                    Iteration time: 1.05s
                      Time elapsed: 00:09:51
                               ETA: 00:17:48

################################################################################
                     [1m Learning iteration 535/1500 [0m                      

                       Computation: 31390 steps/s (collection: 1.095s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0305
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 13.3542
                       Mean reward: 14.78
               Mean episode length: 849.50
Episode_Reward/track_lin_vel_xy_exp: 0.9294
Episode_Reward/track_ang_vel_z_exp: 0.4458
       Episode_Reward/lin_vel_z_l2: -0.0466
      Episode_Reward/ang_vel_xy_l2: -0.1233
     Episode_Reward/dof_torques_l2: -0.0863
         Episode_Reward/dof_acc_l2: -0.1854
     Episode_Reward/action_rate_l2: -0.1594
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4118
Metrics/base_velocity/error_vel_xy: 0.4853
Metrics/base_velocity/error_vel_yaw: 0.5039
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 19296000
                    Iteration time: 1.15s
                      Time elapsed: 00:09:52
                               ETA: 00:17:46

################################################################################
                     [1m Learning iteration 536/1500 [0m                      

                       Computation: 32007 steps/s (collection: 1.073s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0282
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 13.3552
                       Mean reward: 14.47
               Mean episode length: 825.97
Episode_Reward/track_lin_vel_xy_exp: 0.9344
Episode_Reward/track_ang_vel_z_exp: 0.4414
       Episode_Reward/lin_vel_z_l2: -0.0494
      Episode_Reward/ang_vel_xy_l2: -0.1235
     Episode_Reward/dof_torques_l2: -0.0852
         Episode_Reward/dof_acc_l2: -0.1865
     Episode_Reward/action_rate_l2: -0.1589
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4196
Metrics/base_velocity/error_vel_xy: 0.4566
Metrics/base_velocity/error_vel_yaw: 0.5005
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 19332000
                    Iteration time: 1.12s
                      Time elapsed: 00:09:53
                               ETA: 00:17:45

################################################################################
                     [1m Learning iteration 537/1500 [0m                      

                       Computation: 31345 steps/s (collection: 1.096s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0248
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 13.3564
                       Mean reward: 14.04
               Mean episode length: 797.26
Episode_Reward/track_lin_vel_xy_exp: 0.7754
Episode_Reward/track_ang_vel_z_exp: 0.3867
       Episode_Reward/lin_vel_z_l2: -0.0458
      Episode_Reward/ang_vel_xy_l2: -0.1130
     Episode_Reward/dof_torques_l2: -0.0750
         Episode_Reward/dof_acc_l2: -0.1700
     Episode_Reward/action_rate_l2: -0.1419
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4263
Metrics/base_velocity/error_vel_xy: 0.4990
Metrics/base_velocity/error_vel_yaw: 0.4841
      Episode_Termination/time_out: 1.0417
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 19368000
                    Iteration time: 1.15s
                      Time elapsed: 00:09:54
                               ETA: 00:17:44

################################################################################
                     [1m Learning iteration 538/1500 [0m                      

                       Computation: 31034 steps/s (collection: 1.104s, learning 0.056s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0241
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 13.3474
                       Mean reward: 15.26
               Mean episode length: 825.92
Episode_Reward/track_lin_vel_xy_exp: 1.0343
Episode_Reward/track_ang_vel_z_exp: 0.4585
       Episode_Reward/lin_vel_z_l2: -0.0556
      Episode_Reward/ang_vel_xy_l2: -0.1296
     Episode_Reward/dof_torques_l2: -0.0822
         Episode_Reward/dof_acc_l2: -0.2141
     Episode_Reward/action_rate_l2: -0.1665
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4350
Metrics/base_velocity/error_vel_xy: 0.4025
Metrics/base_velocity/error_vel_yaw: 0.5119
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 19404000
                    Iteration time: 1.16s
                      Time elapsed: 00:09:56
                               ETA: 00:17:43

################################################################################
                     [1m Learning iteration 539/1500 [0m                      

                       Computation: 32128 steps/s (collection: 1.069s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0269
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 13.3428
                       Mean reward: 16.54
               Mean episode length: 865.03
Episode_Reward/track_lin_vel_xy_exp: 1.0516
Episode_Reward/track_ang_vel_z_exp: 0.4788
       Episode_Reward/lin_vel_z_l2: -0.0520
      Episode_Reward/ang_vel_xy_l2: -0.1325
     Episode_Reward/dof_torques_l2: -0.0855
         Episode_Reward/dof_acc_l2: -0.2005
     Episode_Reward/action_rate_l2: -0.1719
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4437
Metrics/base_velocity/error_vel_xy: 0.4486
Metrics/base_velocity/error_vel_yaw: 0.5438
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 19440000
                    Iteration time: 1.12s
                      Time elapsed: 00:09:57
                               ETA: 00:17:42

################################################################################
                     [1m Learning iteration 540/1500 [0m                      

                       Computation: 32957 steps/s (collection: 1.038s, learning 0.054s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0261
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 13.3496
                       Mean reward: 15.17
               Mean episode length: 809.20
Episode_Reward/track_lin_vel_xy_exp: 0.7403
Episode_Reward/track_ang_vel_z_exp: 0.3465
       Episode_Reward/lin_vel_z_l2: -0.0538
      Episode_Reward/ang_vel_xy_l2: -0.1124
     Episode_Reward/dof_torques_l2: -0.0646
         Episode_Reward/dof_acc_l2: -0.1620
     Episode_Reward/action_rate_l2: -0.1308
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4518
Metrics/base_velocity/error_vel_xy: 0.3919
Metrics/base_velocity/error_vel_yaw: 0.4562
      Episode_Termination/time_out: 1.0417
  Episode_Termination/base_contact: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 19476000
                    Iteration time: 1.09s
                      Time elapsed: 00:09:58
                               ETA: 00:17:41

################################################################################
                     [1m Learning iteration 541/1500 [0m                      

                       Computation: 32986 steps/s (collection: 1.040s, learning 0.051s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0283
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 13.3639
                       Mean reward: 14.97
               Mean episode length: 811.71
Episode_Reward/track_lin_vel_xy_exp: 0.8264
Episode_Reward/track_ang_vel_z_exp: 0.3952
       Episode_Reward/lin_vel_z_l2: -0.0456
      Episode_Reward/ang_vel_xy_l2: -0.1144
     Episode_Reward/dof_torques_l2: -0.0748
         Episode_Reward/dof_acc_l2: -0.1762
     Episode_Reward/action_rate_l2: -0.1452
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4628
Metrics/base_velocity/error_vel_xy: 0.4409
Metrics/base_velocity/error_vel_yaw: 0.4623
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 19512000
                    Iteration time: 1.09s
                      Time elapsed: 00:09:59
                               ETA: 00:17:40

################################################################################
                     [1m Learning iteration 542/1500 [0m                      

                       Computation: 33147 steps/s (collection: 1.035s, learning 0.051s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0268
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 13.3775
                       Mean reward: 15.02
               Mean episode length: 824.73
Episode_Reward/track_lin_vel_xy_exp: 0.8550
Episode_Reward/track_ang_vel_z_exp: 0.4021
       Episode_Reward/lin_vel_z_l2: -0.0470
      Episode_Reward/ang_vel_xy_l2: -0.1146
     Episode_Reward/dof_torques_l2: -0.0752
         Episode_Reward/dof_acc_l2: -0.1828
     Episode_Reward/action_rate_l2: -0.1501
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4700
Metrics/base_velocity/error_vel_xy: 0.4494
Metrics/base_velocity/error_vel_yaw: 0.4825
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 19548000
                    Iteration time: 1.09s
                      Time elapsed: 00:10:00
                               ETA: 00:17:39

################################################################################
                     [1m Learning iteration 543/1500 [0m                      

                       Computation: 32802 steps/s (collection: 1.048s, learning 0.050s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0300
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 13.3796
                       Mean reward: 14.60
               Mean episode length: 836.20
Episode_Reward/track_lin_vel_xy_exp: 0.8178
Episode_Reward/track_ang_vel_z_exp: 0.3739
       Episode_Reward/lin_vel_z_l2: -0.0428
      Episode_Reward/ang_vel_xy_l2: -0.1081
     Episode_Reward/dof_torques_l2: -0.0692
         Episode_Reward/dof_acc_l2: -0.1713
     Episode_Reward/action_rate_l2: -0.1377
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4753
Metrics/base_velocity/error_vel_xy: 0.3714
Metrics/base_velocity/error_vel_yaw: 0.4338
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 19584000
                    Iteration time: 1.10s
                      Time elapsed: 00:10:01
                               ETA: 00:17:38

################################################################################
                     [1m Learning iteration 544/1500 [0m                      

                       Computation: 33610 steps/s (collection: 1.019s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0252
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 13.3783
                       Mean reward: 14.19
               Mean episode length: 816.23
Episode_Reward/track_lin_vel_xy_exp: 0.8520
Episode_Reward/track_ang_vel_z_exp: 0.4171
       Episode_Reward/lin_vel_z_l2: -0.0484
      Episode_Reward/ang_vel_xy_l2: -0.1216
     Episode_Reward/dof_torques_l2: -0.0828
         Episode_Reward/dof_acc_l2: -0.1850
     Episode_Reward/action_rate_l2: -0.1573
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4849
Metrics/base_velocity/error_vel_xy: 0.5081
Metrics/base_velocity/error_vel_yaw: 0.5222
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 19620000
                    Iteration time: 1.07s
                      Time elapsed: 00:10:02
                               ETA: 00:17:37

################################################################################
                     [1m Learning iteration 545/1500 [0m                      

                       Computation: 33136 steps/s (collection: 1.033s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0260
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 13.3817
                       Mean reward: 15.04
               Mean episode length: 833.59
Episode_Reward/track_lin_vel_xy_exp: 0.9723
Episode_Reward/track_ang_vel_z_exp: 0.4588
       Episode_Reward/lin_vel_z_l2: -0.0489
      Episode_Reward/ang_vel_xy_l2: -0.1286
     Episode_Reward/dof_torques_l2: -0.0826
         Episode_Reward/dof_acc_l2: -0.1848
     Episode_Reward/action_rate_l2: -0.1632
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.4956
Metrics/base_velocity/error_vel_xy: 0.4783
Metrics/base_velocity/error_vel_yaw: 0.5195
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 19656000
                    Iteration time: 1.09s
                      Time elapsed: 00:10:03
                               ETA: 00:17:35

################################################################################
                     [1m Learning iteration 546/1500 [0m                      

                       Computation: 31756 steps/s (collection: 1.082s, learning 0.051s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0278
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 13.3903
                       Mean reward: 16.19
               Mean episode length: 895.84
Episode_Reward/track_lin_vel_xy_exp: 1.0737
Episode_Reward/track_ang_vel_z_exp: 0.5035
       Episode_Reward/lin_vel_z_l2: -0.0519
      Episode_Reward/ang_vel_xy_l2: -0.1397
     Episode_Reward/dof_torques_l2: -0.0936
         Episode_Reward/dof_acc_l2: -0.2045
     Episode_Reward/action_rate_l2: -0.1831
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5093
Metrics/base_velocity/error_vel_xy: 0.5141
Metrics/base_velocity/error_vel_yaw: 0.5739
      Episode_Termination/time_out: 2.0000
  Episode_Termination/base_contact: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 19692000
                    Iteration time: 1.13s
                      Time elapsed: 00:10:04
                               ETA: 00:17:34

################################################################################
                     [1m Learning iteration 547/1500 [0m                      

                       Computation: 32515 steps/s (collection: 1.056s, learning 0.051s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0248
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 13.4048
                       Mean reward: 16.88
               Mean episode length: 919.55
Episode_Reward/track_lin_vel_xy_exp: 1.0634
Episode_Reward/track_ang_vel_z_exp: 0.4773
       Episode_Reward/lin_vel_z_l2: -0.0529
      Episode_Reward/ang_vel_xy_l2: -0.1355
     Episode_Reward/dof_torques_l2: -0.0904
         Episode_Reward/dof_acc_l2: -0.2141
     Episode_Reward/action_rate_l2: -0.1769
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5188
Metrics/base_velocity/error_vel_xy: 0.4535
Metrics/base_velocity/error_vel_yaw: 0.5465
      Episode_Termination/time_out: 1.9583
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 19728000
                    Iteration time: 1.11s
                      Time elapsed: 00:10:05
                               ETA: 00:17:33

################################################################################
                     [1m Learning iteration 548/1500 [0m                      

                       Computation: 32194 steps/s (collection: 1.067s, learning 0.051s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0282
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 13.4159
                       Mean reward: 17.32
               Mean episode length: 889.08
Episode_Reward/track_lin_vel_xy_exp: 1.0006
Episode_Reward/track_ang_vel_z_exp: 0.4459
       Episode_Reward/lin_vel_z_l2: -0.0482
      Episode_Reward/ang_vel_xy_l2: -0.1212
     Episode_Reward/dof_torques_l2: -0.0824
         Episode_Reward/dof_acc_l2: -0.1786
     Episode_Reward/action_rate_l2: -0.1581
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5251
Metrics/base_velocity/error_vel_xy: 0.3743
Metrics/base_velocity/error_vel_yaw: 0.4712
      Episode_Termination/time_out: 1.9167
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 19764000
                    Iteration time: 1.12s
                      Time elapsed: 00:10:07
                               ETA: 00:17:32

################################################################################
                     [1m Learning iteration 549/1500 [0m                      

                       Computation: 30745 steps/s (collection: 1.120s, learning 0.050s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0249
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 13.4119
                       Mean reward: 16.84
               Mean episode length: 861.21
Episode_Reward/track_lin_vel_xy_exp: 1.0911
Episode_Reward/track_ang_vel_z_exp: 0.4935
       Episode_Reward/lin_vel_z_l2: -0.0579
      Episode_Reward/ang_vel_xy_l2: -0.1435
     Episode_Reward/dof_torques_l2: -0.0896
         Episode_Reward/dof_acc_l2: -0.2282
     Episode_Reward/action_rate_l2: -0.1817
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5359
Metrics/base_velocity/error_vel_xy: 0.4339
Metrics/base_velocity/error_vel_yaw: 0.5339
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 19800000
                    Iteration time: 1.17s
                      Time elapsed: 00:10:08
                               ETA: 00:17:31

################################################################################
                     [1m Learning iteration 550/1500 [0m                      

                       Computation: 32556 steps/s (collection: 1.054s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0311
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 13.4032
                       Mean reward: 16.88
               Mean episode length: 910.47
Episode_Reward/track_lin_vel_xy_exp: 0.9985
Episode_Reward/track_ang_vel_z_exp: 0.4987
       Episode_Reward/lin_vel_z_l2: -0.0585
      Episode_Reward/ang_vel_xy_l2: -0.1521
     Episode_Reward/dof_torques_l2: -0.0938
         Episode_Reward/dof_acc_l2: -0.2352
     Episode_Reward/action_rate_l2: -0.1886
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5483
Metrics/base_velocity/error_vel_xy: 0.6399
Metrics/base_velocity/error_vel_yaw: 0.6139
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 19836000
                    Iteration time: 1.11s
                      Time elapsed: 00:10:09
                               ETA: 00:17:30

################################################################################
                     [1m Learning iteration 551/1500 [0m                      

                       Computation: 32618 steps/s (collection: 1.051s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0308
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 13.4132
                       Mean reward: 16.22
               Mean episode length: 869.81
Episode_Reward/track_lin_vel_xy_exp: 1.0194
Episode_Reward/track_ang_vel_z_exp: 0.4527
       Episode_Reward/lin_vel_z_l2: -0.0428
      Episode_Reward/ang_vel_xy_l2: -0.1214
     Episode_Reward/dof_torques_l2: -0.0829
         Episode_Reward/dof_acc_l2: -0.1735
     Episode_Reward/action_rate_l2: -0.1578
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5569
Metrics/base_velocity/error_vel_xy: 0.3714
Metrics/base_velocity/error_vel_yaw: 0.4801
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 19872000
                    Iteration time: 1.10s
                      Time elapsed: 00:10:10
                               ETA: 00:17:29

################################################################################
                     [1m Learning iteration 552/1500 [0m                      

                       Computation: 32098 steps/s (collection: 1.070s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0256
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 13.4214
                       Mean reward: 16.09
               Mean episode length: 857.36
Episode_Reward/track_lin_vel_xy_exp: 0.9374
Episode_Reward/track_ang_vel_z_exp: 0.4394
       Episode_Reward/lin_vel_z_l2: -0.0486
      Episode_Reward/ang_vel_xy_l2: -0.1249
     Episode_Reward/dof_torques_l2: -0.0799
         Episode_Reward/dof_acc_l2: -0.1823
     Episode_Reward/action_rate_l2: -0.1579
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5630
Metrics/base_velocity/error_vel_xy: 0.4409
Metrics/base_velocity/error_vel_yaw: 0.4976
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 19908000
                    Iteration time: 1.12s
                      Time elapsed: 00:10:11
                               ETA: 00:17:28

################################################################################
                     [1m Learning iteration 553/1500 [0m                      

                       Computation: 32889 steps/s (collection: 1.042s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0329
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 13.4209
                       Mean reward: 15.90
               Mean episode length: 843.20
Episode_Reward/track_lin_vel_xy_exp: 0.9660
Episode_Reward/track_ang_vel_z_exp: 0.4423
       Episode_Reward/lin_vel_z_l2: -0.0515
      Episode_Reward/ang_vel_xy_l2: -0.1303
     Episode_Reward/dof_torques_l2: -0.0846
         Episode_Reward/dof_acc_l2: -0.1980
     Episode_Reward/action_rate_l2: -0.1642
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5688
Metrics/base_velocity/error_vel_xy: 0.4397
Metrics/base_velocity/error_vel_yaw: 0.5093
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 19944000
                    Iteration time: 1.09s
                      Time elapsed: 00:10:12
                               ETA: 00:17:27

################################################################################
                     [1m Learning iteration 554/1500 [0m                      

                       Computation: 32492 steps/s (collection: 1.053s, learning 0.055s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0271
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 13.4236
                       Mean reward: 15.84
               Mean episode length: 840.93
Episode_Reward/track_lin_vel_xy_exp: 0.9995
Episode_Reward/track_ang_vel_z_exp: 0.4572
       Episode_Reward/lin_vel_z_l2: -0.0490
      Episode_Reward/ang_vel_xy_l2: -0.1299
     Episode_Reward/dof_torques_l2: -0.0863
         Episode_Reward/dof_acc_l2: -0.1876
     Episode_Reward/action_rate_l2: -0.1644
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5766
Metrics/base_velocity/error_vel_xy: 0.4497
Metrics/base_velocity/error_vel_yaw: 0.5195
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 19980000
                    Iteration time: 1.11s
                      Time elapsed: 00:10:13
                               ETA: 00:17:26

################################################################################
                     [1m Learning iteration 555/1500 [0m                      

                       Computation: 32293 steps/s (collection: 1.062s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0252
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 13.4318
                       Mean reward: 16.26
               Mean episode length: 858.08
Episode_Reward/track_lin_vel_xy_exp: 1.0101
Episode_Reward/track_ang_vel_z_exp: 0.4721
       Episode_Reward/lin_vel_z_l2: -0.0486
      Episode_Reward/ang_vel_xy_l2: -0.1323
     Episode_Reward/dof_torques_l2: -0.0883
         Episode_Reward/dof_acc_l2: -0.2034
     Episode_Reward/action_rate_l2: -0.1710
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5779
Metrics/base_velocity/error_vel_xy: 0.4983
Metrics/base_velocity/error_vel_yaw: 0.5514
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 20016000
                    Iteration time: 1.11s
                      Time elapsed: 00:10:14
                               ETA: 00:17:25

################################################################################
                     [1m Learning iteration 556/1500 [0m                      

                       Computation: 31571 steps/s (collection: 1.087s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0275
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 13.4343
                       Mean reward: 16.00
               Mean episode length: 856.29
Episode_Reward/track_lin_vel_xy_exp: 0.9757
Episode_Reward/track_ang_vel_z_exp: 0.4466
       Episode_Reward/lin_vel_z_l2: -0.0515
      Episode_Reward/ang_vel_xy_l2: -0.1336
     Episode_Reward/dof_torques_l2: -0.0837
         Episode_Reward/dof_acc_l2: -0.2030
     Episode_Reward/action_rate_l2: -0.1676
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5893
Metrics/base_velocity/error_vel_xy: 0.4627
Metrics/base_velocity/error_vel_yaw: 0.5364
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 20052000
                    Iteration time: 1.14s
                      Time elapsed: 00:10:16
                               ETA: 00:17:24

################################################################################
                     [1m Learning iteration 557/1500 [0m                      

                       Computation: 32493 steps/s (collection: 1.055s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0242
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 13.4355
                       Mean reward: 15.50
               Mean episode length: 819.50
Episode_Reward/track_lin_vel_xy_exp: 0.9347
Episode_Reward/track_ang_vel_z_exp: 0.4191
       Episode_Reward/lin_vel_z_l2: -0.0461
      Episode_Reward/ang_vel_xy_l2: -0.1200
     Episode_Reward/dof_torques_l2: -0.0791
         Episode_Reward/dof_acc_l2: -0.1848
     Episode_Reward/action_rate_l2: -0.1536
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5987
Metrics/base_velocity/error_vel_xy: 0.3948
Metrics/base_velocity/error_vel_yaw: 0.4870
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 20088000
                    Iteration time: 1.11s
                      Time elapsed: 00:10:17
                               ETA: 00:17:22

################################################################################
                     [1m Learning iteration 558/1500 [0m                      

                       Computation: 33277 steps/s (collection: 1.028s, learning 0.054s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0255
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 13.4395
                       Mean reward: 16.06
               Mean episode length: 857.34
Episode_Reward/track_lin_vel_xy_exp: 1.0128
Episode_Reward/track_ang_vel_z_exp: 0.4668
       Episode_Reward/lin_vel_z_l2: -0.0492
      Episode_Reward/ang_vel_xy_l2: -0.1337
     Episode_Reward/dof_torques_l2: -0.0883
         Episode_Reward/dof_acc_l2: -0.2075
     Episode_Reward/action_rate_l2: -0.1736
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.5923
Metrics/base_velocity/error_vel_xy: 0.4790
Metrics/base_velocity/error_vel_yaw: 0.5396
      Episode_Termination/time_out: 2.0833
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 20124000
                    Iteration time: 1.08s
                      Time elapsed: 00:10:18
                               ETA: 00:17:21

################################################################################
                     [1m Learning iteration 559/1500 [0m                      

                       Computation: 33869 steps/s (collection: 1.011s, learning 0.051s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0266
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 13.4499
                       Mean reward: 15.97
               Mean episode length: 879.00
Episode_Reward/track_lin_vel_xy_exp: 1.0556
Episode_Reward/track_ang_vel_z_exp: 0.4755
       Episode_Reward/lin_vel_z_l2: -0.0514
      Episode_Reward/ang_vel_xy_l2: -0.1331
     Episode_Reward/dof_torques_l2: -0.0857
         Episode_Reward/dof_acc_l2: -0.2020
     Episode_Reward/action_rate_l2: -0.1706
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6039
Metrics/base_velocity/error_vel_xy: 0.4209
Metrics/base_velocity/error_vel_yaw: 0.5179
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 20160000
                    Iteration time: 1.06s
                      Time elapsed: 00:10:19
                               ETA: 00:17:20

################################################################################
                     [1m Learning iteration 560/1500 [0m                      

                       Computation: 32182 steps/s (collection: 1.065s, learning 0.054s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0255
               Mean surrogate loss: -0.0095
                 Mean entropy loss: 13.4661
                       Mean reward: 14.66
               Mean episode length: 827.35
Episode_Reward/track_lin_vel_xy_exp: 0.8494
Episode_Reward/track_ang_vel_z_exp: 0.3804
       Episode_Reward/lin_vel_z_l2: -0.0452
      Episode_Reward/ang_vel_xy_l2: -0.1133
     Episode_Reward/dof_torques_l2: -0.0743
         Episode_Reward/dof_acc_l2: -0.1807
     Episode_Reward/action_rate_l2: -0.1457
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6150
Metrics/base_velocity/error_vel_xy: 0.3925
Metrics/base_velocity/error_vel_yaw: 0.4826
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 20196000
                    Iteration time: 1.12s
                      Time elapsed: 00:10:20
                               ETA: 00:17:19

################################################################################
                     [1m Learning iteration 561/1500 [0m                      

                       Computation: 32149 steps/s (collection: 1.067s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0240
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 13.4753
                       Mean reward: 14.94
               Mean episode length: 822.44
Episode_Reward/track_lin_vel_xy_exp: 0.9323
Episode_Reward/track_ang_vel_z_exp: 0.4209
       Episode_Reward/lin_vel_z_l2: -0.0535
      Episode_Reward/ang_vel_xy_l2: -0.1225
     Episode_Reward/dof_torques_l2: -0.0789
         Episode_Reward/dof_acc_l2: -0.2008
     Episode_Reward/action_rate_l2: -0.1591
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6247
Metrics/base_velocity/error_vel_xy: 0.3907
Metrics/base_velocity/error_vel_yaw: 0.4679
      Episode_Termination/time_out: 1.1667
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 20232000
                    Iteration time: 1.12s
                      Time elapsed: 00:10:21
                               ETA: 00:17:18

################################################################################
                     [1m Learning iteration 562/1500 [0m                      

                       Computation: 32187 steps/s (collection: 1.067s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0256
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 13.4828
                       Mean reward: 16.03
               Mean episode length: 867.31
Episode_Reward/track_lin_vel_xy_exp: 1.0190
Episode_Reward/track_ang_vel_z_exp: 0.4520
       Episode_Reward/lin_vel_z_l2: -0.0531
      Episode_Reward/ang_vel_xy_l2: -0.1339
     Episode_Reward/dof_torques_l2: -0.0855
         Episode_Reward/dof_acc_l2: -0.1984
     Episode_Reward/action_rate_l2: -0.1683
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6242
Metrics/base_velocity/error_vel_xy: 0.4169
Metrics/base_velocity/error_vel_yaw: 0.5276
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 20268000
                    Iteration time: 1.12s
                      Time elapsed: 00:10:22
                               ETA: 00:17:17

################################################################################
                     [1m Learning iteration 563/1500 [0m                      

                       Computation: 30943 steps/s (collection: 1.112s, learning 0.051s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0231
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 13.4846
                       Mean reward: 15.63
               Mean episode length: 838.87
Episode_Reward/track_lin_vel_xy_exp: 0.9045
Episode_Reward/track_ang_vel_z_exp: 0.4210
       Episode_Reward/lin_vel_z_l2: -0.0497
      Episode_Reward/ang_vel_xy_l2: -0.1228
     Episode_Reward/dof_torques_l2: -0.0791
         Episode_Reward/dof_acc_l2: -0.1852
     Episode_Reward/action_rate_l2: -0.1561
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6337
Metrics/base_velocity/error_vel_xy: 0.4282
Metrics/base_velocity/error_vel_yaw: 0.4736
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 20304000
                    Iteration time: 1.16s
                      Time elapsed: 00:10:23
                               ETA: 00:17:16

################################################################################
                     [1m Learning iteration 564/1500 [0m                      

                       Computation: 29471 steps/s (collection: 1.169s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0252
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 13.4902
                       Mean reward: 15.60
               Mean episode length: 828.60
Episode_Reward/track_lin_vel_xy_exp: 1.0253
Episode_Reward/track_ang_vel_z_exp: 0.4634
       Episode_Reward/lin_vel_z_l2: -0.0516
      Episode_Reward/ang_vel_xy_l2: -0.1287
     Episode_Reward/dof_torques_l2: -0.0861
         Episode_Reward/dof_acc_l2: -0.1926
     Episode_Reward/action_rate_l2: -0.1678
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6444
Metrics/base_velocity/error_vel_xy: 0.4129
Metrics/base_velocity/error_vel_yaw: 0.5110
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 20340000
                    Iteration time: 1.22s
                      Time elapsed: 00:10:25
                               ETA: 00:17:15

################################################################################
                     [1m Learning iteration 565/1500 [0m                      

                       Computation: 31579 steps/s (collection: 1.083s, learning 0.057s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0301
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 13.4895
                       Mean reward: 16.36
               Mean episode length: 840.52
Episode_Reward/track_lin_vel_xy_exp: 1.0029
Episode_Reward/track_ang_vel_z_exp: 0.4521
       Episode_Reward/lin_vel_z_l2: -0.0466
      Episode_Reward/ang_vel_xy_l2: -0.1246
     Episode_Reward/dof_torques_l2: -0.0857
         Episode_Reward/dof_acc_l2: -0.1831
     Episode_Reward/action_rate_l2: -0.1621
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6534
Metrics/base_velocity/error_vel_xy: 0.3886
Metrics/base_velocity/error_vel_yaw: 0.4893
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 20376000
                    Iteration time: 1.14s
                      Time elapsed: 00:10:26
                               ETA: 00:17:14

################################################################################
                     [1m Learning iteration 566/1500 [0m                      

                       Computation: 32197 steps/s (collection: 1.066s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0261
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 13.4879
                       Mean reward: 18.02
               Mean episode length: 885.94
Episode_Reward/track_lin_vel_xy_exp: 1.1518
Episode_Reward/track_ang_vel_z_exp: 0.5261
       Episode_Reward/lin_vel_z_l2: -0.0509
      Episode_Reward/ang_vel_xy_l2: -0.1414
     Episode_Reward/dof_torques_l2: -0.0933
         Episode_Reward/dof_acc_l2: -0.2096
     Episode_Reward/action_rate_l2: -0.1854
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6605
Metrics/base_velocity/error_vel_xy: 0.4468
Metrics/base_velocity/error_vel_yaw: 0.5242
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 20412000
                    Iteration time: 1.12s
                      Time elapsed: 00:10:27
                               ETA: 00:17:13

################################################################################
                     [1m Learning iteration 567/1500 [0m                      

                       Computation: 33079 steps/s (collection: 1.037s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0249
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 13.4776
                       Mean reward: 17.48
               Mean episode length: 885.44
Episode_Reward/track_lin_vel_xy_exp: 0.9758
Episode_Reward/track_ang_vel_z_exp: 0.4585
       Episode_Reward/lin_vel_z_l2: -0.0448
      Episode_Reward/ang_vel_xy_l2: -0.1296
     Episode_Reward/dof_torques_l2: -0.0887
         Episode_Reward/dof_acc_l2: -0.1913
     Episode_Reward/action_rate_l2: -0.1681
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6639
Metrics/base_velocity/error_vel_xy: 0.4716
Metrics/base_velocity/error_vel_yaw: 0.5241
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 20448000
                    Iteration time: 1.09s
                      Time elapsed: 00:10:28
                               ETA: 00:17:12

################################################################################
                     [1m Learning iteration 568/1500 [0m                      

                       Computation: 32408 steps/s (collection: 1.058s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0263
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 13.4706
                       Mean reward: 16.50
               Mean episode length: 859.14
Episode_Reward/track_lin_vel_xy_exp: 0.9753
Episode_Reward/track_ang_vel_z_exp: 0.4400
       Episode_Reward/lin_vel_z_l2: -0.0493
      Episode_Reward/ang_vel_xy_l2: -0.1289
     Episode_Reward/dof_torques_l2: -0.0794
         Episode_Reward/dof_acc_l2: -0.1958
     Episode_Reward/action_rate_l2: -0.1629
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6716
Metrics/base_velocity/error_vel_xy: 0.4055
Metrics/base_velocity/error_vel_yaw: 0.5044
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 20484000
                    Iteration time: 1.11s
                      Time elapsed: 00:10:29
                               ETA: 00:17:11

################################################################################
                     [1m Learning iteration 569/1500 [0m                      

                       Computation: 32291 steps/s (collection: 1.061s, learning 0.054s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0246
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 13.4692
                       Mean reward: 15.70
               Mean episode length: 864.87
Episode_Reward/track_lin_vel_xy_exp: 1.0875
Episode_Reward/track_ang_vel_z_exp: 0.5024
       Episode_Reward/lin_vel_z_l2: -0.0589
      Episode_Reward/ang_vel_xy_l2: -0.1425
     Episode_Reward/dof_torques_l2: -0.0978
         Episode_Reward/dof_acc_l2: -0.2205
     Episode_Reward/action_rate_l2: -0.1869
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6789
Metrics/base_velocity/error_vel_xy: 0.4792
Metrics/base_velocity/error_vel_yaw: 0.5469
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 20520000
                    Iteration time: 1.11s
                      Time elapsed: 00:10:30
                               ETA: 00:17:09

################################################################################
                     [1m Learning iteration 570/1500 [0m                      

                       Computation: 32232 steps/s (collection: 1.066s, learning 0.051s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0288
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 13.4762
                       Mean reward: 16.40
               Mean episode length: 882.06
Episode_Reward/track_lin_vel_xy_exp: 0.9859
Episode_Reward/track_ang_vel_z_exp: 0.4665
       Episode_Reward/lin_vel_z_l2: -0.0464
      Episode_Reward/ang_vel_xy_l2: -0.1335
     Episode_Reward/dof_torques_l2: -0.0850
         Episode_Reward/dof_acc_l2: -0.2014
     Episode_Reward/action_rate_l2: -0.1695
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6829
Metrics/base_velocity/error_vel_xy: 0.4764
Metrics/base_velocity/error_vel_yaw: 0.5194
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 20556000
                    Iteration time: 1.12s
                      Time elapsed: 00:10:31
                               ETA: 00:17:08

################################################################################
                     [1m Learning iteration 571/1500 [0m                      

                       Computation: 32894 steps/s (collection: 1.042s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0247
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 13.4815
                       Mean reward: 16.97
               Mean episode length: 877.81
Episode_Reward/track_lin_vel_xy_exp: 1.0092
Episode_Reward/track_ang_vel_z_exp: 0.4654
       Episode_Reward/lin_vel_z_l2: -0.0497
      Episode_Reward/ang_vel_xy_l2: -0.1304
     Episode_Reward/dof_torques_l2: -0.0862
         Episode_Reward/dof_acc_l2: -0.1920
     Episode_Reward/action_rate_l2: -0.1683
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.6903
Metrics/base_velocity/error_vel_xy: 0.4569
Metrics/base_velocity/error_vel_yaw: 0.5190
      Episode_Termination/time_out: 1.9583
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 20592000
                    Iteration time: 1.09s
                      Time elapsed: 00:10:32
                               ETA: 00:17:07

################################################################################
                     [1m Learning iteration 572/1500 [0m                      

                       Computation: 33017 steps/s (collection: 1.037s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0260
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 13.4714
                       Mean reward: 16.49
               Mean episode length: 871.07
Episode_Reward/track_lin_vel_xy_exp: 1.0714
Episode_Reward/track_ang_vel_z_exp: 0.4834
       Episode_Reward/lin_vel_z_l2: -0.0586
      Episode_Reward/ang_vel_xy_l2: -0.1386
     Episode_Reward/dof_torques_l2: -0.0907
         Episode_Reward/dof_acc_l2: -0.2210
     Episode_Reward/action_rate_l2: -0.1812
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7023
Metrics/base_velocity/error_vel_xy: 0.4383
Metrics/base_velocity/error_vel_yaw: 0.5372
      Episode_Termination/time_out: 1.9583
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 20628000
                    Iteration time: 1.09s
                      Time elapsed: 00:10:33
                               ETA: 00:17:06

################################################################################
                     [1m Learning iteration 573/1500 [0m                      

                       Computation: 33163 steps/s (collection: 1.034s, learning 0.051s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0259
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 13.4662
                       Mean reward: 16.91
               Mean episode length: 872.06
Episode_Reward/track_lin_vel_xy_exp: 1.0398
Episode_Reward/track_ang_vel_z_exp: 0.4749
       Episode_Reward/lin_vel_z_l2: -0.0513
      Episode_Reward/ang_vel_xy_l2: -0.1299
     Episode_Reward/dof_torques_l2: -0.0907
         Episode_Reward/dof_acc_l2: -0.2057
     Episode_Reward/action_rate_l2: -0.1745
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7065
Metrics/base_velocity/error_vel_xy: 0.4385
Metrics/base_velocity/error_vel_yaw: 0.5083
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 20664000
                    Iteration time: 1.09s
                      Time elapsed: 00:10:34
                               ETA: 00:17:05

################################################################################
                     [1m Learning iteration 574/1500 [0m                      

                       Computation: 31934 steps/s (collection: 1.077s, learning 0.051s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0283
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 13.4661
                       Mean reward: 17.54
               Mean episode length: 888.65
Episode_Reward/track_lin_vel_xy_exp: 1.0046
Episode_Reward/track_ang_vel_z_exp: 0.4562
       Episode_Reward/lin_vel_z_l2: -0.0507
      Episode_Reward/ang_vel_xy_l2: -0.1301
     Episode_Reward/dof_torques_l2: -0.0841
         Episode_Reward/dof_acc_l2: -0.1939
     Episode_Reward/action_rate_l2: -0.1669
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7245
Metrics/base_velocity/error_vel_xy: 0.4297
Metrics/base_velocity/error_vel_yaw: 0.5095
      Episode_Termination/time_out: 2.0833
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 20700000
                    Iteration time: 1.13s
                      Time elapsed: 00:10:36
                               ETA: 00:17:04

################################################################################
                     [1m Learning iteration 575/1500 [0m                      

                       Computation: 32822 steps/s (collection: 1.045s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0277
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 13.4667
                       Mean reward: 17.52
               Mean episode length: 915.18
Episode_Reward/track_lin_vel_xy_exp: 0.9928
Episode_Reward/track_ang_vel_z_exp: 0.4651
       Episode_Reward/lin_vel_z_l2: -0.0488
      Episode_Reward/ang_vel_xy_l2: -0.1325
     Episode_Reward/dof_torques_l2: -0.0907
         Episode_Reward/dof_acc_l2: -0.2068
     Episode_Reward/action_rate_l2: -0.1727
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7399
Metrics/base_velocity/error_vel_xy: 0.4997
Metrics/base_velocity/error_vel_yaw: 0.5546
      Episode_Termination/time_out: 1.9583
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 20736000
                    Iteration time: 1.10s
                      Time elapsed: 00:10:37
                               ETA: 00:17:03

################################################################################
                     [1m Learning iteration 576/1500 [0m                      

                       Computation: 32916 steps/s (collection: 1.041s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0252
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 13.4780
                       Mean reward: 16.29
               Mean episode length: 882.24
Episode_Reward/track_lin_vel_xy_exp: 0.9441
Episode_Reward/track_ang_vel_z_exp: 0.4432
       Episode_Reward/lin_vel_z_l2: -0.0455
      Episode_Reward/ang_vel_xy_l2: -0.1233
     Episode_Reward/dof_torques_l2: -0.0864
         Episode_Reward/dof_acc_l2: -0.1791
     Episode_Reward/action_rate_l2: -0.1621
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7493
Metrics/base_velocity/error_vel_xy: 0.4479
Metrics/base_velocity/error_vel_yaw: 0.5051
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 20772000
                    Iteration time: 1.09s
                      Time elapsed: 00:10:38
                               ETA: 00:17:02

################################################################################
                     [1m Learning iteration 577/1500 [0m                      

                       Computation: 32915 steps/s (collection: 1.041s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0268
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 13.4885
                       Mean reward: 15.76
               Mean episode length: 856.65
Episode_Reward/track_lin_vel_xy_exp: 0.9891
Episode_Reward/track_ang_vel_z_exp: 0.4411
       Episode_Reward/lin_vel_z_l2: -0.0448
      Episode_Reward/ang_vel_xy_l2: -0.1217
     Episode_Reward/dof_torques_l2: -0.0868
         Episode_Reward/dof_acc_l2: -0.1807
     Episode_Reward/action_rate_l2: -0.1610
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7555
Metrics/base_velocity/error_vel_xy: 0.3925
Metrics/base_velocity/error_vel_yaw: 0.5086
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 20808000
                    Iteration time: 1.09s
                      Time elapsed: 00:10:39
                               ETA: 00:17:01

################################################################################
                     [1m Learning iteration 578/1500 [0m                      

                       Computation: 33701 steps/s (collection: 1.015s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0293
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 13.4896
                       Mean reward: 16.20
               Mean episode length: 837.63
Episode_Reward/track_lin_vel_xy_exp: 0.9889
Episode_Reward/track_ang_vel_z_exp: 0.4436
       Episode_Reward/lin_vel_z_l2: -0.0448
      Episode_Reward/ang_vel_xy_l2: -0.1253
     Episode_Reward/dof_torques_l2: -0.0819
         Episode_Reward/dof_acc_l2: -0.1882
     Episode_Reward/action_rate_l2: -0.1612
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7623
Metrics/base_velocity/error_vel_xy: 0.3934
Metrics/base_velocity/error_vel_yaw: 0.4899
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 20844000
                    Iteration time: 1.07s
                      Time elapsed: 00:10:40
                               ETA: 00:16:59

################################################################################
                     [1m Learning iteration 579/1500 [0m                      

                       Computation: 33882 steps/s (collection: 1.009s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0279
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 13.4845
                       Mean reward: 15.94
               Mean episode length: 842.25
Episode_Reward/track_lin_vel_xy_exp: 1.0195
Episode_Reward/track_ang_vel_z_exp: 0.4908
       Episode_Reward/lin_vel_z_l2: -0.0546
      Episode_Reward/ang_vel_xy_l2: -0.1423
     Episode_Reward/dof_torques_l2: -0.0958
         Episode_Reward/dof_acc_l2: -0.2237
     Episode_Reward/action_rate_l2: -0.1851
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7761
Metrics/base_velocity/error_vel_xy: 0.5483
Metrics/base_velocity/error_vel_yaw: 0.5575
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 20880000
                    Iteration time: 1.06s
                      Time elapsed: 00:10:41
                               ETA: 00:16:58

################################################################################
                     [1m Learning iteration 580/1500 [0m                      

                       Computation: 34164 steps/s (collection: 1.004s, learning 0.050s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0290
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 13.4703
                       Mean reward: 16.47
               Mean episode length: 873.15
Episode_Reward/track_lin_vel_xy_exp: 1.0066
Episode_Reward/track_ang_vel_z_exp: 0.4608
       Episode_Reward/lin_vel_z_l2: -0.0547
      Episode_Reward/ang_vel_xy_l2: -0.1355
     Episode_Reward/dof_torques_l2: -0.0838
         Episode_Reward/dof_acc_l2: -0.2009
     Episode_Reward/action_rate_l2: -0.1699
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7829
Metrics/base_velocity/error_vel_xy: 0.4550
Metrics/base_velocity/error_vel_yaw: 0.5390
      Episode_Termination/time_out: 1.1667
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 20916000
                    Iteration time: 1.05s
                      Time elapsed: 00:10:42
                               ETA: 00:16:57

################################################################################
                     [1m Learning iteration 581/1500 [0m                      

                       Computation: 32189 steps/s (collection: 1.067s, learning 0.051s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0235
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 13.4647
                       Mean reward: 16.41
               Mean episode length: 874.46
Episode_Reward/track_lin_vel_xy_exp: 0.9824
Episode_Reward/track_ang_vel_z_exp: 0.4414
       Episode_Reward/lin_vel_z_l2: -0.0528
      Episode_Reward/ang_vel_xy_l2: -0.1259
     Episode_Reward/dof_torques_l2: -0.0802
         Episode_Reward/dof_acc_l2: -0.2031
     Episode_Reward/action_rate_l2: -0.1659
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7868
Metrics/base_velocity/error_vel_xy: 0.3973
Metrics/base_velocity/error_vel_yaw: 0.4916
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 20952000
                    Iteration time: 1.12s
                      Time elapsed: 00:10:43
                               ETA: 00:16:56

################################################################################
                     [1m Learning iteration 582/1500 [0m                      

                       Computation: 32967 steps/s (collection: 1.040s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0234
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 13.4575
                       Mean reward: 15.93
               Mean episode length: 863.85
Episode_Reward/track_lin_vel_xy_exp: 1.0252
Episode_Reward/track_ang_vel_z_exp: 0.4694
       Episode_Reward/lin_vel_z_l2: -0.0545
      Episode_Reward/ang_vel_xy_l2: -0.1388
     Episode_Reward/dof_torques_l2: -0.0864
         Episode_Reward/dof_acc_l2: -0.2200
     Episode_Reward/action_rate_l2: -0.1764
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7941
Metrics/base_velocity/error_vel_xy: 0.4584
Metrics/base_velocity/error_vel_yaw: 0.5377
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 20988000
                    Iteration time: 1.09s
                      Time elapsed: 00:10:44
                               ETA: 00:16:55

################################################################################
                     [1m Learning iteration 583/1500 [0m                      

                       Computation: 32246 steps/s (collection: 1.065s, learning 0.051s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0249
               Mean surrogate loss: -0.0096
                 Mean entropy loss: 13.4621
                       Mean reward: 15.50
               Mean episode length: 872.46
Episode_Reward/track_lin_vel_xy_exp: 0.9322
Episode_Reward/track_ang_vel_z_exp: 0.4812
       Episode_Reward/lin_vel_z_l2: -0.0490
      Episode_Reward/ang_vel_xy_l2: -0.1414
     Episode_Reward/dof_torques_l2: -0.0900
         Episode_Reward/dof_acc_l2: -0.2148
     Episode_Reward/action_rate_l2: -0.1794
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7920
Metrics/base_velocity/error_vel_xy: 0.6504
Metrics/base_velocity/error_vel_yaw: 0.5800
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 21024000
                    Iteration time: 1.12s
                      Time elapsed: 00:10:45
                               ETA: 00:16:54

################################################################################
                     [1m Learning iteration 584/1500 [0m                      

                       Computation: 34215 steps/s (collection: 1.000s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0237
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 13.4636
                       Mean reward: 15.58
               Mean episode length: 901.91
Episode_Reward/track_lin_vel_xy_exp: 0.9138
Episode_Reward/track_ang_vel_z_exp: 0.4350
       Episode_Reward/lin_vel_z_l2: -0.0464
      Episode_Reward/ang_vel_xy_l2: -0.1298
     Episode_Reward/dof_torques_l2: -0.0807
         Episode_Reward/dof_acc_l2: -0.1959
     Episode_Reward/action_rate_l2: -0.1624
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7907
Metrics/base_velocity/error_vel_xy: 0.4948
Metrics/base_velocity/error_vel_yaw: 0.5127
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 21060000
                    Iteration time: 1.05s
                      Time elapsed: 00:10:46
                               ETA: 00:16:53

################################################################################
                     [1m Learning iteration 585/1500 [0m                      

                       Computation: 33873 steps/s (collection: 1.009s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0226
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 13.4622
                       Mean reward: 15.67
               Mean episode length: 885.44
Episode_Reward/track_lin_vel_xy_exp: 0.9825
Episode_Reward/track_ang_vel_z_exp: 0.4498
       Episode_Reward/lin_vel_z_l2: -0.0484
      Episode_Reward/ang_vel_xy_l2: -0.1294
     Episode_Reward/dof_torques_l2: -0.0802
         Episode_Reward/dof_acc_l2: -0.1919
     Episode_Reward/action_rate_l2: -0.1629
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.7967
Metrics/base_velocity/error_vel_xy: 0.4271
Metrics/base_velocity/error_vel_yaw: 0.4995
      Episode_Termination/time_out: 1.1250
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 21096000
                    Iteration time: 1.06s
                      Time elapsed: 00:10:48
                               ETA: 00:16:51

################################################################################
                     [1m Learning iteration 586/1500 [0m                      

                       Computation: 34084 steps/s (collection: 1.003s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0278
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 13.4582
                       Mean reward: 16.30
               Mean episode length: 882.19
Episode_Reward/track_lin_vel_xy_exp: 1.0303
Episode_Reward/track_ang_vel_z_exp: 0.4767
       Episode_Reward/lin_vel_z_l2: -0.0520
      Episode_Reward/ang_vel_xy_l2: -0.1395
     Episode_Reward/dof_torques_l2: -0.0919
         Episode_Reward/dof_acc_l2: -0.2106
     Episode_Reward/action_rate_l2: -0.1778
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8047
Metrics/base_velocity/error_vel_xy: 0.4999
Metrics/base_velocity/error_vel_yaw: 0.5675
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 21132000
                    Iteration time: 1.06s
                      Time elapsed: 00:10:49
                               ETA: 00:16:50

################################################################################
                     [1m Learning iteration 587/1500 [0m                      

                       Computation: 34222 steps/s (collection: 0.999s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0278
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 13.4520
                       Mean reward: 16.39
               Mean episode length: 895.71
Episode_Reward/track_lin_vel_xy_exp: 0.9605
Episode_Reward/track_ang_vel_z_exp: 0.4654
       Episode_Reward/lin_vel_z_l2: -0.0511
      Episode_Reward/ang_vel_xy_l2: -0.1353
     Episode_Reward/dof_torques_l2: -0.0876
         Episode_Reward/dof_acc_l2: -0.2046
     Episode_Reward/action_rate_l2: -0.1718
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8120
Metrics/base_velocity/error_vel_xy: 0.4960
Metrics/base_velocity/error_vel_yaw: 0.5061
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 21168000
                    Iteration time: 1.05s
                      Time elapsed: 00:10:50
                               ETA: 00:16:49

################################################################################
                     [1m Learning iteration 588/1500 [0m                      

                       Computation: 34297 steps/s (collection: 0.997s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0289
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 13.4399
                       Mean reward: 16.88
               Mean episode length: 915.70
Episode_Reward/track_lin_vel_xy_exp: 1.0257
Episode_Reward/track_ang_vel_z_exp: 0.4646
       Episode_Reward/lin_vel_z_l2: -0.0522
      Episode_Reward/ang_vel_xy_l2: -0.1361
     Episode_Reward/dof_torques_l2: -0.0876
         Episode_Reward/dof_acc_l2: -0.2100
     Episode_Reward/action_rate_l2: -0.1746
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8177
Metrics/base_velocity/error_vel_xy: 0.4498
Metrics/base_velocity/error_vel_yaw: 0.5305
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 21204000
                    Iteration time: 1.05s
                      Time elapsed: 00:10:51
                               ETA: 00:16:48

################################################################################
                     [1m Learning iteration 589/1500 [0m                      

                       Computation: 33800 steps/s (collection: 1.012s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0313
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 13.4420
                       Mean reward: 16.78
               Mean episode length: 926.20
Episode_Reward/track_lin_vel_xy_exp: 1.0391
Episode_Reward/track_ang_vel_z_exp: 0.5008
       Episode_Reward/lin_vel_z_l2: -0.0513
      Episode_Reward/ang_vel_xy_l2: -0.1409
     Episode_Reward/dof_torques_l2: -0.0981
         Episode_Reward/dof_acc_l2: -0.2068
     Episode_Reward/action_rate_l2: -0.1853
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8240
Metrics/base_velocity/error_vel_xy: 0.5763
Metrics/base_velocity/error_vel_yaw: 0.5679
      Episode_Termination/time_out: 1.9583
  Episode_Termination/base_contact: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 21240000
                    Iteration time: 1.07s
                      Time elapsed: 00:10:52
                               ETA: 00:16:47

################################################################################
                     [1m Learning iteration 590/1500 [0m                      

                       Computation: 34033 steps/s (collection: 1.005s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0245
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 13.4466
                       Mean reward: 16.26
               Mean episode length: 886.17
Episode_Reward/track_lin_vel_xy_exp: 0.9998
Episode_Reward/track_ang_vel_z_exp: 0.4576
       Episode_Reward/lin_vel_z_l2: -0.0518
      Episode_Reward/ang_vel_xy_l2: -0.1302
     Episode_Reward/dof_torques_l2: -0.0871
         Episode_Reward/dof_acc_l2: -0.1901
     Episode_Reward/action_rate_l2: -0.1696
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8281
Metrics/base_velocity/error_vel_xy: 0.4208
Metrics/base_velocity/error_vel_yaw: 0.5005
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 21276000
                    Iteration time: 1.06s
                      Time elapsed: 00:10:53
                               ETA: 00:16:45

################################################################################
                     [1m Learning iteration 591/1500 [0m                      

                       Computation: 33536 steps/s (collection: 1.021s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0256
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 13.4459
                       Mean reward: 16.02
               Mean episode length: 881.01
Episode_Reward/track_lin_vel_xy_exp: 1.0646
Episode_Reward/track_ang_vel_z_exp: 0.4830
       Episode_Reward/lin_vel_z_l2: -0.0519
      Episode_Reward/ang_vel_xy_l2: -0.1404
     Episode_Reward/dof_torques_l2: -0.0904
         Episode_Reward/dof_acc_l2: -0.2096
     Episode_Reward/action_rate_l2: -0.1811
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8246
Metrics/base_velocity/error_vel_xy: 0.4579
Metrics/base_velocity/error_vel_yaw: 0.5434
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 21312000
                    Iteration time: 1.07s
                      Time elapsed: 00:10:54
                               ETA: 00:16:44

################################################################################
                     [1m Learning iteration 592/1500 [0m                      

                       Computation: 33578 steps/s (collection: 1.020s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0281
               Mean surrogate loss: -0.0086
                 Mean entropy loss: 13.4420
                       Mean reward: 15.91
               Mean episode length: 877.51
Episode_Reward/track_lin_vel_xy_exp: 0.9689
Episode_Reward/track_ang_vel_z_exp: 0.4518
       Episode_Reward/lin_vel_z_l2: -0.0506
      Episode_Reward/ang_vel_xy_l2: -0.1345
     Episode_Reward/dof_torques_l2: -0.0889
         Episode_Reward/dof_acc_l2: -0.2068
     Episode_Reward/action_rate_l2: -0.1736
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8266
Metrics/base_velocity/error_vel_xy: 0.4800
Metrics/base_velocity/error_vel_yaw: 0.5339
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 21348000
                    Iteration time: 1.07s
                      Time elapsed: 00:10:55
                               ETA: 00:16:43

################################################################################
                     [1m Learning iteration 593/1500 [0m                      

                       Computation: 33874 steps/s (collection: 1.009s, learning 0.054s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0243
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 13.4431
                       Mean reward: 15.45
               Mean episode length: 872.93
Episode_Reward/track_lin_vel_xy_exp: 0.9251
Episode_Reward/track_ang_vel_z_exp: 0.4524
       Episode_Reward/lin_vel_z_l2: -0.0574
      Episode_Reward/ang_vel_xy_l2: -0.1398
     Episode_Reward/dof_torques_l2: -0.0860
         Episode_Reward/dof_acc_l2: -0.2210
     Episode_Reward/action_rate_l2: -0.1740
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8351
Metrics/base_velocity/error_vel_xy: 0.5344
Metrics/base_velocity/error_vel_yaw: 0.5394
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 21384000
                    Iteration time: 1.06s
                      Time elapsed: 00:10:56
                               ETA: 00:16:42

################################################################################
                     [1m Learning iteration 594/1500 [0m                      

                       Computation: 34024 steps/s (collection: 1.007s, learning 0.051s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0247
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 13.4479
                       Mean reward: 16.19
               Mean episode length: 877.51
Episode_Reward/track_lin_vel_xy_exp: 1.0276
Episode_Reward/track_ang_vel_z_exp: 0.4599
       Episode_Reward/lin_vel_z_l2: -0.0488
      Episode_Reward/ang_vel_xy_l2: -0.1318
     Episode_Reward/dof_torques_l2: -0.0844
         Episode_Reward/dof_acc_l2: -0.2033
     Episode_Reward/action_rate_l2: -0.1690
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8429
Metrics/base_velocity/error_vel_xy: 0.3943
Metrics/base_velocity/error_vel_yaw: 0.4912
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 21420000
                    Iteration time: 1.06s
                      Time elapsed: 00:10:57
                               ETA: 00:16:41

################################################################################
                     [1m Learning iteration 595/1500 [0m                      

                       Computation: 33213 steps/s (collection: 1.033s, learning 0.051s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0281
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 13.4501
                       Mean reward: 16.33
               Mean episode length: 860.52
Episode_Reward/track_lin_vel_xy_exp: 0.9802
Episode_Reward/track_ang_vel_z_exp: 0.4604
       Episode_Reward/lin_vel_z_l2: -0.0453
      Episode_Reward/ang_vel_xy_l2: -0.1295
     Episode_Reward/dof_torques_l2: -0.0891
         Episode_Reward/dof_acc_l2: -0.1924
     Episode_Reward/action_rate_l2: -0.1690
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8552
Metrics/base_velocity/error_vel_xy: 0.4776
Metrics/base_velocity/error_vel_yaw: 0.5171
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 21456000
                    Iteration time: 1.08s
                      Time elapsed: 00:10:58
                               ETA: 00:16:40

################################################################################
                     [1m Learning iteration 596/1500 [0m                      

                       Computation: 33908 steps/s (collection: 1.009s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0292
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 13.4529
                       Mean reward: 16.44
               Mean episode length: 850.40
Episode_Reward/track_lin_vel_xy_exp: 1.0064
Episode_Reward/track_ang_vel_z_exp: 0.4535
       Episode_Reward/lin_vel_z_l2: -0.0459
      Episode_Reward/ang_vel_xy_l2: -0.1257
     Episode_Reward/dof_torques_l2: -0.0867
         Episode_Reward/dof_acc_l2: -0.1807
     Episode_Reward/action_rate_l2: -0.1642
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8669
Metrics/base_velocity/error_vel_xy: 0.4047
Metrics/base_velocity/error_vel_yaw: 0.4882
      Episode_Termination/time_out: 1.9167
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 21492000
                    Iteration time: 1.06s
                      Time elapsed: 00:10:59
                               ETA: 00:16:38

################################################################################
                     [1m Learning iteration 597/1500 [0m                      

                       Computation: 33745 steps/s (collection: 1.013s, learning 0.054s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0221
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 13.4482
                       Mean reward: 15.77
               Mean episode length: 826.30
Episode_Reward/track_lin_vel_xy_exp: 0.9472
Episode_Reward/track_ang_vel_z_exp: 0.4446
       Episode_Reward/lin_vel_z_l2: -0.0444
      Episode_Reward/ang_vel_xy_l2: -0.1255
     Episode_Reward/dof_torques_l2: -0.0820
         Episode_Reward/dof_acc_l2: -0.1796
     Episode_Reward/action_rate_l2: -0.1609
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8706
Metrics/base_velocity/error_vel_xy: 0.4535
Metrics/base_velocity/error_vel_yaw: 0.5015
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 21528000
                    Iteration time: 1.07s
                      Time elapsed: 00:11:00
                               ETA: 00:16:37

################################################################################
                     [1m Learning iteration 598/1500 [0m                      

                       Computation: 33813 steps/s (collection: 1.012s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0271
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 13.4498
                       Mean reward: 15.02
               Mean episode length: 849.84
Episode_Reward/track_lin_vel_xy_exp: 0.9319
Episode_Reward/track_ang_vel_z_exp: 0.4635
       Episode_Reward/lin_vel_z_l2: -0.0529
      Episode_Reward/ang_vel_xy_l2: -0.1415
     Episode_Reward/dof_torques_l2: -0.0860
         Episode_Reward/dof_acc_l2: -0.2097
     Episode_Reward/action_rate_l2: -0.1748
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8726
Metrics/base_velocity/error_vel_xy: 0.5706
Metrics/base_velocity/error_vel_yaw: 0.5424
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 21564000
                    Iteration time: 1.06s
                      Time elapsed: 00:11:01
                               ETA: 00:16:36

################################################################################
                     [1m Learning iteration 599/1500 [0m                      

                       Computation: 34278 steps/s (collection: 0.998s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0303
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 13.4587
                       Mean reward: 15.86
               Mean episode length: 882.99
Episode_Reward/track_lin_vel_xy_exp: 1.0378
Episode_Reward/track_ang_vel_z_exp: 0.4818
       Episode_Reward/lin_vel_z_l2: -0.0471
      Episode_Reward/ang_vel_xy_l2: -0.1347
     Episode_Reward/dof_torques_l2: -0.0933
         Episode_Reward/dof_acc_l2: -0.1971
     Episode_Reward/action_rate_l2: -0.1778
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8851
Metrics/base_velocity/error_vel_xy: 0.4959
Metrics/base_velocity/error_vel_yaw: 0.5740
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 21600000
                    Iteration time: 1.05s
                      Time elapsed: 00:11:02
                               ETA: 00:16:35

################################################################################
                     [1m Learning iteration 600/1500 [0m                      

                       Computation: 33983 steps/s (collection: 1.008s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0272
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 13.4624
                       Mean reward: 16.77
               Mean episode length: 896.42
Episode_Reward/track_lin_vel_xy_exp: 1.0233
Episode_Reward/track_ang_vel_z_exp: 0.4731
       Episode_Reward/lin_vel_z_l2: -0.0501
      Episode_Reward/ang_vel_xy_l2: -0.1380
     Episode_Reward/dof_torques_l2: -0.0847
         Episode_Reward/dof_acc_l2: -0.2024
     Episode_Reward/action_rate_l2: -0.1726
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8919
Metrics/base_velocity/error_vel_xy: 0.4691
Metrics/base_velocity/error_vel_yaw: 0.5333
      Episode_Termination/time_out: 2.1667
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 21636000
                    Iteration time: 1.06s
                      Time elapsed: 00:11:03
                               ETA: 00:16:34

################################################################################
                     [1m Learning iteration 601/1500 [0m                      

                       Computation: 33891 steps/s (collection: 1.009s, learning 0.054s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0250
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 13.4593
                       Mean reward: 16.12
               Mean episode length: 894.48
Episode_Reward/track_lin_vel_xy_exp: 1.0032
Episode_Reward/track_ang_vel_z_exp: 0.4943
       Episode_Reward/lin_vel_z_l2: -0.0502
      Episode_Reward/ang_vel_xy_l2: -0.1443
     Episode_Reward/dof_torques_l2: -0.0925
         Episode_Reward/dof_acc_l2: -0.2118
     Episode_Reward/action_rate_l2: -0.1831
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8983
Metrics/base_velocity/error_vel_xy: 0.5720
Metrics/base_velocity/error_vel_yaw: 0.5665
      Episode_Termination/time_out: 2.0000
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 21672000
                    Iteration time: 1.06s
                      Time elapsed: 00:11:05
                               ETA: 00:16:33

################################################################################
                     [1m Learning iteration 602/1500 [0m                      

                       Computation: 33306 steps/s (collection: 1.028s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0251
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 13.4392
                       Mean reward: 16.00
               Mean episode length: 881.52
Episode_Reward/track_lin_vel_xy_exp: 0.9288
Episode_Reward/track_ang_vel_z_exp: 0.4679
       Episode_Reward/lin_vel_z_l2: -0.0442
      Episode_Reward/ang_vel_xy_l2: -0.1323
     Episode_Reward/dof_torques_l2: -0.0846
         Episode_Reward/dof_acc_l2: -0.1853
     Episode_Reward/action_rate_l2: -0.1678
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.8975
Metrics/base_velocity/error_vel_xy: 0.5551
Metrics/base_velocity/error_vel_yaw: 0.5106
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 21708000
                    Iteration time: 1.08s
                      Time elapsed: 00:11:06
                               ETA: 00:16:31

################################################################################
                     [1m Learning iteration 603/1500 [0m                      

                       Computation: 34275 steps/s (collection: 0.999s, learning 0.051s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0212
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 13.4384
                       Mean reward: 16.18
               Mean episode length: 833.40
Episode_Reward/track_lin_vel_xy_exp: 0.9091
Episode_Reward/track_ang_vel_z_exp: 0.4252
       Episode_Reward/lin_vel_z_l2: -0.0390
      Episode_Reward/ang_vel_xy_l2: -0.1163
     Episode_Reward/dof_torques_l2: -0.0781
         Episode_Reward/dof_acc_l2: -0.1596
     Episode_Reward/action_rate_l2: -0.1492
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9012
Metrics/base_velocity/error_vel_xy: 0.3972
Metrics/base_velocity/error_vel_yaw: 0.4454
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 21744000
                    Iteration time: 1.05s
                      Time elapsed: 00:11:07
                               ETA: 00:16:30

################################################################################
                     [1m Learning iteration 604/1500 [0m                      

                       Computation: 34232 steps/s (collection: 1.002s, learning 0.049s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0297
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 13.4319
                       Mean reward: 16.56
               Mean episode length: 845.19
Episode_Reward/track_lin_vel_xy_exp: 1.0154
Episode_Reward/track_ang_vel_z_exp: 0.4634
       Episode_Reward/lin_vel_z_l2: -0.0471
      Episode_Reward/ang_vel_xy_l2: -0.1290
     Episode_Reward/dof_torques_l2: -0.0835
         Episode_Reward/dof_acc_l2: -0.1911
     Episode_Reward/action_rate_l2: -0.1664
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9038
Metrics/base_velocity/error_vel_xy: 0.4381
Metrics/base_velocity/error_vel_yaw: 0.4877
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 21780000
                    Iteration time: 1.05s
                      Time elapsed: 00:11:08
                               ETA: 00:16:29

################################################################################
                     [1m Learning iteration 605/1500 [0m                      

                       Computation: 33940 steps/s (collection: 1.008s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0262
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 13.4383
                       Mean reward: 16.91
               Mean episode length: 879.00
Episode_Reward/track_lin_vel_xy_exp: 1.0404
Episode_Reward/track_ang_vel_z_exp: 0.4827
       Episode_Reward/lin_vel_z_l2: -0.0497
      Episode_Reward/ang_vel_xy_l2: -0.1347
     Episode_Reward/dof_torques_l2: -0.0905
         Episode_Reward/dof_acc_l2: -0.1886
     Episode_Reward/action_rate_l2: -0.1722
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9037
Metrics/base_velocity/error_vel_xy: 0.4529
Metrics/base_velocity/error_vel_yaw: 0.5145
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 21816000
                    Iteration time: 1.06s
                      Time elapsed: 00:11:09
                               ETA: 00:16:28

################################################################################
                     [1m Learning iteration 606/1500 [0m                      

                       Computation: 32932 steps/s (collection: 1.043s, learning 0.050s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0257
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 13.4457
                       Mean reward: 16.04
               Mean episode length: 838.54
Episode_Reward/track_lin_vel_xy_exp: 0.9384
Episode_Reward/track_ang_vel_z_exp: 0.4352
       Episode_Reward/lin_vel_z_l2: -0.0476
      Episode_Reward/ang_vel_xy_l2: -0.1265
     Episode_Reward/dof_torques_l2: -0.0815
         Episode_Reward/dof_acc_l2: -0.1805
     Episode_Reward/action_rate_l2: -0.1583
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9167
Metrics/base_velocity/error_vel_xy: 0.4130
Metrics/base_velocity/error_vel_yaw: 0.4660
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 21852000
                    Iteration time: 1.09s
                      Time elapsed: 00:11:10
                               ETA: 00:16:27

################################################################################
                     [1m Learning iteration 607/1500 [0m                      

                       Computation: 33446 steps/s (collection: 1.025s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0308
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 13.4547
                       Mean reward: 16.80
               Mean episode length: 856.62
Episode_Reward/track_lin_vel_xy_exp: 1.1228
Episode_Reward/track_ang_vel_z_exp: 0.5102
       Episode_Reward/lin_vel_z_l2: -0.0464
      Episode_Reward/ang_vel_xy_l2: -0.1350
     Episode_Reward/dof_torques_l2: -0.0941
         Episode_Reward/dof_acc_l2: -0.1898
     Episode_Reward/action_rate_l2: -0.1785
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9264
Metrics/base_velocity/error_vel_xy: 0.4137
Metrics/base_velocity/error_vel_yaw: 0.5016
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 21888000
                    Iteration time: 1.08s
                      Time elapsed: 00:11:11
                               ETA: 00:16:26

################################################################################
                     [1m Learning iteration 608/1500 [0m                      

                       Computation: 33603 steps/s (collection: 1.018s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0276
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 13.4587
                       Mean reward: 16.56
               Mean episode length: 860.23
Episode_Reward/track_lin_vel_xy_exp: 0.9476
Episode_Reward/track_ang_vel_z_exp: 0.4596
       Episode_Reward/lin_vel_z_l2: -0.0437
      Episode_Reward/ang_vel_xy_l2: -0.1274
     Episode_Reward/dof_torques_l2: -0.0870
         Episode_Reward/dof_acc_l2: -0.1819
     Episode_Reward/action_rate_l2: -0.1676
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9313
Metrics/base_velocity/error_vel_xy: 0.5051
Metrics/base_velocity/error_vel_yaw: 0.5081
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 21924000
                    Iteration time: 1.07s
                      Time elapsed: 00:11:12
                               ETA: 00:16:25

################################################################################
                     [1m Learning iteration 609/1500 [0m                      

                       Computation: 32277 steps/s (collection: 1.062s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0306
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 13.4567
                       Mean reward: 16.46
               Mean episode length: 891.05
Episode_Reward/track_lin_vel_xy_exp: 0.9620
Episode_Reward/track_ang_vel_z_exp: 0.4496
       Episode_Reward/lin_vel_z_l2: -0.0485
      Episode_Reward/ang_vel_xy_l2: -0.1320
     Episode_Reward/dof_torques_l2: -0.0780
         Episode_Reward/dof_acc_l2: -0.1990
     Episode_Reward/action_rate_l2: -0.1641
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9404
Metrics/base_velocity/error_vel_xy: 0.4532
Metrics/base_velocity/error_vel_yaw: 0.5050
      Episode_Termination/time_out: 2.1250
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 21960000
                    Iteration time: 1.12s
                      Time elapsed: 00:11:13
                               ETA: 00:16:23

################################################################################
                     [1m Learning iteration 610/1500 [0m                      

                       Computation: 32843 steps/s (collection: 1.045s, learning 0.051s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0272
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 13.4672
                       Mean reward: 16.02
               Mean episode length: 850.01
Episode_Reward/track_lin_vel_xy_exp: 0.9873
Episode_Reward/track_ang_vel_z_exp: 0.4542
       Episode_Reward/lin_vel_z_l2: -0.0444
      Episode_Reward/ang_vel_xy_l2: -0.1278
     Episode_Reward/dof_torques_l2: -0.0815
         Episode_Reward/dof_acc_l2: -0.1896
     Episode_Reward/action_rate_l2: -0.1651
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9539
Metrics/base_velocity/error_vel_xy: 0.4177
Metrics/base_velocity/error_vel_yaw: 0.4808
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 21996000
                    Iteration time: 1.10s
                      Time elapsed: 00:11:14
                               ETA: 00:16:22

################################################################################
                     [1m Learning iteration 611/1500 [0m                      

                       Computation: 32863 steps/s (collection: 1.043s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0213
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 13.4738
                       Mean reward: 15.76
               Mean episode length: 839.03
Episode_Reward/track_lin_vel_xy_exp: 0.8840
Episode_Reward/track_ang_vel_z_exp: 0.4030
       Episode_Reward/lin_vel_z_l2: -0.0414
      Episode_Reward/ang_vel_xy_l2: -0.1139
     Episode_Reward/dof_torques_l2: -0.0774
         Episode_Reward/dof_acc_l2: -0.1782
     Episode_Reward/action_rate_l2: -0.1496
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9622
Metrics/base_velocity/error_vel_xy: 0.3776
Metrics/base_velocity/error_vel_yaw: 0.4541
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 22032000
                    Iteration time: 1.10s
                      Time elapsed: 00:11:15
                               ETA: 00:16:21

################################################################################
                     [1m Learning iteration 612/1500 [0m                      

                       Computation: 30605 steps/s (collection: 1.122s, learning 0.054s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0245
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 13.4507
                       Mean reward: 16.50
               Mean episode length: 859.62
Episode_Reward/track_lin_vel_xy_exp: 0.9958
Episode_Reward/track_ang_vel_z_exp: 0.4556
       Episode_Reward/lin_vel_z_l2: -0.0510
      Episode_Reward/ang_vel_xy_l2: -0.1319
     Episode_Reward/dof_torques_l2: -0.0800
         Episode_Reward/dof_acc_l2: -0.1993
     Episode_Reward/action_rate_l2: -0.1673
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9649
Metrics/base_velocity/error_vel_xy: 0.4203
Metrics/base_velocity/error_vel_yaw: 0.5051
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 22068000
                    Iteration time: 1.18s
                      Time elapsed: 00:11:16
                               ETA: 00:16:20

################################################################################
                     [1m Learning iteration 613/1500 [0m                      

                       Computation: 31468 steps/s (collection: 1.091s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0265
               Mean surrogate loss: -0.0097
                 Mean entropy loss: 13.4278
                       Mean reward: 16.63
               Mean episode length: 881.14
Episode_Reward/track_lin_vel_xy_exp: 1.0299
Episode_Reward/track_ang_vel_z_exp: 0.4768
       Episode_Reward/lin_vel_z_l2: -0.0516
      Episode_Reward/ang_vel_xy_l2: -0.1356
     Episode_Reward/dof_torques_l2: -0.0900
         Episode_Reward/dof_acc_l2: -0.1979
     Episode_Reward/action_rate_l2: -0.1777
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9709
Metrics/base_velocity/error_vel_xy: 0.4607
Metrics/base_velocity/error_vel_yaw: 0.5273
      Episode_Termination/time_out: 2.0417
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 22104000
                    Iteration time: 1.14s
                      Time elapsed: 00:11:18
                               ETA: 00:16:19

################################################################################
                     [1m Learning iteration 614/1500 [0m                      

                       Computation: 33076 steps/s (collection: 1.035s, learning 0.054s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0285
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 13.4186
                       Mean reward: 16.52
               Mean episode length: 865.28
Episode_Reward/track_lin_vel_xy_exp: 0.9769
Episode_Reward/track_ang_vel_z_exp: 0.4609
       Episode_Reward/lin_vel_z_l2: -0.0492
      Episode_Reward/ang_vel_xy_l2: -0.1316
     Episode_Reward/dof_torques_l2: -0.0825
         Episode_Reward/dof_acc_l2: -0.1935
     Episode_Reward/action_rate_l2: -0.1690
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9756
Metrics/base_velocity/error_vel_xy: 0.4742
Metrics/base_velocity/error_vel_yaw: 0.5125
      Episode_Termination/time_out: 1.9167
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 22140000
                    Iteration time: 1.09s
                      Time elapsed: 00:11:19
                               ETA: 00:16:18

################################################################################
                     [1m Learning iteration 615/1500 [0m                      

                       Computation: 32033 steps/s (collection: 1.072s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0256
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 13.4114
                       Mean reward: 15.59
               Mean episode length: 837.78
Episode_Reward/track_lin_vel_xy_exp: 0.8676
Episode_Reward/track_ang_vel_z_exp: 0.4213
       Episode_Reward/lin_vel_z_l2: -0.0471
      Episode_Reward/ang_vel_xy_l2: -0.1222
     Episode_Reward/dof_torques_l2: -0.0809
         Episode_Reward/dof_acc_l2: -0.1817
     Episode_Reward/action_rate_l2: -0.1576
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9780
Metrics/base_velocity/error_vel_xy: 0.4719
Metrics/base_velocity/error_vel_yaw: 0.4553
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 22176000
                    Iteration time: 1.12s
                      Time elapsed: 00:11:20
                               ETA: 00:16:17

################################################################################
                     [1m Learning iteration 616/1500 [0m                      

                       Computation: 32453 steps/s (collection: 1.055s, learning 0.054s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0288
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 13.3978
                       Mean reward: 16.52
               Mean episode length: 863.09
Episode_Reward/track_lin_vel_xy_exp: 1.0473
Episode_Reward/track_ang_vel_z_exp: 0.4693
       Episode_Reward/lin_vel_z_l2: -0.0478
      Episode_Reward/ang_vel_xy_l2: -0.1322
     Episode_Reward/dof_torques_l2: -0.0872
         Episode_Reward/dof_acc_l2: -0.1866
     Episode_Reward/action_rate_l2: -0.1704
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 3.9885
Metrics/base_velocity/error_vel_xy: 0.4140
Metrics/base_velocity/error_vel_yaw: 0.5135
      Episode_Termination/time_out: 2.0417
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 22212000
                    Iteration time: 1.11s
                      Time elapsed: 00:11:21
                               ETA: 00:16:16

################################################################################
                     [1m Learning iteration 617/1500 [0m                      

                       Computation: 33522 steps/s (collection: 1.020s, learning 0.054s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0256
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 13.3893
                       Mean reward: 17.65
               Mean episode length: 913.67
Episode_Reward/track_lin_vel_xy_exp: 1.0580
Episode_Reward/track_ang_vel_z_exp: 0.5037
       Episode_Reward/lin_vel_z_l2: -0.0454
      Episode_Reward/ang_vel_xy_l2: -0.1401
     Episode_Reward/dof_torques_l2: -0.0951
         Episode_Reward/dof_acc_l2: -0.1983
     Episode_Reward/action_rate_l2: -0.1828
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0014
Metrics/base_velocity/error_vel_xy: 0.5265
Metrics/base_velocity/error_vel_yaw: 0.5631
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 22248000
                    Iteration time: 1.07s
                      Time elapsed: 00:11:22
                               ETA: 00:16:15

################################################################################
                     [1m Learning iteration 618/1500 [0m                      

                       Computation: 31812 steps/s (collection: 1.079s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0220
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 13.3859
                       Mean reward: 16.69
               Mean episode length: 896.45
Episode_Reward/track_lin_vel_xy_exp: 1.0038
Episode_Reward/track_ang_vel_z_exp: 0.4656
       Episode_Reward/lin_vel_z_l2: -0.0531
      Episode_Reward/ang_vel_xy_l2: -0.1328
     Episode_Reward/dof_torques_l2: -0.0885
         Episode_Reward/dof_acc_l2: -0.2092
     Episode_Reward/action_rate_l2: -0.1757
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0099
Metrics/base_velocity/error_vel_xy: 0.4627
Metrics/base_velocity/error_vel_yaw: 0.5208
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 22284000
                    Iteration time: 1.13s
                      Time elapsed: 00:11:23
                               ETA: 00:16:14

################################################################################
                     [1m Learning iteration 619/1500 [0m                      

                       Computation: 29875 steps/s (collection: 1.137s, learning 0.068s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0225
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 13.3859
                       Mean reward: 15.14
               Mean episode length: 841.28
Episode_Reward/track_lin_vel_xy_exp: 0.8385
Episode_Reward/track_ang_vel_z_exp: 0.4058
       Episode_Reward/lin_vel_z_l2: -0.0420
      Episode_Reward/ang_vel_xy_l2: -0.1133
     Episode_Reward/dof_torques_l2: -0.0777
         Episode_Reward/dof_acc_l2: -0.1631
     Episode_Reward/action_rate_l2: -0.1506
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0088
Metrics/base_velocity/error_vel_xy: 0.4496
Metrics/base_velocity/error_vel_yaw: 0.4567
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 22320000
                    Iteration time: 1.20s
                      Time elapsed: 00:11:24
                               ETA: 00:16:13

################################################################################
                     [1m Learning iteration 620/1500 [0m                      

                       Computation: 30394 steps/s (collection: 1.133s, learning 0.051s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0324
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 13.3775
                       Mean reward: 15.49
               Mean episode length: 842.41
Episode_Reward/track_lin_vel_xy_exp: 1.0123
Episode_Reward/track_ang_vel_z_exp: 0.4706
       Episode_Reward/lin_vel_z_l2: -0.0548
      Episode_Reward/ang_vel_xy_l2: -0.1363
     Episode_Reward/dof_torques_l2: -0.0817
         Episode_Reward/dof_acc_l2: -0.1903
     Episode_Reward/action_rate_l2: -0.1691
      Episode_Reward/feet_air_time: -0.0015
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0139
Metrics/base_velocity/error_vel_xy: 0.4474
Metrics/base_velocity/error_vel_yaw: 0.5153
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 22356000
                    Iteration time: 1.18s
                      Time elapsed: 00:11:26
                               ETA: 00:16:12

################################################################################
                     [1m Learning iteration 621/1500 [0m                      

                       Computation: 31685 steps/s (collection: 1.083s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0287
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 13.3767
                       Mean reward: 16.17
               Mean episode length: 853.00
Episode_Reward/track_lin_vel_xy_exp: 0.9244
Episode_Reward/track_ang_vel_z_exp: 0.4403
       Episode_Reward/lin_vel_z_l2: -0.0421
      Episode_Reward/ang_vel_xy_l2: -0.1223
     Episode_Reward/dof_torques_l2: -0.0760
         Episode_Reward/dof_acc_l2: -0.1678
     Episode_Reward/action_rate_l2: -0.1554
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0098
Metrics/base_velocity/error_vel_xy: 0.4519
Metrics/base_velocity/error_vel_yaw: 0.4778
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 22392000
                    Iteration time: 1.14s
                      Time elapsed: 00:11:27
                               ETA: 00:16:11

################################################################################
                     [1m Learning iteration 622/1500 [0m                      

                       Computation: 32495 steps/s (collection: 1.054s, learning 0.054s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0260
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 13.3899
                       Mean reward: 16.69
               Mean episode length: 822.90
Episode_Reward/track_lin_vel_xy_exp: 1.0194
Episode_Reward/track_ang_vel_z_exp: 0.4380
       Episode_Reward/lin_vel_z_l2: -0.0419
      Episode_Reward/ang_vel_xy_l2: -0.1176
     Episode_Reward/dof_torques_l2: -0.0766
         Episode_Reward/dof_acc_l2: -0.1697
     Episode_Reward/action_rate_l2: -0.1526
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0123
Metrics/base_velocity/error_vel_xy: 0.3126
Metrics/base_velocity/error_vel_yaw: 0.4530
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 22428000
                    Iteration time: 1.11s
                      Time elapsed: 00:11:28
                               ETA: 00:16:10

################################################################################
                     [1m Learning iteration 623/1500 [0m                      

                       Computation: 32112 steps/s (collection: 1.067s, learning 0.054s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0291
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 13.3922
                       Mean reward: 16.80
               Mean episode length: 832.23
Episode_Reward/track_lin_vel_xy_exp: 0.9804
Episode_Reward/track_ang_vel_z_exp: 0.4476
       Episode_Reward/lin_vel_z_l2: -0.0504
      Episode_Reward/ang_vel_xy_l2: -0.1328
     Episode_Reward/dof_torques_l2: -0.0821
         Episode_Reward/dof_acc_l2: -0.2067
     Episode_Reward/action_rate_l2: -0.1700
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0160
Metrics/base_velocity/error_vel_xy: 0.4393
Metrics/base_velocity/error_vel_yaw: 0.5121
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 22464000
                    Iteration time: 1.12s
                      Time elapsed: 00:11:29
                               ETA: 00:16:08

################################################################################
                     [1m Learning iteration 624/1500 [0m                      

                       Computation: 31947 steps/s (collection: 1.074s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0279
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 13.3920
                       Mean reward: 16.17
               Mean episode length: 812.63
Episode_Reward/track_lin_vel_xy_exp: 0.9622
Episode_Reward/track_ang_vel_z_exp: 0.4378
       Episode_Reward/lin_vel_z_l2: -0.0425
      Episode_Reward/ang_vel_xy_l2: -0.1201
     Episode_Reward/dof_torques_l2: -0.0786
         Episode_Reward/dof_acc_l2: -0.1650
     Episode_Reward/action_rate_l2: -0.1546
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0222
Metrics/base_velocity/error_vel_xy: 0.3755
Metrics/base_velocity/error_vel_yaw: 0.4570
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 22500000
                    Iteration time: 1.13s
                      Time elapsed: 00:11:30
                               ETA: 00:16:07

################################################################################
                     [1m Learning iteration 625/1500 [0m                      

                       Computation: 32348 steps/s (collection: 1.060s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0284
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 13.4003
                       Mean reward: 15.53
               Mean episode length: 808.35
Episode_Reward/track_lin_vel_xy_exp: 0.8932
Episode_Reward/track_ang_vel_z_exp: 0.4132
       Episode_Reward/lin_vel_z_l2: -0.0510
      Episode_Reward/ang_vel_xy_l2: -0.1225
     Episode_Reward/dof_torques_l2: -0.0746
         Episode_Reward/dof_acc_l2: -0.1742
     Episode_Reward/action_rate_l2: -0.1501
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0253
Metrics/base_velocity/error_vel_xy: 0.3870
Metrics/base_velocity/error_vel_yaw: 0.4388
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 22536000
                    Iteration time: 1.11s
                      Time elapsed: 00:11:31
                               ETA: 00:16:06

################################################################################
                     [1m Learning iteration 626/1500 [0m                      

                       Computation: 32598 steps/s (collection: 1.051s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0305
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 13.3979
                       Mean reward: 14.89
               Mean episode length: 789.47
Episode_Reward/track_lin_vel_xy_exp: 0.9505
Episode_Reward/track_ang_vel_z_exp: 0.4319
       Episode_Reward/lin_vel_z_l2: -0.0513
      Episode_Reward/ang_vel_xy_l2: -0.1304
     Episode_Reward/dof_torques_l2: -0.0799
         Episode_Reward/dof_acc_l2: -0.1991
     Episode_Reward/action_rate_l2: -0.1619
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0268
Metrics/base_velocity/error_vel_xy: 0.4165
Metrics/base_velocity/error_vel_yaw: 0.5000
      Episode_Termination/time_out: 1.1667
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 22572000
                    Iteration time: 1.10s
                      Time elapsed: 00:11:32
                               ETA: 00:16:05

################################################################################
                     [1m Learning iteration 627/1500 [0m                      

                       Computation: 32295 steps/s (collection: 1.063s, learning 0.051s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0254
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 13.4028
                       Mean reward: 15.30
               Mean episode length: 827.58
Episode_Reward/track_lin_vel_xy_exp: 1.0737
Episode_Reward/track_ang_vel_z_exp: 0.4870
       Episode_Reward/lin_vel_z_l2: -0.0515
      Episode_Reward/ang_vel_xy_l2: -0.1369
     Episode_Reward/dof_torques_l2: -0.0882
         Episode_Reward/dof_acc_l2: -0.2134
     Episode_Reward/action_rate_l2: -0.1797
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0289
Metrics/base_velocity/error_vel_xy: 0.4435
Metrics/base_velocity/error_vel_yaw: 0.5230
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 22608000
                    Iteration time: 1.11s
                      Time elapsed: 00:11:33
                               ETA: 00:16:04

################################################################################
                     [1m Learning iteration 628/1500 [0m                      

                       Computation: 32015 steps/s (collection: 1.071s, learning 0.054s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0294
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 13.3973
                       Mean reward: 16.21
               Mean episode length: 856.37
Episode_Reward/track_lin_vel_xy_exp: 1.0188
Episode_Reward/track_ang_vel_z_exp: 0.4820
       Episode_Reward/lin_vel_z_l2: -0.0474
      Episode_Reward/ang_vel_xy_l2: -0.1319
     Episode_Reward/dof_torques_l2: -0.0883
         Episode_Reward/dof_acc_l2: -0.1956
     Episode_Reward/action_rate_l2: -0.1755
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0369
Metrics/base_velocity/error_vel_xy: 0.4905
Metrics/base_velocity/error_vel_yaw: 0.5201
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 22644000
                    Iteration time: 1.12s
                      Time elapsed: 00:11:34
                               ETA: 00:16:03

################################################################################
                     [1m Learning iteration 629/1500 [0m                      

                       Computation: 31647 steps/s (collection: 1.084s, learning 0.054s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0281
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 13.3903
                       Mean reward: 16.33
               Mean episode length: 858.11
Episode_Reward/track_lin_vel_xy_exp: 0.9511
Episode_Reward/track_ang_vel_z_exp: 0.4463
       Episode_Reward/lin_vel_z_l2: -0.0444
      Episode_Reward/ang_vel_xy_l2: -0.1262
     Episode_Reward/dof_torques_l2: -0.0869
         Episode_Reward/dof_acc_l2: -0.1731
     Episode_Reward/action_rate_l2: -0.1630
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0432
Metrics/base_velocity/error_vel_xy: 0.4770
Metrics/base_velocity/error_vel_yaw: 0.5259
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 22680000
                    Iteration time: 1.14s
                      Time elapsed: 00:11:36
                               ETA: 00:16:02

################################################################################
                     [1m Learning iteration 630/1500 [0m                      

                       Computation: 30835 steps/s (collection: 1.111s, learning 0.056s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0249
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 13.3879
                       Mean reward: 15.89
               Mean episode length: 832.94
Episode_Reward/track_lin_vel_xy_exp: 0.9809
Episode_Reward/track_ang_vel_z_exp: 0.4446
       Episode_Reward/lin_vel_z_l2: -0.0439
      Episode_Reward/ang_vel_xy_l2: -0.1240
     Episode_Reward/dof_torques_l2: -0.0798
         Episode_Reward/dof_acc_l2: -0.1894
     Episode_Reward/action_rate_l2: -0.1595
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0439
Metrics/base_velocity/error_vel_xy: 0.3902
Metrics/base_velocity/error_vel_yaw: 0.4720
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 22716000
                    Iteration time: 1.17s
                      Time elapsed: 00:11:37
                               ETA: 00:16:01

################################################################################
                     [1m Learning iteration 631/1500 [0m                      

                       Computation: 30661 steps/s (collection: 1.119s, learning 0.055s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0321
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 13.3894
                       Mean reward: 15.51
               Mean episode length: 826.44
Episode_Reward/track_lin_vel_xy_exp: 0.9011
Episode_Reward/track_ang_vel_z_exp: 0.4251
       Episode_Reward/lin_vel_z_l2: -0.0434
      Episode_Reward/ang_vel_xy_l2: -0.1165
     Episode_Reward/dof_torques_l2: -0.0811
         Episode_Reward/dof_acc_l2: -0.1700
     Episode_Reward/action_rate_l2: -0.1531
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0454
Metrics/base_velocity/error_vel_xy: 0.4327
Metrics/base_velocity/error_vel_yaw: 0.4670
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 22752000
                    Iteration time: 1.17s
                      Time elapsed: 00:11:38
                               ETA: 00:16:00

################################################################################
                     [1m Learning iteration 632/1500 [0m                      

                       Computation: 31131 steps/s (collection: 1.103s, learning 0.054s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0254
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 13.3881
                       Mean reward: 15.71
               Mean episode length: 849.07
Episode_Reward/track_lin_vel_xy_exp: 0.9964
Episode_Reward/track_ang_vel_z_exp: 0.4618
       Episode_Reward/lin_vel_z_l2: -0.0495
      Episode_Reward/ang_vel_xy_l2: -0.1313
     Episode_Reward/dof_torques_l2: -0.0816
         Episode_Reward/dof_acc_l2: -0.1917
     Episode_Reward/action_rate_l2: -0.1672
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0480
Metrics/base_velocity/error_vel_xy: 0.4393
Metrics/base_velocity/error_vel_yaw: 0.4944
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 22788000
                    Iteration time: 1.16s
                      Time elapsed: 00:11:39
                               ETA: 00:15:59

################################################################################
                     [1m Learning iteration 633/1500 [0m                      

                       Computation: 30842 steps/s (collection: 1.113s, learning 0.054s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0308
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 13.3833
                       Mean reward: 15.78
               Mean episode length: 817.24
Episode_Reward/track_lin_vel_xy_exp: 0.8973
Episode_Reward/track_ang_vel_z_exp: 0.4177
       Episode_Reward/lin_vel_z_l2: -0.0458
      Episode_Reward/ang_vel_xy_l2: -0.1204
     Episode_Reward/dof_torques_l2: -0.0766
         Episode_Reward/dof_acc_l2: -0.1708
     Episode_Reward/action_rate_l2: -0.1540
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0388
Metrics/base_velocity/error_vel_xy: 0.4302
Metrics/base_velocity/error_vel_yaw: 0.4802
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 22824000
                    Iteration time: 1.17s
                      Time elapsed: 00:11:40
                               ETA: 00:15:58

################################################################################
                     [1m Learning iteration 634/1500 [0m                      

                       Computation: 31247 steps/s (collection: 1.091s, learning 0.061s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0285
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 13.3765
                       Mean reward: 15.21
               Mean episode length: 810.15
Episode_Reward/track_lin_vel_xy_exp: 0.9017
Episode_Reward/track_ang_vel_z_exp: 0.4210
       Episode_Reward/lin_vel_z_l2: -0.0498
      Episode_Reward/ang_vel_xy_l2: -0.1253
     Episode_Reward/dof_torques_l2: -0.0742
         Episode_Reward/dof_acc_l2: -0.1904
     Episode_Reward/action_rate_l2: -0.1573
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0460
Metrics/base_velocity/error_vel_xy: 0.4227
Metrics/base_velocity/error_vel_yaw: 0.4617
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 22860000
                    Iteration time: 1.15s
                      Time elapsed: 00:11:41
                               ETA: 00:15:57

################################################################################
                     [1m Learning iteration 635/1500 [0m                      

                       Computation: 29548 steps/s (collection: 1.164s, learning 0.055s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0267
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 13.3679
                       Mean reward: 15.08
               Mean episode length: 812.26
Episode_Reward/track_lin_vel_xy_exp: 1.0168
Episode_Reward/track_ang_vel_z_exp: 0.4583
       Episode_Reward/lin_vel_z_l2: -0.0489
      Episode_Reward/ang_vel_xy_l2: -0.1316
     Episode_Reward/dof_torques_l2: -0.0835
         Episode_Reward/dof_acc_l2: -0.1954
     Episode_Reward/action_rate_l2: -0.1692
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0530
Metrics/base_velocity/error_vel_xy: 0.4104
Metrics/base_velocity/error_vel_yaw: 0.5086
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 22896000
                    Iteration time: 1.22s
                      Time elapsed: 00:11:43
                               ETA: 00:15:56

################################################################################
                     [1m Learning iteration 636/1500 [0m                      

                       Computation: 30475 steps/s (collection: 1.127s, learning 0.054s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0299
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 13.3696
                       Mean reward: 15.94
               Mean episode length: 823.11
Episode_Reward/track_lin_vel_xy_exp: 0.8878
Episode_Reward/track_ang_vel_z_exp: 0.4084
       Episode_Reward/lin_vel_z_l2: -0.0407
      Episode_Reward/ang_vel_xy_l2: -0.1150
     Episode_Reward/dof_torques_l2: -0.0747
         Episode_Reward/dof_acc_l2: -0.1693
     Episode_Reward/action_rate_l2: -0.1472
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0567
Metrics/base_velocity/error_vel_xy: 0.3703
Metrics/base_velocity/error_vel_yaw: 0.4268
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 22932000
                    Iteration time: 1.18s
                      Time elapsed: 00:11:44
                               ETA: 00:15:55

################################################################################
                     [1m Learning iteration 637/1500 [0m                      

                       Computation: 28795 steps/s (collection: 1.193s, learning 0.057s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0255
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 13.3759
                       Mean reward: 16.95
               Mean episode length: 866.93
Episode_Reward/track_lin_vel_xy_exp: 1.0127
Episode_Reward/track_ang_vel_z_exp: 0.4796
       Episode_Reward/lin_vel_z_l2: -0.0493
      Episode_Reward/ang_vel_xy_l2: -0.1349
     Episode_Reward/dof_torques_l2: -0.0895
         Episode_Reward/dof_acc_l2: -0.1969
     Episode_Reward/action_rate_l2: -0.1754
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0649
Metrics/base_velocity/error_vel_xy: 0.5032
Metrics/base_velocity/error_vel_yaw: 0.5388
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 22968000
                    Iteration time: 1.25s
                      Time elapsed: 00:11:45
                               ETA: 00:15:54

################################################################################
                     [1m Learning iteration 638/1500 [0m                      

                       Computation: 30128 steps/s (collection: 1.140s, learning 0.055s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0270
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 13.3800
                       Mean reward: 16.77
               Mean episode length: 850.83
Episode_Reward/track_lin_vel_xy_exp: 0.9579
Episode_Reward/track_ang_vel_z_exp: 0.4273
       Episode_Reward/lin_vel_z_l2: -0.0428
      Episode_Reward/ang_vel_xy_l2: -0.1144
     Episode_Reward/dof_torques_l2: -0.0813
         Episode_Reward/dof_acc_l2: -0.1708
     Episode_Reward/action_rate_l2: -0.1547
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0743
Metrics/base_velocity/error_vel_xy: 0.3578
Metrics/base_velocity/error_vel_yaw: 0.4453
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 23004000
                    Iteration time: 1.19s
                      Time elapsed: 00:11:46
                               ETA: 00:15:53

################################################################################
                     [1m Learning iteration 639/1500 [0m                      

                       Computation: 28344 steps/s (collection: 1.213s, learning 0.057s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0226
               Mean surrogate loss: -0.0097
                 Mean entropy loss: 13.3837
                       Mean reward: 15.94
               Mean episode length: 824.80
Episode_Reward/track_lin_vel_xy_exp: 0.8843
Episode_Reward/track_ang_vel_z_exp: 0.4287
       Episode_Reward/lin_vel_z_l2: -0.0513
      Episode_Reward/ang_vel_xy_l2: -0.1290
     Episode_Reward/dof_torques_l2: -0.0812
         Episode_Reward/dof_acc_l2: -0.1955
     Episode_Reward/action_rate_l2: -0.1636
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0823
Metrics/base_velocity/error_vel_xy: 0.4978
Metrics/base_velocity/error_vel_yaw: 0.5270
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 23040000
                    Iteration time: 1.27s
                      Time elapsed: 00:11:48
                               ETA: 00:15:52

################################################################################
                     [1m Learning iteration 640/1500 [0m                      

                       Computation: 29690 steps/s (collection: 1.160s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0258
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 13.3759
                       Mean reward: 16.22
               Mean episode length: 835.40
Episode_Reward/track_lin_vel_xy_exp: 0.9832
Episode_Reward/track_ang_vel_z_exp: 0.4612
       Episode_Reward/lin_vel_z_l2: -0.0497
      Episode_Reward/ang_vel_xy_l2: -0.1312
     Episode_Reward/dof_torques_l2: -0.0801
         Episode_Reward/dof_acc_l2: -0.1979
     Episode_Reward/action_rate_l2: -0.1672
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0804
Metrics/base_velocity/error_vel_xy: 0.4603
Metrics/base_velocity/error_vel_yaw: 0.5066
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 23076000
                    Iteration time: 1.21s
                      Time elapsed: 00:11:49
                               ETA: 00:15:51

################################################################################
                     [1m Learning iteration 641/1500 [0m                      

                       Computation: 30501 steps/s (collection: 1.128s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0289
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 13.3776
                       Mean reward: 16.56
               Mean episode length: 888.37
Episode_Reward/track_lin_vel_xy_exp: 0.9870
Episode_Reward/track_ang_vel_z_exp: 0.4716
       Episode_Reward/lin_vel_z_l2: -0.0446
      Episode_Reward/ang_vel_xy_l2: -0.1337
     Episode_Reward/dof_torques_l2: -0.0901
         Episode_Reward/dof_acc_l2: -0.1885
     Episode_Reward/action_rate_l2: -0.1726
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0839
Metrics/base_velocity/error_vel_xy: 0.5181
Metrics/base_velocity/error_vel_yaw: 0.5544
      Episode_Termination/time_out: 2.0000
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 23112000
                    Iteration time: 1.18s
                      Time elapsed: 00:11:50
                               ETA: 00:15:50

################################################################################
                     [1m Learning iteration 642/1500 [0m                      

                       Computation: 30344 steps/s (collection: 1.135s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0249
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 13.3853
                       Mean reward: 16.45
               Mean episode length: 882.34
Episode_Reward/track_lin_vel_xy_exp: 0.9025
Episode_Reward/track_ang_vel_z_exp: 0.4206
       Episode_Reward/lin_vel_z_l2: -0.0564
      Episode_Reward/ang_vel_xy_l2: -0.1286
     Episode_Reward/dof_torques_l2: -0.0789
         Episode_Reward/dof_acc_l2: -0.1973
     Episode_Reward/action_rate_l2: -0.1587
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0878
Metrics/base_velocity/error_vel_xy: 0.4216
Metrics/base_velocity/error_vel_yaw: 0.4611
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 23148000
                    Iteration time: 1.19s
                      Time elapsed: 00:11:51
                               ETA: 00:15:49

################################################################################
                     [1m Learning iteration 643/1500 [0m                      

                       Computation: 31327 steps/s (collection: 1.095s, learning 0.054s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0275
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 13.3903
                       Mean reward: 15.99
               Mean episode length: 841.26
Episode_Reward/track_lin_vel_xy_exp: 0.9575
Episode_Reward/track_ang_vel_z_exp: 0.4493
       Episode_Reward/lin_vel_z_l2: -0.0467
      Episode_Reward/ang_vel_xy_l2: -0.1250
     Episode_Reward/dof_torques_l2: -0.0820
         Episode_Reward/dof_acc_l2: -0.1915
     Episode_Reward/action_rate_l2: -0.1630
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.0980
Metrics/base_velocity/error_vel_xy: 0.4358
Metrics/base_velocity/error_vel_yaw: 0.4793
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 23184000
                    Iteration time: 1.15s
                      Time elapsed: 00:11:52
                               ETA: 00:15:48

################################################################################
                     [1m Learning iteration 644/1500 [0m                      

                       Computation: 31004 steps/s (collection: 1.108s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0234
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 13.3999
                       Mean reward: 16.57
               Mean episode length: 863.26
Episode_Reward/track_lin_vel_xy_exp: 1.0438
Episode_Reward/track_ang_vel_z_exp: 0.4607
       Episode_Reward/lin_vel_z_l2: -0.0457
      Episode_Reward/ang_vel_xy_l2: -0.1286
     Episode_Reward/dof_torques_l2: -0.0836
         Episode_Reward/dof_acc_l2: -0.2000
     Episode_Reward/action_rate_l2: -0.1681
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1052
Metrics/base_velocity/error_vel_xy: 0.3865
Metrics/base_velocity/error_vel_yaw: 0.4900
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 23220000
                    Iteration time: 1.16s
                      Time elapsed: 00:11:53
                               ETA: 00:15:47

################################################################################
                     [1m Learning iteration 645/1500 [0m                      

                       Computation: 31043 steps/s (collection: 1.107s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0237
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 13.3995
                       Mean reward: 17.18
               Mean episode length: 878.38
Episode_Reward/track_lin_vel_xy_exp: 1.0400
Episode_Reward/track_ang_vel_z_exp: 0.4736
       Episode_Reward/lin_vel_z_l2: -0.0528
      Episode_Reward/ang_vel_xy_l2: -0.1341
     Episode_Reward/dof_torques_l2: -0.0881
         Episode_Reward/dof_acc_l2: -0.2043
     Episode_Reward/action_rate_l2: -0.1759
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1139
Metrics/base_velocity/error_vel_xy: 0.4438
Metrics/base_velocity/error_vel_yaw: 0.5166
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 23256000
                    Iteration time: 1.16s
                      Time elapsed: 00:11:55
                               ETA: 00:15:46

################################################################################
                     [1m Learning iteration 646/1500 [0m                      

                       Computation: 31417 steps/s (collection: 1.093s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0276
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 13.3955
                       Mean reward: 15.81
               Mean episode length: 832.85
Episode_Reward/track_lin_vel_xy_exp: 0.9810
Episode_Reward/track_ang_vel_z_exp: 0.4365
       Episode_Reward/lin_vel_z_l2: -0.0471
      Episode_Reward/ang_vel_xy_l2: -0.1253
     Episode_Reward/dof_torques_l2: -0.0807
         Episode_Reward/dof_acc_l2: -0.1868
     Episode_Reward/action_rate_l2: -0.1620
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1174
Metrics/base_velocity/error_vel_xy: 0.3959
Metrics/base_velocity/error_vel_yaw: 0.5096
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 23292000
                    Iteration time: 1.15s
                      Time elapsed: 00:11:56
                               ETA: 00:15:45

################################################################################
                     [1m Learning iteration 647/1500 [0m                      

                       Computation: 31531 steps/s (collection: 1.089s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0246
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 13.3936
                       Mean reward: 16.29
               Mean episode length: 824.83
Episode_Reward/track_lin_vel_xy_exp: 1.0420
Episode_Reward/track_ang_vel_z_exp: 0.4597
       Episode_Reward/lin_vel_z_l2: -0.0457
      Episode_Reward/ang_vel_xy_l2: -0.1301
     Episode_Reward/dof_torques_l2: -0.0825
         Episode_Reward/dof_acc_l2: -0.1832
     Episode_Reward/action_rate_l2: -0.1643
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1228
Metrics/base_velocity/error_vel_xy: 0.3729
Metrics/base_velocity/error_vel_yaw: 0.4928
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 23328000
                    Iteration time: 1.14s
                      Time elapsed: 00:11:57
                               ETA: 00:15:44

################################################################################
                     [1m Learning iteration 648/1500 [0m                      

                       Computation: 31103 steps/s (collection: 1.103s, learning 0.054s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0271
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 13.3970
                       Mean reward: 16.72
               Mean episode length: 818.98
Episode_Reward/track_lin_vel_xy_exp: 1.0020
Episode_Reward/track_ang_vel_z_exp: 0.4488
       Episode_Reward/lin_vel_z_l2: -0.0451
      Episode_Reward/ang_vel_xy_l2: -0.1196
     Episode_Reward/dof_torques_l2: -0.0836
         Episode_Reward/dof_acc_l2: -0.1750
     Episode_Reward/action_rate_l2: -0.1602
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1322
Metrics/base_velocity/error_vel_xy: 0.3768
Metrics/base_velocity/error_vel_yaw: 0.4735
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 23364000
                    Iteration time: 1.16s
                      Time elapsed: 00:11:58
                               ETA: 00:15:43

################################################################################
                     [1m Learning iteration 649/1500 [0m                      

                       Computation: 31064 steps/s (collection: 1.106s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0304
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 13.4126
                       Mean reward: 15.79
               Mean episode length: 800.82
Episode_Reward/track_lin_vel_xy_exp: 0.9270
Episode_Reward/track_ang_vel_z_exp: 0.4369
       Episode_Reward/lin_vel_z_l2: -0.0453
      Episode_Reward/ang_vel_xy_l2: -0.1225
     Episode_Reward/dof_torques_l2: -0.0794
         Episode_Reward/dof_acc_l2: -0.1794
     Episode_Reward/action_rate_l2: -0.1566
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1299
Metrics/base_velocity/error_vel_xy: 0.4247
Metrics/base_velocity/error_vel_yaw: 0.4684
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 23400000
                    Iteration time: 1.16s
                      Time elapsed: 00:11:59
                               ETA: 00:15:42

################################################################################
                     [1m Learning iteration 650/1500 [0m                      

                       Computation: 31980 steps/s (collection: 1.074s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0283
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 13.4131
                       Mean reward: 15.43
               Mean episode length: 795.79
Episode_Reward/track_lin_vel_xy_exp: 0.8841
Episode_Reward/track_ang_vel_z_exp: 0.3976
       Episode_Reward/lin_vel_z_l2: -0.0413
      Episode_Reward/ang_vel_xy_l2: -0.1138
     Episode_Reward/dof_torques_l2: -0.0718
         Episode_Reward/dof_acc_l2: -0.1609
     Episode_Reward/action_rate_l2: -0.1446
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1364
Metrics/base_velocity/error_vel_xy: 0.3776
Metrics/base_velocity/error_vel_yaw: 0.4574
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 23436000
                    Iteration time: 1.13s
                      Time elapsed: 00:12:00
                               ETA: 00:15:41

################################################################################
                     [1m Learning iteration 651/1500 [0m                      

                       Computation: 31598 steps/s (collection: 1.085s, learning 0.055s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0267
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 13.4070
                       Mean reward: 14.74
               Mean episode length: 807.10
Episode_Reward/track_lin_vel_xy_exp: 0.8960
Episode_Reward/track_ang_vel_z_exp: 0.4120
       Episode_Reward/lin_vel_z_l2: -0.0477
      Episode_Reward/ang_vel_xy_l2: -0.1243
     Episode_Reward/dof_torques_l2: -0.0756
         Episode_Reward/dof_acc_l2: -0.1921
     Episode_Reward/action_rate_l2: -0.1577
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1361
Metrics/base_velocity/error_vel_xy: 0.4386
Metrics/base_velocity/error_vel_yaw: 0.5163
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 23472000
                    Iteration time: 1.14s
                      Time elapsed: 00:12:01
                               ETA: 00:15:40

################################################################################
                     [1m Learning iteration 652/1500 [0m                      

                       Computation: 29717 steps/s (collection: 1.154s, learning 0.057s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0296
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 13.4075
                       Mean reward: 15.78
               Mean episode length: 840.52
Episode_Reward/track_lin_vel_xy_exp: 1.0316
Episode_Reward/track_ang_vel_z_exp: 0.4663
       Episode_Reward/lin_vel_z_l2: -0.0488
      Episode_Reward/ang_vel_xy_l2: -0.1330
     Episode_Reward/dof_torques_l2: -0.0861
         Episode_Reward/dof_acc_l2: -0.2039
     Episode_Reward/action_rate_l2: -0.1722
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1449
Metrics/base_velocity/error_vel_xy: 0.4311
Metrics/base_velocity/error_vel_yaw: 0.5354
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 23508000
                    Iteration time: 1.21s
                      Time elapsed: 00:12:03
                               ETA: 00:15:39

################################################################################
                     [1m Learning iteration 653/1500 [0m                      

                       Computation: 27311 steps/s (collection: 1.263s, learning 0.055s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0285
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 13.4004
                       Mean reward: 15.38
               Mean episode length: 807.02
Episode_Reward/track_lin_vel_xy_exp: 0.7469
Episode_Reward/track_ang_vel_z_exp: 0.3651
       Episode_Reward/lin_vel_z_l2: -0.0399
      Episode_Reward/ang_vel_xy_l2: -0.1053
     Episode_Reward/dof_torques_l2: -0.0682
         Episode_Reward/dof_acc_l2: -0.1537
     Episode_Reward/action_rate_l2: -0.1347
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1426
Metrics/base_velocity/error_vel_xy: 0.4136
Metrics/base_velocity/error_vel_yaw: 0.4131
      Episode_Termination/time_out: 1.1250
  Episode_Termination/base_contact: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 23544000
                    Iteration time: 1.32s
                      Time elapsed: 00:12:04
                               ETA: 00:15:38

################################################################################
                     [1m Learning iteration 654/1500 [0m                      

                       Computation: 29440 steps/s (collection: 1.155s, learning 0.067s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0334
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 13.4018
                       Mean reward: 15.65
               Mean episode length: 814.70
Episode_Reward/track_lin_vel_xy_exp: 1.0175
Episode_Reward/track_ang_vel_z_exp: 0.4575
       Episode_Reward/lin_vel_z_l2: -0.0439
      Episode_Reward/ang_vel_xy_l2: -0.1252
     Episode_Reward/dof_torques_l2: -0.0798
         Episode_Reward/dof_acc_l2: -0.1848
     Episode_Reward/action_rate_l2: -0.1627
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1464
Metrics/base_velocity/error_vel_xy: 0.3894
Metrics/base_velocity/error_vel_yaw: 0.4829
      Episode_Termination/time_out: 2.0833
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 23580000
                    Iteration time: 1.22s
                      Time elapsed: 00:12:05
                               ETA: 00:15:37

################################################################################
                     [1m Learning iteration 655/1500 [0m                      

                       Computation: 26196 steps/s (collection: 1.313s, learning 0.061s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0220
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 13.4125
                       Mean reward: 16.95
               Mean episode length: 863.88
Episode_Reward/track_lin_vel_xy_exp: 0.9126
Episode_Reward/track_ang_vel_z_exp: 0.4197
       Episode_Reward/lin_vel_z_l2: -0.0453
      Episode_Reward/ang_vel_xy_l2: -0.1202
     Episode_Reward/dof_torques_l2: -0.0770
         Episode_Reward/dof_acc_l2: -0.1761
     Episode_Reward/action_rate_l2: -0.1552
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1477
Metrics/base_velocity/error_vel_xy: 0.3963
Metrics/base_velocity/error_vel_yaw: 0.4637
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 23616000
                    Iteration time: 1.37s
                      Time elapsed: 00:12:07
                               ETA: 00:15:36

################################################################################
                     [1m Learning iteration 656/1500 [0m                      

                       Computation: 27167 steps/s (collection: 1.268s, learning 0.057s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0219
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 13.4167
                       Mean reward: 17.21
               Mean episode length: 874.30
Episode_Reward/track_lin_vel_xy_exp: 1.0259
Episode_Reward/track_ang_vel_z_exp: 0.4598
       Episode_Reward/lin_vel_z_l2: -0.0496
      Episode_Reward/ang_vel_xy_l2: -0.1291
     Episode_Reward/dof_torques_l2: -0.0809
         Episode_Reward/dof_acc_l2: -0.2014
     Episode_Reward/action_rate_l2: -0.1671
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1544
Metrics/base_velocity/error_vel_xy: 0.4000
Metrics/base_velocity/error_vel_yaw: 0.5068
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 23652000
                    Iteration time: 1.33s
                      Time elapsed: 00:12:08
                               ETA: 00:15:35

################################################################################
                     [1m Learning iteration 657/1500 [0m                      

                       Computation: 28891 steps/s (collection: 1.191s, learning 0.055s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0287
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 13.4198
                       Mean reward: 15.67
               Mean episode length: 836.17
Episode_Reward/track_lin_vel_xy_exp: 0.8970
Episode_Reward/track_ang_vel_z_exp: 0.4238
       Episode_Reward/lin_vel_z_l2: -0.0452
      Episode_Reward/ang_vel_xy_l2: -0.1199
     Episode_Reward/dof_torques_l2: -0.0820
         Episode_Reward/dof_acc_l2: -0.1863
     Episode_Reward/action_rate_l2: -0.1571
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1593
Metrics/base_velocity/error_vel_xy: 0.4445
Metrics/base_velocity/error_vel_yaw: 0.4732
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 23688000
                    Iteration time: 1.25s
                      Time elapsed: 00:12:09
                               ETA: 00:15:34

################################################################################
                     [1m Learning iteration 658/1500 [0m                      

                       Computation: 30689 steps/s (collection: 1.116s, learning 0.057s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0315
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 13.4140
                       Mean reward: 14.82
               Mean episode length: 811.29
Episode_Reward/track_lin_vel_xy_exp: 0.9140
Episode_Reward/track_ang_vel_z_exp: 0.4139
       Episode_Reward/lin_vel_z_l2: -0.0451
      Episode_Reward/ang_vel_xy_l2: -0.1198
     Episode_Reward/dof_torques_l2: -0.0774
         Episode_Reward/dof_acc_l2: -0.1732
     Episode_Reward/action_rate_l2: -0.1520
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1581
Metrics/base_velocity/error_vel_xy: 0.4014
Metrics/base_velocity/error_vel_yaw: 0.4829
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 23724000
                    Iteration time: 1.17s
                      Time elapsed: 00:12:10
                               ETA: 00:15:33

################################################################################
                     [1m Learning iteration 659/1500 [0m                      

                       Computation: 29149 steps/s (collection: 1.182s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0255
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 13.4158
                       Mean reward: 16.39
               Mean episode length: 844.19
Episode_Reward/track_lin_vel_xy_exp: 0.9805
Episode_Reward/track_ang_vel_z_exp: 0.4465
       Episode_Reward/lin_vel_z_l2: -0.0514
      Episode_Reward/ang_vel_xy_l2: -0.1350
     Episode_Reward/dof_torques_l2: -0.0815
         Episode_Reward/dof_acc_l2: -0.2003
     Episode_Reward/action_rate_l2: -0.1657
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1610
Metrics/base_velocity/error_vel_xy: 0.4290
Metrics/base_velocity/error_vel_yaw: 0.5078
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 23760000
                    Iteration time: 1.24s
                      Time elapsed: 00:12:12
                               ETA: 00:15:32

################################################################################
                     [1m Learning iteration 660/1500 [0m                      

                       Computation: 30139 steps/s (collection: 1.139s, learning 0.055s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0263
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 13.4113
                       Mean reward: 16.67
               Mean episode length: 843.84
Episode_Reward/track_lin_vel_xy_exp: 0.9778
Episode_Reward/track_ang_vel_z_exp: 0.4473
       Episode_Reward/lin_vel_z_l2: -0.0433
      Episode_Reward/ang_vel_xy_l2: -0.1227
     Episode_Reward/dof_torques_l2: -0.0863
         Episode_Reward/dof_acc_l2: -0.1827
     Episode_Reward/action_rate_l2: -0.1634
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1592
Metrics/base_velocity/error_vel_xy: 0.4003
Metrics/base_velocity/error_vel_yaw: 0.4752
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 23796000
                    Iteration time: 1.19s
                      Time elapsed: 00:12:13
                               ETA: 00:15:31

################################################################################
                     [1m Learning iteration 661/1500 [0m                      

                       Computation: 30502 steps/s (collection: 1.125s, learning 0.055s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0296
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 13.4180
                       Mean reward: 15.40
               Mean episode length: 812.08
Episode_Reward/track_lin_vel_xy_exp: 0.8511
Episode_Reward/track_ang_vel_z_exp: 0.4030
       Episode_Reward/lin_vel_z_l2: -0.0437
      Episode_Reward/ang_vel_xy_l2: -0.1148
     Episode_Reward/dof_torques_l2: -0.0752
         Episode_Reward/dof_acc_l2: -0.1743
     Episode_Reward/action_rate_l2: -0.1478
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1631
Metrics/base_velocity/error_vel_xy: 0.4133
Metrics/base_velocity/error_vel_yaw: 0.4489
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 23832000
                    Iteration time: 1.18s
                      Time elapsed: 00:12:14
                               ETA: 00:15:30

################################################################################
                     [1m Learning iteration 662/1500 [0m                      

                       Computation: 30136 steps/s (collection: 1.140s, learning 0.054s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0292
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 13.4154
                       Mean reward: 14.32
               Mean episode length: 787.74
Episode_Reward/track_lin_vel_xy_exp: 0.7833
Episode_Reward/track_ang_vel_z_exp: 0.3800
       Episode_Reward/lin_vel_z_l2: -0.0410
      Episode_Reward/ang_vel_xy_l2: -0.1128
     Episode_Reward/dof_torques_l2: -0.0706
         Episode_Reward/dof_acc_l2: -0.1621
     Episode_Reward/action_rate_l2: -0.1405
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1671
Metrics/base_velocity/error_vel_xy: 0.4390
Metrics/base_velocity/error_vel_yaw: 0.4440
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 23868000
                    Iteration time: 1.19s
                      Time elapsed: 00:12:15
                               ETA: 00:15:29

################################################################################
                     [1m Learning iteration 663/1500 [0m                      

                       Computation: 30779 steps/s (collection: 1.116s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0255
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 13.4136
                       Mean reward: 14.62
               Mean episode length: 790.90
Episode_Reward/track_lin_vel_xy_exp: 1.0172
Episode_Reward/track_ang_vel_z_exp: 0.4641
       Episode_Reward/lin_vel_z_l2: -0.0437
      Episode_Reward/ang_vel_xy_l2: -0.1295
     Episode_Reward/dof_torques_l2: -0.0820
         Episode_Reward/dof_acc_l2: -0.1889
     Episode_Reward/action_rate_l2: -0.1659
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1699
Metrics/base_velocity/error_vel_xy: 0.4258
Metrics/base_velocity/error_vel_yaw: 0.5197
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 23904000
                    Iteration time: 1.17s
                      Time elapsed: 00:12:16
                               ETA: 00:15:28

################################################################################
                     [1m Learning iteration 664/1500 [0m                      

                       Computation: 30860 steps/s (collection: 1.112s, learning 0.055s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0264
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 13.4272
                       Mean reward: 14.67
               Mean episode length: 821.61
Episode_Reward/track_lin_vel_xy_exp: 0.8994
Episode_Reward/track_ang_vel_z_exp: 0.4341
       Episode_Reward/lin_vel_z_l2: -0.0459
      Episode_Reward/ang_vel_xy_l2: -0.1294
     Episode_Reward/dof_torques_l2: -0.0850
         Episode_Reward/dof_acc_l2: -0.1937
     Episode_Reward/action_rate_l2: -0.1631
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1753
Metrics/base_velocity/error_vel_xy: 0.5022
Metrics/base_velocity/error_vel_yaw: 0.5225
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 23940000
                    Iteration time: 1.17s
                      Time elapsed: 00:12:17
                               ETA: 00:15:27

################################################################################
                     [1m Learning iteration 665/1500 [0m                      

                       Computation: 29604 steps/s (collection: 1.160s, learning 0.056s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0243
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 13.4228
                       Mean reward: 14.88
               Mean episode length: 825.42
Episode_Reward/track_lin_vel_xy_exp: 0.9785
Episode_Reward/track_ang_vel_z_exp: 0.4518
       Episode_Reward/lin_vel_z_l2: -0.0464
      Episode_Reward/ang_vel_xy_l2: -0.1268
     Episode_Reward/dof_torques_l2: -0.0774
         Episode_Reward/dof_acc_l2: -0.1846
     Episode_Reward/action_rate_l2: -0.1600
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1786
Metrics/base_velocity/error_vel_xy: 0.4028
Metrics/base_velocity/error_vel_yaw: 0.4546
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 23976000
                    Iteration time: 1.22s
                      Time elapsed: 00:12:19
                               ETA: 00:15:26

################################################################################
                     [1m Learning iteration 666/1500 [0m                      

                       Computation: 29873 steps/s (collection: 1.150s, learning 0.055s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0250
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 13.4246
                       Mean reward: 15.35
               Mean episode length: 807.25
Episode_Reward/track_lin_vel_xy_exp: 0.9266
Episode_Reward/track_ang_vel_z_exp: 0.4236
       Episode_Reward/lin_vel_z_l2: -0.0455
      Episode_Reward/ang_vel_xy_l2: -0.1186
     Episode_Reward/dof_torques_l2: -0.0785
         Episode_Reward/dof_acc_l2: -0.1911
     Episode_Reward/action_rate_l2: -0.1554
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1866
Metrics/base_velocity/error_vel_xy: 0.3831
Metrics/base_velocity/error_vel_yaw: 0.4603
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 24012000
                    Iteration time: 1.21s
                      Time elapsed: 00:12:20
                               ETA: 00:15:25

################################################################################
                     [1m Learning iteration 667/1500 [0m                      

                       Computation: 30471 steps/s (collection: 1.127s, learning 0.054s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0268
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 13.4325
                       Mean reward: 16.10
               Mean episode length: 831.32
Episode_Reward/track_lin_vel_xy_exp: 0.7706
Episode_Reward/track_ang_vel_z_exp: 0.3667
       Episode_Reward/lin_vel_z_l2: -0.0437
      Episode_Reward/ang_vel_xy_l2: -0.1124
     Episode_Reward/dof_torques_l2: -0.0690
         Episode_Reward/dof_acc_l2: -0.1657
     Episode_Reward/action_rate_l2: -0.1382
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1932
Metrics/base_velocity/error_vel_xy: 0.4258
Metrics/base_velocity/error_vel_yaw: 0.4530
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 24048000
                    Iteration time: 1.18s
                      Time elapsed: 00:12:21
                               ETA: 00:15:24

################################################################################
                     [1m Learning iteration 668/1500 [0m                      

                       Computation: 29252 steps/s (collection: 1.175s, learning 0.056s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0281
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 13.4234
                       Mean reward: 14.78
               Mean episode length: 782.93
Episode_Reward/track_lin_vel_xy_exp: 0.8813
Episode_Reward/track_ang_vel_z_exp: 0.3993
       Episode_Reward/lin_vel_z_l2: -0.0393
      Episode_Reward/ang_vel_xy_l2: -0.1128
     Episode_Reward/dof_torques_l2: -0.0722
         Episode_Reward/dof_acc_l2: -0.1696
     Episode_Reward/action_rate_l2: -0.1443
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1953
Metrics/base_velocity/error_vel_xy: 0.3576
Metrics/base_velocity/error_vel_yaw: 0.4299
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 24084000
                    Iteration time: 1.23s
                      Time elapsed: 00:12:22
                               ETA: 00:15:23

################################################################################
                     [1m Learning iteration 669/1500 [0m                      

                       Computation: 29309 steps/s (collection: 1.174s, learning 0.055s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0241
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 13.4245
                       Mean reward: 15.80
               Mean episode length: 824.02
Episode_Reward/track_lin_vel_xy_exp: 1.1291
Episode_Reward/track_ang_vel_z_exp: 0.4935
       Episode_Reward/lin_vel_z_l2: -0.0492
      Episode_Reward/ang_vel_xy_l2: -0.1376
     Episode_Reward/dof_torques_l2: -0.0892
         Episode_Reward/dof_acc_l2: -0.2034
     Episode_Reward/action_rate_l2: -0.1765
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1952
Metrics/base_velocity/error_vel_xy: 0.3865
Metrics/base_velocity/error_vel_yaw: 0.5312
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 24120000
                    Iteration time: 1.23s
                      Time elapsed: 00:12:24
                               ETA: 00:15:22

################################################################################
                     [1m Learning iteration 670/1500 [0m                      

                       Computation: 31806 steps/s (collection: 1.080s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0282
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 13.4262
                       Mean reward: 16.37
               Mean episode length: 850.24
Episode_Reward/track_lin_vel_xy_exp: 1.0121
Episode_Reward/track_ang_vel_z_exp: 0.4781
       Episode_Reward/lin_vel_z_l2: -0.0449
      Episode_Reward/ang_vel_xy_l2: -0.1342
     Episode_Reward/dof_torques_l2: -0.0872
         Episode_Reward/dof_acc_l2: -0.1938
     Episode_Reward/action_rate_l2: -0.1734
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1971
Metrics/base_velocity/error_vel_xy: 0.4973
Metrics/base_velocity/error_vel_yaw: 0.5392
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 24156000
                    Iteration time: 1.13s
                      Time elapsed: 00:12:25
                               ETA: 00:15:21

################################################################################
                     [1m Learning iteration 671/1500 [0m                      

                       Computation: 31214 steps/s (collection: 1.101s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0304
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 13.4386
                       Mean reward: 16.86
               Mean episode length: 879.70
Episode_Reward/track_lin_vel_xy_exp: 0.9964
Episode_Reward/track_ang_vel_z_exp: 0.4616
       Episode_Reward/lin_vel_z_l2: -0.0473
      Episode_Reward/ang_vel_xy_l2: -0.1311
     Episode_Reward/dof_torques_l2: -0.0853
         Episode_Reward/dof_acc_l2: -0.1975
     Episode_Reward/action_rate_l2: -0.1685
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.1966
Metrics/base_velocity/error_vel_xy: 0.4413
Metrics/base_velocity/error_vel_yaw: 0.5103
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 24192000
                    Iteration time: 1.15s
                      Time elapsed: 00:12:26
                               ETA: 00:15:20

################################################################################
                     [1m Learning iteration 672/1500 [0m                      

                       Computation: 31417 steps/s (collection: 1.092s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0249
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 13.4495
                       Mean reward: 16.15
               Mean episode length: 867.49
Episode_Reward/track_lin_vel_xy_exp: 1.1161
Episode_Reward/track_ang_vel_z_exp: 0.4918
       Episode_Reward/lin_vel_z_l2: -0.0507
      Episode_Reward/ang_vel_xy_l2: -0.1372
     Episode_Reward/dof_torques_l2: -0.0901
         Episode_Reward/dof_acc_l2: -0.1979
     Episode_Reward/action_rate_l2: -0.1763
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2035
Metrics/base_velocity/error_vel_xy: 0.4053
Metrics/base_velocity/error_vel_yaw: 0.5287
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 24228000
                    Iteration time: 1.15s
                      Time elapsed: 00:12:27
                               ETA: 00:15:19

################################################################################
                     [1m Learning iteration 673/1500 [0m                      

                       Computation: 31600 steps/s (collection: 1.086s, learning 0.053s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0273
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 13.4568
                       Mean reward: 17.48
               Mean episode length: 882.46
Episode_Reward/track_lin_vel_xy_exp: 1.0381
Episode_Reward/track_ang_vel_z_exp: 0.4656
       Episode_Reward/lin_vel_z_l2: -0.0464
      Episode_Reward/ang_vel_xy_l2: -0.1301
     Episode_Reward/dof_torques_l2: -0.0899
         Episode_Reward/dof_acc_l2: -0.2009
     Episode_Reward/action_rate_l2: -0.1721
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2059
Metrics/base_velocity/error_vel_xy: 0.4186
Metrics/base_velocity/error_vel_yaw: 0.5176
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 24264000
                    Iteration time: 1.14s
                      Time elapsed: 00:12:28
                               ETA: 00:15:18

################################################################################
                     [1m Learning iteration 674/1500 [0m                      

                       Computation: 32226 steps/s (collection: 1.065s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0303
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 13.4510
                       Mean reward: 18.30
               Mean episode length: 875.34
Episode_Reward/track_lin_vel_xy_exp: 1.1399
Episode_Reward/track_ang_vel_z_exp: 0.5014
       Episode_Reward/lin_vel_z_l2: -0.0413
      Episode_Reward/ang_vel_xy_l2: -0.1281
     Episode_Reward/dof_torques_l2: -0.0870
         Episode_Reward/dof_acc_l2: -0.1888
     Episode_Reward/action_rate_l2: -0.1718
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2101
Metrics/base_velocity/error_vel_xy: 0.3442
Metrics/base_velocity/error_vel_yaw: 0.4638
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 24300000
                    Iteration time: 1.12s
                      Time elapsed: 00:12:29
                               ETA: 00:15:17

################################################################################
                     [1m Learning iteration 675/1500 [0m                      

                       Computation: 28727 steps/s (collection: 1.190s, learning 0.063s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0290
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 13.4541
                       Mean reward: 18.16
               Mean episode length: 891.31
Episode_Reward/track_lin_vel_xy_exp: 1.0594
Episode_Reward/track_ang_vel_z_exp: 0.4897
       Episode_Reward/lin_vel_z_l2: -0.0507
      Episode_Reward/ang_vel_xy_l2: -0.1374
     Episode_Reward/dof_torques_l2: -0.0899
         Episode_Reward/dof_acc_l2: -0.2076
     Episode_Reward/action_rate_l2: -0.1780
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2137
Metrics/base_velocity/error_vel_xy: 0.4630
Metrics/base_velocity/error_vel_yaw: 0.5311
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 24336000
                    Iteration time: 1.25s
                      Time elapsed: 00:12:31
                               ETA: 00:15:16

################################################################################
                     [1m Learning iteration 676/1500 [0m                      

                       Computation: 27406 steps/s (collection: 1.250s, learning 0.064s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0263
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 13.4636
                       Mean reward: 16.22
               Mean episode length: 860.57
Episode_Reward/track_lin_vel_xy_exp: 0.7802
Episode_Reward/track_ang_vel_z_exp: 0.3912
       Episode_Reward/lin_vel_z_l2: -0.0473
      Episode_Reward/ang_vel_xy_l2: -0.1226
     Episode_Reward/dof_torques_l2: -0.0760
         Episode_Reward/dof_acc_l2: -0.1920
     Episode_Reward/action_rate_l2: -0.1488
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2137
Metrics/base_velocity/error_vel_xy: 0.5011
Metrics/base_velocity/error_vel_yaw: 0.4805
      Episode_Termination/time_out: 1.0000
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 24372000
                    Iteration time: 1.31s
                      Time elapsed: 00:12:32
                               ETA: 00:15:15

################################################################################
                     [1m Learning iteration 677/1500 [0m                      

                       Computation: 29080 steps/s (collection: 1.183s, learning 0.055s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0282
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 13.4718
                       Mean reward: 15.25
               Mean episode length: 809.69
Episode_Reward/track_lin_vel_xy_exp: 0.8609
Episode_Reward/track_ang_vel_z_exp: 0.3944
       Episode_Reward/lin_vel_z_l2: -0.0387
      Episode_Reward/ang_vel_xy_l2: -0.1102
     Episode_Reward/dof_torques_l2: -0.0738
         Episode_Reward/dof_acc_l2: -0.1611
     Episode_Reward/action_rate_l2: -0.1439
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2128
Metrics/base_velocity/error_vel_xy: 0.3628
Metrics/base_velocity/error_vel_yaw: 0.4420
      Episode_Termination/time_out: 1.1250
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 24408000
                    Iteration time: 1.24s
                      Time elapsed: 00:12:33
                               ETA: 00:15:14

################################################################################
                     [1m Learning iteration 678/1500 [0m                      

                       Computation: 29620 steps/s (collection: 1.164s, learning 0.052s)
             Mean action noise std: 0.75
          Mean value_function loss: 0.0305
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 13.4731
                       Mean reward: 16.18
               Mean episode length: 836.73
Episode_Reward/track_lin_vel_xy_exp: 1.0544
Episode_Reward/track_ang_vel_z_exp: 0.4976
       Episode_Reward/lin_vel_z_l2: -0.0474
      Episode_Reward/ang_vel_xy_l2: -0.1382
     Episode_Reward/dof_torques_l2: -0.0886
         Episode_Reward/dof_acc_l2: -0.2138
     Episode_Reward/action_rate_l2: -0.1795
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2202
Metrics/base_velocity/error_vel_xy: 0.4994
Metrics/base_velocity/error_vel_yaw: 0.5355
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 24444000
                    Iteration time: 1.22s
                      Time elapsed: 00:12:34
                               ETA: 00:15:13

################################################################################
                     [1m Learning iteration 679/1500 [0m                      

                       Computation: 30826 steps/s (collection: 1.113s, learning 0.055s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0317
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 13.4878
                       Mean reward: 17.17
               Mean episode length: 901.14
Episode_Reward/track_lin_vel_xy_exp: 1.0215
Episode_Reward/track_ang_vel_z_exp: 0.4705
       Episode_Reward/lin_vel_z_l2: -0.0488
      Episode_Reward/ang_vel_xy_l2: -0.1332
     Episode_Reward/dof_torques_l2: -0.0896
         Episode_Reward/dof_acc_l2: -0.2041
     Episode_Reward/action_rate_l2: -0.1731
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2243
Metrics/base_velocity/error_vel_xy: 0.4583
Metrics/base_velocity/error_vel_yaw: 0.5225
      Episode_Termination/time_out: 2.0833
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 24480000
                    Iteration time: 1.17s
                      Time elapsed: 00:12:35
                               ETA: 00:15:12

################################################################################
                     [1m Learning iteration 680/1500 [0m                      

                       Computation: 29608 steps/s (collection: 1.163s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0272
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 13.5099
                       Mean reward: 16.03
               Mean episode length: 845.86
Episode_Reward/track_lin_vel_xy_exp: 0.8800
Episode_Reward/track_ang_vel_z_exp: 0.4190
       Episode_Reward/lin_vel_z_l2: -0.0506
      Episode_Reward/ang_vel_xy_l2: -0.1258
     Episode_Reward/dof_torques_l2: -0.0815
         Episode_Reward/dof_acc_l2: -0.1936
     Episode_Reward/action_rate_l2: -0.1594
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2270
Metrics/base_velocity/error_vel_xy: 0.4923
Metrics/base_velocity/error_vel_yaw: 0.5157
      Episode_Termination/time_out: 1.1667
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 24516000
                    Iteration time: 1.22s
                      Time elapsed: 00:12:37
                               ETA: 00:15:11

################################################################################
                     [1m Learning iteration 681/1500 [0m                      

                       Computation: 30630 steps/s (collection: 1.120s, learning 0.055s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0248
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 13.5032
                       Mean reward: 14.93
               Mean episode length: 803.93
Episode_Reward/track_lin_vel_xy_exp: 1.0306
Episode_Reward/track_ang_vel_z_exp: 0.4585
       Episode_Reward/lin_vel_z_l2: -0.0458
      Episode_Reward/ang_vel_xy_l2: -0.1317
     Episode_Reward/dof_torques_l2: -0.0845
         Episode_Reward/dof_acc_l2: -0.1971
     Episode_Reward/action_rate_l2: -0.1670
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2301
Metrics/base_velocity/error_vel_xy: 0.4040
Metrics/base_velocity/error_vel_yaw: 0.5044
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 24552000
                    Iteration time: 1.18s
                      Time elapsed: 00:12:38
                               ETA: 00:15:10

################################################################################
                     [1m Learning iteration 682/1500 [0m                      

                       Computation: 30499 steps/s (collection: 1.123s, learning 0.057s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0249
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 13.5046
                       Mean reward: 14.66
               Mean episode length: 808.32
Episode_Reward/track_lin_vel_xy_exp: 0.9459
Episode_Reward/track_ang_vel_z_exp: 0.4542
       Episode_Reward/lin_vel_z_l2: -0.0433
      Episode_Reward/ang_vel_xy_l2: -0.1297
     Episode_Reward/dof_torques_l2: -0.0870
         Episode_Reward/dof_acc_l2: -0.2003
     Episode_Reward/action_rate_l2: -0.1698
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2374
Metrics/base_velocity/error_vel_xy: 0.4913
Metrics/base_velocity/error_vel_yaw: 0.5158
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 24588000
                    Iteration time: 1.18s
                      Time elapsed: 00:12:39
                               ETA: 00:15:09

################################################################################
                     [1m Learning iteration 683/1500 [0m                      

                       Computation: 32068 steps/s (collection: 1.069s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0284
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 13.5009
                       Mean reward: 16.19
               Mean episode length: 850.85
Episode_Reward/track_lin_vel_xy_exp: 1.0072
Episode_Reward/track_ang_vel_z_exp: 0.4526
       Episode_Reward/lin_vel_z_l2: -0.0489
      Episode_Reward/ang_vel_xy_l2: -0.1313
     Episode_Reward/dof_torques_l2: -0.0823
         Episode_Reward/dof_acc_l2: -0.1984
     Episode_Reward/action_rate_l2: -0.1664
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2449
Metrics/base_velocity/error_vel_xy: 0.4146
Metrics/base_velocity/error_vel_yaw: 0.5180
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 24624000
                    Iteration time: 1.12s
                      Time elapsed: 00:12:40
                               ETA: 00:15:08

################################################################################
                     [1m Learning iteration 684/1500 [0m                      

                       Computation: 31971 steps/s (collection: 1.072s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0263
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 13.5149
                       Mean reward: 16.71
               Mean episode length: 868.28
Episode_Reward/track_lin_vel_xy_exp: 1.0305
Episode_Reward/track_ang_vel_z_exp: 0.4677
       Episode_Reward/lin_vel_z_l2: -0.0519
      Episode_Reward/ang_vel_xy_l2: -0.1385
     Episode_Reward/dof_torques_l2: -0.0864
         Episode_Reward/dof_acc_l2: -0.1979
     Episode_Reward/action_rate_l2: -0.1710
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2492
Metrics/base_velocity/error_vel_xy: 0.4234
Metrics/base_velocity/error_vel_yaw: 0.4975
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 24660000
                    Iteration time: 1.13s
                      Time elapsed: 00:12:41
                               ETA: 00:15:07

################################################################################
                     [1m Learning iteration 685/1500 [0m                      

                       Computation: 32189 steps/s (collection: 1.066s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0315
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 13.5198
                       Mean reward: 16.35
               Mean episode length: 828.59
Episode_Reward/track_lin_vel_xy_exp: 0.9726
Episode_Reward/track_ang_vel_z_exp: 0.4416
       Episode_Reward/lin_vel_z_l2: -0.0456
      Episode_Reward/ang_vel_xy_l2: -0.1257
     Episode_Reward/dof_torques_l2: -0.0772
         Episode_Reward/dof_acc_l2: -0.1720
     Episode_Reward/action_rate_l2: -0.1555
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2467
Metrics/base_velocity/error_vel_xy: 0.3996
Metrics/base_velocity/error_vel_yaw: 0.4786
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 24696000
                    Iteration time: 1.12s
                      Time elapsed: 00:12:42
                               ETA: 00:15:06

################################################################################
                     [1m Learning iteration 686/1500 [0m                      

                       Computation: 32385 steps/s (collection: 1.059s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0254
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 13.5147
                       Mean reward: 17.27
               Mean episode length: 849.08
Episode_Reward/track_lin_vel_xy_exp: 1.0682
Episode_Reward/track_ang_vel_z_exp: 0.4776
       Episode_Reward/lin_vel_z_l2: -0.0485
      Episode_Reward/ang_vel_xy_l2: -0.1340
     Episode_Reward/dof_torques_l2: -0.0817
         Episode_Reward/dof_acc_l2: -0.2083
     Episode_Reward/action_rate_l2: -0.1705
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2488
Metrics/base_velocity/error_vel_xy: 0.3817
Metrics/base_velocity/error_vel_yaw: 0.4781
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 24732000
                    Iteration time: 1.11s
                      Time elapsed: 00:12:43
                               ETA: 00:15:05

################################################################################
                     [1m Learning iteration 687/1500 [0m                      

                       Computation: 32068 steps/s (collection: 1.071s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0241
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 13.5100
                       Mean reward: 17.51
               Mean episode length: 869.42
Episode_Reward/track_lin_vel_xy_exp: 1.0087
Episode_Reward/track_ang_vel_z_exp: 0.4549
       Episode_Reward/lin_vel_z_l2: -0.0463
      Episode_Reward/ang_vel_xy_l2: -0.1296
     Episode_Reward/dof_torques_l2: -0.0862
         Episode_Reward/dof_acc_l2: -0.1936
     Episode_Reward/action_rate_l2: -0.1705
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2508
Metrics/base_velocity/error_vel_xy: 0.4200
Metrics/base_velocity/error_vel_yaw: 0.5270
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 24768000
                    Iteration time: 1.12s
                      Time elapsed: 00:12:45
                               ETA: 00:15:04

################################################################################
                     [1m Learning iteration 688/1500 [0m                      

                       Computation: 32321 steps/s (collection: 1.063s, learning 0.051s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0231
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 13.5161
                       Mean reward: 16.82
               Mean episode length: 872.33
Episode_Reward/track_lin_vel_xy_exp: 0.9811
Episode_Reward/track_ang_vel_z_exp: 0.4473
       Episode_Reward/lin_vel_z_l2: -0.0479
      Episode_Reward/ang_vel_xy_l2: -0.1301
     Episode_Reward/dof_torques_l2: -0.0908
         Episode_Reward/dof_acc_l2: -0.1978
     Episode_Reward/action_rate_l2: -0.1700
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2585
Metrics/base_velocity/error_vel_xy: 0.4397
Metrics/base_velocity/error_vel_yaw: 0.5316
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 24804000
                    Iteration time: 1.11s
                      Time elapsed: 00:12:46
                               ETA: 00:15:03

################################################################################
                     [1m Learning iteration 689/1500 [0m                      

                       Computation: 32000 steps/s (collection: 1.072s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0270
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 13.5213
                       Mean reward: 16.32
               Mean episode length: 875.41
Episode_Reward/track_lin_vel_xy_exp: 1.0765
Episode_Reward/track_ang_vel_z_exp: 0.4816
       Episode_Reward/lin_vel_z_l2: -0.0457
      Episode_Reward/ang_vel_xy_l2: -0.1353
     Episode_Reward/dof_torques_l2: -0.0880
         Episode_Reward/dof_acc_l2: -0.2034
     Episode_Reward/action_rate_l2: -0.1739
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2673
Metrics/base_velocity/error_vel_xy: 0.4157
Metrics/base_velocity/error_vel_yaw: 0.5177
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 24840000
                    Iteration time: 1.12s
                      Time elapsed: 00:12:47
                               ETA: 00:15:01

################################################################################
                     [1m Learning iteration 690/1500 [0m                      

                       Computation: 32701 steps/s (collection: 1.049s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0265
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 13.5169
                       Mean reward: 16.47
               Mean episode length: 885.92
Episode_Reward/track_lin_vel_xy_exp: 0.9885
Episode_Reward/track_ang_vel_z_exp: 0.4587
       Episode_Reward/lin_vel_z_l2: -0.0498
      Episode_Reward/ang_vel_xy_l2: -0.1311
     Episode_Reward/dof_torques_l2: -0.0931
         Episode_Reward/dof_acc_l2: -0.2005
     Episode_Reward/action_rate_l2: -0.1726
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2730
Metrics/base_velocity/error_vel_xy: 0.4556
Metrics/base_velocity/error_vel_yaw: 0.5299
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 24876000
                    Iteration time: 1.10s
                      Time elapsed: 00:12:48
                               ETA: 00:15:00

################################################################################
                     [1m Learning iteration 691/1500 [0m                      

                       Computation: 32229 steps/s (collection: 1.064s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0252
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 13.5038
                       Mean reward: 15.38
               Mean episode length: 856.08
Episode_Reward/track_lin_vel_xy_exp: 0.8598
Episode_Reward/track_ang_vel_z_exp: 0.4066
       Episode_Reward/lin_vel_z_l2: -0.0455
      Episode_Reward/ang_vel_xy_l2: -0.1212
     Episode_Reward/dof_torques_l2: -0.0773
         Episode_Reward/dof_acc_l2: -0.1969
     Episode_Reward/action_rate_l2: -0.1561
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2762
Metrics/base_velocity/error_vel_xy: 0.4405
Metrics/base_velocity/error_vel_yaw: 0.4789
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 24912000
                    Iteration time: 1.12s
                      Time elapsed: 00:12:49
                               ETA: 00:14:59

################################################################################
                     [1m Learning iteration 692/1500 [0m                      

                       Computation: 32284 steps/s (collection: 1.064s, learning 0.051s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0270
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 13.5024
                       Mean reward: 14.77
               Mean episode length: 843.21
Episode_Reward/track_lin_vel_xy_exp: 0.9891
Episode_Reward/track_ang_vel_z_exp: 0.4720
       Episode_Reward/lin_vel_z_l2: -0.0485
      Episode_Reward/ang_vel_xy_l2: -0.1349
     Episode_Reward/dof_torques_l2: -0.0919
         Episode_Reward/dof_acc_l2: -0.2020
     Episode_Reward/action_rate_l2: -0.1775
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2772
Metrics/base_velocity/error_vel_xy: 0.5375
Metrics/base_velocity/error_vel_yaw: 0.5538
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 24948000
                    Iteration time: 1.12s
                      Time elapsed: 00:12:50
                               ETA: 00:14:58

################################################################################
                     [1m Learning iteration 693/1500 [0m                      

                       Computation: 31888 steps/s (collection: 1.074s, learning 0.055s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0291
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 13.5100
                       Mean reward: 15.99
               Mean episode length: 857.03
Episode_Reward/track_lin_vel_xy_exp: 1.0391
Episode_Reward/track_ang_vel_z_exp: 0.4576
       Episode_Reward/lin_vel_z_l2: -0.0472
      Episode_Reward/ang_vel_xy_l2: -0.1286
     Episode_Reward/dof_torques_l2: -0.0796
         Episode_Reward/dof_acc_l2: -0.1939
     Episode_Reward/action_rate_l2: -0.1653
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2848
Metrics/base_velocity/error_vel_xy: 0.3516
Metrics/base_velocity/error_vel_yaw: 0.4691
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 24984000
                    Iteration time: 1.13s
                      Time elapsed: 00:12:51
                               ETA: 00:14:57

################################################################################
                     [1m Learning iteration 694/1500 [0m                      

                       Computation: 32195 steps/s (collection: 1.067s, learning 0.051s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0271
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 13.5198
                       Mean reward: 16.49
               Mean episode length: 835.08
Episode_Reward/track_lin_vel_xy_exp: 0.9937
Episode_Reward/track_ang_vel_z_exp: 0.4353
       Episode_Reward/lin_vel_z_l2: -0.0487
      Episode_Reward/ang_vel_xy_l2: -0.1248
     Episode_Reward/dof_torques_l2: -0.0811
         Episode_Reward/dof_acc_l2: -0.1901
     Episode_Reward/action_rate_l2: -0.1604
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2766
Metrics/base_velocity/error_vel_xy: 0.3525
Metrics/base_velocity/error_vel_yaw: 0.4821
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 25020000
                    Iteration time: 1.12s
                      Time elapsed: 00:12:52
                               ETA: 00:14:56

################################################################################
                     [1m Learning iteration 695/1500 [0m                      

                       Computation: 32253 steps/s (collection: 1.064s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0291
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 13.5278
                       Mean reward: 16.04
               Mean episode length: 824.46
Episode_Reward/track_lin_vel_xy_exp: 0.9769
Episode_Reward/track_ang_vel_z_exp: 0.4398
       Episode_Reward/lin_vel_z_l2: -0.0496
      Episode_Reward/ang_vel_xy_l2: -0.1284
     Episode_Reward/dof_torques_l2: -0.0789
         Episode_Reward/dof_acc_l2: -0.1983
     Episode_Reward/action_rate_l2: -0.1637
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2704
Metrics/base_velocity/error_vel_xy: 0.3992
Metrics/base_velocity/error_vel_yaw: 0.4977
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 25056000
                    Iteration time: 1.12s
                      Time elapsed: 00:12:54
                               ETA: 00:14:55

################################################################################
                     [1m Learning iteration 696/1500 [0m                      

                       Computation: 31668 steps/s (collection: 1.085s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0281
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 13.5402
                       Mean reward: 16.69
               Mean episode length: 859.09
Episode_Reward/track_lin_vel_xy_exp: 1.0212
Episode_Reward/track_ang_vel_z_exp: 0.4638
       Episode_Reward/lin_vel_z_l2: -0.0475
      Episode_Reward/ang_vel_xy_l2: -0.1318
     Episode_Reward/dof_torques_l2: -0.0867
         Episode_Reward/dof_acc_l2: -0.2004
     Episode_Reward/action_rate_l2: -0.1720
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2673
Metrics/base_velocity/error_vel_xy: 0.4332
Metrics/base_velocity/error_vel_yaw: 0.5123
      Episode_Termination/time_out: 1.9583
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 25092000
                    Iteration time: 1.14s
                      Time elapsed: 00:12:55
                               ETA: 00:14:54

################################################################################
                     [1m Learning iteration 697/1500 [0m                      

                       Computation: 31833 steps/s (collection: 1.078s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0232
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 13.5451
                       Mean reward: 17.12
               Mean episode length: 880.22
Episode_Reward/track_lin_vel_xy_exp: 1.0270
Episode_Reward/track_ang_vel_z_exp: 0.4601
       Episode_Reward/lin_vel_z_l2: -0.0475
      Episode_Reward/ang_vel_xy_l2: -0.1320
     Episode_Reward/dof_torques_l2: -0.0861
         Episode_Reward/dof_acc_l2: -0.2009
     Episode_Reward/action_rate_l2: -0.1708
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2614
Metrics/base_velocity/error_vel_xy: 0.4187
Metrics/base_velocity/error_vel_yaw: 0.5335
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 25128000
                    Iteration time: 1.13s
                      Time elapsed: 00:12:56
                               ETA: 00:14:53

################################################################################
                     [1m Learning iteration 698/1500 [0m                      

                       Computation: 31511 steps/s (collection: 1.084s, learning 0.059s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0266
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 13.5492
                       Mean reward: 15.93
               Mean episode length: 855.75
Episode_Reward/track_lin_vel_xy_exp: 0.9267
Episode_Reward/track_ang_vel_z_exp: 0.4405
       Episode_Reward/lin_vel_z_l2: -0.0455
      Episode_Reward/ang_vel_xy_l2: -0.1307
     Episode_Reward/dof_torques_l2: -0.0863
         Episode_Reward/dof_acc_l2: -0.1940
     Episode_Reward/action_rate_l2: -0.1662
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2634
Metrics/base_velocity/error_vel_xy: 0.4892
Metrics/base_velocity/error_vel_yaw: 0.5402
      Episode_Termination/time_out: 2.0000
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 25164000
                    Iteration time: 1.14s
                      Time elapsed: 00:12:57
                               ETA: 00:14:52

################################################################################
                     [1m Learning iteration 699/1500 [0m                      

                       Computation: 30762 steps/s (collection: 1.117s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0278
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 13.5528
                       Mean reward: 15.82
               Mean episode length: 842.20
Episode_Reward/track_lin_vel_xy_exp: 0.9531
Episode_Reward/track_ang_vel_z_exp: 0.4391
       Episode_Reward/lin_vel_z_l2: -0.0463
      Episode_Reward/ang_vel_xy_l2: -0.1261
     Episode_Reward/dof_torques_l2: -0.0874
         Episode_Reward/dof_acc_l2: -0.1845
     Episode_Reward/action_rate_l2: -0.1641
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2676
Metrics/base_velocity/error_vel_xy: 0.4609
Metrics/base_velocity/error_vel_yaw: 0.5192
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 25200000
                    Iteration time: 1.17s
                      Time elapsed: 00:12:58
                               ETA: 00:14:50

################################################################################
                     [1m Learning iteration 700/1500 [0m                      

                       Computation: 31404 steps/s (collection: 1.094s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0293
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 13.5635
                       Mean reward: 15.99
               Mean episode length: 841.40
Episode_Reward/track_lin_vel_xy_exp: 0.9504
Episode_Reward/track_ang_vel_z_exp: 0.4371
       Episode_Reward/lin_vel_z_l2: -0.0472
      Episode_Reward/ang_vel_xy_l2: -0.1277
     Episode_Reward/dof_torques_l2: -0.0844
         Episode_Reward/dof_acc_l2: -0.1901
     Episode_Reward/action_rate_l2: -0.1621
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2700
Metrics/base_velocity/error_vel_xy: 0.4324
Metrics/base_velocity/error_vel_yaw: 0.4955
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 25236000
                    Iteration time: 1.15s
                      Time elapsed: 00:12:59
                               ETA: 00:14:49

################################################################################
                     [1m Learning iteration 701/1500 [0m                      

                       Computation: 31267 steps/s (collection: 1.097s, learning 0.055s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0234
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 13.5821
                       Mean reward: 15.68
               Mean episode length: 824.76
Episode_Reward/track_lin_vel_xy_exp: 1.0284
Episode_Reward/track_ang_vel_z_exp: 0.4505
       Episode_Reward/lin_vel_z_l2: -0.0463
      Episode_Reward/ang_vel_xy_l2: -0.1259
     Episode_Reward/dof_torques_l2: -0.0818
         Episode_Reward/dof_acc_l2: -0.1979
     Episode_Reward/action_rate_l2: -0.1651
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2588
Metrics/base_velocity/error_vel_xy: 0.3761
Metrics/base_velocity/error_vel_yaw: 0.4966
      Episode_Termination/time_out: 1.0833
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 25272000
                    Iteration time: 1.15s
                      Time elapsed: 00:13:00
                               ETA: 00:14:48

################################################################################
                     [1m Learning iteration 702/1500 [0m                      

                       Computation: 32114 steps/s (collection: 1.069s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0273
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 13.6032
                       Mean reward: 16.55
               Mean episode length: 882.26
Episode_Reward/track_lin_vel_xy_exp: 1.0236
Episode_Reward/track_ang_vel_z_exp: 0.4820
       Episode_Reward/lin_vel_z_l2: -0.0491
      Episode_Reward/ang_vel_xy_l2: -0.1353
     Episode_Reward/dof_torques_l2: -0.0948
         Episode_Reward/dof_acc_l2: -0.2148
     Episode_Reward/action_rate_l2: -0.1802
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2613
Metrics/base_velocity/error_vel_xy: 0.4785
Metrics/base_velocity/error_vel_yaw: 0.5330
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 25308000
                    Iteration time: 1.12s
                      Time elapsed: 00:13:02
                               ETA: 00:14:47

################################################################################
                     [1m Learning iteration 703/1500 [0m                      

                       Computation: 31200 steps/s (collection: 1.099s, learning 0.055s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0274
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 13.6128
                       Mean reward: 15.67
               Mean episode length: 841.74
Episode_Reward/track_lin_vel_xy_exp: 0.8900
Episode_Reward/track_ang_vel_z_exp: 0.4086
       Episode_Reward/lin_vel_z_l2: -0.0497
      Episode_Reward/ang_vel_xy_l2: -0.1244
     Episode_Reward/dof_torques_l2: -0.0784
         Episode_Reward/dof_acc_l2: -0.1850
     Episode_Reward/action_rate_l2: -0.1540
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2555
Metrics/base_velocity/error_vel_xy: 0.4171
Metrics/base_velocity/error_vel_yaw: 0.4891
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 25344000
                    Iteration time: 1.15s
                      Time elapsed: 00:13:03
                               ETA: 00:14:46

################################################################################
                     [1m Learning iteration 704/1500 [0m                      

                       Computation: 31398 steps/s (collection: 1.090s, learning 0.056s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0237
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 13.6289
                       Mean reward: 15.41
               Mean episode length: 823.99
Episode_Reward/track_lin_vel_xy_exp: 1.0524
Episode_Reward/track_ang_vel_z_exp: 0.4790
       Episode_Reward/lin_vel_z_l2: -0.0507
      Episode_Reward/ang_vel_xy_l2: -0.1395
     Episode_Reward/dof_torques_l2: -0.0918
         Episode_Reward/dof_acc_l2: -0.2152
     Episode_Reward/action_rate_l2: -0.1808
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2571
Metrics/base_velocity/error_vel_xy: 0.4598
Metrics/base_velocity/error_vel_yaw: 0.5466
      Episode_Termination/time_out: 1.9167
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 25380000
                    Iteration time: 1.15s
                      Time elapsed: 00:13:04
                               ETA: 00:14:45

################################################################################
                     [1m Learning iteration 705/1500 [0m                      

                       Computation: 32441 steps/s (collection: 1.057s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0243
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 13.6345
                       Mean reward: 15.01
               Mean episode length: 823.89
Episode_Reward/track_lin_vel_xy_exp: 0.9581
Episode_Reward/track_ang_vel_z_exp: 0.4431
       Episode_Reward/lin_vel_z_l2: -0.0546
      Episode_Reward/ang_vel_xy_l2: -0.1345
     Episode_Reward/dof_torques_l2: -0.0814
         Episode_Reward/dof_acc_l2: -0.2045
     Episode_Reward/action_rate_l2: -0.1669
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2622
Metrics/base_velocity/error_vel_xy: 0.4570
Metrics/base_velocity/error_vel_yaw: 0.5217
      Episode_Termination/time_out: 1.1250
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 25416000
                    Iteration time: 1.11s
                      Time elapsed: 00:13:05
                               ETA: 00:14:44

################################################################################
                     [1m Learning iteration 706/1500 [0m                      

                       Computation: 31268 steps/s (collection: 1.097s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0280
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 13.6347
                       Mean reward: 15.96
               Mean episode length: 863.96
Episode_Reward/track_lin_vel_xy_exp: 0.9749
Episode_Reward/track_ang_vel_z_exp: 0.4449
       Episode_Reward/lin_vel_z_l2: -0.0471
      Episode_Reward/ang_vel_xy_l2: -0.1317
     Episode_Reward/dof_torques_l2: -0.0864
         Episode_Reward/dof_acc_l2: -0.2015
     Episode_Reward/action_rate_l2: -0.1704
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2676
Metrics/base_velocity/error_vel_xy: 0.4308
Metrics/base_velocity/error_vel_yaw: 0.5041
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 25452000
                    Iteration time: 1.15s
                      Time elapsed: 00:13:06
                               ETA: 00:14:43

################################################################################
                     [1m Learning iteration 707/1500 [0m                      

                       Computation: 31226 steps/s (collection: 1.091s, learning 0.062s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0235
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 13.6280
                       Mean reward: 15.46
               Mean episode length: 824.26
Episode_Reward/track_lin_vel_xy_exp: 0.9708
Episode_Reward/track_ang_vel_z_exp: 0.4344
       Episode_Reward/lin_vel_z_l2: -0.0482
      Episode_Reward/ang_vel_xy_l2: -0.1271
     Episode_Reward/dof_torques_l2: -0.0799
         Episode_Reward/dof_acc_l2: -0.1938
     Episode_Reward/action_rate_l2: -0.1619
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2738
Metrics/base_velocity/error_vel_xy: 0.3761
Metrics/base_velocity/error_vel_yaw: 0.4667
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 25488000
                    Iteration time: 1.15s
                      Time elapsed: 00:13:07
                               ETA: 00:14:42

################################################################################
                     [1m Learning iteration 708/1500 [0m                      

                       Computation: 31149 steps/s (collection: 1.103s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0314
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 13.6153
                       Mean reward: 16.10
               Mean episode length: 822.61
Episode_Reward/track_lin_vel_xy_exp: 1.1092
Episode_Reward/track_ang_vel_z_exp: 0.4887
       Episode_Reward/lin_vel_z_l2: -0.0493
      Episode_Reward/ang_vel_xy_l2: -0.1413
     Episode_Reward/dof_torques_l2: -0.0900
         Episode_Reward/dof_acc_l2: -0.2180
     Episode_Reward/action_rate_l2: -0.1803
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2828
Metrics/base_velocity/error_vel_xy: 0.4018
Metrics/base_velocity/error_vel_yaw: 0.5166
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 25524000
                    Iteration time: 1.16s
                      Time elapsed: 00:13:08
                               ETA: 00:14:41

################################################################################
                     [1m Learning iteration 709/1500 [0m                      

                       Computation: 30850 steps/s (collection: 1.110s, learning 0.057s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0256
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 13.6186
                       Mean reward: 16.05
               Mean episode length: 848.84
Episode_Reward/track_lin_vel_xy_exp: 0.9131
Episode_Reward/track_ang_vel_z_exp: 0.4299
       Episode_Reward/lin_vel_z_l2: -0.0499
      Episode_Reward/ang_vel_xy_l2: -0.1344
     Episode_Reward/dof_torques_l2: -0.0833
         Episode_Reward/dof_acc_l2: -0.2017
     Episode_Reward/action_rate_l2: -0.1651
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2864
Metrics/base_velocity/error_vel_xy: 0.4712
Metrics/base_velocity/error_vel_yaw: 0.5076
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 25560000
                    Iteration time: 1.17s
                      Time elapsed: 00:13:10
                               ETA: 00:14:40

################################################################################
                     [1m Learning iteration 710/1500 [0m                      

                       Computation: 31265 steps/s (collection: 1.096s, learning 0.055s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0266
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 13.6335
                       Mean reward: 15.31
               Mean episode length: 826.41
Episode_Reward/track_lin_vel_xy_exp: 0.8562
Episode_Reward/track_ang_vel_z_exp: 0.4093
       Episode_Reward/lin_vel_z_l2: -0.0410
      Episode_Reward/ang_vel_xy_l2: -0.1210
     Episode_Reward/dof_torques_l2: -0.0800
         Episode_Reward/dof_acc_l2: -0.1706
     Episode_Reward/action_rate_l2: -0.1529
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2866
Metrics/base_velocity/error_vel_xy: 0.4577
Metrics/base_velocity/error_vel_yaw: 0.4978
      Episode_Termination/time_out: 1.0000
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 25596000
                    Iteration time: 1.15s
                      Time elapsed: 00:13:11
                               ETA: 00:14:39

################################################################################
                     [1m Learning iteration 711/1500 [0m                      

                       Computation: 31394 steps/s (collection: 1.095s, learning 0.051s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0262
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 13.6349
                       Mean reward: 14.31
               Mean episode length: 782.99
Episode_Reward/track_lin_vel_xy_exp: 0.8452
Episode_Reward/track_ang_vel_z_exp: 0.3956
       Episode_Reward/lin_vel_z_l2: -0.0404
      Episode_Reward/ang_vel_xy_l2: -0.1146
     Episode_Reward/dof_torques_l2: -0.0772
         Episode_Reward/dof_acc_l2: -0.1661
     Episode_Reward/action_rate_l2: -0.1479
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2855
Metrics/base_velocity/error_vel_xy: 0.4117
Metrics/base_velocity/error_vel_yaw: 0.4461
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 25632000
                    Iteration time: 1.15s
                      Time elapsed: 00:13:12
                               ETA: 00:14:38

################################################################################
                     [1m Learning iteration 712/1500 [0m                      

                       Computation: 31190 steps/s (collection: 1.102s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0270
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 13.6287
                       Mean reward: 14.96
               Mean episode length: 798.12
Episode_Reward/track_lin_vel_xy_exp: 0.8676
Episode_Reward/track_ang_vel_z_exp: 0.3987
       Episode_Reward/lin_vel_z_l2: -0.0460
      Episode_Reward/ang_vel_xy_l2: -0.1244
     Episode_Reward/dof_torques_l2: -0.0778
         Episode_Reward/dof_acc_l2: -0.1878
     Episode_Reward/action_rate_l2: -0.1558
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2878
Metrics/base_velocity/error_vel_xy: 0.4364
Metrics/base_velocity/error_vel_yaw: 0.4916
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 25668000
                    Iteration time: 1.15s
                      Time elapsed: 00:13:13
                               ETA: 00:14:37

################################################################################
                     [1m Learning iteration 713/1500 [0m                      

                       Computation: 31977 steps/s (collection: 1.072s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0269
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 13.6332
                       Mean reward: 15.13
               Mean episode length: 847.95
Episode_Reward/track_lin_vel_xy_exp: 1.0085
Episode_Reward/track_ang_vel_z_exp: 0.4725
       Episode_Reward/lin_vel_z_l2: -0.0481
      Episode_Reward/ang_vel_xy_l2: -0.1366
     Episode_Reward/dof_torques_l2: -0.0885
         Episode_Reward/dof_acc_l2: -0.2088
     Episode_Reward/action_rate_l2: -0.1771
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.2964
Metrics/base_velocity/error_vel_xy: 0.5001
Metrics/base_velocity/error_vel_yaw: 0.5309
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 25704000
                    Iteration time: 1.13s
                      Time elapsed: 00:13:14
                               ETA: 00:14:35

################################################################################
                     [1m Learning iteration 714/1500 [0m                      

                       Computation: 29974 steps/s (collection: 1.148s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0244
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 13.6340
                       Mean reward: 15.78
               Mean episode length: 846.84
Episode_Reward/track_lin_vel_xy_exp: 0.8502
Episode_Reward/track_ang_vel_z_exp: 0.3869
       Episode_Reward/lin_vel_z_l2: -0.0396
      Episode_Reward/ang_vel_xy_l2: -0.1109
     Episode_Reward/dof_torques_l2: -0.0742
         Episode_Reward/dof_acc_l2: -0.1666
     Episode_Reward/action_rate_l2: -0.1462
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3045
Metrics/base_velocity/error_vel_xy: 0.3699
Metrics/base_velocity/error_vel_yaw: 0.4476
      Episode_Termination/time_out: 1.1667
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 25740000
                    Iteration time: 1.20s
                      Time elapsed: 00:13:15
                               ETA: 00:14:34

################################################################################
                     [1m Learning iteration 715/1500 [0m                      

                       Computation: 30815 steps/s (collection: 1.114s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0278
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 13.6333
                       Mean reward: 16.37
               Mean episode length: 858.78
Episode_Reward/track_lin_vel_xy_exp: 1.0062
Episode_Reward/track_ang_vel_z_exp: 0.4440
       Episode_Reward/lin_vel_z_l2: -0.0473
      Episode_Reward/ang_vel_xy_l2: -0.1276
     Episode_Reward/dof_torques_l2: -0.0863
         Episode_Reward/dof_acc_l2: -0.1973
     Episode_Reward/action_rate_l2: -0.1665
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3116
Metrics/base_velocity/error_vel_xy: 0.3748
Metrics/base_velocity/error_vel_yaw: 0.4870
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 25776000
                    Iteration time: 1.17s
                      Time elapsed: 00:13:17
                               ETA: 00:14:33

################################################################################
                     [1m Learning iteration 716/1500 [0m                      

                       Computation: 31723 steps/s (collection: 1.082s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0277
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 13.6314
                       Mean reward: 16.76
               Mean episode length: 843.35
Episode_Reward/track_lin_vel_xy_exp: 1.1146
Episode_Reward/track_ang_vel_z_exp: 0.4839
       Episode_Reward/lin_vel_z_l2: -0.0496
      Episode_Reward/ang_vel_xy_l2: -0.1325
     Episode_Reward/dof_torques_l2: -0.0810
         Episode_Reward/dof_acc_l2: -0.2063
     Episode_Reward/action_rate_l2: -0.1737
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3164
Metrics/base_velocity/error_vel_xy: 0.3395
Metrics/base_velocity/error_vel_yaw: 0.4782
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 25812000
                    Iteration time: 1.13s
                      Time elapsed: 00:13:18
                               ETA: 00:14:32

################################################################################
                     [1m Learning iteration 717/1500 [0m                      

                       Computation: 32224 steps/s (collection: 1.065s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0291
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 13.6418
                       Mean reward: 16.51
               Mean episode length: 820.92
Episode_Reward/track_lin_vel_xy_exp: 0.9019
Episode_Reward/track_ang_vel_z_exp: 0.4097
       Episode_Reward/lin_vel_z_l2: -0.0361
      Episode_Reward/ang_vel_xy_l2: -0.1109
     Episode_Reward/dof_torques_l2: -0.0779
         Episode_Reward/dof_acc_l2: -0.1620
     Episode_Reward/action_rate_l2: -0.1478
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3205
Metrics/base_velocity/error_vel_xy: 0.3584
Metrics/base_velocity/error_vel_yaw: 0.4386
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 25848000
                    Iteration time: 1.12s
                      Time elapsed: 00:13:19
                               ETA: 00:14:31

################################################################################
                     [1m Learning iteration 718/1500 [0m                      

                       Computation: 32683 steps/s (collection: 1.049s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0279
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.6453
                       Mean reward: 16.66
               Mean episode length: 857.38
Episode_Reward/track_lin_vel_xy_exp: 0.9423
Episode_Reward/track_ang_vel_z_exp: 0.4410
       Episode_Reward/lin_vel_z_l2: -0.0442
      Episode_Reward/ang_vel_xy_l2: -0.1290
     Episode_Reward/dof_torques_l2: -0.0852
         Episode_Reward/dof_acc_l2: -0.1735
     Episode_Reward/action_rate_l2: -0.1611
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3199
Metrics/base_velocity/error_vel_xy: 0.4623
Metrics/base_velocity/error_vel_yaw: 0.5202
      Episode_Termination/time_out: 1.1250
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 25884000
                    Iteration time: 1.10s
                      Time elapsed: 00:13:20
                               ETA: 00:14:30

################################################################################
                     [1m Learning iteration 719/1500 [0m                      

                       Computation: 32058 steps/s (collection: 1.069s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0264
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 13.6539
                       Mean reward: 15.16
               Mean episode length: 824.60
Episode_Reward/track_lin_vel_xy_exp: 1.0579
Episode_Reward/track_ang_vel_z_exp: 0.4636
       Episode_Reward/lin_vel_z_l2: -0.0476
      Episode_Reward/ang_vel_xy_l2: -0.1328
     Episode_Reward/dof_torques_l2: -0.0852
         Episode_Reward/dof_acc_l2: -0.2063
     Episode_Reward/action_rate_l2: -0.1723
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3170
Metrics/base_velocity/error_vel_xy: 0.3837
Metrics/base_velocity/error_vel_yaw: 0.5058
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 25920000
                    Iteration time: 1.12s
                      Time elapsed: 00:13:21
                               ETA: 00:14:29

################################################################################
                     [1m Learning iteration 720/1500 [0m                      

                       Computation: 31043 steps/s (collection: 1.107s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0285
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 13.6555
                       Mean reward: 16.21
               Mean episode length: 849.88
Episode_Reward/track_lin_vel_xy_exp: 1.0713
Episode_Reward/track_ang_vel_z_exp: 0.4713
       Episode_Reward/lin_vel_z_l2: -0.0453
      Episode_Reward/ang_vel_xy_l2: -0.1333
     Episode_Reward/dof_torques_l2: -0.0853
         Episode_Reward/dof_acc_l2: -0.1895
     Episode_Reward/action_rate_l2: -0.1712
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3071
Metrics/base_velocity/error_vel_xy: 0.3873
Metrics/base_velocity/error_vel_yaw: 0.5143
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 25956000
                    Iteration time: 1.16s
                      Time elapsed: 00:13:22
                               ETA: 00:14:28

################################################################################
                     [1m Learning iteration 721/1500 [0m                      

                       Computation: 29956 steps/s (collection: 1.148s, learning 0.054s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0282
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 13.6652
                       Mean reward: 16.86
               Mean episode length: 870.16
Episode_Reward/track_lin_vel_xy_exp: 1.0270
Episode_Reward/track_ang_vel_z_exp: 0.4629
       Episode_Reward/lin_vel_z_l2: -0.0474
      Episode_Reward/ang_vel_xy_l2: -0.1346
     Episode_Reward/dof_torques_l2: -0.0905
         Episode_Reward/dof_acc_l2: -0.2066
     Episode_Reward/action_rate_l2: -0.1757
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3201
Metrics/base_velocity/error_vel_xy: 0.4337
Metrics/base_velocity/error_vel_yaw: 0.5377
      Episode_Termination/time_out: 2.0417
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 25992000
                    Iteration time: 1.20s
                      Time elapsed: 00:13:23
                               ETA: 00:14:27

################################################################################
                     [1m Learning iteration 722/1500 [0m                      

                       Computation: 32316 steps/s (collection: 1.063s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0265
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 13.6660
                       Mean reward: 15.92
               Mean episode length: 844.82
Episode_Reward/track_lin_vel_xy_exp: 0.7778
Episode_Reward/track_ang_vel_z_exp: 0.3714
       Episode_Reward/lin_vel_z_l2: -0.0390
      Episode_Reward/ang_vel_xy_l2: -0.1104
     Episode_Reward/dof_torques_l2: -0.0724
         Episode_Reward/dof_acc_l2: -0.1654
     Episode_Reward/action_rate_l2: -0.1410
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3327
Metrics/base_velocity/error_vel_xy: 0.4094
Metrics/base_velocity/error_vel_yaw: 0.4351
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26028000
                    Iteration time: 1.11s
                      Time elapsed: 00:13:24
                               ETA: 00:14:26

################################################################################
                     [1m Learning iteration 723/1500 [0m                      

                       Computation: 33228 steps/s (collection: 1.031s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0251
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 13.6606
                       Mean reward: 16.01
               Mean episode length: 838.70
Episode_Reward/track_lin_vel_xy_exp: 1.1303
Episode_Reward/track_ang_vel_z_exp: 0.4976
       Episode_Reward/lin_vel_z_l2: -0.0458
      Episode_Reward/ang_vel_xy_l2: -0.1411
     Episode_Reward/dof_torques_l2: -0.0880
         Episode_Reward/dof_acc_l2: -0.2094
     Episode_Reward/action_rate_l2: -0.1840
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3301
Metrics/base_velocity/error_vel_xy: 0.4066
Metrics/base_velocity/error_vel_yaw: 0.5269
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 26064000
                    Iteration time: 1.08s
                      Time elapsed: 00:13:26
                               ETA: 00:14:25

################################################################################
                     [1m Learning iteration 724/1500 [0m                      

                       Computation: 32362 steps/s (collection: 1.062s, learning 0.051s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0309
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 13.6568
                       Mean reward: 16.24
               Mean episode length: 838.64
Episode_Reward/track_lin_vel_xy_exp: 0.9683
Episode_Reward/track_ang_vel_z_exp: 0.4299
       Episode_Reward/lin_vel_z_l2: -0.0431
      Episode_Reward/ang_vel_xy_l2: -0.1270
     Episode_Reward/dof_torques_l2: -0.0831
         Episode_Reward/dof_acc_l2: -0.1767
     Episode_Reward/action_rate_l2: -0.1613
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3240
Metrics/base_velocity/error_vel_xy: 0.3910
Metrics/base_velocity/error_vel_yaw: 0.5086
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 26100000
                    Iteration time: 1.11s
                      Time elapsed: 00:13:27
                               ETA: 00:14:23

################################################################################
                     [1m Learning iteration 725/1500 [0m                      

                       Computation: 32967 steps/s (collection: 1.041s, learning 0.051s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0276
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 13.6581
                       Mean reward: 15.36
               Mean episode length: 818.45
Episode_Reward/track_lin_vel_xy_exp: 0.9655
Episode_Reward/track_ang_vel_z_exp: 0.4353
       Episode_Reward/lin_vel_z_l2: -0.0433
      Episode_Reward/ang_vel_xy_l2: -0.1242
     Episode_Reward/dof_torques_l2: -0.0788
         Episode_Reward/dof_acc_l2: -0.1794
     Episode_Reward/action_rate_l2: -0.1604
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3252
Metrics/base_velocity/error_vel_xy: 0.3809
Metrics/base_velocity/error_vel_yaw: 0.4682
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 26136000
                    Iteration time: 1.09s
                      Time elapsed: 00:13:28
                               ETA: 00:14:22

################################################################################
                     [1m Learning iteration 726/1500 [0m                      

                       Computation: 33052 steps/s (collection: 1.037s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0269
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 13.6588
                       Mean reward: 14.84
               Mean episode length: 810.88
Episode_Reward/track_lin_vel_xy_exp: 0.9510
Episode_Reward/track_ang_vel_z_exp: 0.4436
       Episode_Reward/lin_vel_z_l2: -0.0466
      Episode_Reward/ang_vel_xy_l2: -0.1331
     Episode_Reward/dof_torques_l2: -0.0855
         Episode_Reward/dof_acc_l2: -0.1904
     Episode_Reward/action_rate_l2: -0.1679
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3300
Metrics/base_velocity/error_vel_xy: 0.4694
Metrics/base_velocity/error_vel_yaw: 0.5266
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 26172000
                    Iteration time: 1.09s
                      Time elapsed: 00:13:29
                               ETA: 00:14:21

################################################################################
                     [1m Learning iteration 727/1500 [0m                      

                       Computation: 31707 steps/s (collection: 1.084s, learning 0.051s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0248
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 13.6597
                       Mean reward: 15.59
               Mean episode length: 828.31
Episode_Reward/track_lin_vel_xy_exp: 1.0137
Episode_Reward/track_ang_vel_z_exp: 0.4648
       Episode_Reward/lin_vel_z_l2: -0.0405
      Episode_Reward/ang_vel_xy_l2: -0.1308
     Episode_Reward/dof_torques_l2: -0.0807
         Episode_Reward/dof_acc_l2: -0.1869
     Episode_Reward/action_rate_l2: -0.1677
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3370
Metrics/base_velocity/error_vel_xy: 0.4227
Metrics/base_velocity/error_vel_yaw: 0.5005
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 26208000
                    Iteration time: 1.14s
                      Time elapsed: 00:13:30
                               ETA: 00:14:20

################################################################################
                     [1m Learning iteration 728/1500 [0m                      

                       Computation: 32699 steps/s (collection: 1.047s, learning 0.054s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0263
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 13.6656
                       Mean reward: 16.67
               Mean episode length: 836.51
Episode_Reward/track_lin_vel_xy_exp: 0.9491
Episode_Reward/track_ang_vel_z_exp: 0.4196
       Episode_Reward/lin_vel_z_l2: -0.0425
      Episode_Reward/ang_vel_xy_l2: -0.1180
     Episode_Reward/dof_torques_l2: -0.0742
         Episode_Reward/dof_acc_l2: -0.1664
     Episode_Reward/action_rate_l2: -0.1521
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3319
Metrics/base_velocity/error_vel_xy: 0.3372
Metrics/base_velocity/error_vel_yaw: 0.4410
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 26244000
                    Iteration time: 1.10s
                      Time elapsed: 00:13:31
                               ETA: 00:14:19

################################################################################
                     [1m Learning iteration 729/1500 [0m                      

                       Computation: 32190 steps/s (collection: 1.066s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0280
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 13.6677
                       Mean reward: 16.88
               Mean episode length: 845.58
Episode_Reward/track_lin_vel_xy_exp: 1.0388
Episode_Reward/track_ang_vel_z_exp: 0.4764
       Episode_Reward/lin_vel_z_l2: -0.0436
      Episode_Reward/ang_vel_xy_l2: -0.1343
     Episode_Reward/dof_torques_l2: -0.0915
         Episode_Reward/dof_acc_l2: -0.2013
     Episode_Reward/action_rate_l2: -0.1791
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3288
Metrics/base_velocity/error_vel_xy: 0.4587
Metrics/base_velocity/error_vel_yaw: 0.5239
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 26280000
                    Iteration time: 1.12s
                      Time elapsed: 00:13:32
                               ETA: 00:14:18

################################################################################
                     [1m Learning iteration 730/1500 [0m                      

                       Computation: 32096 steps/s (collection: 1.071s, learning 0.051s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0292
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 13.6614
                       Mean reward: 16.92
               Mean episode length: 856.60
Episode_Reward/track_lin_vel_xy_exp: 1.0505
Episode_Reward/track_ang_vel_z_exp: 0.4571
       Episode_Reward/lin_vel_z_l2: -0.0403
      Episode_Reward/ang_vel_xy_l2: -0.1262
     Episode_Reward/dof_torques_l2: -0.0826
         Episode_Reward/dof_acc_l2: -0.1764
     Episode_Reward/action_rate_l2: -0.1648
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3351
Metrics/base_velocity/error_vel_xy: 0.3500
Metrics/base_velocity/error_vel_yaw: 0.4818
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 26316000
                    Iteration time: 1.12s
                      Time elapsed: 00:13:33
                               ETA: 00:14:17

################################################################################
                     [1m Learning iteration 731/1500 [0m                      

                       Computation: 31898 steps/s (collection: 1.078s, learning 0.051s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0256
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 13.6598
                       Mean reward: 16.66
               Mean episode length: 871.81
Episode_Reward/track_lin_vel_xy_exp: 1.0442
Episode_Reward/track_ang_vel_z_exp: 0.4821
       Episode_Reward/lin_vel_z_l2: -0.0518
      Episode_Reward/ang_vel_xy_l2: -0.1438
     Episode_Reward/dof_torques_l2: -0.0924
         Episode_Reward/dof_acc_l2: -0.2173
     Episode_Reward/action_rate_l2: -0.1841
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3368
Metrics/base_velocity/error_vel_xy: 0.4699
Metrics/base_velocity/error_vel_yaw: 0.5426
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 26352000
                    Iteration time: 1.13s
                      Time elapsed: 00:13:34
                               ETA: 00:14:16

################################################################################
                     [1m Learning iteration 732/1500 [0m                      

                       Computation: 33076 steps/s (collection: 1.036s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0244
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 13.6541
                       Mean reward: 17.19
               Mean episode length: 877.59
Episode_Reward/track_lin_vel_xy_exp: 1.1470
Episode_Reward/track_ang_vel_z_exp: 0.5049
       Episode_Reward/lin_vel_z_l2: -0.0495
      Episode_Reward/ang_vel_xy_l2: -0.1412
     Episode_Reward/dof_torques_l2: -0.0956
         Episode_Reward/dof_acc_l2: -0.2102
     Episode_Reward/action_rate_l2: -0.1875
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3301
Metrics/base_velocity/error_vel_xy: 0.4086
Metrics/base_velocity/error_vel_yaw: 0.5373
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 26388000
                    Iteration time: 1.09s
                      Time elapsed: 00:13:36
                               ETA: 00:14:15

################################################################################
                     [1m Learning iteration 733/1500 [0m                      

                       Computation: 33562 steps/s (collection: 1.020s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0272
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 13.6532
                       Mean reward: 15.71
               Mean episode length: 852.91
Episode_Reward/track_lin_vel_xy_exp: 0.8512
Episode_Reward/track_ang_vel_z_exp: 0.4072
       Episode_Reward/lin_vel_z_l2: -0.0455
      Episode_Reward/ang_vel_xy_l2: -0.1255
     Episode_Reward/dof_torques_l2: -0.0779
         Episode_Reward/dof_acc_l2: -0.1858
     Episode_Reward/action_rate_l2: -0.1561
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3269
Metrics/base_velocity/error_vel_xy: 0.4565
Metrics/base_velocity/error_vel_yaw: 0.4840
      Episode_Termination/time_out: 1.0417
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26424000
                    Iteration time: 1.07s
                      Time elapsed: 00:13:37
                               ETA: 00:14:13

################################################################################
                     [1m Learning iteration 734/1500 [0m                      

                       Computation: 33888 steps/s (collection: 1.011s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0283
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 13.6626
                       Mean reward: 14.34
               Mean episode length: 795.08
Episode_Reward/track_lin_vel_xy_exp: 0.9664
Episode_Reward/track_ang_vel_z_exp: 0.4379
       Episode_Reward/lin_vel_z_l2: -0.0459
      Episode_Reward/ang_vel_xy_l2: -0.1324
     Episode_Reward/dof_torques_l2: -0.0811
         Episode_Reward/dof_acc_l2: -0.1935
     Episode_Reward/action_rate_l2: -0.1652
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3249
Metrics/base_velocity/error_vel_xy: 0.4240
Metrics/base_velocity/error_vel_yaw: 0.5113
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 26460000
                    Iteration time: 1.06s
                      Time elapsed: 00:13:38
                               ETA: 00:14:12

################################################################################
                     [1m Learning iteration 735/1500 [0m                      

                       Computation: 32395 steps/s (collection: 1.061s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0266
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 13.6732
                       Mean reward: 15.22
               Mean episode length: 816.49
Episode_Reward/track_lin_vel_xy_exp: 0.9917
Episode_Reward/track_ang_vel_z_exp: 0.4375
       Episode_Reward/lin_vel_z_l2: -0.0448
      Episode_Reward/ang_vel_xy_l2: -0.1248
     Episode_Reward/dof_torques_l2: -0.0827
         Episode_Reward/dof_acc_l2: -0.1992
     Episode_Reward/action_rate_l2: -0.1661
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3059
Metrics/base_velocity/error_vel_xy: 0.3632
Metrics/base_velocity/error_vel_yaw: 0.4755
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 26496000
                    Iteration time: 1.11s
                      Time elapsed: 00:13:39
                               ETA: 00:14:11

################################################################################
                     [1m Learning iteration 736/1500 [0m                      

                       Computation: 32898 steps/s (collection: 1.043s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0278
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 13.6792
                       Mean reward: 15.83
               Mean episode length: 838.04
Episode_Reward/track_lin_vel_xy_exp: 0.9662
Episode_Reward/track_ang_vel_z_exp: 0.4407
       Episode_Reward/lin_vel_z_l2: -0.0452
      Episode_Reward/ang_vel_xy_l2: -0.1343
     Episode_Reward/dof_torques_l2: -0.0862
         Episode_Reward/dof_acc_l2: -0.1969
     Episode_Reward/action_rate_l2: -0.1692
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3078
Metrics/base_velocity/error_vel_xy: 0.4459
Metrics/base_velocity/error_vel_yaw: 0.5276
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 26532000
                    Iteration time: 1.09s
                      Time elapsed: 00:13:40
                               ETA: 00:14:10

################################################################################
                     [1m Learning iteration 737/1500 [0m                      

                       Computation: 32692 steps/s (collection: 1.049s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0258
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 13.6978
                       Mean reward: 15.85
               Mean episode length: 853.94
Episode_Reward/track_lin_vel_xy_exp: 0.9812
Episode_Reward/track_ang_vel_z_exp: 0.4535
       Episode_Reward/lin_vel_z_l2: -0.0498
      Episode_Reward/ang_vel_xy_l2: -0.1386
     Episode_Reward/dof_torques_l2: -0.0830
         Episode_Reward/dof_acc_l2: -0.2039
     Episode_Reward/action_rate_l2: -0.1738
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3068
Metrics/base_velocity/error_vel_xy: 0.4530
Metrics/base_velocity/error_vel_yaw: 0.5205
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 26568000
                    Iteration time: 1.10s
                      Time elapsed: 00:13:41
                               ETA: 00:14:09

################################################################################
                     [1m Learning iteration 738/1500 [0m                      

                       Computation: 33296 steps/s (collection: 1.030s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0291
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 13.7014
                       Mean reward: 16.33
               Mean episode length: 849.88
Episode_Reward/track_lin_vel_xy_exp: 1.0546
Episode_Reward/track_ang_vel_z_exp: 0.4751
       Episode_Reward/lin_vel_z_l2: -0.0458
      Episode_Reward/ang_vel_xy_l2: -0.1346
     Episode_Reward/dof_torques_l2: -0.0890
         Episode_Reward/dof_acc_l2: -0.2062
     Episode_Reward/action_rate_l2: -0.1789
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3097
Metrics/base_velocity/error_vel_xy: 0.4123
Metrics/base_velocity/error_vel_yaw: 0.5114
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 26604000
                    Iteration time: 1.08s
                      Time elapsed: 00:13:42
                               ETA: 00:14:08

################################################################################
                     [1m Learning iteration 739/1500 [0m                      

                       Computation: 34282 steps/s (collection: 0.997s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0269
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 13.6979
                       Mean reward: 16.85
               Mean episode length: 874.90
Episode_Reward/track_lin_vel_xy_exp: 1.0495
Episode_Reward/track_ang_vel_z_exp: 0.4730
       Episode_Reward/lin_vel_z_l2: -0.0506
      Episode_Reward/ang_vel_xy_l2: -0.1420
     Episode_Reward/dof_torques_l2: -0.0875
         Episode_Reward/dof_acc_l2: -0.2204
     Episode_Reward/action_rate_l2: -0.1809
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3192
Metrics/base_velocity/error_vel_xy: 0.4373
Metrics/base_velocity/error_vel_yaw: 0.5230
      Episode_Termination/time_out: 2.1667
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 26640000
                    Iteration time: 1.05s
                      Time elapsed: 00:13:43
                               ETA: 00:14:06

################################################################################
                     [1m Learning iteration 740/1500 [0m                      

                       Computation: 33670 steps/s (collection: 1.015s, learning 0.054s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0272
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 13.6899
                       Mean reward: 16.44
               Mean episode length: 869.30
Episode_Reward/track_lin_vel_xy_exp: 1.0165
Episode_Reward/track_ang_vel_z_exp: 0.4521
       Episode_Reward/lin_vel_z_l2: -0.0481
      Episode_Reward/ang_vel_xy_l2: -0.1350
     Episode_Reward/dof_torques_l2: -0.0851
         Episode_Reward/dof_acc_l2: -0.1991
     Episode_Reward/action_rate_l2: -0.1713
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3204
Metrics/base_velocity/error_vel_xy: 0.4075
Metrics/base_velocity/error_vel_yaw: 0.5043
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 26676000
                    Iteration time: 1.07s
                      Time elapsed: 00:13:44
                               ETA: 00:14:05

################################################################################
                     [1m Learning iteration 741/1500 [0m                      

                       Computation: 33825 steps/s (collection: 1.013s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0324
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 13.6975
                       Mean reward: 15.39
               Mean episode length: 835.03
Episode_Reward/track_lin_vel_xy_exp: 0.9046
Episode_Reward/track_ang_vel_z_exp: 0.4331
       Episode_Reward/lin_vel_z_l2: -0.0501
      Episode_Reward/ang_vel_xy_l2: -0.1325
     Episode_Reward/dof_torques_l2: -0.0847
         Episode_Reward/dof_acc_l2: -0.1973
     Episode_Reward/action_rate_l2: -0.1665
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3095
Metrics/base_velocity/error_vel_xy: 0.4938
Metrics/base_velocity/error_vel_yaw: 0.5197
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 26712000
                    Iteration time: 1.06s
                      Time elapsed: 00:13:45
                               ETA: 00:14:04

################################################################################
                     [1m Learning iteration 742/1500 [0m                      

                       Computation: 34201 steps/s (collection: 1.003s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0291
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 13.6946
                       Mean reward: 15.98
               Mean episode length: 844.12
Episode_Reward/track_lin_vel_xy_exp: 1.0507
Episode_Reward/track_ang_vel_z_exp: 0.4589
       Episode_Reward/lin_vel_z_l2: -0.0464
      Episode_Reward/ang_vel_xy_l2: -0.1304
     Episode_Reward/dof_torques_l2: -0.0826
         Episode_Reward/dof_acc_l2: -0.1864
     Episode_Reward/action_rate_l2: -0.1686
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3119
Metrics/base_velocity/error_vel_xy: 0.3571
Metrics/base_velocity/error_vel_yaw: 0.4943
      Episode_Termination/time_out: 2.2083
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 26748000
                    Iteration time: 1.05s
                      Time elapsed: 00:13:46
                               ETA: 00:14:03

################################################################################
                     [1m Learning iteration 743/1500 [0m                      

                       Computation: 34198 steps/s (collection: 1.001s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0270
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 13.6887
                       Mean reward: 16.79
               Mean episode length: 856.37
Episode_Reward/track_lin_vel_xy_exp: 1.0358
Episode_Reward/track_ang_vel_z_exp: 0.4518
       Episode_Reward/lin_vel_z_l2: -0.0485
      Episode_Reward/ang_vel_xy_l2: -0.1305
     Episode_Reward/dof_torques_l2: -0.0836
         Episode_Reward/dof_acc_l2: -0.1920
     Episode_Reward/action_rate_l2: -0.1675
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3231
Metrics/base_velocity/error_vel_xy: 0.3435
Metrics/base_velocity/error_vel_yaw: 0.4661
      Episode_Termination/time_out: 1.1250
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 26784000
                    Iteration time: 1.05s
                      Time elapsed: 00:13:47
                               ETA: 00:14:02

################################################################################
                     [1m Learning iteration 744/1500 [0m                      

                       Computation: 33984 steps/s (collection: 1.006s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0285
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.6750
                       Mean reward: 16.88
               Mean episode length: 840.99
Episode_Reward/track_lin_vel_xy_exp: 0.9502
Episode_Reward/track_ang_vel_z_exp: 0.4356
       Episode_Reward/lin_vel_z_l2: -0.0447
      Episode_Reward/ang_vel_xy_l2: -0.1274
     Episode_Reward/dof_torques_l2: -0.0809
         Episode_Reward/dof_acc_l2: -0.1864
     Episode_Reward/action_rate_l2: -0.1616
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3343
Metrics/base_velocity/error_vel_xy: 0.3946
Metrics/base_velocity/error_vel_yaw: 0.4615
      Episode_Termination/time_out: 1.1667
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 26820000
                    Iteration time: 1.06s
                      Time elapsed: 00:13:48
                               ETA: 00:14:01

################################################################################
                     [1m Learning iteration 745/1500 [0m                      

                       Computation: 34059 steps/s (collection: 1.003s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0267
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 13.6698
                       Mean reward: 15.64
               Mean episode length: 830.43
Episode_Reward/track_lin_vel_xy_exp: 1.0027
Episode_Reward/track_ang_vel_z_exp: 0.4551
       Episode_Reward/lin_vel_z_l2: -0.0463
      Episode_Reward/ang_vel_xy_l2: -0.1303
     Episode_Reward/dof_torques_l2: -0.0866
         Episode_Reward/dof_acc_l2: -0.1839
     Episode_Reward/action_rate_l2: -0.1688
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3440
Metrics/base_velocity/error_vel_xy: 0.4207
Metrics/base_velocity/error_vel_yaw: 0.4922
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 26856000
                    Iteration time: 1.06s
                      Time elapsed: 00:13:49
                               ETA: 00:13:59

################################################################################
                     [1m Learning iteration 746/1500 [0m                      

                       Computation: 32983 steps/s (collection: 1.037s, learning 0.055s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0249
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 13.6697
                       Mean reward: 15.24
               Mean episode length: 832.59
Episode_Reward/track_lin_vel_xy_exp: 0.9360
Episode_Reward/track_ang_vel_z_exp: 0.4270
       Episode_Reward/lin_vel_z_l2: -0.0480
      Episode_Reward/ang_vel_xy_l2: -0.1298
     Episode_Reward/dof_torques_l2: -0.0800
         Episode_Reward/dof_acc_l2: -0.1922
     Episode_Reward/action_rate_l2: -0.1634
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3461
Metrics/base_velocity/error_vel_xy: 0.4102
Metrics/base_velocity/error_vel_yaw: 0.4846
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 26892000
                    Iteration time: 1.09s
                      Time elapsed: 00:13:51
                               ETA: 00:13:58

################################################################################
                     [1m Learning iteration 747/1500 [0m                      

                       Computation: 33246 steps/s (collection: 1.032s, learning 0.051s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0299
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 13.6647
                       Mean reward: 14.88
               Mean episode length: 810.89
Episode_Reward/track_lin_vel_xy_exp: 0.8331
Episode_Reward/track_ang_vel_z_exp: 0.3742
       Episode_Reward/lin_vel_z_l2: -0.0429
      Episode_Reward/ang_vel_xy_l2: -0.1118
     Episode_Reward/dof_torques_l2: -0.0710
         Episode_Reward/dof_acc_l2: -0.1630
     Episode_Reward/action_rate_l2: -0.1417
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3499
Metrics/base_velocity/error_vel_xy: 0.3622
Metrics/base_velocity/error_vel_yaw: 0.4403
      Episode_Termination/time_out: 1.0417
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 26928000
                    Iteration time: 1.08s
                      Time elapsed: 00:13:52
                               ETA: 00:13:57

################################################################################
                     [1m Learning iteration 748/1500 [0m                      

                       Computation: 34279 steps/s (collection: 0.999s, learning 0.051s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0254
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 13.6579
                       Mean reward: 15.68
               Mean episode length: 839.69
Episode_Reward/track_lin_vel_xy_exp: 1.0160
Episode_Reward/track_ang_vel_z_exp: 0.4654
       Episode_Reward/lin_vel_z_l2: -0.0452
      Episode_Reward/ang_vel_xy_l2: -0.1365
     Episode_Reward/dof_torques_l2: -0.0881
         Episode_Reward/dof_acc_l2: -0.2061
     Episode_Reward/action_rate_l2: -0.1770
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3528
Metrics/base_velocity/error_vel_xy: 0.4402
Metrics/base_velocity/error_vel_yaw: 0.5201
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 26964000
                    Iteration time: 1.05s
                      Time elapsed: 00:13:53
                               ETA: 00:13:56

################################################################################
                     [1m Learning iteration 749/1500 [0m                      

                       Computation: 33568 steps/s (collection: 1.022s, learning 0.050s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0244
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 13.6635
                       Mean reward: 15.35
               Mean episode length: 845.44
Episode_Reward/track_lin_vel_xy_exp: 0.9957
Episode_Reward/track_ang_vel_z_exp: 0.4619
       Episode_Reward/lin_vel_z_l2: -0.0417
      Episode_Reward/ang_vel_xy_l2: -0.1341
     Episode_Reward/dof_torques_l2: -0.0867
         Episode_Reward/dof_acc_l2: -0.1910
     Episode_Reward/action_rate_l2: -0.1736
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3507
Metrics/base_velocity/error_vel_xy: 0.4780
Metrics/base_velocity/error_vel_yaw: 0.5417
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 27000000
                    Iteration time: 1.07s
                      Time elapsed: 00:13:54
                               ETA: 00:13:55

################################################################################
                     [1m Learning iteration 750/1500 [0m                      

                       Computation: 34098 steps/s (collection: 1.006s, learning 0.050s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0230
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 13.6603
                       Mean reward: 16.41
               Mean episode length: 856.08
Episode_Reward/track_lin_vel_xy_exp: 1.0585
Episode_Reward/track_ang_vel_z_exp: 0.4710
       Episode_Reward/lin_vel_z_l2: -0.0447
      Episode_Reward/ang_vel_xy_l2: -0.1339
     Episode_Reward/dof_torques_l2: -0.0859
         Episode_Reward/dof_acc_l2: -0.1965
     Episode_Reward/action_rate_l2: -0.1724
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3444
Metrics/base_velocity/error_vel_xy: 0.3974
Metrics/base_velocity/error_vel_yaw: 0.4997
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 27036000
                    Iteration time: 1.06s
                      Time elapsed: 00:13:55
                               ETA: 00:13:54

################################################################################
                     [1m Learning iteration 751/1500 [0m                      

                       Computation: 34012 steps/s (collection: 1.007s, learning 0.051s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0290
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 13.6638
                       Mean reward: 16.79
               Mean episode length: 874.72
Episode_Reward/track_lin_vel_xy_exp: 1.0099
Episode_Reward/track_ang_vel_z_exp: 0.4670
       Episode_Reward/lin_vel_z_l2: -0.0467
      Episode_Reward/ang_vel_xy_l2: -0.1373
     Episode_Reward/dof_torques_l2: -0.0897
         Episode_Reward/dof_acc_l2: -0.1948
     Episode_Reward/action_rate_l2: -0.1755
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3501
Metrics/base_velocity/error_vel_xy: 0.4493
Metrics/base_velocity/error_vel_yaw: 0.5208
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 27072000
                    Iteration time: 1.06s
                      Time elapsed: 00:13:56
                               ETA: 00:13:53

################################################################################
                     [1m Learning iteration 752/1500 [0m                      

                       Computation: 34185 steps/s (collection: 1.002s, learning 0.051s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0207
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 13.6663
                       Mean reward: 16.39
               Mean episode length: 863.30
Episode_Reward/track_lin_vel_xy_exp: 0.8867
Episode_Reward/track_ang_vel_z_exp: 0.4136
       Episode_Reward/lin_vel_z_l2: -0.0421
      Episode_Reward/ang_vel_xy_l2: -0.1229
     Episode_Reward/dof_torques_l2: -0.0778
         Episode_Reward/dof_acc_l2: -0.1781
     Episode_Reward/action_rate_l2: -0.1547
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3524
Metrics/base_velocity/error_vel_xy: 0.4129
Metrics/base_velocity/error_vel_yaw: 0.4609
      Episode_Termination/time_out: 1.0833
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 27108000
                    Iteration time: 1.05s
                      Time elapsed: 00:13:57
                               ETA: 00:13:51

################################################################################
                     [1m Learning iteration 753/1500 [0m                      

                       Computation: 32430 steps/s (collection: 1.058s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0268
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 13.6607
                       Mean reward: 16.45
               Mean episode length: 853.78
Episode_Reward/track_lin_vel_xy_exp: 1.0289
Episode_Reward/track_ang_vel_z_exp: 0.4524
       Episode_Reward/lin_vel_z_l2: -0.0444
      Episode_Reward/ang_vel_xy_l2: -0.1276
     Episode_Reward/dof_torques_l2: -0.0816
         Episode_Reward/dof_acc_l2: -0.1838
     Episode_Reward/action_rate_l2: -0.1667
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3398
Metrics/base_velocity/error_vel_xy: 0.3710
Metrics/base_velocity/error_vel_yaw: 0.4939
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 27144000
                    Iteration time: 1.11s
                      Time elapsed: 00:13:58
                               ETA: 00:13:50

################################################################################
                     [1m Learning iteration 754/1500 [0m                      

                       Computation: 33384 steps/s (collection: 1.028s, learning 0.051s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0266
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 13.6662
                       Mean reward: 16.86
               Mean episode length: 866.63
Episode_Reward/track_lin_vel_xy_exp: 1.0059
Episode_Reward/track_ang_vel_z_exp: 0.4535
       Episode_Reward/lin_vel_z_l2: -0.0474
      Episode_Reward/ang_vel_xy_l2: -0.1353
     Episode_Reward/dof_torques_l2: -0.0873
         Episode_Reward/dof_acc_l2: -0.2046
     Episode_Reward/action_rate_l2: -0.1754
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3396
Metrics/base_velocity/error_vel_xy: 0.4327
Metrics/base_velocity/error_vel_yaw: 0.5443
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 27180000
                    Iteration time: 1.08s
                      Time elapsed: 00:13:59
                               ETA: 00:13:49

################################################################################
                     [1m Learning iteration 755/1500 [0m                      

                       Computation: 32988 steps/s (collection: 1.039s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0276
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 13.6682
                       Mean reward: 16.32
               Mean episode length: 880.60
Episode_Reward/track_lin_vel_xy_exp: 0.9825
Episode_Reward/track_ang_vel_z_exp: 0.4586
       Episode_Reward/lin_vel_z_l2: -0.0486
      Episode_Reward/ang_vel_xy_l2: -0.1390
     Episode_Reward/dof_torques_l2: -0.0868
         Episode_Reward/dof_acc_l2: -0.1973
     Episode_Reward/action_rate_l2: -0.1767
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3474
Metrics/base_velocity/error_vel_xy: 0.4930
Metrics/base_velocity/error_vel_yaw: 0.5447
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 27216000
                    Iteration time: 1.09s
                      Time elapsed: 00:14:00
                               ETA: 00:13:48

################################################################################
                     [1m Learning iteration 756/1500 [0m                      

                       Computation: 33205 steps/s (collection: 1.033s, learning 0.051s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0233
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 13.6668
                       Mean reward: 16.79
               Mean episode length: 888.99
Episode_Reward/track_lin_vel_xy_exp: 0.9839
Episode_Reward/track_ang_vel_z_exp: 0.4304
       Episode_Reward/lin_vel_z_l2: -0.0396
      Episode_Reward/ang_vel_xy_l2: -0.1201
     Episode_Reward/dof_torques_l2: -0.0774
         Episode_Reward/dof_acc_l2: -0.1736
     Episode_Reward/action_rate_l2: -0.1573
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3561
Metrics/base_velocity/error_vel_xy: 0.3238
Metrics/base_velocity/error_vel_yaw: 0.4407
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 27252000
                    Iteration time: 1.08s
                      Time elapsed: 00:14:01
                               ETA: 00:13:47

################################################################################
                     [1m Learning iteration 757/1500 [0m                      

                       Computation: 33123 steps/s (collection: 1.033s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0247
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 13.6697
                       Mean reward: 16.17
               Mean episode length: 859.06
Episode_Reward/track_lin_vel_xy_exp: 0.9988
Episode_Reward/track_ang_vel_z_exp: 0.4552
       Episode_Reward/lin_vel_z_l2: -0.0482
      Episode_Reward/ang_vel_xy_l2: -0.1339
     Episode_Reward/dof_torques_l2: -0.0882
         Episode_Reward/dof_acc_l2: -0.2094
     Episode_Reward/action_rate_l2: -0.1779
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3598
Metrics/base_velocity/error_vel_xy: 0.4354
Metrics/base_velocity/error_vel_yaw: 0.5034
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 27288000
                    Iteration time: 1.09s
                      Time elapsed: 00:14:02
                               ETA: 00:13:46

################################################################################
                     [1m Learning iteration 758/1500 [0m                      

                       Computation: 32708 steps/s (collection: 1.046s, learning 0.055s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0246
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 13.6740
                       Mean reward: 16.34
               Mean episode length: 883.39
Episode_Reward/track_lin_vel_xy_exp: 1.0412
Episode_Reward/track_ang_vel_z_exp: 0.4863
       Episode_Reward/lin_vel_z_l2: -0.0528
      Episode_Reward/ang_vel_xy_l2: -0.1470
     Episode_Reward/dof_torques_l2: -0.0918
         Episode_Reward/dof_acc_l2: -0.2242
     Episode_Reward/action_rate_l2: -0.1870
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3606
Metrics/base_velocity/error_vel_xy: 0.4870
Metrics/base_velocity/error_vel_yaw: 0.5431
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 27324000
                    Iteration time: 1.10s
                      Time elapsed: 00:14:03
                               ETA: 00:13:45

################################################################################
                     [1m Learning iteration 759/1500 [0m                      

                       Computation: 31042 steps/s (collection: 1.107s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0261
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 13.6676
                       Mean reward: 15.39
               Mean episode length: 864.66
Episode_Reward/track_lin_vel_xy_exp: 0.8594
Episode_Reward/track_ang_vel_z_exp: 0.4099
       Episode_Reward/lin_vel_z_l2: -0.0508
      Episode_Reward/ang_vel_xy_l2: -0.1296
     Episode_Reward/dof_torques_l2: -0.0781
         Episode_Reward/dof_acc_l2: -0.1869
     Episode_Reward/action_rate_l2: -0.1577
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3598
Metrics/base_velocity/error_vel_xy: 0.4496
Metrics/base_velocity/error_vel_yaw: 0.4885
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 27360000
                    Iteration time: 1.16s
                      Time elapsed: 00:14:05
                               ETA: 00:13:44

################################################################################
                     [1m Learning iteration 760/1500 [0m                      

                       Computation: 33841 steps/s (collection: 1.011s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0258
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 13.6651
                       Mean reward: 14.66
               Mean episode length: 830.73
Episode_Reward/track_lin_vel_xy_exp: 0.9311
Episode_Reward/track_ang_vel_z_exp: 0.4309
       Episode_Reward/lin_vel_z_l2: -0.0424
      Episode_Reward/ang_vel_xy_l2: -0.1298
     Episode_Reward/dof_torques_l2: -0.0848
         Episode_Reward/dof_acc_l2: -0.1818
     Episode_Reward/action_rate_l2: -0.1639
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3580
Metrics/base_velocity/error_vel_xy: 0.4312
Metrics/base_velocity/error_vel_yaw: 0.4977
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 27396000
                    Iteration time: 1.06s
                      Time elapsed: 00:14:06
                               ETA: 00:13:42

################################################################################
                     [1m Learning iteration 761/1500 [0m                      

                       Computation: 33710 steps/s (collection: 1.016s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0252
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 13.6640
                       Mean reward: 15.07
               Mean episode length: 824.82
Episode_Reward/track_lin_vel_xy_exp: 1.0488
Episode_Reward/track_ang_vel_z_exp: 0.4713
       Episode_Reward/lin_vel_z_l2: -0.0496
      Episode_Reward/ang_vel_xy_l2: -0.1388
     Episode_Reward/dof_torques_l2: -0.0901
         Episode_Reward/dof_acc_l2: -0.2109
     Episode_Reward/action_rate_l2: -0.1780
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3599
Metrics/base_velocity/error_vel_xy: 0.4205
Metrics/base_velocity/error_vel_yaw: 0.5285
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 27432000
                    Iteration time: 1.07s
                      Time elapsed: 00:14:07
                               ETA: 00:13:41

################################################################################
                     [1m Learning iteration 762/1500 [0m                      

                       Computation: 32191 steps/s (collection: 1.066s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0267
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 13.6539
                       Mean reward: 16.95
               Mean episode length: 882.11
Episode_Reward/track_lin_vel_xy_exp: 1.1086
Episode_Reward/track_ang_vel_z_exp: 0.4893
       Episode_Reward/lin_vel_z_l2: -0.0496
      Episode_Reward/ang_vel_xy_l2: -0.1449
     Episode_Reward/dof_torques_l2: -0.0890
         Episode_Reward/dof_acc_l2: -0.2154
     Episode_Reward/action_rate_l2: -0.1839
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3645
Metrics/base_velocity/error_vel_xy: 0.3998
Metrics/base_velocity/error_vel_yaw: 0.5256
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 27468000
                    Iteration time: 1.12s
                      Time elapsed: 00:14:08
                               ETA: 00:13:40

################################################################################
                     [1m Learning iteration 763/1500 [0m                      

                       Computation: 32875 steps/s (collection: 1.043s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0241
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 13.6292
                       Mean reward: 17.41
               Mean episode length: 878.12
Episode_Reward/track_lin_vel_xy_exp: 1.0345
Episode_Reward/track_ang_vel_z_exp: 0.4689
       Episode_Reward/lin_vel_z_l2: -0.0470
      Episode_Reward/ang_vel_xy_l2: -0.1332
     Episode_Reward/dof_torques_l2: -0.0883
         Episode_Reward/dof_acc_l2: -0.1832
     Episode_Reward/action_rate_l2: -0.1715
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3661
Metrics/base_velocity/error_vel_xy: 0.4206
Metrics/base_velocity/error_vel_yaw: 0.4934
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 27504000
                    Iteration time: 1.10s
                      Time elapsed: 00:14:09
                               ETA: 00:13:39

################################################################################
                     [1m Learning iteration 764/1500 [0m                      

                       Computation: 32146 steps/s (collection: 1.067s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0252
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 13.6229
                       Mean reward: 18.02
               Mean episode length: 883.79
Episode_Reward/track_lin_vel_xy_exp: 1.0854
Episode_Reward/track_ang_vel_z_exp: 0.4865
       Episode_Reward/lin_vel_z_l2: -0.0489
      Episode_Reward/ang_vel_xy_l2: -0.1420
     Episode_Reward/dof_torques_l2: -0.0885
         Episode_Reward/dof_acc_l2: -0.2009
     Episode_Reward/action_rate_l2: -0.1795
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3675
Metrics/base_velocity/error_vel_xy: 0.4139
Metrics/base_velocity/error_vel_yaw: 0.5144
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 27540000
                    Iteration time: 1.12s
                      Time elapsed: 00:14:10
                               ETA: 00:13:38

################################################################################
                     [1m Learning iteration 765/1500 [0m                      

                       Computation: 29926 steps/s (collection: 1.148s, learning 0.055s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0223
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 13.6238
                       Mean reward: 17.80
               Mean episode length: 864.04
Episode_Reward/track_lin_vel_xy_exp: 0.9804
Episode_Reward/track_ang_vel_z_exp: 0.4164
       Episode_Reward/lin_vel_z_l2: -0.0408
      Episode_Reward/ang_vel_xy_l2: -0.1178
     Episode_Reward/dof_torques_l2: -0.0706
         Episode_Reward/dof_acc_l2: -0.1732
     Episode_Reward/action_rate_l2: -0.1510
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3781
Metrics/base_velocity/error_vel_xy: 0.2705
Metrics/base_velocity/error_vel_yaw: 0.4129
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 27576000
                    Iteration time: 1.20s
                      Time elapsed: 00:14:11
                               ETA: 00:13:37

################################################################################
                     [1m Learning iteration 766/1500 [0m                      

                       Computation: 30510 steps/s (collection: 1.127s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0255
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 13.6285
                       Mean reward: 17.14
               Mean episode length: 865.61
Episode_Reward/track_lin_vel_xy_exp: 1.0878
Episode_Reward/track_ang_vel_z_exp: 0.4916
       Episode_Reward/lin_vel_z_l2: -0.0483
      Episode_Reward/ang_vel_xy_l2: -0.1409
     Episode_Reward/dof_torques_l2: -0.0936
         Episode_Reward/dof_acc_l2: -0.2156
     Episode_Reward/action_rate_l2: -0.1851
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3858
Metrics/base_velocity/error_vel_xy: 0.4512
Metrics/base_velocity/error_vel_yaw: 0.5434
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 27612000
                    Iteration time: 1.18s
                      Time elapsed: 00:14:12
                               ETA: 00:13:36

################################################################################
                     [1m Learning iteration 767/1500 [0m                      

                       Computation: 32157 steps/s (collection: 1.067s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0215
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 13.6229
                       Mean reward: 16.53
               Mean episode length: 877.64
Episode_Reward/track_lin_vel_xy_exp: 1.0985
Episode_Reward/track_ang_vel_z_exp: 0.5020
       Episode_Reward/lin_vel_z_l2: -0.0494
      Episode_Reward/ang_vel_xy_l2: -0.1485
     Episode_Reward/dof_torques_l2: -0.0938
         Episode_Reward/dof_acc_l2: -0.2170
     Episode_Reward/action_rate_l2: -0.1894
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3826
Metrics/base_velocity/error_vel_xy: 0.4873
Metrics/base_velocity/error_vel_yaw: 0.5629
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 27648000
                    Iteration time: 1.12s
                      Time elapsed: 00:14:14
                               ETA: 00:13:35

################################################################################
                     [1m Learning iteration 768/1500 [0m                      

                       Computation: 31426 steps/s (collection: 1.094s, learning 0.051s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0273
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.6205
                       Mean reward: 16.47
               Mean episode length: 857.44
Episode_Reward/track_lin_vel_xy_exp: 0.9381
Episode_Reward/track_ang_vel_z_exp: 0.4321
       Episode_Reward/lin_vel_z_l2: -0.0420
      Episode_Reward/ang_vel_xy_l2: -0.1250
     Episode_Reward/dof_torques_l2: -0.0810
         Episode_Reward/dof_acc_l2: -0.1774
     Episode_Reward/action_rate_l2: -0.1606
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3789
Metrics/base_velocity/error_vel_xy: 0.4106
Metrics/base_velocity/error_vel_yaw: 0.4660
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 27684000
                    Iteration time: 1.15s
                      Time elapsed: 00:14:15
                               ETA: 00:13:34

################################################################################
                     [1m Learning iteration 769/1500 [0m                      

                       Computation: 32467 steps/s (collection: 1.056s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0248
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 13.6124
                       Mean reward: 15.77
               Mean episode length: 831.36
Episode_Reward/track_lin_vel_xy_exp: 0.9596
Episode_Reward/track_ang_vel_z_exp: 0.4323
       Episode_Reward/lin_vel_z_l2: -0.0512
      Episode_Reward/ang_vel_xy_l2: -0.1322
     Episode_Reward/dof_torques_l2: -0.0803
         Episode_Reward/dof_acc_l2: -0.2054
     Episode_Reward/action_rate_l2: -0.1641
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3722
Metrics/base_velocity/error_vel_xy: 0.4101
Metrics/base_velocity/error_vel_yaw: 0.5010
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 27720000
                    Iteration time: 1.11s
                      Time elapsed: 00:14:16
                               ETA: 00:13:32

################################################################################
                     [1m Learning iteration 770/1500 [0m                      

                       Computation: 32038 steps/s (collection: 1.070s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0257
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 13.6140
                       Mean reward: 16.85
               Mean episode length: 848.87
Episode_Reward/track_lin_vel_xy_exp: 1.1014
Episode_Reward/track_ang_vel_z_exp: 0.4997
       Episode_Reward/lin_vel_z_l2: -0.0489
      Episode_Reward/ang_vel_xy_l2: -0.1479
     Episode_Reward/dof_torques_l2: -0.0915
         Episode_Reward/dof_acc_l2: -0.2216
     Episode_Reward/action_rate_l2: -0.1882
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3792
Metrics/base_velocity/error_vel_xy: 0.4590
Metrics/base_velocity/error_vel_yaw: 0.5354
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 27756000
                    Iteration time: 1.12s
                      Time elapsed: 00:14:17
                               ETA: 00:13:31

################################################################################
                     [1m Learning iteration 771/1500 [0m                      

                       Computation: 32049 steps/s (collection: 1.072s, learning 0.051s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0226
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 13.6077
                       Mean reward: 17.19
               Mean episode length: 851.31
Episode_Reward/track_lin_vel_xy_exp: 0.9901
Episode_Reward/track_ang_vel_z_exp: 0.4535
       Episode_Reward/lin_vel_z_l2: -0.0434
      Episode_Reward/ang_vel_xy_l2: -0.1296
     Episode_Reward/dof_torques_l2: -0.0857
         Episode_Reward/dof_acc_l2: -0.1856
     Episode_Reward/action_rate_l2: -0.1674
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3890
Metrics/base_velocity/error_vel_xy: 0.4079
Metrics/base_velocity/error_vel_yaw: 0.4759
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 27792000
                    Iteration time: 1.12s
                      Time elapsed: 00:14:18
                               ETA: 00:13:30

################################################################################
                     [1m Learning iteration 772/1500 [0m                      

                       Computation: 32553 steps/s (collection: 1.043s, learning 0.063s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0254
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 13.6073
                       Mean reward: 17.20
               Mean episode length: 869.59
Episode_Reward/track_lin_vel_xy_exp: 1.0858
Episode_Reward/track_ang_vel_z_exp: 0.4935
       Episode_Reward/lin_vel_z_l2: -0.0507
      Episode_Reward/ang_vel_xy_l2: -0.1453
     Episode_Reward/dof_torques_l2: -0.0935
         Episode_Reward/dof_acc_l2: -0.2212
     Episode_Reward/action_rate_l2: -0.1876
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3879
Metrics/base_velocity/error_vel_xy: 0.4511
Metrics/base_velocity/error_vel_yaw: 0.5379
      Episode_Termination/time_out: 1.9583
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 27828000
                    Iteration time: 1.11s
                      Time elapsed: 00:14:19
                               ETA: 00:13:29

################################################################################
                     [1m Learning iteration 773/1500 [0m                      

                       Computation: 32159 steps/s (collection: 1.066s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0240
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 13.6167
                       Mean reward: 16.07
               Mean episode length: 847.66
Episode_Reward/track_lin_vel_xy_exp: 0.8802
Episode_Reward/track_ang_vel_z_exp: 0.4010
       Episode_Reward/lin_vel_z_l2: -0.0445
      Episode_Reward/ang_vel_xy_l2: -0.1149
     Episode_Reward/dof_torques_l2: -0.0806
         Episode_Reward/dof_acc_l2: -0.1627
     Episode_Reward/action_rate_l2: -0.1504
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3874
Metrics/base_velocity/error_vel_xy: 0.3565
Metrics/base_velocity/error_vel_yaw: 0.4268
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 27864000
                    Iteration time: 1.12s
                      Time elapsed: 00:14:20
                               ETA: 00:13:28

################################################################################
                     [1m Learning iteration 774/1500 [0m                      

                       Computation: 31782 steps/s (collection: 1.081s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0257
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 13.6289
                       Mean reward: 15.80
               Mean episode length: 829.27
Episode_Reward/track_lin_vel_xy_exp: 0.9986
Episode_Reward/track_ang_vel_z_exp: 0.4570
       Episode_Reward/lin_vel_z_l2: -0.0470
      Episode_Reward/ang_vel_xy_l2: -0.1312
     Episode_Reward/dof_torques_l2: -0.0885
         Episode_Reward/dof_acc_l2: -0.2057
     Episode_Reward/action_rate_l2: -0.1740
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.3936
Metrics/base_velocity/error_vel_xy: 0.4340
Metrics/base_velocity/error_vel_yaw: 0.5072
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 27900000
                    Iteration time: 1.13s
                      Time elapsed: 00:14:21
                               ETA: 00:13:27

################################################################################
                     [1m Learning iteration 775/1500 [0m                      

                       Computation: 31999 steps/s (collection: 1.072s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0259
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 13.6252
                       Mean reward: 15.56
               Mean episode length: 839.68
Episode_Reward/track_lin_vel_xy_exp: 0.9846
Episode_Reward/track_ang_vel_z_exp: 0.4589
       Episode_Reward/lin_vel_z_l2: -0.0482
      Episode_Reward/ang_vel_xy_l2: -0.1376
     Episode_Reward/dof_torques_l2: -0.0899
         Episode_Reward/dof_acc_l2: -0.2125
     Episode_Reward/action_rate_l2: -0.1782
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4023
Metrics/base_velocity/error_vel_xy: 0.4588
Metrics/base_velocity/error_vel_yaw: 0.5207
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 27936000
                    Iteration time: 1.13s
                      Time elapsed: 00:14:23
                               ETA: 00:13:26

################################################################################
                     [1m Learning iteration 776/1500 [0m                      

                       Computation: 32847 steps/s (collection: 1.043s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0242
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 13.6235
                       Mean reward: 15.58
               Mean episode length: 870.60
Episode_Reward/track_lin_vel_xy_exp: 0.9697
Episode_Reward/track_ang_vel_z_exp: 0.4552
       Episode_Reward/lin_vel_z_l2: -0.0511
      Episode_Reward/ang_vel_xy_l2: -0.1408
     Episode_Reward/dof_torques_l2: -0.0870
         Episode_Reward/dof_acc_l2: -0.2104
     Episode_Reward/action_rate_l2: -0.1758
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4107
Metrics/base_velocity/error_vel_xy: 0.4828
Metrics/base_velocity/error_vel_yaw: 0.5130
      Episode_Termination/time_out: 2.0417
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 27972000
                    Iteration time: 1.10s
                      Time elapsed: 00:14:24
                               ETA: 00:13:25

################################################################################
                     [1m Learning iteration 777/1500 [0m                      

                       Computation: 32333 steps/s (collection: 1.062s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0278
               Mean surrogate loss: -0.0149
                 Mean entropy loss: 13.6191
                       Mean reward: 16.52
               Mean episode length: 880.10
Episode_Reward/track_lin_vel_xy_exp: 1.0919
Episode_Reward/track_ang_vel_z_exp: 0.4832
       Episode_Reward/lin_vel_z_l2: -0.0479
      Episode_Reward/ang_vel_xy_l2: -0.1367
     Episode_Reward/dof_torques_l2: -0.0898
         Episode_Reward/dof_acc_l2: -0.2021
     Episode_Reward/action_rate_l2: -0.1785
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4107
Metrics/base_velocity/error_vel_xy: 0.3917
Metrics/base_velocity/error_vel_yaw: 0.5093
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 28008000
                    Iteration time: 1.11s
                      Time elapsed: 00:14:25
                               ETA: 00:13:24

################################################################################
                     [1m Learning iteration 778/1500 [0m                      

                       Computation: 32623 steps/s (collection: 1.051s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0224
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 13.6214
                       Mean reward: 15.99
               Mean episode length: 845.43
Episode_Reward/track_lin_vel_xy_exp: 0.9319
Episode_Reward/track_ang_vel_z_exp: 0.4289
       Episode_Reward/lin_vel_z_l2: -0.0482
      Episode_Reward/ang_vel_xy_l2: -0.1290
     Episode_Reward/dof_torques_l2: -0.0857
         Episode_Reward/dof_acc_l2: -0.1915
     Episode_Reward/action_rate_l2: -0.1649
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4131
Metrics/base_velocity/error_vel_xy: 0.4289
Metrics/base_velocity/error_vel_yaw: 0.5100
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 28044000
                    Iteration time: 1.10s
                      Time elapsed: 00:14:26
                               ETA: 00:13:23

################################################################################
                     [1m Learning iteration 779/1500 [0m                      

                       Computation: 32351 steps/s (collection: 1.060s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0280
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 13.6226
                       Mean reward: 16.34
               Mean episode length: 851.32
Episode_Reward/track_lin_vel_xy_exp: 1.1091
Episode_Reward/track_ang_vel_z_exp: 0.4830
       Episode_Reward/lin_vel_z_l2: -0.0468
      Episode_Reward/ang_vel_xy_l2: -0.1363
     Episode_Reward/dof_torques_l2: -0.0836
         Episode_Reward/dof_acc_l2: -0.1986
     Episode_Reward/action_rate_l2: -0.1758
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4131
Metrics/base_velocity/error_vel_xy: 0.3548
Metrics/base_velocity/error_vel_yaw: 0.4947
      Episode_Termination/time_out: 2.2083
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 28080000
                    Iteration time: 1.11s
                      Time elapsed: 00:14:27
                               ETA: 00:13:21

################################################################################
                     [1m Learning iteration 780/1500 [0m                      

                       Computation: 32297 steps/s (collection: 1.065s, learning 0.050s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0258
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 13.6264
                       Mean reward: 17.61
               Mean episode length: 881.43
Episode_Reward/track_lin_vel_xy_exp: 1.0224
Episode_Reward/track_ang_vel_z_exp: 0.4697
       Episode_Reward/lin_vel_z_l2: -0.0477
      Episode_Reward/ang_vel_xy_l2: -0.1356
     Episode_Reward/dof_torques_l2: -0.0870
         Episode_Reward/dof_acc_l2: -0.1919
     Episode_Reward/action_rate_l2: -0.1738
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4132
Metrics/base_velocity/error_vel_xy: 0.4343
Metrics/base_velocity/error_vel_yaw: 0.5040
      Episode_Termination/time_out: 2.0000
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 28116000
                    Iteration time: 1.11s
                      Time elapsed: 00:14:28
                               ETA: 00:13:20

################################################################################
                     [1m Learning iteration 781/1500 [0m                      

                       Computation: 31995 steps/s (collection: 1.073s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0261
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 13.6287
                       Mean reward: 17.28
               Mean episode length: 862.93
Episode_Reward/track_lin_vel_xy_exp: 1.0326
Episode_Reward/track_ang_vel_z_exp: 0.4540
       Episode_Reward/lin_vel_z_l2: -0.0451
      Episode_Reward/ang_vel_xy_l2: -0.1281
     Episode_Reward/dof_torques_l2: -0.0810
         Episode_Reward/dof_acc_l2: -0.1939
     Episode_Reward/action_rate_l2: -0.1670
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4161
Metrics/base_velocity/error_vel_xy: 0.3536
Metrics/base_velocity/error_vel_yaw: 0.4766
      Episode_Termination/time_out: 2.0417
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 28152000
                    Iteration time: 1.13s
                      Time elapsed: 00:14:29
                               ETA: 00:13:19

################################################################################
                     [1m Learning iteration 782/1500 [0m                      

                       Computation: 32060 steps/s (collection: 1.067s, learning 0.055s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0304
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 13.6307
                       Mean reward: 17.14
               Mean episode length: 853.98
Episode_Reward/track_lin_vel_xy_exp: 1.0958
Episode_Reward/track_ang_vel_z_exp: 0.4860
       Episode_Reward/lin_vel_z_l2: -0.0427
      Episode_Reward/ang_vel_xy_l2: -0.1335
     Episode_Reward/dof_torques_l2: -0.0891
         Episode_Reward/dof_acc_l2: -0.1915
     Episode_Reward/action_rate_l2: -0.1757
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4134
Metrics/base_velocity/error_vel_xy: 0.3923
Metrics/base_velocity/error_vel_yaw: 0.5117
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 28188000
                    Iteration time: 1.12s
                      Time elapsed: 00:14:30
                               ETA: 00:13:18

################################################################################
                     [1m Learning iteration 783/1500 [0m                      

                       Computation: 32482 steps/s (collection: 1.055s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0262
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 13.6358
                       Mean reward: 17.38
               Mean episode length: 888.95
Episode_Reward/track_lin_vel_xy_exp: 1.0137
Episode_Reward/track_ang_vel_z_exp: 0.4572
       Episode_Reward/lin_vel_z_l2: -0.0561
      Episode_Reward/ang_vel_xy_l2: -0.1423
     Episode_Reward/dof_torques_l2: -0.0881
         Episode_Reward/dof_acc_l2: -0.2094
     Episode_Reward/action_rate_l2: -0.1757
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4218
Metrics/base_velocity/error_vel_xy: 0.4435
Metrics/base_velocity/error_vel_yaw: 0.5532
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 28224000
                    Iteration time: 1.11s
                      Time elapsed: 00:14:31
                               ETA: 00:13:17

################################################################################
                     [1m Learning iteration 784/1500 [0m                      

                       Computation: 32806 steps/s (collection: 1.044s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0241
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 13.6402
                       Mean reward: 18.16
               Mean episode length: 914.03
Episode_Reward/track_lin_vel_xy_exp: 1.1018
Episode_Reward/track_ang_vel_z_exp: 0.4891
       Episode_Reward/lin_vel_z_l2: -0.0467
      Episode_Reward/ang_vel_xy_l2: -0.1366
     Episode_Reward/dof_torques_l2: -0.0899
         Episode_Reward/dof_acc_l2: -0.2090
     Episode_Reward/action_rate_l2: -0.1813
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4278
Metrics/base_velocity/error_vel_xy: 0.3969
Metrics/base_velocity/error_vel_yaw: 0.5234
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 28260000
                    Iteration time: 1.10s
                      Time elapsed: 00:14:33
                               ETA: 00:13:16

################################################################################
                     [1m Learning iteration 785/1500 [0m                      

                       Computation: 32726 steps/s (collection: 1.046s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0284
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 13.6336
                       Mean reward: 17.30
               Mean episode length: 876.05
Episode_Reward/track_lin_vel_xy_exp: 1.0242
Episode_Reward/track_ang_vel_z_exp: 0.4555
       Episode_Reward/lin_vel_z_l2: -0.0495
      Episode_Reward/ang_vel_xy_l2: -0.1285
     Episode_Reward/dof_torques_l2: -0.0887
         Episode_Reward/dof_acc_l2: -0.1860
     Episode_Reward/action_rate_l2: -0.1677
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4261
Metrics/base_velocity/error_vel_xy: 0.3666
Metrics/base_velocity/error_vel_yaw: 0.4660
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 28296000
                    Iteration time: 1.10s
                      Time elapsed: 00:14:34
                               ETA: 00:13:15

################################################################################
                     [1m Learning iteration 786/1500 [0m                      

                       Computation: 31071 steps/s (collection: 1.108s, learning 0.051s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0273
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 13.6401
                       Mean reward: 16.08
               Mean episode length: 825.78
Episode_Reward/track_lin_vel_xy_exp: 0.8262
Episode_Reward/track_ang_vel_z_exp: 0.3830
       Episode_Reward/lin_vel_z_l2: -0.0393
      Episode_Reward/ang_vel_xy_l2: -0.1142
     Episode_Reward/dof_torques_l2: -0.0741
         Episode_Reward/dof_acc_l2: -0.1667
     Episode_Reward/action_rate_l2: -0.1461
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4317
Metrics/base_velocity/error_vel_xy: 0.3921
Metrics/base_velocity/error_vel_yaw: 0.4437
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28332000
                    Iteration time: 1.16s
                      Time elapsed: 00:14:35
                               ETA: 00:13:14

################################################################################
                     [1m Learning iteration 787/1500 [0m                      

                       Computation: 31851 steps/s (collection: 1.075s, learning 0.056s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0249
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 13.6490
                       Mean reward: 14.64
               Mean episode length: 803.48
Episode_Reward/track_lin_vel_xy_exp: 0.9500
Episode_Reward/track_ang_vel_z_exp: 0.4411
       Episode_Reward/lin_vel_z_l2: -0.0486
      Episode_Reward/ang_vel_xy_l2: -0.1306
     Episode_Reward/dof_torques_l2: -0.0871
         Episode_Reward/dof_acc_l2: -0.1965
     Episode_Reward/action_rate_l2: -0.1698
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4316
Metrics/base_velocity/error_vel_xy: 0.4692
Metrics/base_velocity/error_vel_yaw: 0.5286
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 28368000
                    Iteration time: 1.13s
                      Time elapsed: 00:14:36
                               ETA: 00:13:13

################################################################################
                     [1m Learning iteration 788/1500 [0m                      

                       Computation: 32877 steps/s (collection: 1.044s, learning 0.051s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0257
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 13.6528
                       Mean reward: 15.39
               Mean episode length: 805.34
Episode_Reward/track_lin_vel_xy_exp: 1.0039
Episode_Reward/track_ang_vel_z_exp: 0.4342
       Episode_Reward/lin_vel_z_l2: -0.0517
      Episode_Reward/ang_vel_xy_l2: -0.1300
     Episode_Reward/dof_torques_l2: -0.0777
         Episode_Reward/dof_acc_l2: -0.1869
     Episode_Reward/action_rate_l2: -0.1601
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4344
Metrics/base_velocity/error_vel_xy: 0.3393
Metrics/base_velocity/error_vel_yaw: 0.4843
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 28404000
                    Iteration time: 1.09s
                      Time elapsed: 00:14:37
                               ETA: 00:13:11

################################################################################
                     [1m Learning iteration 789/1500 [0m                      

                       Computation: 32265 steps/s (collection: 1.064s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0287
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 13.6566
                       Mean reward: 16.14
               Mean episode length: 861.87
Episode_Reward/track_lin_vel_xy_exp: 1.0750
Episode_Reward/track_ang_vel_z_exp: 0.5017
       Episode_Reward/lin_vel_z_l2: -0.0487
      Episode_Reward/ang_vel_xy_l2: -0.1458
     Episode_Reward/dof_torques_l2: -0.0974
         Episode_Reward/dof_acc_l2: -0.2193
     Episode_Reward/action_rate_l2: -0.1918
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4358
Metrics/base_velocity/error_vel_xy: 0.5054
Metrics/base_velocity/error_vel_yaw: 0.5657
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 28440000
                    Iteration time: 1.12s
                      Time elapsed: 00:14:38
                               ETA: 00:13:10

################################################################################
                     [1m Learning iteration 790/1500 [0m                      

                       Computation: 32674 steps/s (collection: 1.050s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0275
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 13.6567
                       Mean reward: 16.49
               Mean episode length: 876.42
Episode_Reward/track_lin_vel_xy_exp: 0.9763
Episode_Reward/track_ang_vel_z_exp: 0.4454
       Episode_Reward/lin_vel_z_l2: -0.0508
      Episode_Reward/ang_vel_xy_l2: -0.1348
     Episode_Reward/dof_torques_l2: -0.0852
         Episode_Reward/dof_acc_l2: -0.2072
     Episode_Reward/action_rate_l2: -0.1711
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4398
Metrics/base_velocity/error_vel_xy: 0.4264
Metrics/base_velocity/error_vel_yaw: 0.4898
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 28476000
                    Iteration time: 1.10s
                      Time elapsed: 00:14:39
                               ETA: 00:13:09

################################################################################
                     [1m Learning iteration 791/1500 [0m                      

                       Computation: 31690 steps/s (collection: 1.082s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0292
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 13.6539
                       Mean reward: 15.62
               Mean episode length: 843.44
Episode_Reward/track_lin_vel_xy_exp: 0.9592
Episode_Reward/track_ang_vel_z_exp: 0.4408
       Episode_Reward/lin_vel_z_l2: -0.0463
      Episode_Reward/ang_vel_xy_l2: -0.1290
     Episode_Reward/dof_torques_l2: -0.0856
         Episode_Reward/dof_acc_l2: -0.1989
     Episode_Reward/action_rate_l2: -0.1690
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4442
Metrics/base_velocity/error_vel_xy: 0.4375
Metrics/base_velocity/error_vel_yaw: 0.4982
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 28512000
                    Iteration time: 1.14s
                      Time elapsed: 00:14:40
                               ETA: 00:13:08

################################################################################
                     [1m Learning iteration 792/1500 [0m                      

                       Computation: 31070 steps/s (collection: 1.104s, learning 0.055s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0240
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 13.6528
                       Mean reward: 15.43
               Mean episode length: 845.27
Episode_Reward/track_lin_vel_xy_exp: 0.9010
Episode_Reward/track_ang_vel_z_exp: 0.4313
       Episode_Reward/lin_vel_z_l2: -0.0513
      Episode_Reward/ang_vel_xy_l2: -0.1362
     Episode_Reward/dof_torques_l2: -0.0808
         Episode_Reward/dof_acc_l2: -0.2065
     Episode_Reward/action_rate_l2: -0.1692
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4441
Metrics/base_velocity/error_vel_xy: 0.5085
Metrics/base_velocity/error_vel_yaw: 0.5364
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 28548000
                    Iteration time: 1.16s
                      Time elapsed: 00:14:42
                               ETA: 00:13:07

################################################################################
                     [1m Learning iteration 793/1500 [0m                      

                       Computation: 32266 steps/s (collection: 1.064s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0314
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 13.6543
                       Mean reward: 15.85
               Mean episode length: 839.31
Episode_Reward/track_lin_vel_xy_exp: 1.0944
Episode_Reward/track_ang_vel_z_exp: 0.4867
       Episode_Reward/lin_vel_z_l2: -0.0453
      Episode_Reward/ang_vel_xy_l2: -0.1363
     Episode_Reward/dof_torques_l2: -0.0902
         Episode_Reward/dof_acc_l2: -0.2029
     Episode_Reward/action_rate_l2: -0.1791
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4467
Metrics/base_velocity/error_vel_xy: 0.3981
Metrics/base_velocity/error_vel_yaw: 0.5161
      Episode_Termination/time_out: 1.9167
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 28584000
                    Iteration time: 1.12s
                      Time elapsed: 00:14:43
                               ETA: 00:13:06

################################################################################
                     [1m Learning iteration 794/1500 [0m                      

                       Computation: 32123 steps/s (collection: 1.066s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0281
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 13.6680
                       Mean reward: 16.58
               Mean episode length: 851.87
Episode_Reward/track_lin_vel_xy_exp: 0.9929
Episode_Reward/track_ang_vel_z_exp: 0.4368
       Episode_Reward/lin_vel_z_l2: -0.0490
      Episode_Reward/ang_vel_xy_l2: -0.1303
     Episode_Reward/dof_torques_l2: -0.0787
         Episode_Reward/dof_acc_l2: -0.1996
     Episode_Reward/action_rate_l2: -0.1643
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4491
Metrics/base_velocity/error_vel_xy: 0.3597
Metrics/base_velocity/error_vel_yaw: 0.4699
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 28620000
                    Iteration time: 1.12s
                      Time elapsed: 00:14:44
                               ETA: 00:13:05

################################################################################
                     [1m Learning iteration 795/1500 [0m                      

                       Computation: 30936 steps/s (collection: 1.111s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0274
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 13.6662
                       Mean reward: 16.90
               Mean episode length: 873.27
Episode_Reward/track_lin_vel_xy_exp: 1.0268
Episode_Reward/track_ang_vel_z_exp: 0.4674
       Episode_Reward/lin_vel_z_l2: -0.0500
      Episode_Reward/ang_vel_xy_l2: -0.1391
     Episode_Reward/dof_torques_l2: -0.0915
         Episode_Reward/dof_acc_l2: -0.2220
     Episode_Reward/action_rate_l2: -0.1845
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4472
Metrics/base_velocity/error_vel_xy: 0.4734
Metrics/base_velocity/error_vel_yaw: 0.5418
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 28656000
                    Iteration time: 1.16s
                      Time elapsed: 00:14:45
                               ETA: 00:13:04

################################################################################
                     [1m Learning iteration 796/1500 [0m                      

                       Computation: 31680 steps/s (collection: 1.083s, learning 0.054s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0257
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 13.6758
                       Mean reward: 17.21
               Mean episode length: 906.87
Episode_Reward/track_lin_vel_xy_exp: 1.0236
Episode_Reward/track_ang_vel_z_exp: 0.4636
       Episode_Reward/lin_vel_z_l2: -0.0484
      Episode_Reward/ang_vel_xy_l2: -0.1349
     Episode_Reward/dof_torques_l2: -0.0874
         Episode_Reward/dof_acc_l2: -0.2017
     Episode_Reward/action_rate_l2: -0.1743
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4507
Metrics/base_velocity/error_vel_xy: 0.4449
Metrics/base_velocity/error_vel_yaw: 0.5327
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 28692000
                    Iteration time: 1.14s
                      Time elapsed: 00:14:46
                               ETA: 00:13:03

################################################################################
                     [1m Learning iteration 797/1500 [0m                      

                       Computation: 32596 steps/s (collection: 1.051s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0276
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 13.6837
                       Mean reward: 14.44
               Mean episode length: 830.89
Episode_Reward/track_lin_vel_xy_exp: 0.7846
Episode_Reward/track_ang_vel_z_exp: 0.3816
       Episode_Reward/lin_vel_z_l2: -0.0496
      Episode_Reward/ang_vel_xy_l2: -0.1204
     Episode_Reward/dof_torques_l2: -0.0754
         Episode_Reward/dof_acc_l2: -0.1662
     Episode_Reward/action_rate_l2: -0.1462
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4430
Metrics/base_velocity/error_vel_xy: 0.4672
Metrics/base_velocity/error_vel_yaw: 0.4723
      Episode_Termination/time_out: 1.0833
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 28728000
                    Iteration time: 1.10s
                      Time elapsed: 00:14:47
                               ETA: 00:13:02

################################################################################
                     [1m Learning iteration 798/1500 [0m                      

                       Computation: 33223 steps/s (collection: 1.031s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0333
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 13.6878
                       Mean reward: 14.80
               Mean episode length: 810.59
Episode_Reward/track_lin_vel_xy_exp: 0.9250
Episode_Reward/track_ang_vel_z_exp: 0.4278
       Episode_Reward/lin_vel_z_l2: -0.0441
      Episode_Reward/ang_vel_xy_l2: -0.1230
     Episode_Reward/dof_torques_l2: -0.0820
         Episode_Reward/dof_acc_l2: -0.1812
     Episode_Reward/action_rate_l2: -0.1602
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4413
Metrics/base_velocity/error_vel_xy: 0.4179
Metrics/base_velocity/error_vel_yaw: 0.4787
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 28764000
                    Iteration time: 1.08s
                      Time elapsed: 00:14:48
                               ETA: 00:13:00

################################################################################
                     [1m Learning iteration 799/1500 [0m                      

                       Computation: 31301 steps/s (collection: 1.099s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0278
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 13.6928
                       Mean reward: 14.46
               Mean episode length: 800.19
Episode_Reward/track_lin_vel_xy_exp: 0.8863
Episode_Reward/track_ang_vel_z_exp: 0.4009
       Episode_Reward/lin_vel_z_l2: -0.0449
      Episode_Reward/ang_vel_xy_l2: -0.1235
     Episode_Reward/dof_torques_l2: -0.0773
         Episode_Reward/dof_acc_l2: -0.1830
     Episode_Reward/action_rate_l2: -0.1593
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4458
Metrics/base_velocity/error_vel_xy: 0.4156
Metrics/base_velocity/error_vel_yaw: 0.5031
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 28800000
                    Iteration time: 1.15s
                      Time elapsed: 00:14:49
                               ETA: 00:12:59

################################################################################
                     [1m Learning iteration 800/1500 [0m                      

                       Computation: 31900 steps/s (collection: 1.074s, learning 0.054s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0270
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 13.6953
                       Mean reward: 16.50
               Mean episode length: 843.01
Episode_Reward/track_lin_vel_xy_exp: 1.1379
Episode_Reward/track_ang_vel_z_exp: 0.4841
       Episode_Reward/lin_vel_z_l2: -0.0494
      Episode_Reward/ang_vel_xy_l2: -0.1359
     Episode_Reward/dof_torques_l2: -0.0863
         Episode_Reward/dof_acc_l2: -0.1968
     Episode_Reward/action_rate_l2: -0.1783
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4512
Metrics/base_velocity/error_vel_xy: 0.3309
Metrics/base_velocity/error_vel_yaw: 0.4972
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 28836000
                    Iteration time: 1.13s
                      Time elapsed: 00:14:51
                               ETA: 00:12:58

################################################################################
                     [1m Learning iteration 801/1500 [0m                      

                       Computation: 30637 steps/s (collection: 1.122s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0253
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 13.7012
                       Mean reward: 17.14
               Mean episode length: 861.64
Episode_Reward/track_lin_vel_xy_exp: 1.0093
Episode_Reward/track_ang_vel_z_exp: 0.4575
       Episode_Reward/lin_vel_z_l2: -0.0534
      Episode_Reward/ang_vel_xy_l2: -0.1334
     Episode_Reward/dof_torques_l2: -0.0873
         Episode_Reward/dof_acc_l2: -0.2200
     Episode_Reward/action_rate_l2: -0.1753
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4582
Metrics/base_velocity/error_vel_xy: 0.4361
Metrics/base_velocity/error_vel_yaw: 0.5036
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 28872000
                    Iteration time: 1.18s
                      Time elapsed: 00:14:52
                               ETA: 00:12:57

################################################################################
                     [1m Learning iteration 802/1500 [0m                      

                       Computation: 32671 steps/s (collection: 1.052s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0277
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 13.7000
                       Mean reward: 15.99
               Mean episode length: 814.41
Episode_Reward/track_lin_vel_xy_exp: 0.9573
Episode_Reward/track_ang_vel_z_exp: 0.4346
       Episode_Reward/lin_vel_z_l2: -0.0465
      Episode_Reward/ang_vel_xy_l2: -0.1228
     Episode_Reward/dof_torques_l2: -0.0829
         Episode_Reward/dof_acc_l2: -0.1922
     Episode_Reward/action_rate_l2: -0.1642
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4624
Metrics/base_velocity/error_vel_xy: 0.3953
Metrics/base_velocity/error_vel_yaw: 0.4857
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 28908000
                    Iteration time: 1.10s
                      Time elapsed: 00:14:53
                               ETA: 00:12:56

################################################################################
                     [1m Learning iteration 803/1500 [0m                      

                       Computation: 31755 steps/s (collection: 1.081s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0260
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 13.7047
                       Mean reward: 15.98
               Mean episode length: 830.18
Episode_Reward/track_lin_vel_xy_exp: 0.9884
Episode_Reward/track_ang_vel_z_exp: 0.4511
       Episode_Reward/lin_vel_z_l2: -0.0467
      Episode_Reward/ang_vel_xy_l2: -0.1311
     Episode_Reward/dof_torques_l2: -0.0839
         Episode_Reward/dof_acc_l2: -0.2084
     Episode_Reward/action_rate_l2: -0.1728
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4679
Metrics/base_velocity/error_vel_xy: 0.4325
Metrics/base_velocity/error_vel_yaw: 0.5107
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 28944000
                    Iteration time: 1.13s
                      Time elapsed: 00:14:54
                               ETA: 00:12:55

################################################################################
                     [1m Learning iteration 804/1500 [0m                      

                       Computation: 32306 steps/s (collection: 1.061s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0264
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 13.7030
                       Mean reward: 16.82
               Mean episode length: 875.14
Episode_Reward/track_lin_vel_xy_exp: 1.1296
Episode_Reward/track_ang_vel_z_exp: 0.5011
       Episode_Reward/lin_vel_z_l2: -0.0510
      Episode_Reward/ang_vel_xy_l2: -0.1410
     Episode_Reward/dof_torques_l2: -0.0949
         Episode_Reward/dof_acc_l2: -0.2183
     Episode_Reward/action_rate_l2: -0.1876
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4701
Metrics/base_velocity/error_vel_xy: 0.4151
Metrics/base_velocity/error_vel_yaw: 0.5384
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 28980000
                    Iteration time: 1.11s
                      Time elapsed: 00:14:55
                               ETA: 00:12:54

################################################################################
                     [1m Learning iteration 805/1500 [0m                      

                       Computation: 31339 steps/s (collection: 1.096s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0261
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 13.6996
                       Mean reward: 17.68
               Mean episode length: 897.76
Episode_Reward/track_lin_vel_xy_exp: 1.0596
Episode_Reward/track_ang_vel_z_exp: 0.4691
       Episode_Reward/lin_vel_z_l2: -0.0411
      Episode_Reward/ang_vel_xy_l2: -0.1303
     Episode_Reward/dof_torques_l2: -0.0842
         Episode_Reward/dof_acc_l2: -0.1935
     Episode_Reward/action_rate_l2: -0.1718
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4606
Metrics/base_velocity/error_vel_xy: 0.3884
Metrics/base_velocity/error_vel_yaw: 0.5105
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 29016000
                    Iteration time: 1.15s
                      Time elapsed: 00:14:56
                               ETA: 00:12:53

################################################################################
                     [1m Learning iteration 806/1500 [0m                      

                       Computation: 32431 steps/s (collection: 1.059s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0241
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 13.7005
                       Mean reward: 16.20
               Mean episode length: 855.76
Episode_Reward/track_lin_vel_xy_exp: 0.9376
Episode_Reward/track_ang_vel_z_exp: 0.4324
       Episode_Reward/lin_vel_z_l2: -0.0515
      Episode_Reward/ang_vel_xy_l2: -0.1332
     Episode_Reward/dof_torques_l2: -0.0817
         Episode_Reward/dof_acc_l2: -0.2030
     Episode_Reward/action_rate_l2: -0.1662
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4548
Metrics/base_velocity/error_vel_xy: 0.4355
Metrics/base_velocity/error_vel_yaw: 0.5002
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 29052000
                    Iteration time: 1.11s
                      Time elapsed: 00:14:57
                               ETA: 00:12:52

################################################################################
                     [1m Learning iteration 807/1500 [0m                      

                       Computation: 31897 steps/s (collection: 1.076s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0262
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 13.6980
                       Mean reward: 15.42
               Mean episode length: 836.80
Episode_Reward/track_lin_vel_xy_exp: 0.9827
Episode_Reward/track_ang_vel_z_exp: 0.4613
       Episode_Reward/lin_vel_z_l2: -0.0488
      Episode_Reward/ang_vel_xy_l2: -0.1352
     Episode_Reward/dof_torques_l2: -0.0923
         Episode_Reward/dof_acc_l2: -0.1942
     Episode_Reward/action_rate_l2: -0.1749
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4501
Metrics/base_velocity/error_vel_xy: 0.4791
Metrics/base_velocity/error_vel_yaw: 0.5258
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 29088000
                    Iteration time: 1.13s
                      Time elapsed: 00:14:59
                               ETA: 00:12:51

################################################################################
                     [1m Learning iteration 808/1500 [0m                      

                       Computation: 32165 steps/s (collection: 1.067s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0271
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 13.6989
                       Mean reward: 15.97
               Mean episode length: 869.59
Episode_Reward/track_lin_vel_xy_exp: 1.0216
Episode_Reward/track_ang_vel_z_exp: 0.4684
       Episode_Reward/lin_vel_z_l2: -0.0516
      Episode_Reward/ang_vel_xy_l2: -0.1377
     Episode_Reward/dof_torques_l2: -0.0854
         Episode_Reward/dof_acc_l2: -0.2079
     Episode_Reward/action_rate_l2: -0.1761
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4562
Metrics/base_velocity/error_vel_xy: 0.4354
Metrics/base_velocity/error_vel_yaw: 0.4992
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 29124000
                    Iteration time: 1.12s
                      Time elapsed: 00:15:00
                               ETA: 00:12:49

################################################################################
                     [1m Learning iteration 809/1500 [0m                      

                       Computation: 32695 steps/s (collection: 1.049s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0244
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 13.7031
                       Mean reward: 15.76
               Mean episode length: 860.62
Episode_Reward/track_lin_vel_xy_exp: 0.9161
Episode_Reward/track_ang_vel_z_exp: 0.4163
       Episode_Reward/lin_vel_z_l2: -0.0465
      Episode_Reward/ang_vel_xy_l2: -0.1192
     Episode_Reward/dof_torques_l2: -0.0817
         Episode_Reward/dof_acc_l2: -0.1906
     Episode_Reward/action_rate_l2: -0.1589
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4575
Metrics/base_velocity/error_vel_xy: 0.3966
Metrics/base_velocity/error_vel_yaw: 0.4706
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 29160000
                    Iteration time: 1.10s
                      Time elapsed: 00:15:01
                               ETA: 00:12:48

################################################################################
                     [1m Learning iteration 810/1500 [0m                      

                       Computation: 31472 steps/s (collection: 1.091s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0250
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.6996
                       Mean reward: 15.78
               Mean episode length: 841.55
Episode_Reward/track_lin_vel_xy_exp: 1.0157
Episode_Reward/track_ang_vel_z_exp: 0.4695
       Episode_Reward/lin_vel_z_l2: -0.0485
      Episode_Reward/ang_vel_xy_l2: -0.1386
     Episode_Reward/dof_torques_l2: -0.0883
         Episode_Reward/dof_acc_l2: -0.1995
     Episode_Reward/action_rate_l2: -0.1769
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4556
Metrics/base_velocity/error_vel_xy: 0.4622
Metrics/base_velocity/error_vel_yaw: 0.5374
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 29196000
                    Iteration time: 1.14s
                      Time elapsed: 00:15:02
                               ETA: 00:12:47

################################################################################
                     [1m Learning iteration 811/1500 [0m                      

                       Computation: 32476 steps/s (collection: 1.053s, learning 0.056s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0250
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 13.6940
                       Mean reward: 16.16
               Mean episode length: 857.72
Episode_Reward/track_lin_vel_xy_exp: 1.0485
Episode_Reward/track_ang_vel_z_exp: 0.4645
       Episode_Reward/lin_vel_z_l2: -0.0486
      Episode_Reward/ang_vel_xy_l2: -0.1326
     Episode_Reward/dof_torques_l2: -0.0840
         Episode_Reward/dof_acc_l2: -0.2014
     Episode_Reward/action_rate_l2: -0.1734
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4598
Metrics/base_velocity/error_vel_xy: 0.3748
Metrics/base_velocity/error_vel_yaw: 0.4868
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 29232000
                    Iteration time: 1.11s
                      Time elapsed: 00:15:03
                               ETA: 00:12:46

################################################################################
                     [1m Learning iteration 812/1500 [0m                      

                       Computation: 32637 steps/s (collection: 1.051s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0280
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 13.6936
                       Mean reward: 16.63
               Mean episode length: 862.33
Episode_Reward/track_lin_vel_xy_exp: 0.9962
Episode_Reward/track_ang_vel_z_exp: 0.4491
       Episode_Reward/lin_vel_z_l2: -0.0418
      Episode_Reward/ang_vel_xy_l2: -0.1275
     Episode_Reward/dof_torques_l2: -0.0902
         Episode_Reward/dof_acc_l2: -0.1961
     Episode_Reward/action_rate_l2: -0.1717
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4534
Metrics/base_velocity/error_vel_xy: 0.4346
Metrics/base_velocity/error_vel_yaw: 0.5131
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 29268000
                    Iteration time: 1.10s
                      Time elapsed: 00:15:04
                               ETA: 00:12:45

################################################################################
                     [1m Learning iteration 813/1500 [0m                      

                       Computation: 32500 steps/s (collection: 1.055s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0232
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 13.6994
                       Mean reward: 16.93
               Mean episode length: 873.38
Episode_Reward/track_lin_vel_xy_exp: 1.0594
Episode_Reward/track_ang_vel_z_exp: 0.4676
       Episode_Reward/lin_vel_z_l2: -0.0458
      Episode_Reward/ang_vel_xy_l2: -0.1329
     Episode_Reward/dof_torques_l2: -0.0881
         Episode_Reward/dof_acc_l2: -0.1969
     Episode_Reward/action_rate_l2: -0.1728
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4405
Metrics/base_velocity/error_vel_xy: 0.3931
Metrics/base_velocity/error_vel_yaw: 0.5124
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 29304000
                    Iteration time: 1.11s
                      Time elapsed: 00:15:05
                               ETA: 00:12:44

################################################################################
                     [1m Learning iteration 814/1500 [0m                      

                       Computation: 32341 steps/s (collection: 1.062s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0237
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 13.7024
                       Mean reward: 16.50
               Mean episode length: 881.88
Episode_Reward/track_lin_vel_xy_exp: 0.9726
Episode_Reward/track_ang_vel_z_exp: 0.4450
       Episode_Reward/lin_vel_z_l2: -0.0527
      Episode_Reward/ang_vel_xy_l2: -0.1352
     Episode_Reward/dof_torques_l2: -0.0880
         Episode_Reward/dof_acc_l2: -0.2144
     Episode_Reward/action_rate_l2: -0.1751
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4363
Metrics/base_velocity/error_vel_xy: 0.4503
Metrics/base_velocity/error_vel_yaw: 0.5379
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 29340000
                    Iteration time: 1.11s
                      Time elapsed: 00:15:06
                               ETA: 00:12:43

################################################################################
                     [1m Learning iteration 815/1500 [0m                      

                       Computation: 32663 steps/s (collection: 1.050s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0220
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 13.6985
                       Mean reward: 16.32
               Mean episode length: 895.00
Episode_Reward/track_lin_vel_xy_exp: 1.0678
Episode_Reward/track_ang_vel_z_exp: 0.4884
       Episode_Reward/lin_vel_z_l2: -0.0539
      Episode_Reward/ang_vel_xy_l2: -0.1404
     Episode_Reward/dof_torques_l2: -0.0991
         Episode_Reward/dof_acc_l2: -0.2165
     Episode_Reward/action_rate_l2: -0.1881
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4409
Metrics/base_velocity/error_vel_xy: 0.4741
Metrics/base_velocity/error_vel_yaw: 0.5500
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 29376000
                    Iteration time: 1.10s
                      Time elapsed: 00:15:07
                               ETA: 00:12:42

################################################################################
                     [1m Learning iteration 816/1500 [0m                      

                       Computation: 33225 steps/s (collection: 1.031s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0266
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 13.6939
                       Mean reward: 16.32
               Mean episode length: 883.75
Episode_Reward/track_lin_vel_xy_exp: 1.0922
Episode_Reward/track_ang_vel_z_exp: 0.5036
       Episode_Reward/lin_vel_z_l2: -0.0478
      Episode_Reward/ang_vel_xy_l2: -0.1397
     Episode_Reward/dof_torques_l2: -0.0920
         Episode_Reward/dof_acc_l2: -0.2055
     Episode_Reward/action_rate_l2: -0.1852
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4398
Metrics/base_velocity/error_vel_xy: 0.4628
Metrics/base_velocity/error_vel_yaw: 0.5182
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 29412000
                    Iteration time: 1.08s
                      Time elapsed: 00:15:08
                               ETA: 00:12:41

################################################################################
                     [1m Learning iteration 817/1500 [0m                      

                       Computation: 33206 steps/s (collection: 1.034s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0238
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 13.6987
                       Mean reward: 17.94
               Mean episode length: 931.59
Episode_Reward/track_lin_vel_xy_exp: 1.1222
Episode_Reward/track_ang_vel_z_exp: 0.5142
       Episode_Reward/lin_vel_z_l2: -0.0468
      Episode_Reward/ang_vel_xy_l2: -0.1430
     Episode_Reward/dof_torques_l2: -0.0953
         Episode_Reward/dof_acc_l2: -0.2060
     Episode_Reward/action_rate_l2: -0.1901
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4455
Metrics/base_velocity/error_vel_xy: 0.4743
Metrics/base_velocity/error_vel_yaw: 0.5648
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 29448000
                    Iteration time: 1.08s
                      Time elapsed: 00:15:10
                               ETA: 00:12:39

################################################################################
                     [1m Learning iteration 818/1500 [0m                      

                       Computation: 32350 steps/s (collection: 1.060s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0250
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 13.7050
                       Mean reward: 18.87
               Mean episode length: 924.58
Episode_Reward/track_lin_vel_xy_exp: 1.1161
Episode_Reward/track_ang_vel_z_exp: 0.4979
       Episode_Reward/lin_vel_z_l2: -0.0449
      Episode_Reward/ang_vel_xy_l2: -0.1378
     Episode_Reward/dof_torques_l2: -0.0900
         Episode_Reward/dof_acc_l2: -0.2107
     Episode_Reward/action_rate_l2: -0.1825
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4497
Metrics/base_velocity/error_vel_xy: 0.3954
Metrics/base_velocity/error_vel_yaw: 0.5054
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 29484000
                    Iteration time: 1.11s
                      Time elapsed: 00:15:11
                               ETA: 00:12:38

################################################################################
                     [1m Learning iteration 819/1500 [0m                      

                       Computation: 31151 steps/s (collection: 1.099s, learning 0.057s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0269
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 13.7096
                       Mean reward: 18.53
               Mean episode length: 924.09
Episode_Reward/track_lin_vel_xy_exp: 1.1049
Episode_Reward/track_ang_vel_z_exp: 0.5061
       Episode_Reward/lin_vel_z_l2: -0.0529
      Episode_Reward/ang_vel_xy_l2: -0.1498
     Episode_Reward/dof_torques_l2: -0.0992
         Episode_Reward/dof_acc_l2: -0.2194
     Episode_Reward/action_rate_l2: -0.1930
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4585
Metrics/base_velocity/error_vel_xy: 0.4930
Metrics/base_velocity/error_vel_yaw: 0.5701
      Episode_Termination/time_out: 2.0000
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 29520000
                    Iteration time: 1.16s
                      Time elapsed: 00:15:12
                               ETA: 00:12:37

################################################################################
                     [1m Learning iteration 820/1500 [0m                      

                       Computation: 31701 steps/s (collection: 1.084s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0307
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 13.7153
                       Mean reward: 17.21
               Mean episode length: 894.11
Episode_Reward/track_lin_vel_xy_exp: 1.0489
Episode_Reward/track_ang_vel_z_exp: 0.4803
       Episode_Reward/lin_vel_z_l2: -0.0484
      Episode_Reward/ang_vel_xy_l2: -0.1380
     Episode_Reward/dof_torques_l2: -0.0885
         Episode_Reward/dof_acc_l2: -0.2020
     Episode_Reward/action_rate_l2: -0.1805
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4698
Metrics/base_velocity/error_vel_xy: 0.4477
Metrics/base_velocity/error_vel_yaw: 0.5243
      Episode_Termination/time_out: 1.9583
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 29556000
                    Iteration time: 1.14s
                      Time elapsed: 00:15:13
                               ETA: 00:12:36

################################################################################
                     [1m Learning iteration 821/1500 [0m                      

                       Computation: 32339 steps/s (collection: 1.060s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0289
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 13.7186
                       Mean reward: 17.63
               Mean episode length: 881.19
Episode_Reward/track_lin_vel_xy_exp: 1.0291
Episode_Reward/track_ang_vel_z_exp: 0.4516
       Episode_Reward/lin_vel_z_l2: -0.0444
      Episode_Reward/ang_vel_xy_l2: -0.1296
     Episode_Reward/dof_torques_l2: -0.0807
         Episode_Reward/dof_acc_l2: -0.1985
     Episode_Reward/action_rate_l2: -0.1688
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4776
Metrics/base_velocity/error_vel_xy: 0.3578
Metrics/base_velocity/error_vel_yaw: 0.4759
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 29592000
                    Iteration time: 1.11s
                      Time elapsed: 00:15:14
                               ETA: 00:12:35

################################################################################
                     [1m Learning iteration 822/1500 [0m                      

                       Computation: 32238 steps/s (collection: 1.065s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0254
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 13.7140
                       Mean reward: 16.35
               Mean episode length: 841.00
Episode_Reward/track_lin_vel_xy_exp: 0.8723
Episode_Reward/track_ang_vel_z_exp: 0.4102
       Episode_Reward/lin_vel_z_l2: -0.0433
      Episode_Reward/ang_vel_xy_l2: -0.1248
     Episode_Reward/dof_torques_l2: -0.0787
         Episode_Reward/dof_acc_l2: -0.1825
     Episode_Reward/action_rate_l2: -0.1577
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4805
Metrics/base_velocity/error_vel_xy: 0.4246
Metrics/base_velocity/error_vel_yaw: 0.4605
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 29628000
                    Iteration time: 1.12s
                      Time elapsed: 00:15:15
                               ETA: 00:12:34

################################################################################
                     [1m Learning iteration 823/1500 [0m                      

                       Computation: 32217 steps/s (collection: 1.064s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0271
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 13.7051
                       Mean reward: 15.11
               Mean episode length: 814.86
Episode_Reward/track_lin_vel_xy_exp: 0.9760
Episode_Reward/track_ang_vel_z_exp: 0.4450
       Episode_Reward/lin_vel_z_l2: -0.0445
      Episode_Reward/ang_vel_xy_l2: -0.1316
     Episode_Reward/dof_torques_l2: -0.0875
         Episode_Reward/dof_acc_l2: -0.2085
     Episode_Reward/action_rate_l2: -0.1728
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4913
Metrics/base_velocity/error_vel_xy: 0.4501
Metrics/base_velocity/error_vel_yaw: 0.5271
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 29664000
                    Iteration time: 1.12s
                      Time elapsed: 00:15:16
                               ETA: 00:12:33

################################################################################
                     [1m Learning iteration 824/1500 [0m                      

                       Computation: 32357 steps/s (collection: 1.059s, learning 0.054s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0271
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 13.7016
                       Mean reward: 16.55
               Mean episode length: 864.39
Episode_Reward/track_lin_vel_xy_exp: 1.0424
Episode_Reward/track_ang_vel_z_exp: 0.4733
       Episode_Reward/lin_vel_z_l2: -0.0463
      Episode_Reward/ang_vel_xy_l2: -0.1351
     Episode_Reward/dof_torques_l2: -0.0898
         Episode_Reward/dof_acc_l2: -0.1990
     Episode_Reward/action_rate_l2: -0.1778
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4953
Metrics/base_velocity/error_vel_xy: 0.4534
Metrics/base_velocity/error_vel_yaw: 0.5383
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 29700000
                    Iteration time: 1.11s
                      Time elapsed: 00:15:17
                               ETA: 00:12:32

################################################################################
                     [1m Learning iteration 825/1500 [0m                      

                       Computation: 32982 steps/s (collection: 1.038s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0261
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 13.7046
                       Mean reward: 16.30
               Mean episode length: 862.46
Episode_Reward/track_lin_vel_xy_exp: 0.9969
Episode_Reward/track_ang_vel_z_exp: 0.4700
       Episode_Reward/lin_vel_z_l2: -0.0459
      Episode_Reward/ang_vel_xy_l2: -0.1323
     Episode_Reward/dof_torques_l2: -0.0923
         Episode_Reward/dof_acc_l2: -0.1977
     Episode_Reward/action_rate_l2: -0.1772
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4928
Metrics/base_velocity/error_vel_xy: 0.4870
Metrics/base_velocity/error_vel_yaw: 0.5114
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 29736000
                    Iteration time: 1.09s
                      Time elapsed: 00:15:19
                               ETA: 00:12:31

################################################################################
                     [1m Learning iteration 826/1500 [0m                      

                       Computation: 33085 steps/s (collection: 1.036s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0269
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 13.7067
                       Mean reward: 15.91
               Mean episode length: 836.45
Episode_Reward/track_lin_vel_xy_exp: 0.9960
Episode_Reward/track_ang_vel_z_exp: 0.4447
       Episode_Reward/lin_vel_z_l2: -0.0460
      Episode_Reward/ang_vel_xy_l2: -0.1256
     Episode_Reward/dof_torques_l2: -0.0852
         Episode_Reward/dof_acc_l2: -0.1974
     Episode_Reward/action_rate_l2: -0.1687
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4872
Metrics/base_velocity/error_vel_xy: 0.3839
Metrics/base_velocity/error_vel_yaw: 0.4806
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 29772000
                    Iteration time: 1.09s
                      Time elapsed: 00:15:20
                               ETA: 00:12:29

################################################################################
                     [1m Learning iteration 827/1500 [0m                      

                       Computation: 32643 steps/s (collection: 1.051s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0310
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 13.7001
                       Mean reward: 15.53
               Mean episode length: 820.93
Episode_Reward/track_lin_vel_xy_exp: 0.8639
Episode_Reward/track_ang_vel_z_exp: 0.4062
       Episode_Reward/lin_vel_z_l2: -0.0491
      Episode_Reward/ang_vel_xy_l2: -0.1286
     Episode_Reward/dof_torques_l2: -0.0803
         Episode_Reward/dof_acc_l2: -0.1858
     Episode_Reward/action_rate_l2: -0.1593
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4849
Metrics/base_velocity/error_vel_xy: 0.4826
Metrics/base_velocity/error_vel_yaw: 0.5384
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 29808000
                    Iteration time: 1.10s
                      Time elapsed: 00:15:21
                               ETA: 00:12:28

################################################################################
                     [1m Learning iteration 828/1500 [0m                      

                       Computation: 32166 steps/s (collection: 1.066s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0282
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 13.6953
                       Mean reward: 14.68
               Mean episode length: 802.20
Episode_Reward/track_lin_vel_xy_exp: 0.9668
Episode_Reward/track_ang_vel_z_exp: 0.4330
       Episode_Reward/lin_vel_z_l2: -0.0456
      Episode_Reward/ang_vel_xy_l2: -0.1254
     Episode_Reward/dof_torques_l2: -0.0849
         Episode_Reward/dof_acc_l2: -0.2008
     Episode_Reward/action_rate_l2: -0.1666
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4885
Metrics/base_velocity/error_vel_xy: 0.4129
Metrics/base_velocity/error_vel_yaw: 0.5118
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 29844000
                    Iteration time: 1.12s
                      Time elapsed: 00:15:22
                               ETA: 00:12:27

################################################################################
                     [1m Learning iteration 829/1500 [0m                      

                       Computation: 32558 steps/s (collection: 1.055s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0264
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 13.6928
                       Mean reward: 15.73
               Mean episode length: 856.60
Episode_Reward/track_lin_vel_xy_exp: 1.1253
Episode_Reward/track_ang_vel_z_exp: 0.4967
       Episode_Reward/lin_vel_z_l2: -0.0484
      Episode_Reward/ang_vel_xy_l2: -0.1427
     Episode_Reward/dof_torques_l2: -0.0937
         Episode_Reward/dof_acc_l2: -0.2108
     Episode_Reward/action_rate_l2: -0.1871
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4956
Metrics/base_velocity/error_vel_xy: 0.4299
Metrics/base_velocity/error_vel_yaw: 0.5525
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 29880000
                    Iteration time: 1.11s
                      Time elapsed: 00:15:23
                               ETA: 00:12:26

################################################################################
                     [1m Learning iteration 830/1500 [0m                      

                       Computation: 33570 steps/s (collection: 1.019s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0247
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 13.6970
                       Mean reward: 15.89
               Mean episode length: 865.19
Episode_Reward/track_lin_vel_xy_exp: 0.9598
Episode_Reward/track_ang_vel_z_exp: 0.4505
       Episode_Reward/lin_vel_z_l2: -0.0479
      Episode_Reward/ang_vel_xy_l2: -0.1345
     Episode_Reward/dof_torques_l2: -0.0878
         Episode_Reward/dof_acc_l2: -0.2044
     Episode_Reward/action_rate_l2: -0.1750
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5015
Metrics/base_velocity/error_vel_xy: 0.5004
Metrics/base_velocity/error_vel_yaw: 0.5356
      Episode_Termination/time_out: 1.1250
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 29916000
                    Iteration time: 1.07s
                      Time elapsed: 00:15:24
                               ETA: 00:12:25

################################################################################
                     [1m Learning iteration 831/1500 [0m                      

                       Computation: 32119 steps/s (collection: 1.069s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0253
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 13.6969
                       Mean reward: 15.03
               Mean episode length: 845.21
Episode_Reward/track_lin_vel_xy_exp: 0.9299
Episode_Reward/track_ang_vel_z_exp: 0.4508
       Episode_Reward/lin_vel_z_l2: -0.0504
      Episode_Reward/ang_vel_xy_l2: -0.1388
     Episode_Reward/dof_torques_l2: -0.0885
         Episode_Reward/dof_acc_l2: -0.2091
     Episode_Reward/action_rate_l2: -0.1772
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5076
Metrics/base_velocity/error_vel_xy: 0.5431
Metrics/base_velocity/error_vel_yaw: 0.5653
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 29952000
                    Iteration time: 1.12s
                      Time elapsed: 00:15:25
                               ETA: 00:12:24

################################################################################
                     [1m Learning iteration 832/1500 [0m                      

                       Computation: 32476 steps/s (collection: 1.057s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0244
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 13.6978
                       Mean reward: 15.54
               Mean episode length: 829.14
Episode_Reward/track_lin_vel_xy_exp: 1.0335
Episode_Reward/track_ang_vel_z_exp: 0.4491
       Episode_Reward/lin_vel_z_l2: -0.0423
      Episode_Reward/ang_vel_xy_l2: -0.1265
     Episode_Reward/dof_torques_l2: -0.0825
         Episode_Reward/dof_acc_l2: -0.1918
     Episode_Reward/action_rate_l2: -0.1677
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5102
Metrics/base_velocity/error_vel_xy: 0.3427
Metrics/base_velocity/error_vel_yaw: 0.4791
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 29988000
                    Iteration time: 1.11s
                      Time elapsed: 00:15:26
                               ETA: 00:12:23

################################################################################
                     [1m Learning iteration 833/1500 [0m                      

                       Computation: 31968 steps/s (collection: 1.073s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0239
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 13.6972
                       Mean reward: 16.19
               Mean episode length: 841.03
Episode_Reward/track_lin_vel_xy_exp: 0.9573
Episode_Reward/track_ang_vel_z_exp: 0.4405
       Episode_Reward/lin_vel_z_l2: -0.0474
      Episode_Reward/ang_vel_xy_l2: -0.1257
     Episode_Reward/dof_torques_l2: -0.0845
         Episode_Reward/dof_acc_l2: -0.1920
     Episode_Reward/action_rate_l2: -0.1664
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5111
Metrics/base_velocity/error_vel_xy: 0.4247
Metrics/base_velocity/error_vel_yaw: 0.4882
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 30024000
                    Iteration time: 1.13s
                      Time elapsed: 00:15:27
                               ETA: 00:12:22

################################################################################
                     [1m Learning iteration 834/1500 [0m                      

                       Computation: 32795 steps/s (collection: 1.046s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0255
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 13.6989
                       Mean reward: 15.40
               Mean episode length: 831.14
Episode_Reward/track_lin_vel_xy_exp: 0.9019
Episode_Reward/track_ang_vel_z_exp: 0.4286
       Episode_Reward/lin_vel_z_l2: -0.0472
      Episode_Reward/ang_vel_xy_l2: -0.1289
     Episode_Reward/dof_torques_l2: -0.0820
         Episode_Reward/dof_acc_l2: -0.1805
     Episode_Reward/action_rate_l2: -0.1624
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.4997
Metrics/base_velocity/error_vel_xy: 0.4728
Metrics/base_velocity/error_vel_yaw: 0.5007
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 30060000
                    Iteration time: 1.10s
                      Time elapsed: 00:15:28
                               ETA: 00:12:20

################################################################################
                     [1m Learning iteration 835/1500 [0m                      

                       Computation: 32719 steps/s (collection: 1.048s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0236
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 13.6912
                       Mean reward: 15.07
               Mean episode length: 833.01
Episode_Reward/track_lin_vel_xy_exp: 0.9507
Episode_Reward/track_ang_vel_z_exp: 0.4431
       Episode_Reward/lin_vel_z_l2: -0.0512
      Episode_Reward/ang_vel_xy_l2: -0.1316
     Episode_Reward/dof_torques_l2: -0.0834
         Episode_Reward/dof_acc_l2: -0.2068
     Episode_Reward/action_rate_l2: -0.1712
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5088
Metrics/base_velocity/error_vel_xy: 0.4607
Metrics/base_velocity/error_vel_yaw: 0.5198
      Episode_Termination/time_out: 1.1250
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 30096000
                    Iteration time: 1.10s
                      Time elapsed: 00:15:30
                               ETA: 00:12:19

################################################################################
                     [1m Learning iteration 836/1500 [0m                      

                       Computation: 32241 steps/s (collection: 1.065s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0250
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 13.6824
                       Mean reward: 16.12
               Mean episode length: 856.20
Episode_Reward/track_lin_vel_xy_exp: 1.0527
Episode_Reward/track_ang_vel_z_exp: 0.4753
       Episode_Reward/lin_vel_z_l2: -0.0477
      Episode_Reward/ang_vel_xy_l2: -0.1376
     Episode_Reward/dof_torques_l2: -0.0853
         Episode_Reward/dof_acc_l2: -0.2162
     Episode_Reward/action_rate_l2: -0.1799
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5099
Metrics/base_velocity/error_vel_xy: 0.4359
Metrics/base_velocity/error_vel_yaw: 0.5177
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 30132000
                    Iteration time: 1.12s
                      Time elapsed: 00:15:31
                               ETA: 00:12:18

################################################################################
                     [1m Learning iteration 837/1500 [0m                      

                       Computation: 33132 steps/s (collection: 1.034s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0249
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 13.6756
                       Mean reward: 15.89
               Mean episode length: 863.53
Episode_Reward/track_lin_vel_xy_exp: 0.9404
Episode_Reward/track_ang_vel_z_exp: 0.4397
       Episode_Reward/lin_vel_z_l2: -0.0482
      Episode_Reward/ang_vel_xy_l2: -0.1337
     Episode_Reward/dof_torques_l2: -0.0868
         Episode_Reward/dof_acc_l2: -0.1969
     Episode_Reward/action_rate_l2: -0.1699
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5138
Metrics/base_velocity/error_vel_xy: 0.4789
Metrics/base_velocity/error_vel_yaw: 0.5320
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 30168000
                    Iteration time: 1.09s
                      Time elapsed: 00:15:32
                               ETA: 00:12:17

################################################################################
                     [1m Learning iteration 838/1500 [0m                      

                       Computation: 33651 steps/s (collection: 1.017s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0230
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 13.6731
                       Mean reward: 16.01
               Mean episode length: 866.32
Episode_Reward/track_lin_vel_xy_exp: 0.9624
Episode_Reward/track_ang_vel_z_exp: 0.4434
       Episode_Reward/lin_vel_z_l2: -0.0434
      Episode_Reward/ang_vel_xy_l2: -0.1270
     Episode_Reward/dof_torques_l2: -0.0868
         Episode_Reward/dof_acc_l2: -0.1863
     Episode_Reward/action_rate_l2: -0.1686
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5205
Metrics/base_velocity/error_vel_xy: 0.4402
Metrics/base_velocity/error_vel_yaw: 0.5094
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 30204000
                    Iteration time: 1.07s
                      Time elapsed: 00:15:33
                               ETA: 00:12:16

################################################################################
                     [1m Learning iteration 839/1500 [0m                      

                       Computation: 31762 steps/s (collection: 1.081s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0241
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 13.6803
                       Mean reward: 14.73
               Mean episode length: 848.86
Episode_Reward/track_lin_vel_xy_exp: 0.9197
Episode_Reward/track_ang_vel_z_exp: 0.4128
       Episode_Reward/lin_vel_z_l2: -0.0442
      Episode_Reward/ang_vel_xy_l2: -0.1248
     Episode_Reward/dof_torques_l2: -0.0776
         Episode_Reward/dof_acc_l2: -0.1875
     Episode_Reward/action_rate_l2: -0.1584
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5227
Metrics/base_velocity/error_vel_xy: 0.3942
Metrics/base_velocity/error_vel_yaw: 0.4919
      Episode_Termination/time_out: 1.0833
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 30240000
                    Iteration time: 1.13s
                      Time elapsed: 00:15:34
                               ETA: 00:12:15

################################################################################
                     [1m Learning iteration 840/1500 [0m                      

                       Computation: 34020 steps/s (collection: 1.004s, learning 0.055s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0287
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 13.6913
                       Mean reward: 15.81
               Mean episode length: 840.43
Episode_Reward/track_lin_vel_xy_exp: 1.0329
Episode_Reward/track_ang_vel_z_exp: 0.4683
       Episode_Reward/lin_vel_z_l2: -0.0445
      Episode_Reward/ang_vel_xy_l2: -0.1291
     Episode_Reward/dof_torques_l2: -0.0869
         Episode_Reward/dof_acc_l2: -0.1851
     Episode_Reward/action_rate_l2: -0.1723
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5221
Metrics/base_velocity/error_vel_xy: 0.4096
Metrics/base_velocity/error_vel_yaw: 0.4954
      Episode_Termination/time_out: 2.0000
  Episode_Termination/base_contact: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 30276000
                    Iteration time: 1.06s
                      Time elapsed: 00:15:35
                               ETA: 00:12:14

################################################################################
                     [1m Learning iteration 841/1500 [0m                      

                       Computation: 33336 steps/s (collection: 1.027s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0292
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 13.6945
                       Mean reward: 16.82
               Mean episode length: 861.20
Episode_Reward/track_lin_vel_xy_exp: 1.0134
Episode_Reward/track_ang_vel_z_exp: 0.4505
       Episode_Reward/lin_vel_z_l2: -0.0461
      Episode_Reward/ang_vel_xy_l2: -0.1290
     Episode_Reward/dof_torques_l2: -0.0783
         Episode_Reward/dof_acc_l2: -0.1957
     Episode_Reward/action_rate_l2: -0.1686
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5084
Metrics/base_velocity/error_vel_xy: 0.3812
Metrics/base_velocity/error_vel_yaw: 0.4812
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 30312000
                    Iteration time: 1.08s
                      Time elapsed: 00:15:36
                               ETA: 00:12:13

################################################################################
                     [1m Learning iteration 842/1500 [0m                      

                       Computation: 34022 steps/s (collection: 1.005s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0265
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 13.6933
                       Mean reward: 16.77
               Mean episode length: 872.44
Episode_Reward/track_lin_vel_xy_exp: 1.0256
Episode_Reward/track_ang_vel_z_exp: 0.4641
       Episode_Reward/lin_vel_z_l2: -0.0494
      Episode_Reward/ang_vel_xy_l2: -0.1335
     Episode_Reward/dof_torques_l2: -0.0942
         Episode_Reward/dof_acc_l2: -0.2104
     Episode_Reward/action_rate_l2: -0.1809
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5068
Metrics/base_velocity/error_vel_xy: 0.4623
Metrics/base_velocity/error_vel_yaw: 0.5490
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 30348000
                    Iteration time: 1.06s
                      Time elapsed: 00:15:37
                               ETA: 00:12:11

################################################################################
                     [1m Learning iteration 843/1500 [0m                      

                       Computation: 33876 steps/s (collection: 1.009s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0247
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 13.6953
                       Mean reward: 15.63
               Mean episode length: 839.28
Episode_Reward/track_lin_vel_xy_exp: 0.8223
Episode_Reward/track_ang_vel_z_exp: 0.3842
       Episode_Reward/lin_vel_z_l2: -0.0427
      Episode_Reward/ang_vel_xy_l2: -0.1171
     Episode_Reward/dof_torques_l2: -0.0775
         Episode_Reward/dof_acc_l2: -0.1792
     Episode_Reward/action_rate_l2: -0.1516
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5031
Metrics/base_velocity/error_vel_xy: 0.4273
Metrics/base_velocity/error_vel_yaw: 0.4844
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 30384000
                    Iteration time: 1.06s
                      Time elapsed: 00:15:38
                               ETA: 00:12:10

################################################################################
                     [1m Learning iteration 844/1500 [0m                      

                       Computation: 33903 steps/s (collection: 1.008s, learning 0.054s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0274
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 13.6971
                       Mean reward: 14.89
               Mean episode length: 854.25
Episode_Reward/track_lin_vel_xy_exp: 0.9271
Episode_Reward/track_ang_vel_z_exp: 0.4509
       Episode_Reward/lin_vel_z_l2: -0.0474
      Episode_Reward/ang_vel_xy_l2: -0.1362
     Episode_Reward/dof_torques_l2: -0.0942
         Episode_Reward/dof_acc_l2: -0.2039
     Episode_Reward/action_rate_l2: -0.1787
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5089
Metrics/base_velocity/error_vel_xy: 0.5449
Metrics/base_velocity/error_vel_yaw: 0.5580
      Episode_Termination/time_out: 1.9167
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 30420000
                    Iteration time: 1.06s
                      Time elapsed: 00:15:39
                               ETA: 00:12:09

################################################################################
                     [1m Learning iteration 845/1500 [0m                      

                       Computation: 34235 steps/s (collection: 0.999s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0274
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 13.6860
                       Mean reward: 16.08
               Mean episode length: 905.55
Episode_Reward/track_lin_vel_xy_exp: 1.1246
Episode_Reward/track_ang_vel_z_exp: 0.4949
       Episode_Reward/lin_vel_z_l2: -0.0500
      Episode_Reward/ang_vel_xy_l2: -0.1449
     Episode_Reward/dof_torques_l2: -0.0930
         Episode_Reward/dof_acc_l2: -0.2148
     Episode_Reward/action_rate_l2: -0.1888
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5133
Metrics/base_velocity/error_vel_xy: 0.4522
Metrics/base_velocity/error_vel_yaw: 0.5710
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 30456000
                    Iteration time: 1.05s
                      Time elapsed: 00:15:40
                               ETA: 00:12:08

################################################################################
                     [1m Learning iteration 846/1500 [0m                      

                       Computation: 33836 steps/s (collection: 1.011s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0288
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 13.6888
                       Mean reward: 16.75
               Mean episode length: 875.57
Episode_Reward/track_lin_vel_xy_exp: 0.9952
Episode_Reward/track_ang_vel_z_exp: 0.4491
       Episode_Reward/lin_vel_z_l2: -0.0408
      Episode_Reward/ang_vel_xy_l2: -0.1227
     Episode_Reward/dof_torques_l2: -0.0849
         Episode_Reward/dof_acc_l2: -0.1745
     Episode_Reward/action_rate_l2: -0.1658
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5177
Metrics/base_velocity/error_vel_xy: 0.3992
Metrics/base_velocity/error_vel_yaw: 0.4798
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 30492000
                    Iteration time: 1.06s
                      Time elapsed: 00:15:41
                               ETA: 00:12:07

################################################################################
                     [1m Learning iteration 847/1500 [0m                      

                       Computation: 33635 steps/s (collection: 1.017s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0252
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 13.7003
                       Mean reward: 15.39
               Mean episode length: 824.14
Episode_Reward/track_lin_vel_xy_exp: 0.9452
Episode_Reward/track_ang_vel_z_exp: 0.4409
       Episode_Reward/lin_vel_z_l2: -0.0473
      Episode_Reward/ang_vel_xy_l2: -0.1318
     Episode_Reward/dof_torques_l2: -0.0870
         Episode_Reward/dof_acc_l2: -0.1942
     Episode_Reward/action_rate_l2: -0.1698
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5220
Metrics/base_velocity/error_vel_xy: 0.4660
Metrics/base_velocity/error_vel_yaw: 0.5262
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 30528000
                    Iteration time: 1.07s
                      Time elapsed: 00:15:42
                               ETA: 00:12:06

################################################################################
                     [1m Learning iteration 848/1500 [0m                      

                       Computation: 34133 steps/s (collection: 1.005s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0243
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 13.6974
                       Mean reward: 15.86
               Mean episode length: 831.80
Episode_Reward/track_lin_vel_xy_exp: 0.9406
Episode_Reward/track_ang_vel_z_exp: 0.4142
       Episode_Reward/lin_vel_z_l2: -0.0420
      Episode_Reward/ang_vel_xy_l2: -0.1188
     Episode_Reward/dof_torques_l2: -0.0745
         Episode_Reward/dof_acc_l2: -0.1712
     Episode_Reward/action_rate_l2: -0.1551
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5285
Metrics/base_velocity/error_vel_xy: 0.3407
Metrics/base_velocity/error_vel_yaw: 0.4481
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 30564000
                    Iteration time: 1.05s
                      Time elapsed: 00:15:44
                               ETA: 00:12:04

################################################################################
                     [1m Learning iteration 849/1500 [0m                      

                       Computation: 33921 steps/s (collection: 1.009s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0260
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.6848
                       Mean reward: 15.92
               Mean episode length: 852.10
Episode_Reward/track_lin_vel_xy_exp: 0.9972
Episode_Reward/track_ang_vel_z_exp: 0.4592
       Episode_Reward/lin_vel_z_l2: -0.0438
      Episode_Reward/ang_vel_xy_l2: -0.1300
     Episode_Reward/dof_torques_l2: -0.0942
         Episode_Reward/dof_acc_l2: -0.1972
     Episode_Reward/action_rate_l2: -0.1771
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5262
Metrics/base_velocity/error_vel_xy: 0.4703
Metrics/base_velocity/error_vel_yaw: 0.5443
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 30600000
                    Iteration time: 1.06s
                      Time elapsed: 00:15:45
                               ETA: 00:12:03

################################################################################
                     [1m Learning iteration 850/1500 [0m                      

                       Computation: 34044 steps/s (collection: 1.005s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0242
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 13.6823
                       Mean reward: 16.29
               Mean episode length: 862.50
Episode_Reward/track_lin_vel_xy_exp: 1.0737
Episode_Reward/track_ang_vel_z_exp: 0.4755
       Episode_Reward/lin_vel_z_l2: -0.0439
      Episode_Reward/ang_vel_xy_l2: -0.1361
     Episode_Reward/dof_torques_l2: -0.0860
         Episode_Reward/dof_acc_l2: -0.2036
     Episode_Reward/action_rate_l2: -0.1763
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5213
Metrics/base_velocity/error_vel_xy: 0.3869
Metrics/base_velocity/error_vel_yaw: 0.5047
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 30636000
                    Iteration time: 1.06s
                      Time elapsed: 00:15:46
                               ETA: 00:12:02

################################################################################
                     [1m Learning iteration 851/1500 [0m                      

                       Computation: 34728 steps/s (collection: 0.985s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0266
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 13.6705
                       Mean reward: 17.36
               Mean episode length: 881.02
Episode_Reward/track_lin_vel_xy_exp: 1.0055
Episode_Reward/track_ang_vel_z_exp: 0.4548
       Episode_Reward/lin_vel_z_l2: -0.0455
      Episode_Reward/ang_vel_xy_l2: -0.1323
     Episode_Reward/dof_torques_l2: -0.0892
         Episode_Reward/dof_acc_l2: -0.1958
     Episode_Reward/action_rate_l2: -0.1742
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5261
Metrics/base_velocity/error_vel_xy: 0.4456
Metrics/base_velocity/error_vel_yaw: 0.5224
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 30672000
                    Iteration time: 1.04s
                      Time elapsed: 00:15:47
                               ETA: 00:12:01

################################################################################
                     [1m Learning iteration 852/1500 [0m                      

                       Computation: 33855 steps/s (collection: 1.012s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0248
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 13.6681
                       Mean reward: 17.12
               Mean episode length: 869.99
Episode_Reward/track_lin_vel_xy_exp: 1.0593
Episode_Reward/track_ang_vel_z_exp: 0.4711
       Episode_Reward/lin_vel_z_l2: -0.0488
      Episode_Reward/ang_vel_xy_l2: -0.1371
     Episode_Reward/dof_torques_l2: -0.0895
         Episode_Reward/dof_acc_l2: -0.2089
     Episode_Reward/action_rate_l2: -0.1813
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5251
Metrics/base_velocity/error_vel_xy: 0.4362
Metrics/base_velocity/error_vel_yaw: 0.5413
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 30708000
                    Iteration time: 1.06s
                      Time elapsed: 00:15:48
                               ETA: 00:12:00

################################################################################
                     [1m Learning iteration 853/1500 [0m                      

                       Computation: 33701 steps/s (collection: 1.017s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0282
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 13.6751
                       Mean reward: 17.44
               Mean episode length: 888.39
Episode_Reward/track_lin_vel_xy_exp: 1.1019
Episode_Reward/track_ang_vel_z_exp: 0.4810
       Episode_Reward/lin_vel_z_l2: -0.0496
      Episode_Reward/ang_vel_xy_l2: -0.1352
     Episode_Reward/dof_torques_l2: -0.0901
         Episode_Reward/dof_acc_l2: -0.2164
     Episode_Reward/action_rate_l2: -0.1844
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5251
Metrics/base_velocity/error_vel_xy: 0.3738
Metrics/base_velocity/error_vel_yaw: 0.5107
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 30744000
                    Iteration time: 1.07s
                      Time elapsed: 00:15:49
                               ETA: 00:11:59

################################################################################
                     [1m Learning iteration 854/1500 [0m                      

                       Computation: 33400 steps/s (collection: 1.025s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0280
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 13.6828
                       Mean reward: 17.00
               Mean episode length: 886.59
Episode_Reward/track_lin_vel_xy_exp: 0.9780
Episode_Reward/track_ang_vel_z_exp: 0.4460
       Episode_Reward/lin_vel_z_l2: -0.0486
      Episode_Reward/ang_vel_xy_l2: -0.1274
     Episode_Reward/dof_torques_l2: -0.0878
         Episode_Reward/dof_acc_l2: -0.1902
     Episode_Reward/action_rate_l2: -0.1701
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5245
Metrics/base_velocity/error_vel_xy: 0.4214
Metrics/base_velocity/error_vel_yaw: 0.4973
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 30780000
                    Iteration time: 1.08s
                      Time elapsed: 00:15:50
                               ETA: 00:11:58

################################################################################
                     [1m Learning iteration 855/1500 [0m                      

                       Computation: 33881 steps/s (collection: 1.008s, learning 0.054s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0270
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 13.6844
                       Mean reward: 17.91
               Mean episode length: 903.40
Episode_Reward/track_lin_vel_xy_exp: 1.1613
Episode_Reward/track_ang_vel_z_exp: 0.5007
       Episode_Reward/lin_vel_z_l2: -0.0451
      Episode_Reward/ang_vel_xy_l2: -0.1353
     Episode_Reward/dof_torques_l2: -0.0977
         Episode_Reward/dof_acc_l2: -0.2103
     Episode_Reward/action_rate_l2: -0.1892
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5292
Metrics/base_velocity/error_vel_xy: 0.3879
Metrics/base_velocity/error_vel_yaw: 0.5469
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 30816000
                    Iteration time: 1.06s
                      Time elapsed: 00:15:51
                               ETA: 00:11:56

################################################################################
                     [1m Learning iteration 856/1500 [0m                      

                       Computation: 34001 steps/s (collection: 1.007s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0244
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 13.6805
                       Mean reward: 16.95
               Mean episode length: 871.97
Episode_Reward/track_lin_vel_xy_exp: 0.9227
Episode_Reward/track_ang_vel_z_exp: 0.4394
       Episode_Reward/lin_vel_z_l2: -0.0433
      Episode_Reward/ang_vel_xy_l2: -0.1272
     Episode_Reward/dof_torques_l2: -0.0890
         Episode_Reward/dof_acc_l2: -0.1835
     Episode_Reward/action_rate_l2: -0.1690
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5314
Metrics/base_velocity/error_vel_xy: 0.4646
Metrics/base_velocity/error_vel_yaw: 0.5109
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 30852000
                    Iteration time: 1.06s
                      Time elapsed: 00:15:52
                               ETA: 00:11:55

################################################################################
                     [1m Learning iteration 857/1500 [0m                      

                       Computation: 34023 steps/s (collection: 1.006s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0234
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.6622
                       Mean reward: 17.30
               Mean episode length: 885.83
Episode_Reward/track_lin_vel_xy_exp: 1.0427
Episode_Reward/track_ang_vel_z_exp: 0.4713
       Episode_Reward/lin_vel_z_l2: -0.0479
      Episode_Reward/ang_vel_xy_l2: -0.1283
     Episode_Reward/dof_torques_l2: -0.0951
         Episode_Reward/dof_acc_l2: -0.2019
     Episode_Reward/action_rate_l2: -0.1789
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5227
Metrics/base_velocity/error_vel_xy: 0.4337
Metrics/base_velocity/error_vel_yaw: 0.5159
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 30888000
                    Iteration time: 1.06s
                      Time elapsed: 00:15:53
                               ETA: 00:11:54

################################################################################
                     [1m Learning iteration 858/1500 [0m                      

                       Computation: 34554 steps/s (collection: 0.989s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0235
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.6467
                       Mean reward: 16.60
               Mean episode length: 877.21
Episode_Reward/track_lin_vel_xy_exp: 0.9707
Episode_Reward/track_ang_vel_z_exp: 0.4422
       Episode_Reward/lin_vel_z_l2: -0.0423
      Episode_Reward/ang_vel_xy_l2: -0.1251
     Episode_Reward/dof_torques_l2: -0.0867
         Episode_Reward/dof_acc_l2: -0.1915
     Episode_Reward/action_rate_l2: -0.1695
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5193
Metrics/base_velocity/error_vel_xy: 0.4117
Metrics/base_velocity/error_vel_yaw: 0.5012
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 30924000
                    Iteration time: 1.04s
                      Time elapsed: 00:15:54
                               ETA: 00:11:53

################################################################################
                     [1m Learning iteration 859/1500 [0m                      

                       Computation: 33624 steps/s (collection: 1.018s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0252
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 13.6337
                       Mean reward: 16.54
               Mean episode length: 856.53
Episode_Reward/track_lin_vel_xy_exp: 0.9581
Episode_Reward/track_ang_vel_z_exp: 0.4311
       Episode_Reward/lin_vel_z_l2: -0.0490
      Episode_Reward/ang_vel_xy_l2: -0.1311
     Episode_Reward/dof_torques_l2: -0.0808
         Episode_Reward/dof_acc_l2: -0.1990
     Episode_Reward/action_rate_l2: -0.1681
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5183
Metrics/base_velocity/error_vel_xy: 0.4086
Metrics/base_velocity/error_vel_yaw: 0.5086
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 30960000
                    Iteration time: 1.07s
                      Time elapsed: 00:15:55
                               ETA: 00:11:52

################################################################################
                     [1m Learning iteration 860/1500 [0m                      

                       Computation: 33517 steps/s (collection: 1.021s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0260
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 13.6357
                       Mean reward: 15.76
               Mean episode length: 816.43
Episode_Reward/track_lin_vel_xy_exp: 0.9324
Episode_Reward/track_ang_vel_z_exp: 0.4334
       Episode_Reward/lin_vel_z_l2: -0.0456
      Episode_Reward/ang_vel_xy_l2: -0.1220
     Episode_Reward/dof_torques_l2: -0.0836
         Episode_Reward/dof_acc_l2: -0.1927
     Episode_Reward/action_rate_l2: -0.1660
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5207
Metrics/base_velocity/error_vel_xy: 0.4290
Metrics/base_velocity/error_vel_yaw: 0.4771
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 30996000
                    Iteration time: 1.07s
                      Time elapsed: 00:15:56
                               ETA: 00:11:51

################################################################################
                     [1m Learning iteration 861/1500 [0m                      

                       Computation: 33933 steps/s (collection: 1.008s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0256
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 13.6387
                       Mean reward: 16.24
               Mean episode length: 863.63
Episode_Reward/track_lin_vel_xy_exp: 0.9953
Episode_Reward/track_ang_vel_z_exp: 0.4621
       Episode_Reward/lin_vel_z_l2: -0.0463
      Episode_Reward/ang_vel_xy_l2: -0.1313
     Episode_Reward/dof_torques_l2: -0.0894
         Episode_Reward/dof_acc_l2: -0.1982
     Episode_Reward/action_rate_l2: -0.1764
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5280
Metrics/base_velocity/error_vel_xy: 0.4769
Metrics/base_velocity/error_vel_yaw: 0.5428
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 31032000
                    Iteration time: 1.06s
                      Time elapsed: 00:15:57
                               ETA: 00:11:50

################################################################################
                     [1m Learning iteration 862/1500 [0m                      

                       Computation: 34161 steps/s (collection: 1.001s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0277
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 13.6370
                       Mean reward: 16.11
               Mean episode length: 863.33
Episode_Reward/track_lin_vel_xy_exp: 0.9397
Episode_Reward/track_ang_vel_z_exp: 0.4331
       Episode_Reward/lin_vel_z_l2: -0.0499
      Episode_Reward/ang_vel_xy_l2: -0.1315
     Episode_Reward/dof_torques_l2: -0.0809
         Episode_Reward/dof_acc_l2: -0.2043
     Episode_Reward/action_rate_l2: -0.1691
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5383
Metrics/base_velocity/error_vel_xy: 0.4677
Metrics/base_velocity/error_vel_yaw: 0.5147
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 31068000
                    Iteration time: 1.05s
                      Time elapsed: 00:15:58
                               ETA: 00:11:48

################################################################################
                     [1m Learning iteration 863/1500 [0m                      

                       Computation: 33685 steps/s (collection: 1.016s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0255
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 13.6446
                       Mean reward: 17.81
               Mean episode length: 905.65
Episode_Reward/track_lin_vel_xy_exp: 1.1642
Episode_Reward/track_ang_vel_z_exp: 0.5117
       Episode_Reward/lin_vel_z_l2: -0.0478
      Episode_Reward/ang_vel_xy_l2: -0.1410
     Episode_Reward/dof_torques_l2: -0.0953
         Episode_Reward/dof_acc_l2: -0.2141
     Episode_Reward/action_rate_l2: -0.1922
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5507
Metrics/base_velocity/error_vel_xy: 0.4371
Metrics/base_velocity/error_vel_yaw: 0.5844
      Episode_Termination/time_out: 2.0833
  Episode_Termination/base_contact: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 31104000
                    Iteration time: 1.07s
                      Time elapsed: 00:15:59
                               ETA: 00:11:47

################################################################################
                     [1m Learning iteration 864/1500 [0m                      

                       Computation: 33929 steps/s (collection: 1.008s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0278
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 13.6303
                       Mean reward: 18.45
               Mean episode length: 902.70
Episode_Reward/track_lin_vel_xy_exp: 0.9144
Episode_Reward/track_ang_vel_z_exp: 0.3963
       Episode_Reward/lin_vel_z_l2: -0.0442
      Episode_Reward/ang_vel_xy_l2: -0.1211
     Episode_Reward/dof_torques_l2: -0.0739
         Episode_Reward/dof_acc_l2: -0.1690
     Episode_Reward/action_rate_l2: -0.1518
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5572
Metrics/base_velocity/error_vel_xy: 0.3683
Metrics/base_velocity/error_vel_yaw: 0.4825
      Episode_Termination/time_out: 2.0000
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 31140000
                    Iteration time: 1.06s
                      Time elapsed: 00:16:01
                               ETA: 00:11:46

################################################################################
                     [1m Learning iteration 865/1500 [0m                      

                       Computation: 34191 steps/s (collection: 0.999s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0235
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.6245
                       Mean reward: 17.62
               Mean episode length: 867.30
Episode_Reward/track_lin_vel_xy_exp: 1.0316
Episode_Reward/track_ang_vel_z_exp: 0.4595
       Episode_Reward/lin_vel_z_l2: -0.0400
      Episode_Reward/ang_vel_xy_l2: -0.1257
     Episode_Reward/dof_torques_l2: -0.0850
         Episode_Reward/dof_acc_l2: -0.1839
     Episode_Reward/action_rate_l2: -0.1694
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5554
Metrics/base_velocity/error_vel_xy: 0.3988
Metrics/base_velocity/error_vel_yaw: 0.5134
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 31176000
                    Iteration time: 1.05s
                      Time elapsed: 00:16:02
                               ETA: 00:11:45

################################################################################
                     [1m Learning iteration 866/1500 [0m                      

                       Computation: 33615 steps/s (collection: 1.017s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0213
               Mean surrogate loss: -0.0156
                 Mean entropy loss: 13.6262
                       Mean reward: 17.35
               Mean episode length: 867.87
Episode_Reward/track_lin_vel_xy_exp: 0.9258
Episode_Reward/track_ang_vel_z_exp: 0.4284
       Episode_Reward/lin_vel_z_l2: -0.0429
      Episode_Reward/ang_vel_xy_l2: -0.1209
     Episode_Reward/dof_torques_l2: -0.0809
         Episode_Reward/dof_acc_l2: -0.1713
     Episode_Reward/action_rate_l2: -0.1593
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5595
Metrics/base_velocity/error_vel_xy: 0.4201
Metrics/base_velocity/error_vel_yaw: 0.4781
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 31212000
                    Iteration time: 1.07s
                      Time elapsed: 00:16:03
                               ETA: 00:11:44

################################################################################
                     [1m Learning iteration 867/1500 [0m                      

                       Computation: 34291 steps/s (collection: 0.996s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0260
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 13.6221
                       Mean reward: 15.35
               Mean episode length: 841.63
Episode_Reward/track_lin_vel_xy_exp: 0.9750
Episode_Reward/track_ang_vel_z_exp: 0.4632
       Episode_Reward/lin_vel_z_l2: -0.0492
      Episode_Reward/ang_vel_xy_l2: -0.1354
     Episode_Reward/dof_torques_l2: -0.0974
         Episode_Reward/dof_acc_l2: -0.2119
     Episode_Reward/action_rate_l2: -0.1812
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5587
Metrics/base_velocity/error_vel_xy: 0.5076
Metrics/base_velocity/error_vel_yaw: 0.5512
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 31248000
                    Iteration time: 1.05s
                      Time elapsed: 00:16:04
                               ETA: 00:11:43

################################################################################
                     [1m Learning iteration 868/1500 [0m                      

                       Computation: 33904 steps/s (collection: 1.009s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0268
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 13.6112
                       Mean reward: 15.65
               Mean episode length: 845.62
Episode_Reward/track_lin_vel_xy_exp: 0.9109
Episode_Reward/track_ang_vel_z_exp: 0.4194
       Episode_Reward/lin_vel_z_l2: -0.0472
      Episode_Reward/ang_vel_xy_l2: -0.1271
     Episode_Reward/dof_torques_l2: -0.0873
         Episode_Reward/dof_acc_l2: -0.1946
     Episode_Reward/action_rate_l2: -0.1670
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5588
Metrics/base_velocity/error_vel_xy: 0.4611
Metrics/base_velocity/error_vel_yaw: 0.5598
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 31284000
                    Iteration time: 1.06s
                      Time elapsed: 00:16:05
                               ETA: 00:11:41

################################################################################
                     [1m Learning iteration 869/1500 [0m                      

                       Computation: 33731 steps/s (collection: 1.014s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0236
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 13.6031
                       Mean reward: 16.53
               Mean episode length: 874.97
Episode_Reward/track_lin_vel_xy_exp: 1.1484
Episode_Reward/track_ang_vel_z_exp: 0.4972
       Episode_Reward/lin_vel_z_l2: -0.0543
      Episode_Reward/ang_vel_xy_l2: -0.1404
     Episode_Reward/dof_torques_l2: -0.0940
         Episode_Reward/dof_acc_l2: -0.2159
     Episode_Reward/action_rate_l2: -0.1881
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5670
Metrics/base_velocity/error_vel_xy: 0.3901
Metrics/base_velocity/error_vel_yaw: 0.5368
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 31320000
                    Iteration time: 1.07s
                      Time elapsed: 00:16:06
                               ETA: 00:11:40

################################################################################
                     [1m Learning iteration 870/1500 [0m                      

                       Computation: 33969 steps/s (collection: 1.008s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0209
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 13.5951
                       Mean reward: 16.63
               Mean episode length: 870.47
Episode_Reward/track_lin_vel_xy_exp: 0.9211
Episode_Reward/track_ang_vel_z_exp: 0.4290
       Episode_Reward/lin_vel_z_l2: -0.0468
      Episode_Reward/ang_vel_xy_l2: -0.1263
     Episode_Reward/dof_torques_l2: -0.0929
         Episode_Reward/dof_acc_l2: -0.1942
     Episode_Reward/action_rate_l2: -0.1705
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5777
Metrics/base_velocity/error_vel_xy: 0.4643
Metrics/base_velocity/error_vel_yaw: 0.5308
      Episode_Termination/time_out: 1.1667
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 31356000
                    Iteration time: 1.06s
                      Time elapsed: 00:16:07
                               ETA: 00:11:39

################################################################################
                     [1m Learning iteration 871/1500 [0m                      

                       Computation: 33439 steps/s (collection: 1.024s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0231
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 13.5834
                       Mean reward: 17.39
               Mean episode length: 893.90
Episode_Reward/track_lin_vel_xy_exp: 1.1473
Episode_Reward/track_ang_vel_z_exp: 0.5074
       Episode_Reward/lin_vel_z_l2: -0.0479
      Episode_Reward/ang_vel_xy_l2: -0.1415
     Episode_Reward/dof_torques_l2: -0.0915
         Episode_Reward/dof_acc_l2: -0.2219
     Episode_Reward/action_rate_l2: -0.1892
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5857
Metrics/base_velocity/error_vel_xy: 0.4039
Metrics/base_velocity/error_vel_yaw: 0.5231
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 31392000
                    Iteration time: 1.08s
                      Time elapsed: 00:16:08
                               ETA: 00:11:38

################################################################################
                     [1m Learning iteration 872/1500 [0m                      

                       Computation: 32946 steps/s (collection: 1.039s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0257
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.5899
                       Mean reward: 16.54
               Mean episode length: 862.88
Episode_Reward/track_lin_vel_xy_exp: 1.0374
Episode_Reward/track_ang_vel_z_exp: 0.4618
       Episode_Reward/lin_vel_z_l2: -0.0452
      Episode_Reward/ang_vel_xy_l2: -0.1282
     Episode_Reward/dof_torques_l2: -0.0869
         Episode_Reward/dof_acc_l2: -0.1981
     Episode_Reward/action_rate_l2: -0.1734
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5889
Metrics/base_velocity/error_vel_xy: 0.3960
Metrics/base_velocity/error_vel_yaw: 0.5066
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 31428000
                    Iteration time: 1.09s
                      Time elapsed: 00:16:09
                               ETA: 00:11:37

################################################################################
                     [1m Learning iteration 873/1500 [0m                      

                       Computation: 33217 steps/s (collection: 1.031s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0260
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 13.5894
                       Mean reward: 15.96
               Mean episode length: 834.89
Episode_Reward/track_lin_vel_xy_exp: 0.9355
Episode_Reward/track_ang_vel_z_exp: 0.4408
       Episode_Reward/lin_vel_z_l2: -0.0442
      Episode_Reward/ang_vel_xy_l2: -0.1265
     Episode_Reward/dof_torques_l2: -0.0827
         Episode_Reward/dof_acc_l2: -0.1789
     Episode_Reward/action_rate_l2: -0.1640
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5930
Metrics/base_velocity/error_vel_xy: 0.4627
Metrics/base_velocity/error_vel_yaw: 0.4995
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 31464000
                    Iteration time: 1.08s
                      Time elapsed: 00:16:10
                               ETA: 00:11:36

################################################################################
                     [1m Learning iteration 874/1500 [0m                      

                       Computation: 34039 steps/s (collection: 1.005s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0270
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 13.5838
                       Mean reward: 16.00
               Mean episode length: 845.97
Episode_Reward/track_lin_vel_xy_exp: 1.0301
Episode_Reward/track_ang_vel_z_exp: 0.4692
       Episode_Reward/lin_vel_z_l2: -0.0500
      Episode_Reward/ang_vel_xy_l2: -0.1374
     Episode_Reward/dof_torques_l2: -0.0924
         Episode_Reward/dof_acc_l2: -0.2196
     Episode_Reward/action_rate_l2: -0.1846
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5845
Metrics/base_velocity/error_vel_xy: 0.4811
Metrics/base_velocity/error_vel_yaw: 0.5606
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 31500000
                    Iteration time: 1.06s
                      Time elapsed: 00:16:11
                               ETA: 00:11:35

################################################################################
                     [1m Learning iteration 875/1500 [0m                      

                       Computation: 34522 steps/s (collection: 0.990s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0244
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 13.5796
                       Mean reward: 16.80
               Mean episode length: 888.82
Episode_Reward/track_lin_vel_xy_exp: 1.0546
Episode_Reward/track_ang_vel_z_exp: 0.4946
       Episode_Reward/lin_vel_z_l2: -0.0480
      Episode_Reward/ang_vel_xy_l2: -0.1394
     Episode_Reward/dof_torques_l2: -0.0974
         Episode_Reward/dof_acc_l2: -0.2152
     Episode_Reward/action_rate_l2: -0.1894
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5943
Metrics/base_velocity/error_vel_xy: 0.5393
Metrics/base_velocity/error_vel_yaw: 0.5910
      Episode_Termination/time_out: 1.9167
  Episode_Termination/base_contact: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 31536000
                    Iteration time: 1.04s
                      Time elapsed: 00:16:12
                               ETA: 00:11:34

################################################################################
                     [1m Learning iteration 876/1500 [0m                      

                       Computation: 34106 steps/s (collection: 1.002s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0241
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 13.5766
                       Mean reward: 16.90
               Mean episode length: 910.43
Episode_Reward/track_lin_vel_xy_exp: 0.9856
Episode_Reward/track_ang_vel_z_exp: 0.4526
       Episode_Reward/lin_vel_z_l2: -0.0443
      Episode_Reward/ang_vel_xy_l2: -0.1306
     Episode_Reward/dof_torques_l2: -0.0881
         Episode_Reward/dof_acc_l2: -0.1846
     Episode_Reward/action_rate_l2: -0.1713
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6024
Metrics/base_velocity/error_vel_xy: 0.4406
Metrics/base_velocity/error_vel_yaw: 0.5039
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 31572000
                    Iteration time: 1.06s
                      Time elapsed: 00:16:13
                               ETA: 00:11:32

################################################################################
                     [1m Learning iteration 877/1500 [0m                      

                       Computation: 33261 steps/s (collection: 1.029s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0238
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 13.5938
                       Mean reward: 16.31
               Mean episode length: 876.33
Episode_Reward/track_lin_vel_xy_exp: 0.9054
Episode_Reward/track_ang_vel_z_exp: 0.4174
       Episode_Reward/lin_vel_z_l2: -0.0436
      Episode_Reward/ang_vel_xy_l2: -0.1184
     Episode_Reward/dof_torques_l2: -0.0766
         Episode_Reward/dof_acc_l2: -0.1736
     Episode_Reward/action_rate_l2: -0.1553
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6004
Metrics/base_velocity/error_vel_xy: 0.4167
Metrics/base_velocity/error_vel_yaw: 0.4861
      Episode_Termination/time_out: 0.9583
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 31608000
                    Iteration time: 1.08s
                      Time elapsed: 00:16:14
                               ETA: 00:11:31

################################################################################
                     [1m Learning iteration 878/1500 [0m                      

                       Computation: 34324 steps/s (collection: 0.995s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0273
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 13.6041
                       Mean reward: 15.76
               Mean episode length: 843.37
Episode_Reward/track_lin_vel_xy_exp: 0.9487
Episode_Reward/track_ang_vel_z_exp: 0.4314
       Episode_Reward/lin_vel_z_l2: -0.0478
      Episode_Reward/ang_vel_xy_l2: -0.1220
     Episode_Reward/dof_torques_l2: -0.0833
         Episode_Reward/dof_acc_l2: -0.1835
     Episode_Reward/action_rate_l2: -0.1623
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6018
Metrics/base_velocity/error_vel_xy: 0.3966
Metrics/base_velocity/error_vel_yaw: 0.4692
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 31644000
                    Iteration time: 1.05s
                      Time elapsed: 00:16:15
                               ETA: 00:11:30

################################################################################
                     [1m Learning iteration 879/1500 [0m                      

                       Computation: 33943 steps/s (collection: 1.008s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0251
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 13.6050
                       Mean reward: 16.15
               Mean episode length: 841.68
Episode_Reward/track_lin_vel_xy_exp: 0.9295
Episode_Reward/track_ang_vel_z_exp: 0.4267
       Episode_Reward/lin_vel_z_l2: -0.0432
      Episode_Reward/ang_vel_xy_l2: -0.1198
     Episode_Reward/dof_torques_l2: -0.0802
         Episode_Reward/dof_acc_l2: -0.1844
     Episode_Reward/action_rate_l2: -0.1615
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5932
Metrics/base_velocity/error_vel_xy: 0.4189
Metrics/base_velocity/error_vel_yaw: 0.4741
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 31680000
                    Iteration time: 1.06s
                      Time elapsed: 00:16:16
                               ETA: 00:11:29

################################################################################
                     [1m Learning iteration 880/1500 [0m                      

                       Computation: 34070 steps/s (collection: 1.003s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0254
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 13.6108
                       Mean reward: 16.62
               Mean episode length: 842.38
Episode_Reward/track_lin_vel_xy_exp: 1.0774
Episode_Reward/track_ang_vel_z_exp: 0.4939
       Episode_Reward/lin_vel_z_l2: -0.0458
      Episode_Reward/ang_vel_xy_l2: -0.1362
     Episode_Reward/dof_torques_l2: -0.0866
         Episode_Reward/dof_acc_l2: -0.2006
     Episode_Reward/action_rate_l2: -0.1788
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5861
Metrics/base_velocity/error_vel_xy: 0.4189
Metrics/base_velocity/error_vel_yaw: 0.5007
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 31716000
                    Iteration time: 1.06s
                      Time elapsed: 00:16:18
                               ETA: 00:11:28

################################################################################
                     [1m Learning iteration 881/1500 [0m                      

                       Computation: 34447 steps/s (collection: 0.992s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0266
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 13.6165
                       Mean reward: 16.53
               Mean episode length: 856.27
Episode_Reward/track_lin_vel_xy_exp: 0.9673
Episode_Reward/track_ang_vel_z_exp: 0.4472
       Episode_Reward/lin_vel_z_l2: -0.0463
      Episode_Reward/ang_vel_xy_l2: -0.1256
     Episode_Reward/dof_torques_l2: -0.0898
         Episode_Reward/dof_acc_l2: -0.2015
     Episode_Reward/action_rate_l2: -0.1710
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5867
Metrics/base_velocity/error_vel_xy: 0.4390
Metrics/base_velocity/error_vel_yaw: 0.4977
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 31752000
                    Iteration time: 1.05s
                      Time elapsed: 00:16:19
                               ETA: 00:11:27

################################################################################
                     [1m Learning iteration 882/1500 [0m                      

                       Computation: 33802 steps/s (collection: 1.014s, learning 0.051s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0290
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 13.6248
                       Mean reward: 17.18
               Mean episode length: 868.40
Episode_Reward/track_lin_vel_xy_exp: 1.0686
Episode_Reward/track_ang_vel_z_exp: 0.4698
       Episode_Reward/lin_vel_z_l2: -0.0488
      Episode_Reward/ang_vel_xy_l2: -0.1302
     Episode_Reward/dof_torques_l2: -0.0861
         Episode_Reward/dof_acc_l2: -0.1869
     Episode_Reward/action_rate_l2: -0.1725
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5947
Metrics/base_velocity/error_vel_xy: 0.3743
Metrics/base_velocity/error_vel_yaw: 0.4955
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 31788000
                    Iteration time: 1.07s
                      Time elapsed: 00:16:20
                               ETA: 00:11:25

################################################################################
                     [1m Learning iteration 883/1500 [0m                      

                       Computation: 33568 steps/s (collection: 1.019s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0274
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 13.6339
                       Mean reward: 16.80
               Mean episode length: 852.73
Episode_Reward/track_lin_vel_xy_exp: 1.0217
Episode_Reward/track_ang_vel_z_exp: 0.4492
       Episode_Reward/lin_vel_z_l2: -0.0479
      Episode_Reward/ang_vel_xy_l2: -0.1273
     Episode_Reward/dof_torques_l2: -0.0828
         Episode_Reward/dof_acc_l2: -0.1975
     Episode_Reward/action_rate_l2: -0.1703
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.5999
Metrics/base_velocity/error_vel_xy: 0.3697
Metrics/base_velocity/error_vel_yaw: 0.4811
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 31824000
                    Iteration time: 1.07s
                      Time elapsed: 00:16:21
                               ETA: 00:11:24

################################################################################
                     [1m Learning iteration 884/1500 [0m                      

                       Computation: 33725 steps/s (collection: 1.014s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0289
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 13.6462
                       Mean reward: 15.61
               Mean episode length: 845.65
Episode_Reward/track_lin_vel_xy_exp: 0.9515
Episode_Reward/track_ang_vel_z_exp: 0.4638
       Episode_Reward/lin_vel_z_l2: -0.0498
      Episode_Reward/ang_vel_xy_l2: -0.1347
     Episode_Reward/dof_torques_l2: -0.0914
         Episode_Reward/dof_acc_l2: -0.2045
     Episode_Reward/action_rate_l2: -0.1778
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6057
Metrics/base_velocity/error_vel_xy: 0.5341
Metrics/base_velocity/error_vel_yaw: 0.5370
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 31860000
                    Iteration time: 1.07s
                      Time elapsed: 00:16:22
                               ETA: 00:11:23

################################################################################
                     [1m Learning iteration 885/1500 [0m                      

                       Computation: 34444 steps/s (collection: 0.991s, learning 0.054s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0241
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 13.6555
                       Mean reward: 16.16
               Mean episode length: 873.87
Episode_Reward/track_lin_vel_xy_exp: 1.0396
Episode_Reward/track_ang_vel_z_exp: 0.4750
       Episode_Reward/lin_vel_z_l2: -0.0475
      Episode_Reward/ang_vel_xy_l2: -0.1359
     Episode_Reward/dof_torques_l2: -0.0911
         Episode_Reward/dof_acc_l2: -0.2005
     Episode_Reward/action_rate_l2: -0.1788
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6134
Metrics/base_velocity/error_vel_xy: 0.4525
Metrics/base_velocity/error_vel_yaw: 0.5375
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 31896000
                    Iteration time: 1.05s
                      Time elapsed: 00:16:23
                               ETA: 00:11:22

################################################################################
                     [1m Learning iteration 886/1500 [0m                      

                       Computation: 33227 steps/s (collection: 1.029s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0243
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 13.6552
                       Mean reward: 15.91
               Mean episode length: 852.01
Episode_Reward/track_lin_vel_xy_exp: 0.9536
Episode_Reward/track_ang_vel_z_exp: 0.4304
       Episode_Reward/lin_vel_z_l2: -0.0478
      Episode_Reward/ang_vel_xy_l2: -0.1266
     Episode_Reward/dof_torques_l2: -0.0880
         Episode_Reward/dof_acc_l2: -0.1956
     Episode_Reward/action_rate_l2: -0.1678
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6177
Metrics/base_velocity/error_vel_xy: 0.4138
Metrics/base_velocity/error_vel_yaw: 0.5149
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 31932000
                    Iteration time: 1.08s
                      Time elapsed: 00:16:24
                               ETA: 00:11:21

################################################################################
                     [1m Learning iteration 887/1500 [0m                      

                       Computation: 31910 steps/s (collection: 1.077s, learning 0.051s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0240
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 13.6500
                       Mean reward: 15.82
               Mean episode length: 870.95
Episode_Reward/track_lin_vel_xy_exp: 0.9600
Episode_Reward/track_ang_vel_z_exp: 0.4604
       Episode_Reward/lin_vel_z_l2: -0.0474
      Episode_Reward/ang_vel_xy_l2: -0.1341
     Episode_Reward/dof_torques_l2: -0.0911
         Episode_Reward/dof_acc_l2: -0.1983
     Episode_Reward/action_rate_l2: -0.1758
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6220
Metrics/base_velocity/error_vel_xy: 0.5297
Metrics/base_velocity/error_vel_yaw: 0.5566
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 31968000
                    Iteration time: 1.13s
                      Time elapsed: 00:16:25
                               ETA: 00:11:20

################################################################################
                     [1m Learning iteration 888/1500 [0m                      

                       Computation: 33770 steps/s (collection: 1.014s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0262
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 13.6482
                       Mean reward: 16.38
               Mean episode length: 867.77
Episode_Reward/track_lin_vel_xy_exp: 1.0467
Episode_Reward/track_ang_vel_z_exp: 0.4653
       Episode_Reward/lin_vel_z_l2: -0.0399
      Episode_Reward/ang_vel_xy_l2: -0.1213
     Episode_Reward/dof_torques_l2: -0.0973
         Episode_Reward/dof_acc_l2: -0.1746
     Episode_Reward/action_rate_l2: -0.1727
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6294
Metrics/base_velocity/error_vel_xy: 0.4009
Metrics/base_velocity/error_vel_yaw: 0.4934
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 32004000
                    Iteration time: 1.07s
                      Time elapsed: 00:16:26
                               ETA: 00:11:19

################################################################################
                     [1m Learning iteration 889/1500 [0m                      

                       Computation: 33206 steps/s (collection: 1.032s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0255
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 13.6467
                       Mean reward: 16.88
               Mean episode length: 869.21
Episode_Reward/track_lin_vel_xy_exp: 1.0406
Episode_Reward/track_ang_vel_z_exp: 0.4669
       Episode_Reward/lin_vel_z_l2: -0.0437
      Episode_Reward/ang_vel_xy_l2: -0.1274
     Episode_Reward/dof_torques_l2: -0.0897
         Episode_Reward/dof_acc_l2: -0.1876
     Episode_Reward/action_rate_l2: -0.1732
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6268
Metrics/base_velocity/error_vel_xy: 0.3976
Metrics/base_velocity/error_vel_yaw: 0.5015
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 32040000
                    Iteration time: 1.08s
                      Time elapsed: 00:16:27
                               ETA: 00:11:18

################################################################################
                     [1m Learning iteration 890/1500 [0m                      

                       Computation: 32789 steps/s (collection: 1.048s, learning 0.050s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0308
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 13.6509
                       Mean reward: 17.20
               Mean episode length: 834.57
Episode_Reward/track_lin_vel_xy_exp: 1.0423
Episode_Reward/track_ang_vel_z_exp: 0.4534
       Episode_Reward/lin_vel_z_l2: -0.0400
      Episode_Reward/ang_vel_xy_l2: -0.1186
     Episode_Reward/dof_torques_l2: -0.0831
         Episode_Reward/dof_acc_l2: -0.1701
     Episode_Reward/action_rate_l2: -0.1612
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6270
Metrics/base_velocity/error_vel_xy: 0.3272
Metrics/base_velocity/error_vel_yaw: 0.4648
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 32076000
                    Iteration time: 1.10s
                      Time elapsed: 00:16:28
                               ETA: 00:11:16

################################################################################
                     [1m Learning iteration 891/1500 [0m                      

                       Computation: 33005 steps/s (collection: 1.038s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0280
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 13.6579
                       Mean reward: 16.78
               Mean episode length: 829.95
Episode_Reward/track_lin_vel_xy_exp: 1.0041
Episode_Reward/track_ang_vel_z_exp: 0.4714
       Episode_Reward/lin_vel_z_l2: -0.0457
      Episode_Reward/ang_vel_xy_l2: -0.1288
     Episode_Reward/dof_torques_l2: -0.0887
         Episode_Reward/dof_acc_l2: -0.1936
     Episode_Reward/action_rate_l2: -0.1732
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6294
Metrics/base_velocity/error_vel_xy: 0.4558
Metrics/base_velocity/error_vel_yaw: 0.4961
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 32112000
                    Iteration time: 1.09s
                      Time elapsed: 00:16:29
                               ETA: 00:11:15

################################################################################
                     [1m Learning iteration 892/1500 [0m                      

                       Computation: 31110 steps/s (collection: 1.105s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0248
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 13.6564
                       Mean reward: 16.42
               Mean episode length: 823.29
Episode_Reward/track_lin_vel_xy_exp: 0.9243
Episode_Reward/track_ang_vel_z_exp: 0.4165
       Episode_Reward/lin_vel_z_l2: -0.0436
      Episode_Reward/ang_vel_xy_l2: -0.1177
     Episode_Reward/dof_torques_l2: -0.0809
         Episode_Reward/dof_acc_l2: -0.1766
     Episode_Reward/action_rate_l2: -0.1550
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6313
Metrics/base_velocity/error_vel_xy: 0.3706
Metrics/base_velocity/error_vel_yaw: 0.4607
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 32148000
                    Iteration time: 1.16s
                      Time elapsed: 00:16:31
                               ETA: 00:11:14

################################################################################
                     [1m Learning iteration 893/1500 [0m                      

                       Computation: 32403 steps/s (collection: 1.058s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0269
               Mean surrogate loss: -0.0152
                 Mean entropy loss: 13.6433
                       Mean reward: 16.07
               Mean episode length: 835.19
Episode_Reward/track_lin_vel_xy_exp: 0.8541
Episode_Reward/track_ang_vel_z_exp: 0.4058
       Episode_Reward/lin_vel_z_l2: -0.0418
      Episode_Reward/ang_vel_xy_l2: -0.1173
     Episode_Reward/dof_torques_l2: -0.0847
         Episode_Reward/dof_acc_l2: -0.1786
     Episode_Reward/action_rate_l2: -0.1570
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6302
Metrics/base_velocity/error_vel_xy: 0.4575
Metrics/base_velocity/error_vel_yaw: 0.4921
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 32184000
                    Iteration time: 1.11s
                      Time elapsed: 00:16:32
                               ETA: 00:11:13

################################################################################
                     [1m Learning iteration 894/1500 [0m                      

                       Computation: 32855 steps/s (collection: 1.044s, learning 0.051s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0274
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 13.6354
                       Mean reward: 16.15
               Mean episode length: 833.58
Episode_Reward/track_lin_vel_xy_exp: 1.1288
Episode_Reward/track_ang_vel_z_exp: 0.4962
       Episode_Reward/lin_vel_z_l2: -0.0490
      Episode_Reward/ang_vel_xy_l2: -0.1361
     Episode_Reward/dof_torques_l2: -0.0896
         Episode_Reward/dof_acc_l2: -0.2047
     Episode_Reward/action_rate_l2: -0.1817
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6377
Metrics/base_velocity/error_vel_xy: 0.3855
Metrics/base_velocity/error_vel_yaw: 0.5063
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 32220000
                    Iteration time: 1.10s
                      Time elapsed: 00:16:33
                               ETA: 00:11:12

################################################################################
                     [1m Learning iteration 895/1500 [0m                      

                       Computation: 32427 steps/s (collection: 1.059s, learning 0.051s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0306
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 13.6355
                       Mean reward: 17.43
               Mean episode length: 874.40
Episode_Reward/track_lin_vel_xy_exp: 1.0260
Episode_Reward/track_ang_vel_z_exp: 0.4566
       Episode_Reward/lin_vel_z_l2: -0.0493
      Episode_Reward/ang_vel_xy_l2: -0.1299
     Episode_Reward/dof_torques_l2: -0.0833
         Episode_Reward/dof_acc_l2: -0.1955
     Episode_Reward/action_rate_l2: -0.1699
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6209
Metrics/base_velocity/error_vel_xy: 0.3775
Metrics/base_velocity/error_vel_yaw: 0.4842
      Episode_Termination/time_out: 2.0417
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 32256000
                    Iteration time: 1.11s
                      Time elapsed: 00:16:34
                               ETA: 00:11:11

################################################################################
                     [1m Learning iteration 896/1500 [0m                      

                       Computation: 32498 steps/s (collection: 1.056s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0235
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 13.6371
                       Mean reward: 16.97
               Mean episode length: 857.10
Episode_Reward/track_lin_vel_xy_exp: 1.0440
Episode_Reward/track_ang_vel_z_exp: 0.4636
       Episode_Reward/lin_vel_z_l2: -0.0424
      Episode_Reward/ang_vel_xy_l2: -0.1270
     Episode_Reward/dof_torques_l2: -0.0905
         Episode_Reward/dof_acc_l2: -0.1842
     Episode_Reward/action_rate_l2: -0.1730
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6212
Metrics/base_velocity/error_vel_xy: 0.4173
Metrics/base_velocity/error_vel_yaw: 0.5258
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 32292000
                    Iteration time: 1.11s
                      Time elapsed: 00:16:35
                               ETA: 00:11:10

################################################################################
                     [1m Learning iteration 897/1500 [0m                      

                       Computation: 31463 steps/s (collection: 1.091s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0267
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 13.6424
                       Mean reward: 17.31
               Mean episode length: 879.61
Episode_Reward/track_lin_vel_xy_exp: 0.9972
Episode_Reward/track_ang_vel_z_exp: 0.4515
       Episode_Reward/lin_vel_z_l2: -0.0436
      Episode_Reward/ang_vel_xy_l2: -0.1245
     Episode_Reward/dof_torques_l2: -0.0874
         Episode_Reward/dof_acc_l2: -0.1856
     Episode_Reward/action_rate_l2: -0.1669
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6187
Metrics/base_velocity/error_vel_xy: 0.4102
Metrics/base_velocity/error_vel_yaw: 0.5025
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 32328000
                    Iteration time: 1.14s
                      Time elapsed: 00:16:36
                               ETA: 00:11:09

################################################################################
                     [1m Learning iteration 898/1500 [0m                      

                       Computation: 31902 steps/s (collection: 1.075s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0247
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 13.6419
                       Mean reward: 15.83
               Mean episode length: 833.10
Episode_Reward/track_lin_vel_xy_exp: 0.9272
Episode_Reward/track_ang_vel_z_exp: 0.4309
       Episode_Reward/lin_vel_z_l2: -0.0440
      Episode_Reward/ang_vel_xy_l2: -0.1217
     Episode_Reward/dof_torques_l2: -0.0906
         Episode_Reward/dof_acc_l2: -0.1874
     Episode_Reward/action_rate_l2: -0.1650
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6204
Metrics/base_velocity/error_vel_xy: 0.4410
Metrics/base_velocity/error_vel_yaw: 0.5020
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 32364000
                    Iteration time: 1.13s
                      Time elapsed: 00:16:37
                               ETA: 00:11:08

################################################################################
                     [1m Learning iteration 899/1500 [0m                      

                       Computation: 31106 steps/s (collection: 1.104s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0247
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 13.6440
                       Mean reward: 16.93
               Mean episode length: 842.84
Episode_Reward/track_lin_vel_xy_exp: 0.9978
Episode_Reward/track_ang_vel_z_exp: 0.4460
       Episode_Reward/lin_vel_z_l2: -0.0412
      Episode_Reward/ang_vel_xy_l2: -0.1169
     Episode_Reward/dof_torques_l2: -0.0833
         Episode_Reward/dof_acc_l2: -0.1695
     Episode_Reward/action_rate_l2: -0.1601
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6246
Metrics/base_velocity/error_vel_xy: 0.3724
Metrics/base_velocity/error_vel_yaw: 0.4631
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 32400000
                    Iteration time: 1.16s
                      Time elapsed: 00:16:38
                               ETA: 00:11:07

################################################################################
                     [1m Learning iteration 900/1500 [0m                      

                       Computation: 31244 steps/s (collection: 1.094s, learning 0.058s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0281
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.6460
                       Mean reward: 16.59
               Mean episode length: 843.59
Episode_Reward/track_lin_vel_xy_exp: 0.9903
Episode_Reward/track_ang_vel_z_exp: 0.4415
       Episode_Reward/lin_vel_z_l2: -0.0440
      Episode_Reward/ang_vel_xy_l2: -0.1216
     Episode_Reward/dof_torques_l2: -0.0822
         Episode_Reward/dof_acc_l2: -0.1720
     Episode_Reward/action_rate_l2: -0.1635
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6245
Metrics/base_velocity/error_vel_xy: 0.3962
Metrics/base_velocity/error_vel_yaw: 0.4942
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 32436000
                    Iteration time: 1.15s
                      Time elapsed: 00:16:40
                               ETA: 00:11:05

################################################################################
                     [1m Learning iteration 901/1500 [0m                      

                       Computation: 30155 steps/s (collection: 1.137s, learning 0.057s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0284
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 13.6526
                       Mean reward: 17.79
               Mean episode length: 877.77
Episode_Reward/track_lin_vel_xy_exp: 1.0920
Episode_Reward/track_ang_vel_z_exp: 0.4821
       Episode_Reward/lin_vel_z_l2: -0.0464
      Episode_Reward/ang_vel_xy_l2: -0.1285
     Episode_Reward/dof_torques_l2: -0.0870
         Episode_Reward/dof_acc_l2: -0.1871
     Episode_Reward/action_rate_l2: -0.1751
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6192
Metrics/base_velocity/error_vel_xy: 0.3839
Metrics/base_velocity/error_vel_yaw: 0.5100
      Episode_Termination/time_out: 2.1667
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 32472000
                    Iteration time: 1.19s
                      Time elapsed: 00:16:41
                               ETA: 00:11:04

################################################################################
                     [1m Learning iteration 902/1500 [0m                      

                       Computation: 28915 steps/s (collection: 1.188s, learning 0.057s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0263
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 13.6612
                       Mean reward: 18.28
               Mean episode length: 891.15
Episode_Reward/track_lin_vel_xy_exp: 1.1094
Episode_Reward/track_ang_vel_z_exp: 0.4876
       Episode_Reward/lin_vel_z_l2: -0.0484
      Episode_Reward/ang_vel_xy_l2: -0.1322
     Episode_Reward/dof_torques_l2: -0.0889
         Episode_Reward/dof_acc_l2: -0.1947
     Episode_Reward/action_rate_l2: -0.1767
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6198
Metrics/base_velocity/error_vel_xy: 0.3853
Metrics/base_velocity/error_vel_yaw: 0.5151
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 32508000
                    Iteration time: 1.25s
                      Time elapsed: 00:16:42
                               ETA: 00:11:03

################################################################################
                     [1m Learning iteration 903/1500 [0m                      

                       Computation: 29348 steps/s (collection: 1.172s, learning 0.054s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0283
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 13.6638
                       Mean reward: 16.24
               Mean episode length: 826.60
Episode_Reward/track_lin_vel_xy_exp: 0.8753
Episode_Reward/track_ang_vel_z_exp: 0.4082
       Episode_Reward/lin_vel_z_l2: -0.0402
      Episode_Reward/ang_vel_xy_l2: -0.1161
     Episode_Reward/dof_torques_l2: -0.0810
         Episode_Reward/dof_acc_l2: -0.1712
     Episode_Reward/action_rate_l2: -0.1554
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6281
Metrics/base_velocity/error_vel_xy: 0.4304
Metrics/base_velocity/error_vel_yaw: 0.4733
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 32544000
                    Iteration time: 1.23s
                      Time elapsed: 00:16:43
                               ETA: 00:11:02

################################################################################
                     [1m Learning iteration 904/1500 [0m                      

                       Computation: 30112 steps/s (collection: 1.141s, learning 0.054s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0290
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 13.6704
                       Mean reward: 15.80
               Mean episode length: 829.04
Episode_Reward/track_lin_vel_xy_exp: 0.9159
Episode_Reward/track_ang_vel_z_exp: 0.4207
       Episode_Reward/lin_vel_z_l2: -0.0436
      Episode_Reward/ang_vel_xy_l2: -0.1229
     Episode_Reward/dof_torques_l2: -0.0800
         Episode_Reward/dof_acc_l2: -0.1848
     Episode_Reward/action_rate_l2: -0.1595
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6286
Metrics/base_velocity/error_vel_xy: 0.4308
Metrics/base_velocity/error_vel_yaw: 0.4959
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 32580000
                    Iteration time: 1.20s
                      Time elapsed: 00:16:44
                               ETA: 00:11:01

################################################################################
                     [1m Learning iteration 905/1500 [0m                      

                       Computation: 32308 steps/s (collection: 1.063s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0256
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 13.6703
                       Mean reward: 16.99
               Mean episode length: 868.63
Episode_Reward/track_lin_vel_xy_exp: 1.0288
Episode_Reward/track_ang_vel_z_exp: 0.4614
       Episode_Reward/lin_vel_z_l2: -0.0448
      Episode_Reward/ang_vel_xy_l2: -0.1243
     Episode_Reward/dof_torques_l2: -0.0866
         Episode_Reward/dof_acc_l2: -0.1881
     Episode_Reward/action_rate_l2: -0.1690
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6318
Metrics/base_velocity/error_vel_xy: 0.3929
Metrics/base_velocity/error_vel_yaw: 0.4855
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 32616000
                    Iteration time: 1.11s
                      Time elapsed: 00:16:46
                               ETA: 00:11:00

################################################################################
                     [1m Learning iteration 906/1500 [0m                      

                       Computation: 30198 steps/s (collection: 1.139s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0258
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 13.6721
                       Mean reward: 17.96
               Mean episode length: 901.13
Episode_Reward/track_lin_vel_xy_exp: 1.0726
Episode_Reward/track_ang_vel_z_exp: 0.4871
       Episode_Reward/lin_vel_z_l2: -0.0443
      Episode_Reward/ang_vel_xy_l2: -0.1304
     Episode_Reward/dof_torques_l2: -0.0939
         Episode_Reward/dof_acc_l2: -0.1924
     Episode_Reward/action_rate_l2: -0.1799
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6334
Metrics/base_velocity/error_vel_xy: 0.4413
Metrics/base_velocity/error_vel_yaw: 0.5334
      Episode_Termination/time_out: 2.0833
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 32652000
                    Iteration time: 1.19s
                      Time elapsed: 00:16:47
                               ETA: 00:10:59

################################################################################
                     [1m Learning iteration 907/1500 [0m                      

                       Computation: 30771 steps/s (collection: 1.118s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0283
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 13.6699
                       Mean reward: 17.53
               Mean episode length: 911.82
Episode_Reward/track_lin_vel_xy_exp: 1.1228
Episode_Reward/track_ang_vel_z_exp: 0.5043
       Episode_Reward/lin_vel_z_l2: -0.0491
      Episode_Reward/ang_vel_xy_l2: -0.1381
     Episode_Reward/dof_torques_l2: -0.0964
         Episode_Reward/dof_acc_l2: -0.2104
     Episode_Reward/action_rate_l2: -0.1892
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6261
Metrics/base_velocity/error_vel_xy: 0.4503
Metrics/base_velocity/error_vel_yaw: 0.5592
      Episode_Termination/time_out: 1.9583
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 32688000
                    Iteration time: 1.17s
                      Time elapsed: 00:16:48
                               ETA: 00:10:58

################################################################################
                     [1m Learning iteration 908/1500 [0m                      

                       Computation: 31191 steps/s (collection: 1.101s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0297
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 13.6776
                       Mean reward: 17.14
               Mean episode length: 892.03
Episode_Reward/track_lin_vel_xy_exp: 0.9650
Episode_Reward/track_ang_vel_z_exp: 0.4367
       Episode_Reward/lin_vel_z_l2: -0.0474
      Episode_Reward/ang_vel_xy_l2: -0.1275
     Episode_Reward/dof_torques_l2: -0.0890
         Episode_Reward/dof_acc_l2: -0.1860
     Episode_Reward/action_rate_l2: -0.1678
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6247
Metrics/base_velocity/error_vel_xy: 0.4219
Metrics/base_velocity/error_vel_yaw: 0.5124
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 32724000
                    Iteration time: 1.15s
                      Time elapsed: 00:16:49
                               ETA: 00:10:57

################################################################################
                     [1m Learning iteration 909/1500 [0m                      

                       Computation: 30934 steps/s (collection: 1.111s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0240
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 13.6820
                       Mean reward: 15.26
               Mean episode length: 808.54
Episode_Reward/track_lin_vel_xy_exp: 0.8969
Episode_Reward/track_ang_vel_z_exp: 0.4069
       Episode_Reward/lin_vel_z_l2: -0.0435
      Episode_Reward/ang_vel_xy_l2: -0.1182
     Episode_Reward/dof_torques_l2: -0.0794
         Episode_Reward/dof_acc_l2: -0.1751
     Episode_Reward/action_rate_l2: -0.1541
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6263
Metrics/base_velocity/error_vel_xy: 0.3888
Metrics/base_velocity/error_vel_yaw: 0.4721
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 32760000
                    Iteration time: 1.16s
                      Time elapsed: 00:16:50
                               ETA: 00:10:56

################################################################################
                     [1m Learning iteration 910/1500 [0m                      

                       Computation: 31882 steps/s (collection: 1.075s, learning 0.054s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0282
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 13.6904
                       Mean reward: 15.57
               Mean episode length: 803.73
Episode_Reward/track_lin_vel_xy_exp: 1.0059
Episode_Reward/track_ang_vel_z_exp: 0.4566
       Episode_Reward/lin_vel_z_l2: -0.0432
      Episode_Reward/ang_vel_xy_l2: -0.1207
     Episode_Reward/dof_torques_l2: -0.0919
         Episode_Reward/dof_acc_l2: -0.1866
     Episode_Reward/action_rate_l2: -0.1702
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6272
Metrics/base_velocity/error_vel_xy: 0.4205
Metrics/base_velocity/error_vel_yaw: 0.4971
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 32796000
                    Iteration time: 1.13s
                      Time elapsed: 00:16:51
                               ETA: 00:10:55

################################################################################
                     [1m Learning iteration 911/1500 [0m                      

                       Computation: 31637 steps/s (collection: 1.084s, learning 0.054s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0291
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 13.6874
                       Mean reward: 15.13
               Mean episode length: 771.58
Episode_Reward/track_lin_vel_xy_exp: 0.8793
Episode_Reward/track_ang_vel_z_exp: 0.3902
       Episode_Reward/lin_vel_z_l2: -0.0443
      Episode_Reward/ang_vel_xy_l2: -0.1100
     Episode_Reward/dof_torques_l2: -0.0692
         Episode_Reward/dof_acc_l2: -0.1508
     Episode_Reward/action_rate_l2: -0.1404
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6279
Metrics/base_velocity/error_vel_xy: 0.3212
Metrics/base_velocity/error_vel_yaw: 0.4142
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 32832000
                    Iteration time: 1.14s
                      Time elapsed: 00:16:52
                               ETA: 00:10:54

################################################################################
                     [1m Learning iteration 912/1500 [0m                      

                       Computation: 33319 steps/s (collection: 1.029s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0267
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 13.6952
                       Mean reward: 15.58
               Mean episode length: 755.37
Episode_Reward/track_lin_vel_xy_exp: 0.7999
Episode_Reward/track_ang_vel_z_exp: 0.3524
       Episode_Reward/lin_vel_z_l2: -0.0356
      Episode_Reward/ang_vel_xy_l2: -0.0986
     Episode_Reward/dof_torques_l2: -0.0707
         Episode_Reward/dof_acc_l2: -0.1369
     Episode_Reward/action_rate_l2: -0.1311
      Episode_Reward/feet_air_time: -0.0009
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6201
Metrics/base_velocity/error_vel_xy: 0.3181
Metrics/base_velocity/error_vel_yaw: 0.4104
      Episode_Termination/time_out: 1.0000
  Episode_Termination/base_contact: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 32868000
                    Iteration time: 1.08s
                      Time elapsed: 00:16:54
                               ETA: 00:10:53

################################################################################
                     [1m Learning iteration 913/1500 [0m                      

                       Computation: 30468 steps/s (collection: 1.129s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0256
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 13.7024
                       Mean reward: 15.63
               Mean episode length: 779.99
Episode_Reward/track_lin_vel_xy_exp: 1.0037
Episode_Reward/track_ang_vel_z_exp: 0.4470
       Episode_Reward/lin_vel_z_l2: -0.0461
      Episode_Reward/ang_vel_xy_l2: -0.1230
     Episode_Reward/dof_torques_l2: -0.0853
         Episode_Reward/dof_acc_l2: -0.1924
     Episode_Reward/action_rate_l2: -0.1688
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6293
Metrics/base_velocity/error_vel_xy: 0.3976
Metrics/base_velocity/error_vel_yaw: 0.4903
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 32904000
                    Iteration time: 1.18s
                      Time elapsed: 00:16:55
                               ETA: 00:10:52

################################################################################
                     [1m Learning iteration 914/1500 [0m                      

                       Computation: 30570 steps/s (collection: 1.126s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0249
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 13.7090
                       Mean reward: 15.93
               Mean episode length: 843.74
Episode_Reward/track_lin_vel_xy_exp: 1.0156
Episode_Reward/track_ang_vel_z_exp: 0.4757
       Episode_Reward/lin_vel_z_l2: -0.0450
      Episode_Reward/ang_vel_xy_l2: -0.1334
     Episode_Reward/dof_torques_l2: -0.0925
         Episode_Reward/dof_acc_l2: -0.2023
     Episode_Reward/action_rate_l2: -0.1800
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6364
Metrics/base_velocity/error_vel_xy: 0.5058
Metrics/base_velocity/error_vel_yaw: 0.5600
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 32940000
                    Iteration time: 1.18s
                      Time elapsed: 00:16:56
                               ETA: 00:10:50

################################################################################
                     [1m Learning iteration 915/1500 [0m                      

                       Computation: 30448 steps/s (collection: 1.126s, learning 0.056s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0280
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 13.7209
                       Mean reward: 16.29
               Mean episode length: 843.95
Episode_Reward/track_lin_vel_xy_exp: 0.9892
Episode_Reward/track_ang_vel_z_exp: 0.4364
       Episode_Reward/lin_vel_z_l2: -0.0426
      Episode_Reward/ang_vel_xy_l2: -0.1173
     Episode_Reward/dof_torques_l2: -0.0804
         Episode_Reward/dof_acc_l2: -0.1765
     Episode_Reward/action_rate_l2: -0.1608
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6332
Metrics/base_velocity/error_vel_xy: 0.3451
Metrics/base_velocity/error_vel_yaw: 0.4561
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 32976000
                    Iteration time: 1.18s
                      Time elapsed: 00:16:57
                               ETA: 00:10:49

################################################################################
                     [1m Learning iteration 916/1500 [0m                      

                       Computation: 30048 steps/s (collection: 1.146s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0267
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 13.7341
                       Mean reward: 16.85
               Mean episode length: 855.46
Episode_Reward/track_lin_vel_xy_exp: 1.0878
Episode_Reward/track_ang_vel_z_exp: 0.4940
       Episode_Reward/lin_vel_z_l2: -0.0518
      Episode_Reward/ang_vel_xy_l2: -0.1390
     Episode_Reward/dof_torques_l2: -0.0918
         Episode_Reward/dof_acc_l2: -0.2106
     Episode_Reward/action_rate_l2: -0.1824
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6385
Metrics/base_velocity/error_vel_xy: 0.4576
Metrics/base_velocity/error_vel_yaw: 0.5511
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 33012000
                    Iteration time: 1.20s
                      Time elapsed: 00:16:58
                               ETA: 00:10:48

################################################################################
                     [1m Learning iteration 917/1500 [0m                      

                       Computation: 30307 steps/s (collection: 1.136s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0231
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 13.7473
                       Mean reward: 17.69
               Mean episode length: 884.78
Episode_Reward/track_lin_vel_xy_exp: 0.9538
Episode_Reward/track_ang_vel_z_exp: 0.4351
       Episode_Reward/lin_vel_z_l2: -0.0411
      Episode_Reward/ang_vel_xy_l2: -0.1152
     Episode_Reward/dof_torques_l2: -0.0805
         Episode_Reward/dof_acc_l2: -0.1755
     Episode_Reward/action_rate_l2: -0.1582
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6443
Metrics/base_velocity/error_vel_xy: 0.4009
Metrics/base_velocity/error_vel_yaw: 0.4582
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 33048000
                    Iteration time: 1.19s
                      Time elapsed: 00:16:59
                               ETA: 00:10:47

################################################################################
                     [1m Learning iteration 918/1500 [0m                      

                       Computation: 30181 steps/s (collection: 1.133s, learning 0.059s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0244
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 13.7432
                       Mean reward: 16.19
               Mean episode length: 839.97
Episode_Reward/track_lin_vel_xy_exp: 0.7796
Episode_Reward/track_ang_vel_z_exp: 0.3660
       Episode_Reward/lin_vel_z_l2: -0.0406
      Episode_Reward/ang_vel_xy_l2: -0.1105
     Episode_Reward/dof_torques_l2: -0.0731
         Episode_Reward/dof_acc_l2: -0.1577
     Episode_Reward/action_rate_l2: -0.1414
      Episode_Reward/feet_air_time: -0.0009
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6518
Metrics/base_velocity/error_vel_xy: 0.4246
Metrics/base_velocity/error_vel_yaw: 0.4813
      Episode_Termination/time_out: 1.0833
  Episode_Termination/base_contact: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 33084000
                    Iteration time: 1.19s
                      Time elapsed: 00:17:01
                               ETA: 00:10:46

################################################################################
                     [1m Learning iteration 919/1500 [0m                      

                       Computation: 30765 steps/s (collection: 1.118s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0300
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 13.7408
                       Mean reward: 15.68
               Mean episode length: 804.75
Episode_Reward/track_lin_vel_xy_exp: 0.9547
Episode_Reward/track_ang_vel_z_exp: 0.4324
       Episode_Reward/lin_vel_z_l2: -0.0473
      Episode_Reward/ang_vel_xy_l2: -0.1217
     Episode_Reward/dof_torques_l2: -0.0807
         Episode_Reward/dof_acc_l2: -0.1908
     Episode_Reward/action_rate_l2: -0.1619
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6520
Metrics/base_velocity/error_vel_xy: 0.4141
Metrics/base_velocity/error_vel_yaw: 0.4939
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 33120000
                    Iteration time: 1.17s
                      Time elapsed: 00:17:02
                               ETA: 00:10:45

################################################################################
                     [1m Learning iteration 920/1500 [0m                      

                       Computation: 31780 steps/s (collection: 1.081s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0247
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 13.7412
                       Mean reward: 16.37
               Mean episode length: 836.61
Episode_Reward/track_lin_vel_xy_exp: 1.0520
Episode_Reward/track_ang_vel_z_exp: 0.4745
       Episode_Reward/lin_vel_z_l2: -0.0521
      Episode_Reward/ang_vel_xy_l2: -0.1323
     Episode_Reward/dof_torques_l2: -0.0881
         Episode_Reward/dof_acc_l2: -0.1997
     Episode_Reward/action_rate_l2: -0.1768
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6573
Metrics/base_velocity/error_vel_xy: 0.4414
Metrics/base_velocity/error_vel_yaw: 0.5328
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 33156000
                    Iteration time: 1.13s
                      Time elapsed: 00:17:03
                               ETA: 00:10:44

################################################################################
                     [1m Learning iteration 921/1500 [0m                      

                       Computation: 31238 steps/s (collection: 1.099s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0225
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.7375
                       Mean reward: 16.58
               Mean episode length: 830.79
Episode_Reward/track_lin_vel_xy_exp: 0.9412
Episode_Reward/track_ang_vel_z_exp: 0.4254
       Episode_Reward/lin_vel_z_l2: -0.0467
      Episode_Reward/ang_vel_xy_l2: -0.1218
     Episode_Reward/dof_torques_l2: -0.0802
         Episode_Reward/dof_acc_l2: -0.1857
     Episode_Reward/action_rate_l2: -0.1599
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6644
Metrics/base_velocity/error_vel_xy: 0.3821
Metrics/base_velocity/error_vel_yaw: 0.4672
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 33192000
                    Iteration time: 1.15s
                      Time elapsed: 00:17:04
                               ETA: 00:10:43

################################################################################
                     [1m Learning iteration 922/1500 [0m                      

                       Computation: 31947 steps/s (collection: 1.071s, learning 0.056s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0269
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 13.7363
                       Mean reward: 15.31
               Mean episode length: 816.66
Episode_Reward/track_lin_vel_xy_exp: 0.9781
Episode_Reward/track_ang_vel_z_exp: 0.4531
       Episode_Reward/lin_vel_z_l2: -0.0517
      Episode_Reward/ang_vel_xy_l2: -0.1318
     Episode_Reward/dof_torques_l2: -0.0835
         Episode_Reward/dof_acc_l2: -0.2076
     Episode_Reward/action_rate_l2: -0.1738
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6690
Metrics/base_velocity/error_vel_xy: 0.4651
Metrics/base_velocity/error_vel_yaw: 0.5272
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 33228000
                    Iteration time: 1.13s
                      Time elapsed: 00:17:05
                               ETA: 00:10:42

################################################################################
                     [1m Learning iteration 923/1500 [0m                      

                       Computation: 31632 steps/s (collection: 1.085s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0304
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 13.7411
                       Mean reward: 16.02
               Mean episode length: 852.55
Episode_Reward/track_lin_vel_xy_exp: 1.1028
Episode_Reward/track_ang_vel_z_exp: 0.4979
       Episode_Reward/lin_vel_z_l2: -0.0439
      Episode_Reward/ang_vel_xy_l2: -0.1339
     Episode_Reward/dof_torques_l2: -0.0973
         Episode_Reward/dof_acc_l2: -0.2028
     Episode_Reward/action_rate_l2: -0.1863
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6706
Metrics/base_velocity/error_vel_xy: 0.4475
Metrics/base_velocity/error_vel_yaw: 0.5479
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 33264000
                    Iteration time: 1.14s
                      Time elapsed: 00:17:06
                               ETA: 00:10:41

################################################################################
                     [1m Learning iteration 924/1500 [0m                      

                       Computation: 31420 steps/s (collection: 1.090s, learning 0.056s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0251
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 13.7371
                       Mean reward: 17.37
               Mean episode length: 883.96
Episode_Reward/track_lin_vel_xy_exp: 1.0920
Episode_Reward/track_ang_vel_z_exp: 0.4807
       Episode_Reward/lin_vel_z_l2: -0.0531
      Episode_Reward/ang_vel_xy_l2: -0.1295
     Episode_Reward/dof_torques_l2: -0.0848
         Episode_Reward/dof_acc_l2: -0.1982
     Episode_Reward/action_rate_l2: -0.1753
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6793
Metrics/base_velocity/error_vel_xy: 0.3595
Metrics/base_velocity/error_vel_yaw: 0.4830
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 33300000
                    Iteration time: 1.15s
                      Time elapsed: 00:17:08
                               ETA: 00:10:40

################################################################################
                     [1m Learning iteration 925/1500 [0m                      

                       Computation: 29629 steps/s (collection: 1.148s, learning 0.067s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0238
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 13.7320
                       Mean reward: 17.08
               Mean episode length: 868.43
Episode_Reward/track_lin_vel_xy_exp: 1.0235
Episode_Reward/track_ang_vel_z_exp: 0.4650
       Episode_Reward/lin_vel_z_l2: -0.0464
      Episode_Reward/ang_vel_xy_l2: -0.1287
     Episode_Reward/dof_torques_l2: -0.0866
         Episode_Reward/dof_acc_l2: -0.1879
     Episode_Reward/action_rate_l2: -0.1720
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6797
Metrics/base_velocity/error_vel_xy: 0.4241
Metrics/base_velocity/error_vel_yaw: 0.5009
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 33336000
                    Iteration time: 1.21s
                      Time elapsed: 00:17:09
                               ETA: 00:10:39

################################################################################
                     [1m Learning iteration 926/1500 [0m                      

                       Computation: 31705 steps/s (collection: 1.084s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0215
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 13.7220
                       Mean reward: 16.87
               Mean episode length: 869.16
Episode_Reward/track_lin_vel_xy_exp: 1.0571
Episode_Reward/track_ang_vel_z_exp: 0.4837
       Episode_Reward/lin_vel_z_l2: -0.0514
      Episode_Reward/ang_vel_xy_l2: -0.1396
     Episode_Reward/dof_torques_l2: -0.0923
         Episode_Reward/dof_acc_l2: -0.2010
     Episode_Reward/action_rate_l2: -0.1838
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6829
Metrics/base_velocity/error_vel_xy: 0.4763
Metrics/base_velocity/error_vel_yaw: 0.5615
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 33372000
                    Iteration time: 1.14s
                      Time elapsed: 00:17:10
                               ETA: 00:10:38

################################################################################
                     [1m Learning iteration 927/1500 [0m                      

                       Computation: 30950 steps/s (collection: 1.111s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0244
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 13.7102
                       Mean reward: 16.83
               Mean episode length: 898.72
Episode_Reward/track_lin_vel_xy_exp: 1.0266
Episode_Reward/track_ang_vel_z_exp: 0.4757
       Episode_Reward/lin_vel_z_l2: -0.0536
      Episode_Reward/ang_vel_xy_l2: -0.1372
     Episode_Reward/dof_torques_l2: -0.1005
         Episode_Reward/dof_acc_l2: -0.2175
     Episode_Reward/action_rate_l2: -0.1856
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6856
Metrics/base_velocity/error_vel_xy: 0.5330
Metrics/base_velocity/error_vel_yaw: 0.5956
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 33408000
                    Iteration time: 1.16s
                      Time elapsed: 00:17:11
                               ETA: 00:10:36

################################################################################
                     [1m Learning iteration 928/1500 [0m                      

                       Computation: 30233 steps/s (collection: 1.138s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0233
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 13.7225
                       Mean reward: 15.78
               Mean episode length: 855.46
Episode_Reward/track_lin_vel_xy_exp: 0.9893
Episode_Reward/track_ang_vel_z_exp: 0.4470
       Episode_Reward/lin_vel_z_l2: -0.0469
      Episode_Reward/ang_vel_xy_l2: -0.1266
     Episode_Reward/dof_torques_l2: -0.0879
         Episode_Reward/dof_acc_l2: -0.1986
     Episode_Reward/action_rate_l2: -0.1722
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6850
Metrics/base_velocity/error_vel_xy: 0.4103
Metrics/base_velocity/error_vel_yaw: 0.4906
      Episode_Termination/time_out: 1.1250
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 33444000
                    Iteration time: 1.19s
                      Time elapsed: 00:17:12
                               ETA: 00:10:35

################################################################################
                     [1m Learning iteration 929/1500 [0m                      

                       Computation: 30926 steps/s (collection: 1.113s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0260
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 13.7280
                       Mean reward: 16.21
               Mean episode length: 841.86
Episode_Reward/track_lin_vel_xy_exp: 1.0300
Episode_Reward/track_ang_vel_z_exp: 0.4733
       Episode_Reward/lin_vel_z_l2: -0.0505
      Episode_Reward/ang_vel_xy_l2: -0.1283
     Episode_Reward/dof_torques_l2: -0.0922
         Episode_Reward/dof_acc_l2: -0.1925
     Episode_Reward/action_rate_l2: -0.1790
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6915
Metrics/base_velocity/error_vel_xy: 0.4582
Metrics/base_velocity/error_vel_yaw: 0.5280
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 33480000
                    Iteration time: 1.16s
                      Time elapsed: 00:17:13
                               ETA: 00:10:34

################################################################################
                     [1m Learning iteration 930/1500 [0m                      

                       Computation: 30564 steps/s (collection: 1.126s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0246
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 13.7305
                       Mean reward: 17.37
               Mean episode length: 842.17
Episode_Reward/track_lin_vel_xy_exp: 1.1088
Episode_Reward/track_ang_vel_z_exp: 0.4914
       Episode_Reward/lin_vel_z_l2: -0.0437
      Episode_Reward/ang_vel_xy_l2: -0.1248
     Episode_Reward/dof_torques_l2: -0.0933
         Episode_Reward/dof_acc_l2: -0.1842
     Episode_Reward/action_rate_l2: -0.1778
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6934
Metrics/base_velocity/error_vel_xy: 0.3670
Metrics/base_velocity/error_vel_yaw: 0.4854
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 33516000
                    Iteration time: 1.18s
                      Time elapsed: 00:17:15
                               ETA: 00:10:33

################################################################################
                     [1m Learning iteration 931/1500 [0m                      

                       Computation: 30527 steps/s (collection: 1.121s, learning 0.058s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0282
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 13.7255
                       Mean reward: 18.73
               Mean episode length: 887.73
Episode_Reward/track_lin_vel_xy_exp: 1.0814
Episode_Reward/track_ang_vel_z_exp: 0.4859
       Episode_Reward/lin_vel_z_l2: -0.0476
      Episode_Reward/ang_vel_xy_l2: -0.1339
     Episode_Reward/dof_torques_l2: -0.0908
         Episode_Reward/dof_acc_l2: -0.1989
     Episode_Reward/action_rate_l2: -0.1832
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7030
Metrics/base_velocity/error_vel_xy: 0.4492
Metrics/base_velocity/error_vel_yaw: 0.5377
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 33552000
                    Iteration time: 1.18s
                      Time elapsed: 00:17:16
                               ETA: 00:10:32

################################################################################
                     [1m Learning iteration 932/1500 [0m                      

                       Computation: 30423 steps/s (collection: 1.130s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0276
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 13.7278
                       Mean reward: 17.56
               Mean episode length: 879.97
Episode_Reward/track_lin_vel_xy_exp: 0.9547
Episode_Reward/track_ang_vel_z_exp: 0.4368
       Episode_Reward/lin_vel_z_l2: -0.0457
      Episode_Reward/ang_vel_xy_l2: -0.1230
     Episode_Reward/dof_torques_l2: -0.0892
         Episode_Reward/dof_acc_l2: -0.1964
     Episode_Reward/action_rate_l2: -0.1694
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6937
Metrics/base_velocity/error_vel_xy: 0.4380
Metrics/base_velocity/error_vel_yaw: 0.5158
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 33588000
                    Iteration time: 1.18s
                      Time elapsed: 00:17:17
                               ETA: 00:10:31

################################################################################
                     [1m Learning iteration 933/1500 [0m                      

                       Computation: 30206 steps/s (collection: 1.140s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0289
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 13.7371
                       Mean reward: 15.31
               Mean episode length: 833.04
Episode_Reward/track_lin_vel_xy_exp: 0.9627
Episode_Reward/track_ang_vel_z_exp: 0.4519
       Episode_Reward/lin_vel_z_l2: -0.0477
      Episode_Reward/ang_vel_xy_l2: -0.1284
     Episode_Reward/dof_torques_l2: -0.0854
         Episode_Reward/dof_acc_l2: -0.1982
     Episode_Reward/action_rate_l2: -0.1716
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6952
Metrics/base_velocity/error_vel_xy: 0.4626
Metrics/base_velocity/error_vel_yaw: 0.5069
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33624000
                    Iteration time: 1.19s
                      Time elapsed: 00:17:18
                               ETA: 00:10:30

################################################################################
                     [1m Learning iteration 934/1500 [0m                      

                       Computation: 29844 steps/s (collection: 1.156s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0267
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 13.7413
                       Mean reward: 15.94
               Mean episode length: 831.66
Episode_Reward/track_lin_vel_xy_exp: 0.9830
Episode_Reward/track_ang_vel_z_exp: 0.4366
       Episode_Reward/lin_vel_z_l2: -0.0440
      Episode_Reward/ang_vel_xy_l2: -0.1212
     Episode_Reward/dof_torques_l2: -0.0800
         Episode_Reward/dof_acc_l2: -0.1833
     Episode_Reward/action_rate_l2: -0.1625
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6921
Metrics/base_velocity/error_vel_xy: 0.3692
Metrics/base_velocity/error_vel_yaw: 0.4722
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 33660000
                    Iteration time: 1.21s
                      Time elapsed: 00:17:19
                               ETA: 00:10:29

################################################################################
                     [1m Learning iteration 935/1500 [0m                      

                       Computation: 31258 steps/s (collection: 1.099s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0288
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 13.7478
                       Mean reward: 15.37
               Mean episode length: 809.27
Episode_Reward/track_lin_vel_xy_exp: 0.9368
Episode_Reward/track_ang_vel_z_exp: 0.4296
       Episode_Reward/lin_vel_z_l2: -0.0461
      Episode_Reward/ang_vel_xy_l2: -0.1238
     Episode_Reward/dof_torques_l2: -0.0843
         Episode_Reward/dof_acc_l2: -0.1803
     Episode_Reward/action_rate_l2: -0.1646
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6967
Metrics/base_velocity/error_vel_xy: 0.4214
Metrics/base_velocity/error_vel_yaw: 0.4928
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 33696000
                    Iteration time: 1.15s
                      Time elapsed: 00:17:20
                               ETA: 00:10:28

################################################################################
                     [1m Learning iteration 936/1500 [0m                      

                       Computation: 31080 steps/s (collection: 1.105s, learning 0.054s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0254
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 13.7518
                       Mean reward: 17.10
               Mean episode length: 848.44
Episode_Reward/track_lin_vel_xy_exp: 1.0870
Episode_Reward/track_ang_vel_z_exp: 0.4749
       Episode_Reward/lin_vel_z_l2: -0.0503
      Episode_Reward/ang_vel_xy_l2: -0.1304
     Episode_Reward/dof_torques_l2: -0.0867
         Episode_Reward/dof_acc_l2: -0.1978
     Episode_Reward/action_rate_l2: -0.1759
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6901
Metrics/base_velocity/error_vel_xy: 0.3453
Metrics/base_velocity/error_vel_yaw: 0.4722
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 33732000
                    Iteration time: 1.16s
                      Time elapsed: 00:17:22
                               ETA: 00:10:27

################################################################################
                     [1m Learning iteration 937/1500 [0m                      

                       Computation: 30654 steps/s (collection: 1.121s, learning 0.054s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0256
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 13.7527
                       Mean reward: 17.09
               Mean episode length: 856.64
Episode_Reward/track_lin_vel_xy_exp: 0.9734
Episode_Reward/track_ang_vel_z_exp: 0.4326
       Episode_Reward/lin_vel_z_l2: -0.0453
      Episode_Reward/ang_vel_xy_l2: -0.1213
     Episode_Reward/dof_torques_l2: -0.0838
         Episode_Reward/dof_acc_l2: -0.1794
     Episode_Reward/action_rate_l2: -0.1628
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6837
Metrics/base_velocity/error_vel_xy: 0.3779
Metrics/base_velocity/error_vel_yaw: 0.4822
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 33768000
                    Iteration time: 1.17s
                      Time elapsed: 00:17:23
                               ETA: 00:10:26

################################################################################
                     [1m Learning iteration 938/1500 [0m                      

                       Computation: 30818 steps/s (collection: 1.116s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0272
               Mean surrogate loss: -0.0149
                 Mean entropy loss: 13.7507
                       Mean reward: 17.30
               Mean episode length: 864.05
Episode_Reward/track_lin_vel_xy_exp: 1.0070
Episode_Reward/track_ang_vel_z_exp: 0.4420
       Episode_Reward/lin_vel_z_l2: -0.0398
      Episode_Reward/ang_vel_xy_l2: -0.1195
     Episode_Reward/dof_torques_l2: -0.0830
         Episode_Reward/dof_acc_l2: -0.1750
     Episode_Reward/action_rate_l2: -0.1641
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6876
Metrics/base_velocity/error_vel_xy: 0.3779
Metrics/base_velocity/error_vel_yaw: 0.5016
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 33804000
                    Iteration time: 1.17s
                      Time elapsed: 00:17:24
                               ETA: 00:10:25

################################################################################
                     [1m Learning iteration 939/1500 [0m                      

                       Computation: 31276 steps/s (collection: 1.099s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0266
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 13.7605
                       Mean reward: 17.64
               Mean episode length: 861.79
Episode_Reward/track_lin_vel_xy_exp: 1.0270
Episode_Reward/track_ang_vel_z_exp: 0.4520
       Episode_Reward/lin_vel_z_l2: -0.0423
      Episode_Reward/ang_vel_xy_l2: -0.1188
     Episode_Reward/dof_torques_l2: -0.0845
         Episode_Reward/dof_acc_l2: -0.1770
     Episode_Reward/action_rate_l2: -0.1668
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6901
Metrics/base_velocity/error_vel_xy: 0.3598
Metrics/base_velocity/error_vel_yaw: 0.4765
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 33840000
                    Iteration time: 1.15s
                      Time elapsed: 00:17:25
                               ETA: 00:10:24

################################################################################
                     [1m Learning iteration 940/1500 [0m                      

                       Computation: 31104 steps/s (collection: 1.101s, learning 0.056s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0307
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 13.7716
                       Mean reward: 17.56
               Mean episode length: 866.53
Episode_Reward/track_lin_vel_xy_exp: 1.0306
Episode_Reward/track_ang_vel_z_exp: 0.4681
       Episode_Reward/lin_vel_z_l2: -0.0468
      Episode_Reward/ang_vel_xy_l2: -0.1292
     Episode_Reward/dof_torques_l2: -0.0913
         Episode_Reward/dof_acc_l2: -0.1902
     Episode_Reward/action_rate_l2: -0.1750
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6903
Metrics/base_velocity/error_vel_xy: 0.4280
Metrics/base_velocity/error_vel_yaw: 0.5136
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 33876000
                    Iteration time: 1.16s
                      Time elapsed: 00:17:26
                               ETA: 00:10:22

################################################################################
                     [1m Learning iteration 941/1500 [0m                      

                       Computation: 30781 steps/s (collection: 1.106s, learning 0.063s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0290
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 13.7809
                       Mean reward: 16.40
               Mean episode length: 813.56
Episode_Reward/track_lin_vel_xy_exp: 0.8934
Episode_Reward/track_ang_vel_z_exp: 0.3902
       Episode_Reward/lin_vel_z_l2: -0.0462
      Episode_Reward/ang_vel_xy_l2: -0.1064
     Episode_Reward/dof_torques_l2: -0.0681
         Episode_Reward/dof_acc_l2: -0.1507
     Episode_Reward/action_rate_l2: -0.1393
      Episode_Reward/feet_air_time: -0.0009
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6949
Metrics/base_velocity/error_vel_xy: 0.2891
Metrics/base_velocity/error_vel_yaw: 0.3965
      Episode_Termination/time_out: 1.1667
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 33912000
                    Iteration time: 1.17s
                      Time elapsed: 00:17:27
                               ETA: 00:10:21

################################################################################
                     [1m Learning iteration 942/1500 [0m                      

                       Computation: 30496 steps/s (collection: 1.128s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0251
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 13.7875
                       Mean reward: 15.91
               Mean episode length: 802.79
Episode_Reward/track_lin_vel_xy_exp: 0.9878
Episode_Reward/track_ang_vel_z_exp: 0.4479
       Episode_Reward/lin_vel_z_l2: -0.0474
      Episode_Reward/ang_vel_xy_l2: -0.1287
     Episode_Reward/dof_torques_l2: -0.0863
         Episode_Reward/dof_acc_l2: -0.1867
     Episode_Reward/action_rate_l2: -0.1703
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6969
Metrics/base_velocity/error_vel_xy: 0.4330
Metrics/base_velocity/error_vel_yaw: 0.5279
      Episode_Termination/time_out: 1.1667
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 33948000
                    Iteration time: 1.18s
                      Time elapsed: 00:17:29
                               ETA: 00:10:20

################################################################################
                     [1m Learning iteration 943/1500 [0m                      

                       Computation: 31068 steps/s (collection: 1.106s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0288
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 13.7788
                       Mean reward: 17.14
               Mean episode length: 864.05
Episode_Reward/track_lin_vel_xy_exp: 0.9972
Episode_Reward/track_ang_vel_z_exp: 0.4546
       Episode_Reward/lin_vel_z_l2: -0.0456
      Episode_Reward/ang_vel_xy_l2: -0.1253
     Episode_Reward/dof_torques_l2: -0.0828
         Episode_Reward/dof_acc_l2: -0.1878
     Episode_Reward/action_rate_l2: -0.1693
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7044
Metrics/base_velocity/error_vel_xy: 0.4308
Metrics/base_velocity/error_vel_yaw: 0.4910
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 33984000
                    Iteration time: 1.16s
                      Time elapsed: 00:17:30
                               ETA: 00:10:19

################################################################################
                     [1m Learning iteration 944/1500 [0m                      

                       Computation: 30951 steps/s (collection: 1.111s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0261
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 13.7765
                       Mean reward: 17.52
               Mean episode length: 887.91
Episode_Reward/track_lin_vel_xy_exp: 1.0409
Episode_Reward/track_ang_vel_z_exp: 0.4681
       Episode_Reward/lin_vel_z_l2: -0.0455
      Episode_Reward/ang_vel_xy_l2: -0.1282
     Episode_Reward/dof_torques_l2: -0.0876
         Episode_Reward/dof_acc_l2: -0.1998
     Episode_Reward/action_rate_l2: -0.1761
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6864
Metrics/base_velocity/error_vel_xy: 0.4048
Metrics/base_velocity/error_vel_yaw: 0.5000
      Episode_Termination/time_out: 2.0000
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 34020000
                    Iteration time: 1.16s
                      Time elapsed: 00:17:31
                               ETA: 00:10:18

################################################################################
                     [1m Learning iteration 945/1500 [0m                      

                       Computation: 31149 steps/s (collection: 1.103s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0220
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 13.7781
                       Mean reward: 18.17
               Mean episode length: 883.60
Episode_Reward/track_lin_vel_xy_exp: 1.1155
Episode_Reward/track_ang_vel_z_exp: 0.4865
       Episode_Reward/lin_vel_z_l2: -0.0459
      Episode_Reward/ang_vel_xy_l2: -0.1311
     Episode_Reward/dof_torques_l2: -0.0893
         Episode_Reward/dof_acc_l2: -0.1951
     Episode_Reward/action_rate_l2: -0.1810
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6848
Metrics/base_velocity/error_vel_xy: 0.3880
Metrics/base_velocity/error_vel_yaw: 0.5266
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 34056000
                    Iteration time: 1.16s
                      Time elapsed: 00:17:32
                               ETA: 00:10:17

################################################################################
                     [1m Learning iteration 946/1500 [0m                      

                       Computation: 30608 steps/s (collection: 1.120s, learning 0.056s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0241
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 13.7681
                       Mean reward: 18.17
               Mean episode length: 869.81
Episode_Reward/track_lin_vel_xy_exp: 1.0768
Episode_Reward/track_ang_vel_z_exp: 0.4747
       Episode_Reward/lin_vel_z_l2: -0.0476
      Episode_Reward/ang_vel_xy_l2: -0.1325
     Episode_Reward/dof_torques_l2: -0.0861
         Episode_Reward/dof_acc_l2: -0.2018
     Episode_Reward/action_rate_l2: -0.1787
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6914
Metrics/base_velocity/error_vel_xy: 0.3949
Metrics/base_velocity/error_vel_yaw: 0.5248
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 34092000
                    Iteration time: 1.18s
                      Time elapsed: 00:17:33
                               ETA: 00:10:16

################################################################################
                     [1m Learning iteration 947/1500 [0m                      

                       Computation: 32491 steps/s (collection: 1.054s, learning 0.054s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0227
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 13.7587
                       Mean reward: 17.54
               Mean episode length: 861.65
Episode_Reward/track_lin_vel_xy_exp: 0.9976
Episode_Reward/track_ang_vel_z_exp: 0.4478
       Episode_Reward/lin_vel_z_l2: -0.0436
      Episode_Reward/ang_vel_xy_l2: -0.1276
     Episode_Reward/dof_torques_l2: -0.0896
         Episode_Reward/dof_acc_l2: -0.1944
     Episode_Reward/action_rate_l2: -0.1757
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6900
Metrics/base_velocity/error_vel_xy: 0.4237
Metrics/base_velocity/error_vel_yaw: 0.5183
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 34128000
                    Iteration time: 1.11s
                      Time elapsed: 00:17:34
                               ETA: 00:10:15

################################################################################
                     [1m Learning iteration 948/1500 [0m                      

                       Computation: 32437 steps/s (collection: 1.055s, learning 0.055s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0279
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 13.7565
                       Mean reward: 18.08
               Mean episode length: 902.30
Episode_Reward/track_lin_vel_xy_exp: 1.0957
Episode_Reward/track_ang_vel_z_exp: 0.4919
       Episode_Reward/lin_vel_z_l2: -0.0546
      Episode_Reward/ang_vel_xy_l2: -0.1417
     Episode_Reward/dof_torques_l2: -0.0935
         Episode_Reward/dof_acc_l2: -0.2132
     Episode_Reward/action_rate_l2: -0.1895
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.6954
Metrics/base_velocity/error_vel_xy: 0.4719
Metrics/base_velocity/error_vel_yaw: 0.5650
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 34164000
                    Iteration time: 1.11s
                      Time elapsed: 00:17:36
                               ETA: 00:10:14

################################################################################
                     [1m Learning iteration 949/1500 [0m                      

                       Computation: 32741 steps/s (collection: 1.047s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0257
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 13.7485
                       Mean reward: 18.28
               Mean episode length: 903.38
Episode_Reward/track_lin_vel_xy_exp: 1.1165
Episode_Reward/track_ang_vel_z_exp: 0.4894
       Episode_Reward/lin_vel_z_l2: -0.0475
      Episode_Reward/ang_vel_xy_l2: -0.1363
     Episode_Reward/dof_torques_l2: -0.0958
         Episode_Reward/dof_acc_l2: -0.2079
     Episode_Reward/action_rate_l2: -0.1865
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7054
Metrics/base_velocity/error_vel_xy: 0.4024
Metrics/base_velocity/error_vel_yaw: 0.5263
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 34200000
                    Iteration time: 1.10s
                      Time elapsed: 00:17:37
                               ETA: 00:10:13

################################################################################
                     [1m Learning iteration 950/1500 [0m                      

                       Computation: 31314 steps/s (collection: 1.097s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0256
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 13.7272
                       Mean reward: 16.73
               Mean episode length: 878.31
Episode_Reward/track_lin_vel_xy_exp: 0.9884
Episode_Reward/track_ang_vel_z_exp: 0.4567
       Episode_Reward/lin_vel_z_l2: -0.0522
      Episode_Reward/ang_vel_xy_l2: -0.1353
     Episode_Reward/dof_torques_l2: -0.0891
         Episode_Reward/dof_acc_l2: -0.2109
     Episode_Reward/action_rate_l2: -0.1788
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7154
Metrics/base_velocity/error_vel_xy: 0.4793
Metrics/base_velocity/error_vel_yaw: 0.5457
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 34236000
                    Iteration time: 1.15s
                      Time elapsed: 00:17:38
                               ETA: 00:10:12

################################################################################
                     [1m Learning iteration 951/1500 [0m                      

                       Computation: 31972 steps/s (collection: 1.073s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0248
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 13.7188
                       Mean reward: 16.11
               Mean episode length: 889.71
Episode_Reward/track_lin_vel_xy_exp: 1.0083
Episode_Reward/track_ang_vel_z_exp: 0.4718
       Episode_Reward/lin_vel_z_l2: -0.0490
      Episode_Reward/ang_vel_xy_l2: -0.1361
     Episode_Reward/dof_torques_l2: -0.0986
         Episode_Reward/dof_acc_l2: -0.2189
     Episode_Reward/action_rate_l2: -0.1854
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7159
Metrics/base_velocity/error_vel_xy: 0.5055
Metrics/base_velocity/error_vel_yaw: 0.5513
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 34272000
                    Iteration time: 1.13s
                      Time elapsed: 00:17:39
                               ETA: 00:10:10

################################################################################
                     [1m Learning iteration 952/1500 [0m                      

                       Computation: 31556 steps/s (collection: 1.089s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0249
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 13.7130
                       Mean reward: 15.26
               Mean episode length: 857.69
Episode_Reward/track_lin_vel_xy_exp: 0.8973
Episode_Reward/track_ang_vel_z_exp: 0.4245
       Episode_Reward/lin_vel_z_l2: -0.0485
      Episode_Reward/ang_vel_xy_l2: -0.1282
     Episode_Reward/dof_torques_l2: -0.0843
         Episode_Reward/dof_acc_l2: -0.1882
     Episode_Reward/action_rate_l2: -0.1658
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7083
Metrics/base_velocity/error_vel_xy: 0.4786
Metrics/base_velocity/error_vel_yaw: 0.5238
      Episode_Termination/time_out: 1.1250
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 34308000
                    Iteration time: 1.14s
                      Time elapsed: 00:17:40
                               ETA: 00:10:09

################################################################################
                     [1m Learning iteration 953/1500 [0m                      

                       Computation: 34384 steps/s (collection: 0.995s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0261
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.7108
                       Mean reward: 15.64
               Mean episode length: 854.70
Episode_Reward/track_lin_vel_xy_exp: 1.1220
Episode_Reward/track_ang_vel_z_exp: 0.4899
       Episode_Reward/lin_vel_z_l2: -0.0450
      Episode_Reward/ang_vel_xy_l2: -0.1345
     Episode_Reward/dof_torques_l2: -0.0943
         Episode_Reward/dof_acc_l2: -0.1984
     Episode_Reward/action_rate_l2: -0.1840
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7154
Metrics/base_velocity/error_vel_xy: 0.3980
Metrics/base_velocity/error_vel_yaw: 0.5353
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 34344000
                    Iteration time: 1.05s
                      Time elapsed: 00:17:41
                               ETA: 00:10:08

################################################################################
                     [1m Learning iteration 954/1500 [0m                      

                       Computation: 33849 steps/s (collection: 1.011s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0252
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 13.7032
                       Mean reward: 17.14
               Mean episode length: 878.47
Episode_Reward/track_lin_vel_xy_exp: 1.1423
Episode_Reward/track_ang_vel_z_exp: 0.5068
       Episode_Reward/lin_vel_z_l2: -0.0479
      Episode_Reward/ang_vel_xy_l2: -0.1381
     Episode_Reward/dof_torques_l2: -0.1016
         Episode_Reward/dof_acc_l2: -0.2077
     Episode_Reward/action_rate_l2: -0.1926
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7234
Metrics/base_velocity/error_vel_xy: 0.4236
Metrics/base_velocity/error_vel_yaw: 0.5408
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 34380000
                    Iteration time: 1.06s
                      Time elapsed: 00:17:42
                               ETA: 00:10:07

################################################################################
                     [1m Learning iteration 955/1500 [0m                      

                       Computation: 32869 steps/s (collection: 1.043s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0241
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 13.6988
                       Mean reward: 15.52
               Mean episode length: 842.97
Episode_Reward/track_lin_vel_xy_exp: 0.8687
Episode_Reward/track_ang_vel_z_exp: 0.4226
       Episode_Reward/lin_vel_z_l2: -0.0537
      Episode_Reward/ang_vel_xy_l2: -0.1284
     Episode_Reward/dof_torques_l2: -0.0874
         Episode_Reward/dof_acc_l2: -0.1995
     Episode_Reward/action_rate_l2: -0.1667
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7144
Metrics/base_velocity/error_vel_xy: 0.5237
Metrics/base_velocity/error_vel_yaw: 0.5145
      Episode_Termination/time_out: 1.1667
  Episode_Termination/base_contact: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 34416000
                    Iteration time: 1.10s
                      Time elapsed: 00:17:43
                               ETA: 00:10:06

################################################################################
                     [1m Learning iteration 956/1500 [0m                      

                       Computation: 32797 steps/s (collection: 1.045s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0260
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 13.7057
                       Mean reward: 15.41
               Mean episode length: 851.49
Episode_Reward/track_lin_vel_xy_exp: 1.0388
Episode_Reward/track_ang_vel_z_exp: 0.4702
       Episode_Reward/lin_vel_z_l2: -0.0491
      Episode_Reward/ang_vel_xy_l2: -0.1374
     Episode_Reward/dof_torques_l2: -0.0918
         Episode_Reward/dof_acc_l2: -0.2016
     Episode_Reward/action_rate_l2: -0.1834
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7168
Metrics/base_velocity/error_vel_xy: 0.4739
Metrics/base_velocity/error_vel_yaw: 0.5557
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 34452000
                    Iteration time: 1.10s
                      Time elapsed: 00:17:44
                               ETA: 00:10:05

################################################################################
                     [1m Learning iteration 957/1500 [0m                      

                       Computation: 30139 steps/s (collection: 1.138s, learning 0.056s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0228
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 13.7158
                       Mean reward: 16.97
               Mean episode length: 882.36
Episode_Reward/track_lin_vel_xy_exp: 1.0146
Episode_Reward/track_ang_vel_z_exp: 0.4528
       Episode_Reward/lin_vel_z_l2: -0.0489
      Episode_Reward/ang_vel_xy_l2: -0.1276
     Episode_Reward/dof_torques_l2: -0.0842
         Episode_Reward/dof_acc_l2: -0.1969
     Episode_Reward/action_rate_l2: -0.1724
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7210
Metrics/base_velocity/error_vel_xy: 0.3921
Metrics/base_velocity/error_vel_yaw: 0.4958
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 34488000
                    Iteration time: 1.19s
                      Time elapsed: 00:17:46
                               ETA: 00:10:04

################################################################################
                     [1m Learning iteration 958/1500 [0m                      

                       Computation: 32016 steps/s (collection: 1.072s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0292
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 13.7206
                       Mean reward: 16.74
               Mean episode length: 855.03
Episode_Reward/track_lin_vel_xy_exp: 0.9859
Episode_Reward/track_ang_vel_z_exp: 0.4354
       Episode_Reward/lin_vel_z_l2: -0.0496
      Episode_Reward/ang_vel_xy_l2: -0.1265
     Episode_Reward/dof_torques_l2: -0.0821
         Episode_Reward/dof_acc_l2: -0.1966
     Episode_Reward/action_rate_l2: -0.1668
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7264
Metrics/base_velocity/error_vel_xy: 0.3740
Metrics/base_velocity/error_vel_yaw: 0.4802
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 34524000
                    Iteration time: 1.12s
                      Time elapsed: 00:17:47
                               ETA: 00:10:03

################################################################################
                     [1m Learning iteration 959/1500 [0m                      

                       Computation: 32512 steps/s (collection: 1.053s, learning 0.054s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0246
               Mean surrogate loss: -0.0153
                 Mean entropy loss: 13.7212
                       Mean reward: 15.17
               Mean episode length: 800.55
Episode_Reward/track_lin_vel_xy_exp: 0.9090
Episode_Reward/track_ang_vel_z_exp: 0.4083
       Episode_Reward/lin_vel_z_l2: -0.0437
      Episode_Reward/ang_vel_xy_l2: -0.1240
     Episode_Reward/dof_torques_l2: -0.0801
         Episode_Reward/dof_acc_l2: -0.1794
     Episode_Reward/action_rate_l2: -0.1586
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7267
Metrics/base_velocity/error_vel_xy: 0.4028
Metrics/base_velocity/error_vel_yaw: 0.4892
      Episode_Termination/time_out: 1.0417
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 34560000
                    Iteration time: 1.11s
                      Time elapsed: 00:17:48
                               ETA: 00:10:02

################################################################################
                     [1m Learning iteration 960/1500 [0m                      

                       Computation: 31981 steps/s (collection: 1.075s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0265
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 13.7148
                       Mean reward: 15.17
               Mean episode length: 806.72
Episode_Reward/track_lin_vel_xy_exp: 1.0386
Episode_Reward/track_ang_vel_z_exp: 0.4681
       Episode_Reward/lin_vel_z_l2: -0.0518
      Episode_Reward/ang_vel_xy_l2: -0.1350
     Episode_Reward/dof_torques_l2: -0.0904
         Episode_Reward/dof_acc_l2: -0.1963
     Episode_Reward/action_rate_l2: -0.1791
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7234
Metrics/base_velocity/error_vel_xy: 0.4475
Metrics/base_velocity/error_vel_yaw: 0.5523
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 34596000
                    Iteration time: 1.13s
                      Time elapsed: 00:17:49
                               ETA: 00:10:00

################################################################################
                     [1m Learning iteration 961/1500 [0m                      

                       Computation: 31658 steps/s (collection: 1.084s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0276
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 13.7113
                       Mean reward: 17.16
               Mean episode length: 870.87
Episode_Reward/track_lin_vel_xy_exp: 1.0000
Episode_Reward/track_ang_vel_z_exp: 0.4544
       Episode_Reward/lin_vel_z_l2: -0.0460
      Episode_Reward/ang_vel_xy_l2: -0.1284
     Episode_Reward/dof_torques_l2: -0.0857
         Episode_Reward/dof_acc_l2: -0.1875
     Episode_Reward/action_rate_l2: -0.1712
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7198
Metrics/base_velocity/error_vel_xy: 0.4237
Metrics/base_velocity/error_vel_yaw: 0.5057
      Episode_Termination/time_out: 1.9583
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 34632000
                    Iteration time: 1.14s
                      Time elapsed: 00:17:50
                               ETA: 00:09:59

################################################################################
                     [1m Learning iteration 962/1500 [0m                      

                       Computation: 31667 steps/s (collection: 1.084s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0259
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 13.7156
                       Mean reward: 17.27
               Mean episode length: 885.60
Episode_Reward/track_lin_vel_xy_exp: 0.9093
Episode_Reward/track_ang_vel_z_exp: 0.4271
       Episode_Reward/lin_vel_z_l2: -0.0426
      Episode_Reward/ang_vel_xy_l2: -0.1188
     Episode_Reward/dof_torques_l2: -0.0876
         Episode_Reward/dof_acc_l2: -0.1845
     Episode_Reward/action_rate_l2: -0.1667
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7219
Metrics/base_velocity/error_vel_xy: 0.4534
Metrics/base_velocity/error_vel_yaw: 0.4862
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 34668000
                    Iteration time: 1.14s
                      Time elapsed: 00:17:51
                               ETA: 00:09:58

################################################################################
                     [1m Learning iteration 963/1500 [0m                      

                       Computation: 32243 steps/s (collection: 1.061s, learning 0.055s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0236
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.7159
                       Mean reward: 15.97
               Mean episode length: 859.72
Episode_Reward/track_lin_vel_xy_exp: 1.0159
Episode_Reward/track_ang_vel_z_exp: 0.4831
       Episode_Reward/lin_vel_z_l2: -0.0441
      Episode_Reward/ang_vel_xy_l2: -0.1296
     Episode_Reward/dof_torques_l2: -0.0925
         Episode_Reward/dof_acc_l2: -0.1932
     Episode_Reward/action_rate_l2: -0.1809
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7305
Metrics/base_velocity/error_vel_xy: 0.5125
Metrics/base_velocity/error_vel_yaw: 0.5222
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 34704000
                    Iteration time: 1.12s
                      Time elapsed: 00:17:52
                               ETA: 00:09:57

################################################################################
                     [1m Learning iteration 964/1500 [0m                      

                       Computation: 32199 steps/s (collection: 1.066s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0221
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 13.7098
                       Mean reward: 16.16
               Mean episode length: 880.18
Episode_Reward/track_lin_vel_xy_exp: 0.8265
Episode_Reward/track_ang_vel_z_exp: 0.3867
       Episode_Reward/lin_vel_z_l2: -0.0451
      Episode_Reward/ang_vel_xy_l2: -0.1191
     Episode_Reward/dof_torques_l2: -0.0768
         Episode_Reward/dof_acc_l2: -0.1893
     Episode_Reward/action_rate_l2: -0.1527
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7365
Metrics/base_velocity/error_vel_xy: 0.4487
Metrics/base_velocity/error_vel_yaw: 0.5036
      Episode_Termination/time_out: 0.8750
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 34740000
                    Iteration time: 1.12s
                      Time elapsed: 00:17:53
                               ETA: 00:09:56

################################################################################
                     [1m Learning iteration 965/1500 [0m                      

                       Computation: 31756 steps/s (collection: 1.078s, learning 0.055s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0240
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 13.7105
                       Mean reward: 16.98
               Mean episode length: 871.10
Episode_Reward/track_lin_vel_xy_exp: 1.0637
Episode_Reward/track_ang_vel_z_exp: 0.4828
       Episode_Reward/lin_vel_z_l2: -0.0440
      Episode_Reward/ang_vel_xy_l2: -0.1304
     Episode_Reward/dof_torques_l2: -0.0935
         Episode_Reward/dof_acc_l2: -0.1921
     Episode_Reward/action_rate_l2: -0.1794
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7397
Metrics/base_velocity/error_vel_xy: 0.4289
Metrics/base_velocity/error_vel_yaw: 0.5132
      Episode_Termination/time_out: 2.0833
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 34776000
                    Iteration time: 1.13s
                      Time elapsed: 00:17:55
                               ETA: 00:09:55

################################################################################
                     [1m Learning iteration 966/1500 [0m                      

                       Computation: 30734 steps/s (collection: 1.119s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0237
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 13.7079
                       Mean reward: 18.72
               Mean episode length: 900.35
Episode_Reward/track_lin_vel_xy_exp: 1.1355
Episode_Reward/track_ang_vel_z_exp: 0.4930
       Episode_Reward/lin_vel_z_l2: -0.0464
      Episode_Reward/ang_vel_xy_l2: -0.1330
     Episode_Reward/dof_torques_l2: -0.0844
         Episode_Reward/dof_acc_l2: -0.1928
     Episode_Reward/action_rate_l2: -0.1796
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7393
Metrics/base_velocity/error_vel_xy: 0.3475
Metrics/base_velocity/error_vel_yaw: 0.4844
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 34812000
                    Iteration time: 1.17s
                      Time elapsed: 00:17:56
                               ETA: 00:09:54

################################################################################
                     [1m Learning iteration 967/1500 [0m                      

                       Computation: 30380 steps/s (collection: 1.134s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0210
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 13.6993
                       Mean reward: 18.12
               Mean episode length: 903.95
Episode_Reward/track_lin_vel_xy_exp: 0.9928
Episode_Reward/track_ang_vel_z_exp: 0.4450
       Episode_Reward/lin_vel_z_l2: -0.0417
      Episode_Reward/ang_vel_xy_l2: -0.1226
     Episode_Reward/dof_torques_l2: -0.0886
         Episode_Reward/dof_acc_l2: -0.1743
     Episode_Reward/action_rate_l2: -0.1703
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7326
Metrics/base_velocity/error_vel_xy: 0.4126
Metrics/base_velocity/error_vel_yaw: 0.5068
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 34848000
                    Iteration time: 1.18s
                      Time elapsed: 00:17:57
                               ETA: 00:09:53

################################################################################
                     [1m Learning iteration 968/1500 [0m                      

                       Computation: 32270 steps/s (collection: 1.064s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0259
               Mean surrogate loss: -0.0158
                 Mean entropy loss: 13.6910
                       Mean reward: 17.09
               Mean episode length: 878.27
Episode_Reward/track_lin_vel_xy_exp: 0.9244
Episode_Reward/track_ang_vel_z_exp: 0.4246
       Episode_Reward/lin_vel_z_l2: -0.0474
      Episode_Reward/ang_vel_xy_l2: -0.1260
     Episode_Reward/dof_torques_l2: -0.0791
         Episode_Reward/dof_acc_l2: -0.1811
     Episode_Reward/action_rate_l2: -0.1633
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7349
Metrics/base_velocity/error_vel_xy: 0.4417
Metrics/base_velocity/error_vel_yaw: 0.5102
      Episode_Termination/time_out: 1.0833
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 34884000
                    Iteration time: 1.12s
                      Time elapsed: 00:17:58
                               ETA: 00:09:52

################################################################################
                     [1m Learning iteration 969/1500 [0m                      

                       Computation: 31460 steps/s (collection: 1.092s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0260
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 13.6830
                       Mean reward: 15.01
               Mean episode length: 827.69
Episode_Reward/track_lin_vel_xy_exp: 0.8985
Episode_Reward/track_ang_vel_z_exp: 0.4140
       Episode_Reward/lin_vel_z_l2: -0.0451
      Episode_Reward/ang_vel_xy_l2: -0.1178
     Episode_Reward/dof_torques_l2: -0.0878
         Episode_Reward/dof_acc_l2: -0.1823
     Episode_Reward/action_rate_l2: -0.1642
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7366
Metrics/base_velocity/error_vel_xy: 0.4420
Metrics/base_velocity/error_vel_yaw: 0.5036
      Episode_Termination/time_out: 1.9167
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 34920000
                    Iteration time: 1.14s
                      Time elapsed: 00:17:59
                               ETA: 00:09:51

################################################################################
                     [1m Learning iteration 970/1500 [0m                      

                       Computation: 33483 steps/s (collection: 1.023s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0246
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 13.6832
                       Mean reward: 15.63
               Mean episode length: 822.78
Episode_Reward/track_lin_vel_xy_exp: 1.0715
Episode_Reward/track_ang_vel_z_exp: 0.4760
       Episode_Reward/lin_vel_z_l2: -0.0462
      Episode_Reward/ang_vel_xy_l2: -0.1290
     Episode_Reward/dof_torques_l2: -0.0887
         Episode_Reward/dof_acc_l2: -0.1949
     Episode_Reward/action_rate_l2: -0.1780
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7366
Metrics/base_velocity/error_vel_xy: 0.3876
Metrics/base_velocity/error_vel_yaw: 0.4988
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 34956000
                    Iteration time: 1.08s
                      Time elapsed: 00:18:00
                               ETA: 00:09:49

################################################################################
                     [1m Learning iteration 971/1500 [0m                      

                       Computation: 32928 steps/s (collection: 1.040s, learning 0.054s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0220
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 13.6813
                       Mean reward: 17.05
               Mean episode length: 867.59
Episode_Reward/track_lin_vel_xy_exp: 1.1174
Episode_Reward/track_ang_vel_z_exp: 0.4938
       Episode_Reward/lin_vel_z_l2: -0.0465
      Episode_Reward/ang_vel_xy_l2: -0.1310
     Episode_Reward/dof_torques_l2: -0.0979
         Episode_Reward/dof_acc_l2: -0.2044
     Episode_Reward/action_rate_l2: -0.1884
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7409
Metrics/base_velocity/error_vel_xy: 0.4096
Metrics/base_velocity/error_vel_yaw: 0.5279
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 34992000
                    Iteration time: 1.09s
                      Time elapsed: 00:18:01
                               ETA: 00:09:48

################################################################################
                     [1m Learning iteration 972/1500 [0m                      

                       Computation: 33254 steps/s (collection: 1.031s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0237
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 13.6729
                       Mean reward: 17.68
               Mean episode length: 898.10
Episode_Reward/track_lin_vel_xy_exp: 1.0228
Episode_Reward/track_ang_vel_z_exp: 0.4600
       Episode_Reward/lin_vel_z_l2: -0.0456
      Episode_Reward/ang_vel_xy_l2: -0.1287
     Episode_Reward/dof_torques_l2: -0.0889
         Episode_Reward/dof_acc_l2: -0.1943
     Episode_Reward/action_rate_l2: -0.1744
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7491
Metrics/base_velocity/error_vel_xy: 0.4290
Metrics/base_velocity/error_vel_yaw: 0.5308
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 35028000
                    Iteration time: 1.08s
                      Time elapsed: 00:18:02
                               ETA: 00:09:47

################################################################################
                     [1m Learning iteration 973/1500 [0m                      

                       Computation: 31484 steps/s (collection: 1.090s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0229
               Mean surrogate loss: -0.0155
                 Mean entropy loss: 13.6623
                       Mean reward: 17.50
               Mean episode length: 912.74
Episode_Reward/track_lin_vel_xy_exp: 1.0662
Episode_Reward/track_ang_vel_z_exp: 0.4803
       Episode_Reward/lin_vel_z_l2: -0.0483
      Episode_Reward/ang_vel_xy_l2: -0.1343
     Episode_Reward/dof_torques_l2: -0.0920
         Episode_Reward/dof_acc_l2: -0.2083
     Episode_Reward/action_rate_l2: -0.1847
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7564
Metrics/base_velocity/error_vel_xy: 0.4502
Metrics/base_velocity/error_vel_yaw: 0.5358
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 35064000
                    Iteration time: 1.14s
                      Time elapsed: 00:18:04
                               ETA: 00:09:46

################################################################################
                     [1m Learning iteration 974/1500 [0m                      

                       Computation: 32146 steps/s (collection: 1.068s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0205
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 13.6592
                       Mean reward: 18.48
               Mean episode length: 930.95
Episode_Reward/track_lin_vel_xy_exp: 1.1200
Episode_Reward/track_ang_vel_z_exp: 0.4896
       Episode_Reward/lin_vel_z_l2: -0.0477
      Episode_Reward/ang_vel_xy_l2: -0.1285
     Episode_Reward/dof_torques_l2: -0.0981
         Episode_Reward/dof_acc_l2: -0.1956
     Episode_Reward/action_rate_l2: -0.1857
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7659
Metrics/base_velocity/error_vel_xy: 0.3946
Metrics/base_velocity/error_vel_yaw: 0.5257
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 35100000
                    Iteration time: 1.12s
                      Time elapsed: 00:18:05
                               ETA: 00:09:45

################################################################################
                     [1m Learning iteration 975/1500 [0m                      

                       Computation: 30921 steps/s (collection: 1.112s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0263
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 13.6591
                       Mean reward: 17.00
               Mean episode length: 871.87
Episode_Reward/track_lin_vel_xy_exp: 0.9851
Episode_Reward/track_ang_vel_z_exp: 0.4431
       Episode_Reward/lin_vel_z_l2: -0.0427
      Episode_Reward/ang_vel_xy_l2: -0.1218
     Episode_Reward/dof_torques_l2: -0.0821
         Episode_Reward/dof_acc_l2: -0.1792
     Episode_Reward/action_rate_l2: -0.1653
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7767
Metrics/base_velocity/error_vel_xy: 0.3905
Metrics/base_velocity/error_vel_yaw: 0.4843
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 35136000
                    Iteration time: 1.16s
                      Time elapsed: 00:18:06
                               ETA: 00:09:44

################################################################################
                     [1m Learning iteration 976/1500 [0m                      

                       Computation: 32933 steps/s (collection: 1.042s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0253
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.6606
                       Mean reward: 16.55
               Mean episode length: 829.78
Episode_Reward/track_lin_vel_xy_exp: 0.8794
Episode_Reward/track_ang_vel_z_exp: 0.3966
       Episode_Reward/lin_vel_z_l2: -0.0397
      Episode_Reward/ang_vel_xy_l2: -0.1118
     Episode_Reward/dof_torques_l2: -0.0749
         Episode_Reward/dof_acc_l2: -0.1596
     Episode_Reward/action_rate_l2: -0.1485
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7735
Metrics/base_velocity/error_vel_xy: 0.3728
Metrics/base_velocity/error_vel_yaw: 0.4409
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 35172000
                    Iteration time: 1.09s
                      Time elapsed: 00:18:07
                               ETA: 00:09:43

################################################################################
                     [1m Learning iteration 977/1500 [0m                      

                       Computation: 33753 steps/s (collection: 1.014s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0265
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 13.6567
                       Mean reward: 16.09
               Mean episode length: 788.12
Episode_Reward/track_lin_vel_xy_exp: 1.0419
Episode_Reward/track_ang_vel_z_exp: 0.4449
       Episode_Reward/lin_vel_z_l2: -0.0400
      Episode_Reward/ang_vel_xy_l2: -0.1216
     Episode_Reward/dof_torques_l2: -0.0793
         Episode_Reward/dof_acc_l2: -0.1669
     Episode_Reward/action_rate_l2: -0.1640
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7693
Metrics/base_velocity/error_vel_xy: 0.3211
Metrics/base_velocity/error_vel_yaw: 0.4802
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 35208000
                    Iteration time: 1.07s
                      Time elapsed: 00:18:08
                               ETA: 00:09:42

################################################################################
                     [1m Learning iteration 978/1500 [0m                      

                       Computation: 33989 steps/s (collection: 1.005s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0250
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 13.6468
                       Mean reward: 16.60
               Mean episode length: 820.01
Episode_Reward/track_lin_vel_xy_exp: 1.1104
Episode_Reward/track_ang_vel_z_exp: 0.4892
       Episode_Reward/lin_vel_z_l2: -0.0477
      Episode_Reward/ang_vel_xy_l2: -0.1343
     Episode_Reward/dof_torques_l2: -0.0949
         Episode_Reward/dof_acc_l2: -0.2010
     Episode_Reward/action_rate_l2: -0.1877
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7696
Metrics/base_velocity/error_vel_xy: 0.4202
Metrics/base_velocity/error_vel_yaw: 0.5462
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 35244000
                    Iteration time: 1.06s
                      Time elapsed: 00:18:09
                               ETA: 00:09:40

################################################################################
                     [1m Learning iteration 979/1500 [0m                      

                       Computation: 34172 steps/s (collection: 1.002s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0259
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 13.6504
                       Mean reward: 16.94
               Mean episode length: 868.11
Episode_Reward/track_lin_vel_xy_exp: 0.9815
Episode_Reward/track_ang_vel_z_exp: 0.4463
       Episode_Reward/lin_vel_z_l2: -0.0513
      Episode_Reward/ang_vel_xy_l2: -0.1269
     Episode_Reward/dof_torques_l2: -0.0860
         Episode_Reward/dof_acc_l2: -0.1937
     Episode_Reward/action_rate_l2: -0.1704
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7731
Metrics/base_velocity/error_vel_xy: 0.4031
Metrics/base_velocity/error_vel_yaw: 0.4836
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 35280000
                    Iteration time: 1.05s
                      Time elapsed: 00:18:10
                               ETA: 00:09:39

################################################################################
                     [1m Learning iteration 980/1500 [0m                      

                       Computation: 33068 steps/s (collection: 1.039s, learning 0.050s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0220
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 13.6538
                       Mean reward: 16.84
               Mean episode length: 895.36
Episode_Reward/track_lin_vel_xy_exp: 0.9992
Episode_Reward/track_ang_vel_z_exp: 0.4598
       Episode_Reward/lin_vel_z_l2: -0.0539
      Episode_Reward/ang_vel_xy_l2: -0.1324
     Episode_Reward/dof_torques_l2: -0.0931
         Episode_Reward/dof_acc_l2: -0.2095
     Episode_Reward/action_rate_l2: -0.1807
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7840
Metrics/base_velocity/error_vel_xy: 0.4537
Metrics/base_velocity/error_vel_yaw: 0.5303
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 35316000
                    Iteration time: 1.09s
                      Time elapsed: 00:18:11
                               ETA: 00:09:38

################################################################################
                     [1m Learning iteration 981/1500 [0m                      

                       Computation: 33595 steps/s (collection: 1.019s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0229
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 13.6529
                       Mean reward: 17.02
               Mean episode length: 877.41
Episode_Reward/track_lin_vel_xy_exp: 1.0790
Episode_Reward/track_ang_vel_z_exp: 0.4742
       Episode_Reward/lin_vel_z_l2: -0.0445
      Episode_Reward/ang_vel_xy_l2: -0.1253
     Episode_Reward/dof_torques_l2: -0.0912
         Episode_Reward/dof_acc_l2: -0.1998
     Episode_Reward/action_rate_l2: -0.1779
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7976
Metrics/base_velocity/error_vel_xy: 0.3652
Metrics/base_velocity/error_vel_yaw: 0.4852
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 35352000
                    Iteration time: 1.07s
                      Time elapsed: 00:18:12
                               ETA: 00:09:37

################################################################################
                     [1m Learning iteration 982/1500 [0m                      

                       Computation: 32712 steps/s (collection: 1.048s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0252
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 13.6481
                       Mean reward: 16.25
               Mean episode length: 842.72
Episode_Reward/track_lin_vel_xy_exp: 0.9405
Episode_Reward/track_ang_vel_z_exp: 0.4458
       Episode_Reward/lin_vel_z_l2: -0.0488
      Episode_Reward/ang_vel_xy_l2: -0.1264
     Episode_Reward/dof_torques_l2: -0.0937
         Episode_Reward/dof_acc_l2: -0.1865
     Episode_Reward/action_rate_l2: -0.1744
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8048
Metrics/base_velocity/error_vel_xy: 0.4999
Metrics/base_velocity/error_vel_yaw: 0.5122
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 35388000
                    Iteration time: 1.10s
                      Time elapsed: 00:18:13
                               ETA: 00:09:36

################################################################################
                     [1m Learning iteration 983/1500 [0m                      

                       Computation: 31889 steps/s (collection: 1.078s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0287
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 13.6547
                       Mean reward: 16.38
               Mean episode length: 853.68
Episode_Reward/track_lin_vel_xy_exp: 0.9616
Episode_Reward/track_ang_vel_z_exp: 0.4382
       Episode_Reward/lin_vel_z_l2: -0.0477
      Episode_Reward/ang_vel_xy_l2: -0.1242
     Episode_Reward/dof_torques_l2: -0.0819
         Episode_Reward/dof_acc_l2: -0.1774
     Episode_Reward/action_rate_l2: -0.1630
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8068
Metrics/base_velocity/error_vel_xy: 0.4033
Metrics/base_velocity/error_vel_yaw: 0.4834
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 35424000
                    Iteration time: 1.13s
                      Time elapsed: 00:18:14
                               ETA: 00:09:35

################################################################################
                     [1m Learning iteration 984/1500 [0m                      

                       Computation: 32202 steps/s (collection: 1.063s, learning 0.054s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0247
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.6629
                       Mean reward: 15.95
               Mean episode length: 845.90
Episode_Reward/track_lin_vel_xy_exp: 0.9921
Episode_Reward/track_ang_vel_z_exp: 0.4549
       Episode_Reward/lin_vel_z_l2: -0.0456
      Episode_Reward/ang_vel_xy_l2: -0.1262
     Episode_Reward/dof_torques_l2: -0.0878
         Episode_Reward/dof_acc_l2: -0.1857
     Episode_Reward/action_rate_l2: -0.1720
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8046
Metrics/base_velocity/error_vel_xy: 0.4441
Metrics/base_velocity/error_vel_yaw: 0.5158
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 35460000
                    Iteration time: 1.12s
                      Time elapsed: 00:18:16
                               ETA: 00:09:34

################################################################################
                     [1m Learning iteration 985/1500 [0m                      

                       Computation: 30584 steps/s (collection: 1.123s, learning 0.054s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0278
               Mean surrogate loss: -0.0153
                 Mean entropy loss: 13.6679
                       Mean reward: 17.53
               Mean episode length: 875.53
Episode_Reward/track_lin_vel_xy_exp: 1.1041
Episode_Reward/track_ang_vel_z_exp: 0.4913
       Episode_Reward/lin_vel_z_l2: -0.0472
      Episode_Reward/ang_vel_xy_l2: -0.1330
     Episode_Reward/dof_torques_l2: -0.0900
         Episode_Reward/dof_acc_l2: -0.2059
     Episode_Reward/action_rate_l2: -0.1852
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8075
Metrics/base_velocity/error_vel_xy: 0.4217
Metrics/base_velocity/error_vel_yaw: 0.5160
      Episode_Termination/time_out: 2.0417
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 35496000
                    Iteration time: 1.18s
                      Time elapsed: 00:18:17
                               ETA: 00:09:33

################################################################################
                     [1m Learning iteration 986/1500 [0m                      

                       Computation: 31698 steps/s (collection: 1.085s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0251
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 13.6676
                       Mean reward: 18.43
               Mean episode length: 871.90
Episode_Reward/track_lin_vel_xy_exp: 1.0316
Episode_Reward/track_ang_vel_z_exp: 0.4579
       Episode_Reward/lin_vel_z_l2: -0.0450
      Episode_Reward/ang_vel_xy_l2: -0.1238
     Episode_Reward/dof_torques_l2: -0.0852
         Episode_Reward/dof_acc_l2: -0.1864
     Episode_Reward/action_rate_l2: -0.1702
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8100
Metrics/base_velocity/error_vel_xy: 0.3899
Metrics/base_velocity/error_vel_yaw: 0.4971
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 35532000
                    Iteration time: 1.14s
                      Time elapsed: 00:18:18
                               ETA: 00:09:32

################################################################################
                     [1m Learning iteration 987/1500 [0m                      

                       Computation: 33059 steps/s (collection: 1.035s, learning 0.054s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0230
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 13.6628
                       Mean reward: 17.09
               Mean episode length: 857.16
Episode_Reward/track_lin_vel_xy_exp: 0.9524
Episode_Reward/track_ang_vel_z_exp: 0.4437
       Episode_Reward/lin_vel_z_l2: -0.0440
      Episode_Reward/ang_vel_xy_l2: -0.1235
     Episode_Reward/dof_torques_l2: -0.0859
         Episode_Reward/dof_acc_l2: -0.1779
     Episode_Reward/action_rate_l2: -0.1679
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.7991
Metrics/base_velocity/error_vel_xy: 0.4411
Metrics/base_velocity/error_vel_yaw: 0.4990
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 35568000
                    Iteration time: 1.09s
                      Time elapsed: 00:18:19
                               ETA: 00:09:30

################################################################################
                     [1m Learning iteration 988/1500 [0m                      

                       Computation: 34327 steps/s (collection: 0.997s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0255
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.6541
                       Mean reward: 16.67
               Mean episode length: 857.61
Episode_Reward/track_lin_vel_xy_exp: 1.0099
Episode_Reward/track_ang_vel_z_exp: 0.4569
       Episode_Reward/lin_vel_z_l2: -0.0470
      Episode_Reward/ang_vel_xy_l2: -0.1307
     Episode_Reward/dof_torques_l2: -0.0891
         Episode_Reward/dof_acc_l2: -0.1975
     Episode_Reward/action_rate_l2: -0.1768
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8012
Metrics/base_velocity/error_vel_xy: 0.4274
Metrics/base_velocity/error_vel_yaw: 0.5185
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 35604000
                    Iteration time: 1.05s
                      Time elapsed: 00:18:20
                               ETA: 00:09:29

################################################################################
                     [1m Learning iteration 989/1500 [0m                      

                       Computation: 34098 steps/s (collection: 1.002s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0219
               Mean surrogate loss: -0.0152
                 Mean entropy loss: 13.6482
                       Mean reward: 18.02
               Mean episode length: 890.09
Episode_Reward/track_lin_vel_xy_exp: 1.0436
Episode_Reward/track_ang_vel_z_exp: 0.4619
       Episode_Reward/lin_vel_z_l2: -0.0502
      Episode_Reward/ang_vel_xy_l2: -0.1289
     Episode_Reward/dof_torques_l2: -0.0901
         Episode_Reward/dof_acc_l2: -0.1868
     Episode_Reward/action_rate_l2: -0.1756
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8044
Metrics/base_velocity/error_vel_xy: 0.4110
Metrics/base_velocity/error_vel_yaw: 0.5297
      Episode_Termination/time_out: 2.0417
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 35640000
                    Iteration time: 1.06s
                      Time elapsed: 00:18:21
                               ETA: 00:09:28

################################################################################
                     [1m Learning iteration 990/1500 [0m                      

                       Computation: 33900 steps/s (collection: 1.009s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0258
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 13.6421
                       Mean reward: 17.87
               Mean episode length: 899.79
Episode_Reward/track_lin_vel_xy_exp: 0.9937
Episode_Reward/track_ang_vel_z_exp: 0.4536
       Episode_Reward/lin_vel_z_l2: -0.0470
      Episode_Reward/ang_vel_xy_l2: -0.1234
     Episode_Reward/dof_torques_l2: -0.0863
         Episode_Reward/dof_acc_l2: -0.1802
     Episode_Reward/action_rate_l2: -0.1687
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8111
Metrics/base_velocity/error_vel_xy: 0.4182
Metrics/base_velocity/error_vel_yaw: 0.4720
      Episode_Termination/time_out: 1.9167
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 35676000
                    Iteration time: 1.06s
                      Time elapsed: 00:18:22
                               ETA: 00:09:27

################################################################################
                     [1m Learning iteration 991/1500 [0m                      

                       Computation: 31662 steps/s (collection: 1.085s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0247
               Mean surrogate loss: -0.0155
                 Mean entropy loss: 13.6349
                       Mean reward: 17.10
               Mean episode length: 854.38
Episode_Reward/track_lin_vel_xy_exp: 1.0269
Episode_Reward/track_ang_vel_z_exp: 0.4558
       Episode_Reward/lin_vel_z_l2: -0.0450
      Episode_Reward/ang_vel_xy_l2: -0.1238
     Episode_Reward/dof_torques_l2: -0.0824
         Episode_Reward/dof_acc_l2: -0.1781
     Episode_Reward/action_rate_l2: -0.1663
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8149
Metrics/base_velocity/error_vel_xy: 0.3666
Metrics/base_velocity/error_vel_yaw: 0.4710
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 35712000
                    Iteration time: 1.14s
                      Time elapsed: 00:18:23
                               ETA: 00:09:26

################################################################################
                     [1m Learning iteration 992/1500 [0m                      

                       Computation: 32807 steps/s (collection: 1.044s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0242
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 13.6305
                       Mean reward: 16.90
               Mean episode length: 878.54
Episode_Reward/track_lin_vel_xy_exp: 0.9916
Episode_Reward/track_ang_vel_z_exp: 0.4799
       Episode_Reward/lin_vel_z_l2: -0.0485
      Episode_Reward/ang_vel_xy_l2: -0.1302
     Episode_Reward/dof_torques_l2: -0.0965
         Episode_Reward/dof_acc_l2: -0.1925
     Episode_Reward/action_rate_l2: -0.1810
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8182
Metrics/base_velocity/error_vel_xy: 0.5281
Metrics/base_velocity/error_vel_yaw: 0.5222
      Episode_Termination/time_out: 1.9167
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 35748000
                    Iteration time: 1.10s
                      Time elapsed: 00:18:24
                               ETA: 00:09:25

################################################################################
                     [1m Learning iteration 993/1500 [0m                      

                       Computation: 32630 steps/s (collection: 1.052s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0250
               Mean surrogate loss: -0.0157
                 Mean entropy loss: 13.6400
                       Mean reward: 16.32
               Mean episode length: 866.18
Episode_Reward/track_lin_vel_xy_exp: 0.9446
Episode_Reward/track_ang_vel_z_exp: 0.4338
       Episode_Reward/lin_vel_z_l2: -0.0463
      Episode_Reward/ang_vel_xy_l2: -0.1213
     Episode_Reward/dof_torques_l2: -0.0842
         Episode_Reward/dof_acc_l2: -0.1730
     Episode_Reward/action_rate_l2: -0.1635
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8230
Metrics/base_velocity/error_vel_xy: 0.4207
Metrics/base_velocity/error_vel_yaw: 0.4899
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 35784000
                    Iteration time: 1.10s
                      Time elapsed: 00:18:26
                               ETA: 00:09:24

################################################################################
                     [1m Learning iteration 994/1500 [0m                      

                       Computation: 32707 steps/s (collection: 1.041s, learning 0.059s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0247
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 13.6544
                       Mean reward: 16.61
               Mean episode length: 866.85
Episode_Reward/track_lin_vel_xy_exp: 0.9808
Episode_Reward/track_ang_vel_z_exp: 0.4533
       Episode_Reward/lin_vel_z_l2: -0.0463
      Episode_Reward/ang_vel_xy_l2: -0.1258
     Episode_Reward/dof_torques_l2: -0.0863
         Episode_Reward/dof_acc_l2: -0.1910
     Episode_Reward/action_rate_l2: -0.1739
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8240
Metrics/base_velocity/error_vel_xy: 0.4572
Metrics/base_velocity/error_vel_yaw: 0.5159
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 35820000
                    Iteration time: 1.10s
                      Time elapsed: 00:18:27
                               ETA: 00:09:23

################################################################################
                     [1m Learning iteration 995/1500 [0m                      

                       Computation: 32900 steps/s (collection: 1.042s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0264
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 13.6502
                       Mean reward: 17.36
               Mean episode length: 863.27
Episode_Reward/track_lin_vel_xy_exp: 1.0216
Episode_Reward/track_ang_vel_z_exp: 0.4509
       Episode_Reward/lin_vel_z_l2: -0.0380
      Episode_Reward/ang_vel_xy_l2: -0.1170
     Episode_Reward/dof_torques_l2: -0.0833
         Episode_Reward/dof_acc_l2: -0.1724
     Episode_Reward/action_rate_l2: -0.1626
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8201
Metrics/base_velocity/error_vel_xy: 0.3532
Metrics/base_velocity/error_vel_yaw: 0.4722
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 35856000
                    Iteration time: 1.09s
                      Time elapsed: 00:18:28
                               ETA: 00:09:21

################################################################################
                     [1m Learning iteration 996/1500 [0m                      

                       Computation: 33121 steps/s (collection: 1.034s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0249
               Mean surrogate loss: -0.0155
                 Mean entropy loss: 13.6488
                       Mean reward: 17.57
               Mean episode length: 873.04
Episode_Reward/track_lin_vel_xy_exp: 1.0381
Episode_Reward/track_ang_vel_z_exp: 0.4599
       Episode_Reward/lin_vel_z_l2: -0.0433
      Episode_Reward/ang_vel_xy_l2: -0.1249
     Episode_Reward/dof_torques_l2: -0.0857
         Episode_Reward/dof_acc_l2: -0.1841
     Episode_Reward/action_rate_l2: -0.1696
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8182
Metrics/base_velocity/error_vel_xy: 0.3791
Metrics/base_velocity/error_vel_yaw: 0.4972
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 35892000
                    Iteration time: 1.09s
                      Time elapsed: 00:18:29
                               ETA: 00:09:20

################################################################################
                     [1m Learning iteration 997/1500 [0m                      

                       Computation: 32985 steps/s (collection: 1.040s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0244
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 13.6547
                       Mean reward: 17.19
               Mean episode length: 862.30
Episode_Reward/track_lin_vel_xy_exp: 0.9664
Episode_Reward/track_ang_vel_z_exp: 0.4459
       Episode_Reward/lin_vel_z_l2: -0.0441
      Episode_Reward/ang_vel_xy_l2: -0.1213
     Episode_Reward/dof_torques_l2: -0.0818
         Episode_Reward/dof_acc_l2: -0.1714
     Episode_Reward/action_rate_l2: -0.1642
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8216
Metrics/base_velocity/error_vel_xy: 0.4159
Metrics/base_velocity/error_vel_yaw: 0.4741
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 35928000
                    Iteration time: 1.09s
                      Time elapsed: 00:18:30
                               ETA: 00:09:19

################################################################################
                     [1m Learning iteration 998/1500 [0m                      

                       Computation: 32803 steps/s (collection: 1.044s, learning 0.054s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0271
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 13.6612
                       Mean reward: 17.10
               Mean episode length: 879.19
Episode_Reward/track_lin_vel_xy_exp: 1.0310
Episode_Reward/track_ang_vel_z_exp: 0.4832
       Episode_Reward/lin_vel_z_l2: -0.0500
      Episode_Reward/ang_vel_xy_l2: -0.1330
     Episode_Reward/dof_torques_l2: -0.0872
         Episode_Reward/dof_acc_l2: -0.1947
     Episode_Reward/action_rate_l2: -0.1785
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8222
Metrics/base_velocity/error_vel_xy: 0.4921
Metrics/base_velocity/error_vel_yaw: 0.5336
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 35964000
                    Iteration time: 1.10s
                      Time elapsed: 00:18:31
                               ETA: 00:09:18

################################################################################
                     [1m Learning iteration 999/1500 [0m                      

                       Computation: 33321 steps/s (collection: 1.027s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0243
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 13.6573
                       Mean reward: 16.65
               Mean episode length: 834.13
Episode_Reward/track_lin_vel_xy_exp: 0.8958
Episode_Reward/track_ang_vel_z_exp: 0.3964
       Episode_Reward/lin_vel_z_l2: -0.0446
      Episode_Reward/ang_vel_xy_l2: -0.1121
     Episode_Reward/dof_torques_l2: -0.0694
         Episode_Reward/dof_acc_l2: -0.1558
     Episode_Reward/action_rate_l2: -0.1452
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8207
Metrics/base_velocity/error_vel_xy: 0.3370
Metrics/base_velocity/error_vel_yaw: 0.4180
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 36000000
                    Iteration time: 1.08s
                      Time elapsed: 00:18:32
                               ETA: 00:09:17

################################################################################
                     [1m Learning iteration 1000/1500 [0m                     

                       Computation: 33911 steps/s (collection: 1.010s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0242
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 13.6388
                       Mean reward: 16.41
               Mean episode length: 818.95
Episode_Reward/track_lin_vel_xy_exp: 0.9681
Episode_Reward/track_ang_vel_z_exp: 0.4524
       Episode_Reward/lin_vel_z_l2: -0.0449
      Episode_Reward/ang_vel_xy_l2: -0.1246
     Episode_Reward/dof_torques_l2: -0.0864
         Episode_Reward/dof_acc_l2: -0.2004
     Episode_Reward/action_rate_l2: -0.1734
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8152
Metrics/base_velocity/error_vel_xy: 0.4654
Metrics/base_velocity/error_vel_yaw: 0.4986
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 36036000
                    Iteration time: 1.06s
                      Time elapsed: 00:18:33
                               ETA: 00:09:16

################################################################################
                     [1m Learning iteration 1001/1500 [0m                     

                       Computation: 33546 steps/s (collection: 1.019s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0252
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 13.6365
                       Mean reward: 16.79
               Mean episode length: 824.54
Episode_Reward/track_lin_vel_xy_exp: 0.9784
Episode_Reward/track_ang_vel_z_exp: 0.4501
       Episode_Reward/lin_vel_z_l2: -0.0416
      Episode_Reward/ang_vel_xy_l2: -0.1224
     Episode_Reward/dof_torques_l2: -0.0838
         Episode_Reward/dof_acc_l2: -0.1822
     Episode_Reward/action_rate_l2: -0.1670
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8153
Metrics/base_velocity/error_vel_xy: 0.4234
Metrics/base_velocity/error_vel_yaw: 0.4843
      Episode_Termination/time_out: 1.1667
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 36072000
                    Iteration time: 1.07s
                      Time elapsed: 00:18:34
                               ETA: 00:09:15

################################################################################
                     [1m Learning iteration 1002/1500 [0m                     

                       Computation: 32642 steps/s (collection: 1.050s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0243
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 13.6462
                       Mean reward: 16.81
               Mean episode length: 845.99
Episode_Reward/track_lin_vel_xy_exp: 0.9829
Episode_Reward/track_ang_vel_z_exp: 0.4633
       Episode_Reward/lin_vel_z_l2: -0.0481
      Episode_Reward/ang_vel_xy_l2: -0.1309
     Episode_Reward/dof_torques_l2: -0.0826
         Episode_Reward/dof_acc_l2: -0.1876
     Episode_Reward/action_rate_l2: -0.1731
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8172
Metrics/base_velocity/error_vel_xy: 0.4853
Metrics/base_velocity/error_vel_yaw: 0.5171
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 36108000
                    Iteration time: 1.10s
                      Time elapsed: 00:18:35
                               ETA: 00:09:14

################################################################################
                     [1m Learning iteration 1003/1500 [0m                     

                       Computation: 31163 steps/s (collection: 1.101s, learning 0.054s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0263
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 13.6540
                       Mean reward: 17.27
               Mean episode length: 876.22
Episode_Reward/track_lin_vel_xy_exp: 1.0809
Episode_Reward/track_ang_vel_z_exp: 0.4805
       Episode_Reward/lin_vel_z_l2: -0.0463
      Episode_Reward/ang_vel_xy_l2: -0.1276
     Episode_Reward/dof_torques_l2: -0.0898
         Episode_Reward/dof_acc_l2: -0.1875
     Episode_Reward/action_rate_l2: -0.1790
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8200
Metrics/base_velocity/error_vel_xy: 0.4188
Metrics/base_velocity/error_vel_yaw: 0.5237
      Episode_Termination/time_out: 1.9583
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 36144000
                    Iteration time: 1.16s
                      Time elapsed: 00:18:36
                               ETA: 00:09:12

################################################################################
                     [1m Learning iteration 1004/1500 [0m                     

                       Computation: 32402 steps/s (collection: 1.056s, learning 0.055s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0244
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 13.6604
                       Mean reward: 17.48
               Mean episode length: 893.92
Episode_Reward/track_lin_vel_xy_exp: 1.0537
Episode_Reward/track_ang_vel_z_exp: 0.4982
       Episode_Reward/lin_vel_z_l2: -0.0475
      Episode_Reward/ang_vel_xy_l2: -0.1314
     Episode_Reward/dof_torques_l2: -0.0980
         Episode_Reward/dof_acc_l2: -0.2006
     Episode_Reward/action_rate_l2: -0.1862
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8233
Metrics/base_velocity/error_vel_xy: 0.5019
Metrics/base_velocity/error_vel_yaw: 0.5495
      Episode_Termination/time_out: 1.9583
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 36180000
                    Iteration time: 1.11s
                      Time elapsed: 00:18:38
                               ETA: 00:09:11

################################################################################
                     [1m Learning iteration 1005/1500 [0m                     

                       Computation: 33610 steps/s (collection: 1.020s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0249
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 13.6615
                       Mean reward: 15.94
               Mean episode length: 835.26
Episode_Reward/track_lin_vel_xy_exp: 0.8758
Episode_Reward/track_ang_vel_z_exp: 0.3986
       Episode_Reward/lin_vel_z_l2: -0.0418
      Episode_Reward/ang_vel_xy_l2: -0.1092
     Episode_Reward/dof_torques_l2: -0.0775
         Episode_Reward/dof_acc_l2: -0.1612
     Episode_Reward/action_rate_l2: -0.1503
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8187
Metrics/base_velocity/error_vel_xy: 0.3612
Metrics/base_velocity/error_vel_yaw: 0.4460
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 36216000
                    Iteration time: 1.07s
                      Time elapsed: 00:18:39
                               ETA: 00:09:10

################################################################################
                     [1m Learning iteration 1006/1500 [0m                     

                       Computation: 31472 steps/s (collection: 1.092s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0265
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 13.6623
                       Mean reward: 15.94
               Mean episode length: 812.62
Episode_Reward/track_lin_vel_xy_exp: 0.8851
Episode_Reward/track_ang_vel_z_exp: 0.4096
       Episode_Reward/lin_vel_z_l2: -0.0449
      Episode_Reward/ang_vel_xy_l2: -0.1185
     Episode_Reward/dof_torques_l2: -0.0790
         Episode_Reward/dof_acc_l2: -0.1724
     Episode_Reward/action_rate_l2: -0.1569
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8192
Metrics/base_velocity/error_vel_xy: 0.4353
Metrics/base_velocity/error_vel_yaw: 0.4918
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 36252000
                    Iteration time: 1.14s
                      Time elapsed: 00:18:40
                               ETA: 00:09:09

################################################################################
                     [1m Learning iteration 1007/1500 [0m                     

                       Computation: 31020 steps/s (collection: 1.108s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0242
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 13.6612
                       Mean reward: 16.20
               Mean episode length: 829.12
Episode_Reward/track_lin_vel_xy_exp: 1.0137
Episode_Reward/track_ang_vel_z_exp: 0.4677
       Episode_Reward/lin_vel_z_l2: -0.0460
      Episode_Reward/ang_vel_xy_l2: -0.1242
     Episode_Reward/dof_torques_l2: -0.0868
         Episode_Reward/dof_acc_l2: -0.1863
     Episode_Reward/action_rate_l2: -0.1715
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8219
Metrics/base_velocity/error_vel_xy: 0.4197
Metrics/base_velocity/error_vel_yaw: 0.4725
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 36288000
                    Iteration time: 1.16s
                      Time elapsed: 00:18:41
                               ETA: 00:09:08

################################################################################
                     [1m Learning iteration 1008/1500 [0m                     

                       Computation: 30483 steps/s (collection: 1.128s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0284
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.6574
                       Mean reward: 17.55
               Mean episode length: 862.25
Episode_Reward/track_lin_vel_xy_exp: 1.0318
Episode_Reward/track_ang_vel_z_exp: 0.4640
       Episode_Reward/lin_vel_z_l2: -0.0445
      Episode_Reward/ang_vel_xy_l2: -0.1227
     Episode_Reward/dof_torques_l2: -0.0843
         Episode_Reward/dof_acc_l2: -0.1919
     Episode_Reward/action_rate_l2: -0.1726
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8333
Metrics/base_velocity/error_vel_xy: 0.3937
Metrics/base_velocity/error_vel_yaw: 0.4840
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 36324000
                    Iteration time: 1.18s
                      Time elapsed: 00:18:42
                               ETA: 00:09:07

################################################################################
                     [1m Learning iteration 1009/1500 [0m                     

                       Computation: 30332 steps/s (collection: 1.134s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0226
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 13.6630
                       Mean reward: 17.25
               Mean episode length: 858.74
Episode_Reward/track_lin_vel_xy_exp: 0.9684
Episode_Reward/track_ang_vel_z_exp: 0.4716
       Episode_Reward/lin_vel_z_l2: -0.0428
      Episode_Reward/ang_vel_xy_l2: -0.1273
     Episode_Reward/dof_torques_l2: -0.0848
         Episode_Reward/dof_acc_l2: -0.1823
     Episode_Reward/action_rate_l2: -0.1717
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8383
Metrics/base_velocity/error_vel_xy: 0.4790
Metrics/base_velocity/error_vel_yaw: 0.4841
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 36360000
                    Iteration time: 1.19s
                      Time elapsed: 00:18:43
                               ETA: 00:09:06

################################################################################
                     [1m Learning iteration 1010/1500 [0m                     

                       Computation: 30619 steps/s (collection: 1.124s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0246
               Mean surrogate loss: -0.0156
                 Mean entropy loss: 13.6601
                       Mean reward: 17.56
               Mean episode length: 854.93
Episode_Reward/track_lin_vel_xy_exp: 0.9929
Episode_Reward/track_ang_vel_z_exp: 0.4451
       Episode_Reward/lin_vel_z_l2: -0.0431
      Episode_Reward/ang_vel_xy_l2: -0.1190
     Episode_Reward/dof_torques_l2: -0.0820
         Episode_Reward/dof_acc_l2: -0.1744
     Episode_Reward/action_rate_l2: -0.1643
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8437
Metrics/base_velocity/error_vel_xy: 0.3811
Metrics/base_velocity/error_vel_yaw: 0.4754
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 36396000
                    Iteration time: 1.18s
                      Time elapsed: 00:18:44
                               ETA: 00:09:05

################################################################################
                     [1m Learning iteration 1011/1500 [0m                     

                       Computation: 31634 steps/s (collection: 1.082s, learning 0.056s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0239
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 13.6533
                       Mean reward: 17.12
               Mean episode length: 854.52
Episode_Reward/track_lin_vel_xy_exp: 0.9173
Episode_Reward/track_ang_vel_z_exp: 0.4280
       Episode_Reward/lin_vel_z_l2: -0.0419
      Episode_Reward/ang_vel_xy_l2: -0.1173
     Episode_Reward/dof_torques_l2: -0.0866
         Episode_Reward/dof_acc_l2: -0.1762
     Episode_Reward/action_rate_l2: -0.1641
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8497
Metrics/base_velocity/error_vel_xy: 0.4500
Metrics/base_velocity/error_vel_yaw: 0.4983
      Episode_Termination/time_out: 2.2500
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 36432000
                    Iteration time: 1.14s
                      Time elapsed: 00:18:46
                               ETA: 00:09:04

################################################################################
                     [1m Learning iteration 1012/1500 [0m                     

                       Computation: 31330 steps/s (collection: 1.095s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0213
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 13.6446
                       Mean reward: 17.19
               Mean episode length: 846.99
Episode_Reward/track_lin_vel_xy_exp: 1.0174
Episode_Reward/track_ang_vel_z_exp: 0.4666
       Episode_Reward/lin_vel_z_l2: -0.0461
      Episode_Reward/ang_vel_xy_l2: -0.1245
     Episode_Reward/dof_torques_l2: -0.0941
         Episode_Reward/dof_acc_l2: -0.1848
     Episode_Reward/action_rate_l2: -0.1772
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8483
Metrics/base_velocity/error_vel_xy: 0.4212
Metrics/base_velocity/error_vel_yaw: 0.4923
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 36468000
                    Iteration time: 1.15s
                      Time elapsed: 00:18:47
                               ETA: 00:09:03

################################################################################
                     [1m Learning iteration 1013/1500 [0m                     

                       Computation: 30972 steps/s (collection: 1.103s, learning 0.060s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0249
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 13.6377
                       Mean reward: 17.79
               Mean episode length: 855.93
Episode_Reward/track_lin_vel_xy_exp: 0.9958
Episode_Reward/track_ang_vel_z_exp: 0.4438
       Episode_Reward/lin_vel_z_l2: -0.0443
      Episode_Reward/ang_vel_xy_l2: -0.1179
     Episode_Reward/dof_torques_l2: -0.0858
         Episode_Reward/dof_acc_l2: -0.1711
     Episode_Reward/action_rate_l2: -0.1648
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8426
Metrics/base_velocity/error_vel_xy: 0.3660
Metrics/base_velocity/error_vel_yaw: 0.4595
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 36504000
                    Iteration time: 1.16s
                      Time elapsed: 00:18:48
                               ETA: 00:09:01

################################################################################
                     [1m Learning iteration 1014/1500 [0m                     

                       Computation: 30865 steps/s (collection: 1.115s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0251
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 13.6408
                       Mean reward: 18.46
               Mean episode length: 887.99
Episode_Reward/track_lin_vel_xy_exp: 1.0653
Episode_Reward/track_ang_vel_z_exp: 0.5042
       Episode_Reward/lin_vel_z_l2: -0.0503
      Episode_Reward/ang_vel_xy_l2: -0.1344
     Episode_Reward/dof_torques_l2: -0.0947
         Episode_Reward/dof_acc_l2: -0.2068
     Episode_Reward/action_rate_l2: -0.1890
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8449
Metrics/base_velocity/error_vel_xy: 0.5144
Metrics/base_velocity/error_vel_yaw: 0.5124
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 36540000
                    Iteration time: 1.17s
                      Time elapsed: 00:18:49
                               ETA: 00:09:00

################################################################################
                     [1m Learning iteration 1015/1500 [0m                     

                       Computation: 30888 steps/s (collection: 1.113s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0234
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 13.6360
                       Mean reward: 18.52
               Mean episode length: 891.07
Episode_Reward/track_lin_vel_xy_exp: 1.0294
Episode_Reward/track_ang_vel_z_exp: 0.4669
       Episode_Reward/lin_vel_z_l2: -0.0458
      Episode_Reward/ang_vel_xy_l2: -0.1271
     Episode_Reward/dof_torques_l2: -0.0842
         Episode_Reward/dof_acc_l2: -0.1765
     Episode_Reward/action_rate_l2: -0.1707
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8539
Metrics/base_velocity/error_vel_xy: 0.4059
Metrics/base_velocity/error_vel_yaw: 0.4794
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 36576000
                    Iteration time: 1.17s
                      Time elapsed: 00:18:50
                               ETA: 00:08:59

################################################################################
                     [1m Learning iteration 1016/1500 [0m                     

                       Computation: 30412 steps/s (collection: 1.131s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0272
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 13.6413
                       Mean reward: 17.24
               Mean episode length: 868.77
Episode_Reward/track_lin_vel_xy_exp: 0.9457
Episode_Reward/track_ang_vel_z_exp: 0.4302
       Episode_Reward/lin_vel_z_l2: -0.0430
      Episode_Reward/ang_vel_xy_l2: -0.1185
     Episode_Reward/dof_torques_l2: -0.0822
         Episode_Reward/dof_acc_l2: -0.1815
     Episode_Reward/action_rate_l2: -0.1628
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8678
Metrics/base_velocity/error_vel_xy: 0.3947
Metrics/base_velocity/error_vel_yaw: 0.4746
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 36612000
                    Iteration time: 1.18s
                      Time elapsed: 00:18:51
                               ETA: 00:08:58

################################################################################
                     [1m Learning iteration 1017/1500 [0m                     

                       Computation: 30332 steps/s (collection: 1.132s, learning 0.055s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0255
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 13.6457
                       Mean reward: 17.25
               Mean episode length: 873.80
Episode_Reward/track_lin_vel_xy_exp: 1.1076
Episode_Reward/track_ang_vel_z_exp: 0.5070
       Episode_Reward/lin_vel_z_l2: -0.0481
      Episode_Reward/ang_vel_xy_l2: -0.1349
     Episode_Reward/dof_torques_l2: -0.0950
         Episode_Reward/dof_acc_l2: -0.2010
     Episode_Reward/action_rate_l2: -0.1879
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8782
Metrics/base_velocity/error_vel_xy: 0.4531
Metrics/base_velocity/error_vel_yaw: 0.5379
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 36648000
                    Iteration time: 1.19s
                      Time elapsed: 00:18:53
                               ETA: 00:08:57

################################################################################
                     [1m Learning iteration 1018/1500 [0m                     

                       Computation: 31250 steps/s (collection: 1.097s, learning 0.055s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0229
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 13.6492
                       Mean reward: 17.70
               Mean episode length: 888.86
Episode_Reward/track_lin_vel_xy_exp: 1.0020
Episode_Reward/track_ang_vel_z_exp: 0.4509
       Episode_Reward/lin_vel_z_l2: -0.0433
      Episode_Reward/ang_vel_xy_l2: -0.1185
     Episode_Reward/dof_torques_l2: -0.0835
         Episode_Reward/dof_acc_l2: -0.1675
     Episode_Reward/action_rate_l2: -0.1643
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8749
Metrics/base_velocity/error_vel_xy: 0.3927
Metrics/base_velocity/error_vel_yaw: 0.4849
      Episode_Termination/time_out: 1.1667
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 36684000
                    Iteration time: 1.15s
                      Time elapsed: 00:18:54
                               ETA: 00:08:56

################################################################################
                     [1m Learning iteration 1019/1500 [0m                     

                       Computation: 31087 steps/s (collection: 1.107s, learning 0.051s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0258
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 13.6417
                       Mean reward: 17.19
               Mean episode length: 879.80
Episode_Reward/track_lin_vel_xy_exp: 1.0129
Episode_Reward/track_ang_vel_z_exp: 0.4734
       Episode_Reward/lin_vel_z_l2: -0.0504
      Episode_Reward/ang_vel_xy_l2: -0.1314
     Episode_Reward/dof_torques_l2: -0.0879
         Episode_Reward/dof_acc_l2: -0.1943
     Episode_Reward/action_rate_l2: -0.1772
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8722
Metrics/base_velocity/error_vel_xy: 0.4461
Metrics/base_velocity/error_vel_yaw: 0.4899
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 36720000
                    Iteration time: 1.16s
                      Time elapsed: 00:18:55
                               ETA: 00:08:55

################################################################################
                     [1m Learning iteration 1020/1500 [0m                     

                       Computation: 31112 steps/s (collection: 1.106s, learning 0.051s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0262
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 13.6334
                       Mean reward: 17.64
               Mean episode length: 887.04
Episode_Reward/track_lin_vel_xy_exp: 1.0113
Episode_Reward/track_ang_vel_z_exp: 0.4611
       Episode_Reward/lin_vel_z_l2: -0.0398
      Episode_Reward/ang_vel_xy_l2: -0.1210
     Episode_Reward/dof_torques_l2: -0.0862
         Episode_Reward/dof_acc_l2: -0.1705
     Episode_Reward/action_rate_l2: -0.1686
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8741
Metrics/base_velocity/error_vel_xy: 0.4157
Metrics/base_velocity/error_vel_yaw: 0.4786
      Episode_Termination/time_out: 2.0417
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 36756000
                    Iteration time: 1.16s
                      Time elapsed: 00:18:56
                               ETA: 00:08:54

################################################################################
                     [1m Learning iteration 1021/1500 [0m                     

                       Computation: 30187 steps/s (collection: 1.125s, learning 0.068s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0232
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 13.6413
                       Mean reward: 17.46
               Mean episode length: 874.32
Episode_Reward/track_lin_vel_xy_exp: 0.9759
Episode_Reward/track_ang_vel_z_exp: 0.4576
       Episode_Reward/lin_vel_z_l2: -0.0423
      Episode_Reward/ang_vel_xy_l2: -0.1225
     Episode_Reward/dof_torques_l2: -0.0848
         Episode_Reward/dof_acc_l2: -0.1746
     Episode_Reward/action_rate_l2: -0.1679
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8736
Metrics/base_velocity/error_vel_xy: 0.4445
Metrics/base_velocity/error_vel_yaw: 0.4976
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 36792000
                    Iteration time: 1.19s
                      Time elapsed: 00:18:57
                               ETA: 00:08:53

################################################################################
                     [1m Learning iteration 1022/1500 [0m                     

                       Computation: 30023 steps/s (collection: 1.139s, learning 0.060s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0203
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 13.6521
                       Mean reward: 17.63
               Mean episode length: 868.29
Episode_Reward/track_lin_vel_xy_exp: 1.1244
Episode_Reward/track_ang_vel_z_exp: 0.5030
       Episode_Reward/lin_vel_z_l2: -0.0436
      Episode_Reward/ang_vel_xy_l2: -0.1291
     Episode_Reward/dof_torques_l2: -0.0903
         Episode_Reward/dof_acc_l2: -0.1900
     Episode_Reward/action_rate_l2: -0.1802
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8724
Metrics/base_velocity/error_vel_xy: 0.3926
Metrics/base_velocity/error_vel_yaw: 0.4973
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 36828000
                    Iteration time: 1.20s
                      Time elapsed: 00:18:58
                               ETA: 00:08:52

################################################################################
                     [1m Learning iteration 1023/1500 [0m                     

                       Computation: 30353 steps/s (collection: 1.135s, learning 0.051s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0203
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 13.6483
                       Mean reward: 17.40
               Mean episode length: 866.25
Episode_Reward/track_lin_vel_xy_exp: 1.0039
Episode_Reward/track_ang_vel_z_exp: 0.4587
       Episode_Reward/lin_vel_z_l2: -0.0420
      Episode_Reward/ang_vel_xy_l2: -0.1195
     Episode_Reward/dof_torques_l2: -0.0875
         Episode_Reward/dof_acc_l2: -0.1669
     Episode_Reward/action_rate_l2: -0.1667
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8752
Metrics/base_velocity/error_vel_xy: 0.4034
Metrics/base_velocity/error_vel_yaw: 0.4751
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 1.19s
                      Time elapsed: 00:19:00
                               ETA: 00:08:51

################################################################################
                     [1m Learning iteration 1024/1500 [0m                     

                       Computation: 30514 steps/s (collection: 1.126s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0285
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 13.6448
                       Mean reward: 17.84
               Mean episode length: 888.81
Episode_Reward/track_lin_vel_xy_exp: 1.0402
Episode_Reward/track_ang_vel_z_exp: 0.5013
       Episode_Reward/lin_vel_z_l2: -0.0469
      Episode_Reward/ang_vel_xy_l2: -0.1344
     Episode_Reward/dof_torques_l2: -0.0969
         Episode_Reward/dof_acc_l2: -0.1978
     Episode_Reward/action_rate_l2: -0.1864
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8810
Metrics/base_velocity/error_vel_xy: 0.5184
Metrics/base_velocity/error_vel_yaw: 0.5267
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 36900000
                    Iteration time: 1.18s
                      Time elapsed: 00:19:01
                               ETA: 00:08:50

################################################################################
                     [1m Learning iteration 1025/1500 [0m                     

                       Computation: 31010 steps/s (collection: 1.110s, learning 0.051s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0226
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 13.6401
                       Mean reward: 17.34
               Mean episode length: 868.13
Episode_Reward/track_lin_vel_xy_exp: 1.0103
Episode_Reward/track_ang_vel_z_exp: 0.4617
       Episode_Reward/lin_vel_z_l2: -0.0472
      Episode_Reward/ang_vel_xy_l2: -0.1284
     Episode_Reward/dof_torques_l2: -0.0850
         Episode_Reward/dof_acc_l2: -0.1970
     Episode_Reward/action_rate_l2: -0.1751
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8831
Metrics/base_velocity/error_vel_xy: 0.4366
Metrics/base_velocity/error_vel_yaw: 0.5035
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 36936000
                    Iteration time: 1.16s
                      Time elapsed: 00:19:02
                               ETA: 00:08:48

################################################################################
                     [1m Learning iteration 1026/1500 [0m                     

                       Computation: 30318 steps/s (collection: 1.135s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0254
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 13.6483
                       Mean reward: 16.21
               Mean episode length: 841.93
Episode_Reward/track_lin_vel_xy_exp: 0.9267
Episode_Reward/track_ang_vel_z_exp: 0.4344
       Episode_Reward/lin_vel_z_l2: -0.0426
      Episode_Reward/ang_vel_xy_l2: -0.1208
     Episode_Reward/dof_torques_l2: -0.0827
         Episode_Reward/dof_acc_l2: -0.1763
     Episode_Reward/action_rate_l2: -0.1624
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8843
Metrics/base_velocity/error_vel_xy: 0.4369
Metrics/base_velocity/error_vel_yaw: 0.4821
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 36972000
                    Iteration time: 1.19s
                      Time elapsed: 00:19:03
                               ETA: 00:08:47

################################################################################
                     [1m Learning iteration 1027/1500 [0m                     

                       Computation: 31571 steps/s (collection: 1.089s, learning 0.051s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0250
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 13.6490
                       Mean reward: 17.32
               Mean episode length: 872.23
Episode_Reward/track_lin_vel_xy_exp: 1.1631
Episode_Reward/track_ang_vel_z_exp: 0.5264
       Episode_Reward/lin_vel_z_l2: -0.0473
      Episode_Reward/ang_vel_xy_l2: -0.1359
     Episode_Reward/dof_torques_l2: -0.0934
         Episode_Reward/dof_acc_l2: -0.2061
     Episode_Reward/action_rate_l2: -0.1919
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8793
Metrics/base_velocity/error_vel_xy: 0.4073
Metrics/base_velocity/error_vel_yaw: 0.5020
      Episode_Termination/time_out: 1.9167
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 37008000
                    Iteration time: 1.14s
                      Time elapsed: 00:19:04
                               ETA: 00:08:46

################################################################################
                     [1m Learning iteration 1028/1500 [0m                     

                       Computation: 31008 steps/s (collection: 1.102s, learning 0.059s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0269
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 13.6375
                       Mean reward: 18.52
               Mean episode length: 891.26
Episode_Reward/track_lin_vel_xy_exp: 0.9895
Episode_Reward/track_ang_vel_z_exp: 0.4819
       Episode_Reward/lin_vel_z_l2: -0.0470
      Episode_Reward/ang_vel_xy_l2: -0.1317
     Episode_Reward/dof_torques_l2: -0.0967
         Episode_Reward/dof_acc_l2: -0.1826
     Episode_Reward/action_rate_l2: -0.1809
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8796
Metrics/base_velocity/error_vel_xy: 0.5427
Metrics/base_velocity/error_vel_yaw: 0.5342
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 37044000
                    Iteration time: 1.16s
                      Time elapsed: 00:19:06
                               ETA: 00:08:45

################################################################################
                     [1m Learning iteration 1029/1500 [0m                     

                       Computation: 28070 steps/s (collection: 1.223s, learning 0.059s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0262
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 13.6298
                       Mean reward: 17.46
               Mean episode length: 881.53
Episode_Reward/track_lin_vel_xy_exp: 0.9351
Episode_Reward/track_ang_vel_z_exp: 0.4384
       Episode_Reward/lin_vel_z_l2: -0.0464
      Episode_Reward/ang_vel_xy_l2: -0.1226
     Episode_Reward/dof_torques_l2: -0.0863
         Episode_Reward/dof_acc_l2: -0.1826
     Episode_Reward/action_rate_l2: -0.1671
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8810
Metrics/base_velocity/error_vel_xy: 0.4531
Metrics/base_velocity/error_vel_yaw: 0.5014
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 37080000
                    Iteration time: 1.28s
                      Time elapsed: 00:19:07
                               ETA: 00:08:44

################################################################################
                     [1m Learning iteration 1030/1500 [0m                     

                       Computation: 27349 steps/s (collection: 1.263s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0256
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 13.6244
                       Mean reward: 16.75
               Mean episode length: 858.36
Episode_Reward/track_lin_vel_xy_exp: 0.9817
Episode_Reward/track_ang_vel_z_exp: 0.4595
       Episode_Reward/lin_vel_z_l2: -0.0413
      Episode_Reward/ang_vel_xy_l2: -0.1226
     Episode_Reward/dof_torques_l2: -0.0855
         Episode_Reward/dof_acc_l2: -0.1847
     Episode_Reward/action_rate_l2: -0.1691
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8850
Metrics/base_velocity/error_vel_xy: 0.4355
Metrics/base_velocity/error_vel_yaw: 0.4719
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 37116000
                    Iteration time: 1.32s
                      Time elapsed: 00:19:08
                               ETA: 00:08:43

################################################################################
                     [1m Learning iteration 1031/1500 [0m                     

                       Computation: 30473 steps/s (collection: 1.130s, learning 0.051s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0264
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 13.6221
                       Mean reward: 17.33
               Mean episode length: 876.25
Episode_Reward/track_lin_vel_xy_exp: 1.0115
Episode_Reward/track_ang_vel_z_exp: 0.4665
       Episode_Reward/lin_vel_z_l2: -0.0488
      Episode_Reward/ang_vel_xy_l2: -0.1281
     Episode_Reward/dof_torques_l2: -0.0858
         Episode_Reward/dof_acc_l2: -0.1885
     Episode_Reward/action_rate_l2: -0.1740
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8841
Metrics/base_velocity/error_vel_xy: 0.4411
Metrics/base_velocity/error_vel_yaw: 0.5030
      Episode_Termination/time_out: 2.2083
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 37152000
                    Iteration time: 1.18s
                      Time elapsed: 00:19:09
                               ETA: 00:08:42

################################################################################
                     [1m Learning iteration 1032/1500 [0m                     

                       Computation: 29046 steps/s (collection: 1.188s, learning 0.051s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0276
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 13.6286
                       Mean reward: 17.67
               Mean episode length: 889.18
Episode_Reward/track_lin_vel_xy_exp: 1.0731
Episode_Reward/track_ang_vel_z_exp: 0.4961
       Episode_Reward/lin_vel_z_l2: -0.0452
      Episode_Reward/ang_vel_xy_l2: -0.1323
     Episode_Reward/dof_torques_l2: -0.0918
         Episode_Reward/dof_acc_l2: -0.1998
     Episode_Reward/action_rate_l2: -0.1827
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.8948
Metrics/base_velocity/error_vel_xy: 0.4369
Metrics/base_velocity/error_vel_yaw: 0.5063
      Episode_Termination/time_out: 1.9167
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 37188000
                    Iteration time: 1.24s
                      Time elapsed: 00:19:11
                               ETA: 00:08:41

################################################################################
                     [1m Learning iteration 1033/1500 [0m                     

                       Computation: 29299 steps/s (collection: 1.176s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0208
               Mean surrogate loss: -0.0151
                 Mean entropy loss: 13.6411
                       Mean reward: 17.92
               Mean episode length: 894.96
Episode_Reward/track_lin_vel_xy_exp: 1.0680
Episode_Reward/track_ang_vel_z_exp: 0.4858
       Episode_Reward/lin_vel_z_l2: -0.0410
      Episode_Reward/ang_vel_xy_l2: -0.1252
     Episode_Reward/dof_torques_l2: -0.0912
         Episode_Reward/dof_acc_l2: -0.1742
     Episode_Reward/action_rate_l2: -0.1745
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9083
Metrics/base_velocity/error_vel_xy: 0.4096
Metrics/base_velocity/error_vel_yaw: 0.4957
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 37224000
                    Iteration time: 1.23s
                      Time elapsed: 00:19:12
                               ETA: 00:08:40

################################################################################
                     [1m Learning iteration 1034/1500 [0m                     

                       Computation: 29424 steps/s (collection: 1.166s, learning 0.057s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0242
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 13.6405
                       Mean reward: 17.53
               Mean episode length: 887.89
Episode_Reward/track_lin_vel_xy_exp: 0.9973
Episode_Reward/track_ang_vel_z_exp: 0.4664
       Episode_Reward/lin_vel_z_l2: -0.0437
      Episode_Reward/ang_vel_xy_l2: -0.1265
     Episode_Reward/dof_torques_l2: -0.0882
         Episode_Reward/dof_acc_l2: -0.1942
     Episode_Reward/action_rate_l2: -0.1743
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9114
Metrics/base_velocity/error_vel_xy: 0.4563
Metrics/base_velocity/error_vel_yaw: 0.5062
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 37260000
                    Iteration time: 1.22s
                      Time elapsed: 00:19:13
                               ETA: 00:08:39

################################################################################
                     [1m Learning iteration 1035/1500 [0m                     

                       Computation: 30305 steps/s (collection: 1.133s, learning 0.055s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0233
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.6421
                       Mean reward: 17.39
               Mean episode length: 887.38
Episode_Reward/track_lin_vel_xy_exp: 1.0202
Episode_Reward/track_ang_vel_z_exp: 0.4884
       Episode_Reward/lin_vel_z_l2: -0.0467
      Episode_Reward/ang_vel_xy_l2: -0.1326
     Episode_Reward/dof_torques_l2: -0.0920
         Episode_Reward/dof_acc_l2: -0.1953
     Episode_Reward/action_rate_l2: -0.1814
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9174
Metrics/base_velocity/error_vel_xy: 0.5023
Metrics/base_velocity/error_vel_yaw: 0.5272
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 37296000
                    Iteration time: 1.19s
                      Time elapsed: 00:19:14
                               ETA: 00:08:38

################################################################################
                     [1m Learning iteration 1036/1500 [0m                     

                       Computation: 30216 steps/s (collection: 1.138s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0238
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 13.6442
                       Mean reward: 16.05
               Mean episode length: 835.81
Episode_Reward/track_lin_vel_xy_exp: 0.9681
Episode_Reward/track_ang_vel_z_exp: 0.4319
       Episode_Reward/lin_vel_z_l2: -0.0449
      Episode_Reward/ang_vel_xy_l2: -0.1160
     Episode_Reward/dof_torques_l2: -0.0787
         Episode_Reward/dof_acc_l2: -0.1717
     Episode_Reward/action_rate_l2: -0.1590
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9141
Metrics/base_velocity/error_vel_xy: 0.3627
Metrics/base_velocity/error_vel_yaw: 0.4568
      Episode_Termination/time_out: 1.1250
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 37332000
                    Iteration time: 1.19s
                      Time elapsed: 00:19:15
                               ETA: 00:08:37

################################################################################
                     [1m Learning iteration 1037/1500 [0m                     

                       Computation: 30251 steps/s (collection: 1.138s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0239
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.6529
                       Mean reward: 16.47
               Mean episode length: 841.57
Episode_Reward/track_lin_vel_xy_exp: 1.0386
Episode_Reward/track_ang_vel_z_exp: 0.4814
       Episode_Reward/lin_vel_z_l2: -0.0436
      Episode_Reward/ang_vel_xy_l2: -0.1316
     Episode_Reward/dof_torques_l2: -0.0901
         Episode_Reward/dof_acc_l2: -0.1917
     Episode_Reward/action_rate_l2: -0.1782
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9186
Metrics/base_velocity/error_vel_xy: 0.4659
Metrics/base_velocity/error_vel_yaw: 0.5276
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 37368000
                    Iteration time: 1.19s
                      Time elapsed: 00:19:17
                               ETA: 00:08:36

################################################################################
                     [1m Learning iteration 1038/1500 [0m                     

                       Computation: 31850 steps/s (collection: 1.079s, learning 0.051s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0221
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 13.6531
                       Mean reward: 17.81
               Mean episode length: 860.23
Episode_Reward/track_lin_vel_xy_exp: 1.1377
Episode_Reward/track_ang_vel_z_exp: 0.5161
       Episode_Reward/lin_vel_z_l2: -0.0459
      Episode_Reward/ang_vel_xy_l2: -0.1339
     Episode_Reward/dof_torques_l2: -0.0939
         Episode_Reward/dof_acc_l2: -0.2052
     Episode_Reward/action_rate_l2: -0.1866
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9259
Metrics/base_velocity/error_vel_xy: 0.4113
Metrics/base_velocity/error_vel_yaw: 0.5052
      Episode_Termination/time_out: 1.9167
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 37404000
                    Iteration time: 1.13s
                      Time elapsed: 00:19:18
                               ETA: 00:08:34

################################################################################
                     [1m Learning iteration 1039/1500 [0m                     

                       Computation: 32636 steps/s (collection: 1.051s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0248
               Mean surrogate loss: -0.0149
                 Mean entropy loss: 13.6434
                       Mean reward: 18.63
               Mean episode length: 907.89
Episode_Reward/track_lin_vel_xy_exp: 1.0658
Episode_Reward/track_ang_vel_z_exp: 0.4859
       Episode_Reward/lin_vel_z_l2: -0.0454
      Episode_Reward/ang_vel_xy_l2: -0.1358
     Episode_Reward/dof_torques_l2: -0.0871
         Episode_Reward/dof_acc_l2: -0.2057
     Episode_Reward/action_rate_l2: -0.1823
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9268
Metrics/base_velocity/error_vel_xy: 0.4346
Metrics/base_velocity/error_vel_yaw: 0.5230
      Episode_Termination/time_out: 1.9583
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 37440000
                    Iteration time: 1.10s
                      Time elapsed: 00:19:19
                               ETA: 00:08:33

################################################################################
                     [1m Learning iteration 1040/1500 [0m                     

                       Computation: 32105 steps/s (collection: 1.069s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0288
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.6289
                       Mean reward: 17.17
               Mean episode length: 885.51
Episode_Reward/track_lin_vel_xy_exp: 1.0103
Episode_Reward/track_ang_vel_z_exp: 0.4639
       Episode_Reward/lin_vel_z_l2: -0.0460
      Episode_Reward/ang_vel_xy_l2: -0.1282
     Episode_Reward/dof_torques_l2: -0.0823
         Episode_Reward/dof_acc_l2: -0.1893
     Episode_Reward/action_rate_l2: -0.1728
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9288
Metrics/base_velocity/error_vel_xy: 0.4318
Metrics/base_velocity/error_vel_yaw: 0.5023
      Episode_Termination/time_out: 1.9583
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 37476000
                    Iteration time: 1.12s
                      Time elapsed: 00:19:20
                               ETA: 00:08:32

################################################################################
                     [1m Learning iteration 1041/1500 [0m                     

                       Computation: 32143 steps/s (collection: 1.067s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0268
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.6282
                       Mean reward: 16.99
               Mean episode length: 860.93
Episode_Reward/track_lin_vel_xy_exp: 1.0290
Episode_Reward/track_ang_vel_z_exp: 0.4718
       Episode_Reward/lin_vel_z_l2: -0.0489
      Episode_Reward/ang_vel_xy_l2: -0.1296
     Episode_Reward/dof_torques_l2: -0.0822
         Episode_Reward/dof_acc_l2: -0.1912
     Episode_Reward/action_rate_l2: -0.1734
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9291
Metrics/base_velocity/error_vel_xy: 0.4167
Metrics/base_velocity/error_vel_yaw: 0.4871
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 37512000
                    Iteration time: 1.12s
                      Time elapsed: 00:19:21
                               ETA: 00:08:31

################################################################################
                     [1m Learning iteration 1042/1500 [0m                     

                       Computation: 33629 steps/s (collection: 1.017s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0214
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 13.6300
                       Mean reward: 17.11
               Mean episode length: 856.85
Episode_Reward/track_lin_vel_xy_exp: 0.9620
Episode_Reward/track_ang_vel_z_exp: 0.4666
       Episode_Reward/lin_vel_z_l2: -0.0441
      Episode_Reward/ang_vel_xy_l2: -0.1301
     Episode_Reward/dof_torques_l2: -0.0858
         Episode_Reward/dof_acc_l2: -0.1901
     Episode_Reward/action_rate_l2: -0.1740
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9213
Metrics/base_velocity/error_vel_xy: 0.4816
Metrics/base_velocity/error_vel_yaw: 0.4924
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 37548000
                    Iteration time: 1.07s
                      Time elapsed: 00:19:22
                               ETA: 00:08:30

################################################################################
                     [1m Learning iteration 1043/1500 [0m                     

                       Computation: 32834 steps/s (collection: 1.043s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0255
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 13.6329
                       Mean reward: 16.22
               Mean episode length: 862.16
Episode_Reward/track_lin_vel_xy_exp: 0.9586
Episode_Reward/track_ang_vel_z_exp: 0.4717
       Episode_Reward/lin_vel_z_l2: -0.0490
      Episode_Reward/ang_vel_xy_l2: -0.1361
     Episode_Reward/dof_torques_l2: -0.0949
         Episode_Reward/dof_acc_l2: -0.1991
     Episode_Reward/action_rate_l2: -0.1801
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9229
Metrics/base_velocity/error_vel_xy: 0.5452
Metrics/base_velocity/error_vel_yaw: 0.5487
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 37584000
                    Iteration time: 1.10s
                      Time elapsed: 00:19:23
                               ETA: 00:08:29

################################################################################
                     [1m Learning iteration 1044/1500 [0m                     

                       Computation: 33265 steps/s (collection: 1.028s, learning 0.055s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0289
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 13.6293
                       Mean reward: 16.31
               Mean episode length: 852.02
Episode_Reward/track_lin_vel_xy_exp: 1.0196
Episode_Reward/track_ang_vel_z_exp: 0.4775
       Episode_Reward/lin_vel_z_l2: -0.0489
      Episode_Reward/ang_vel_xy_l2: -0.1286
     Episode_Reward/dof_torques_l2: -0.0860
         Episode_Reward/dof_acc_l2: -0.1855
     Episode_Reward/action_rate_l2: -0.1738
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9308
Metrics/base_velocity/error_vel_xy: 0.4291
Metrics/base_velocity/error_vel_yaw: 0.4771
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 37620000
                    Iteration time: 1.08s
                      Time elapsed: 00:19:24
                               ETA: 00:08:28

################################################################################
                     [1m Learning iteration 1045/1500 [0m                     

                       Computation: 31563 steps/s (collection: 1.091s, learning 0.050s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0245
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 13.6259
                       Mean reward: 17.93
               Mean episode length: 893.34
Episode_Reward/track_lin_vel_xy_exp: 1.1072
Episode_Reward/track_ang_vel_z_exp: 0.5063
       Episode_Reward/lin_vel_z_l2: -0.0441
      Episode_Reward/ang_vel_xy_l2: -0.1365
     Episode_Reward/dof_torques_l2: -0.0941
         Episode_Reward/dof_acc_l2: -0.2004
     Episode_Reward/action_rate_l2: -0.1879
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9333
Metrics/base_velocity/error_vel_xy: 0.4493
Metrics/base_velocity/error_vel_yaw: 0.5246
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 37656000
                    Iteration time: 1.14s
                      Time elapsed: 00:19:25
                               ETA: 00:08:27

################################################################################
                     [1m Learning iteration 1046/1500 [0m                     

                       Computation: 32037 steps/s (collection: 1.073s, learning 0.050s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0234
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 13.6266
                       Mean reward: 17.66
               Mean episode length: 868.96
Episode_Reward/track_lin_vel_xy_exp: 0.9527
Episode_Reward/track_ang_vel_z_exp: 0.4266
       Episode_Reward/lin_vel_z_l2: -0.0365
      Episode_Reward/ang_vel_xy_l2: -0.1114
     Episode_Reward/dof_torques_l2: -0.0784
         Episode_Reward/dof_acc_l2: -0.1658
     Episode_Reward/action_rate_l2: -0.1559
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9346
Metrics/base_velocity/error_vel_xy: 0.3554
Metrics/base_velocity/error_vel_yaw: 0.4515
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 37692000
                    Iteration time: 1.12s
                      Time elapsed: 00:19:27
                               ETA: 00:08:26

################################################################################
                     [1m Learning iteration 1047/1500 [0m                     

                       Computation: 33251 steps/s (collection: 1.031s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0245
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 13.6327
                       Mean reward: 17.52
               Mean episode length: 879.60
Episode_Reward/track_lin_vel_xy_exp: 1.0591
Episode_Reward/track_ang_vel_z_exp: 0.4770
       Episode_Reward/lin_vel_z_l2: -0.0419
      Episode_Reward/ang_vel_xy_l2: -0.1305
     Episode_Reward/dof_torques_l2: -0.0830
         Episode_Reward/dof_acc_l2: -0.1790
     Episode_Reward/action_rate_l2: -0.1714
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9347
Metrics/base_velocity/error_vel_xy: 0.4104
Metrics/base_velocity/error_vel_yaw: 0.5068
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 37728000
                    Iteration time: 1.08s
                      Time elapsed: 00:19:28
                               ETA: 00:08:24

################################################################################
                     [1m Learning iteration 1048/1500 [0m                     

                       Computation: 31347 steps/s (collection: 1.097s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0242
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 13.6439
                       Mean reward: 18.41
               Mean episode length: 905.06
Episode_Reward/track_lin_vel_xy_exp: 1.1777
Episode_Reward/track_ang_vel_z_exp: 0.5189
       Episode_Reward/lin_vel_z_l2: -0.0472
      Episode_Reward/ang_vel_xy_l2: -0.1380
     Episode_Reward/dof_torques_l2: -0.0945
         Episode_Reward/dof_acc_l2: -0.2023
     Episode_Reward/action_rate_l2: -0.1891
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9292
Metrics/base_velocity/error_vel_xy: 0.3993
Metrics/base_velocity/error_vel_yaw: 0.5294
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 37764000
                    Iteration time: 1.15s
                      Time elapsed: 00:19:29
                               ETA: 00:08:23

################################################################################
                     [1m Learning iteration 1049/1500 [0m                     

                       Computation: 32051 steps/s (collection: 1.072s, learning 0.051s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0254
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 13.6524
                       Mean reward: 17.47
               Mean episode length: 869.55
Episode_Reward/track_lin_vel_xy_exp: 0.9592
Episode_Reward/track_ang_vel_z_exp: 0.4447
       Episode_Reward/lin_vel_z_l2: -0.0441
      Episode_Reward/ang_vel_xy_l2: -0.1227
     Episode_Reward/dof_torques_l2: -0.0801
         Episode_Reward/dof_acc_l2: -0.1758
     Episode_Reward/action_rate_l2: -0.1635
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9292
Metrics/base_velocity/error_vel_xy: 0.3962
Metrics/base_velocity/error_vel_yaw: 0.4377
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 37800000
                    Iteration time: 1.12s
                      Time elapsed: 00:19:30
                               ETA: 00:08:22

################################################################################
                     [1m Learning iteration 1050/1500 [0m                     

                       Computation: 32396 steps/s (collection: 1.057s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0271
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 13.6543
                       Mean reward: 17.11
               Mean episode length: 861.03
Episode_Reward/track_lin_vel_xy_exp: 1.1334
Episode_Reward/track_ang_vel_z_exp: 0.5052
       Episode_Reward/lin_vel_z_l2: -0.0504
      Episode_Reward/ang_vel_xy_l2: -0.1389
     Episode_Reward/dof_torques_l2: -0.0903
         Episode_Reward/dof_acc_l2: -0.1896
     Episode_Reward/action_rate_l2: -0.1820
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9303
Metrics/base_velocity/error_vel_xy: 0.4294
Metrics/base_velocity/error_vel_yaw: 0.5447
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 37836000
                    Iteration time: 1.11s
                      Time elapsed: 00:19:31
                               ETA: 00:08:21

################################################################################
                     [1m Learning iteration 1051/1500 [0m                     

                       Computation: 32135 steps/s (collection: 1.068s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0221
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 13.6642
                       Mean reward: 17.15
               Mean episode length: 889.03
Episode_Reward/track_lin_vel_xy_exp: 0.9831
Episode_Reward/track_ang_vel_z_exp: 0.4525
       Episode_Reward/lin_vel_z_l2: -0.0460
      Episode_Reward/ang_vel_xy_l2: -0.1271
     Episode_Reward/dof_torques_l2: -0.0873
         Episode_Reward/dof_acc_l2: -0.1965
     Episode_Reward/action_rate_l2: -0.1729
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9307
Metrics/base_velocity/error_vel_xy: 0.4412
Metrics/base_velocity/error_vel_yaw: 0.5131
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 37872000
                    Iteration time: 1.12s
                      Time elapsed: 00:19:32
                               ETA: 00:08:20

################################################################################
                     [1m Learning iteration 1052/1500 [0m                     

                       Computation: 32704 steps/s (collection: 1.051s, learning 0.050s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0256
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 13.6627
                       Mean reward: 17.27
               Mean episode length: 873.10
Episode_Reward/track_lin_vel_xy_exp: 0.9051
Episode_Reward/track_ang_vel_z_exp: 0.4198
       Episode_Reward/lin_vel_z_l2: -0.0390
      Episode_Reward/ang_vel_xy_l2: -0.1093
     Episode_Reward/dof_torques_l2: -0.0856
         Episode_Reward/dof_acc_l2: -0.1559
     Episode_Reward/action_rate_l2: -0.1547
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9285
Metrics/base_velocity/error_vel_xy: 0.3844
Metrics/base_velocity/error_vel_yaw: 0.4577
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 37908000
                    Iteration time: 1.10s
                      Time elapsed: 00:19:33
                               ETA: 00:08:19

################################################################################
                     [1m Learning iteration 1053/1500 [0m                     

                       Computation: 33287 steps/s (collection: 1.032s, learning 0.050s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0221
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 13.6656
                       Mean reward: 16.31
               Mean episode length: 848.02
Episode_Reward/track_lin_vel_xy_exp: 0.9324
Episode_Reward/track_ang_vel_z_exp: 0.4205
       Episode_Reward/lin_vel_z_l2: -0.0394
      Episode_Reward/ang_vel_xy_l2: -0.1093
     Episode_Reward/dof_torques_l2: -0.0794
         Episode_Reward/dof_acc_l2: -0.1693
     Episode_Reward/action_rate_l2: -0.1543
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9359
Metrics/base_velocity/error_vel_xy: 0.3614
Metrics/base_velocity/error_vel_yaw: 0.4329
      Episode_Termination/time_out: 1.1667
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 37944000
                    Iteration time: 1.08s
                      Time elapsed: 00:19:34
                               ETA: 00:08:18

################################################################################
                     [1m Learning iteration 1054/1500 [0m                     

                       Computation: 32778 steps/s (collection: 1.046s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0283
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 13.6583
                       Mean reward: 17.41
               Mean episode length: 870.53
Episode_Reward/track_lin_vel_xy_exp: 1.0394
Episode_Reward/track_ang_vel_z_exp: 0.4728
       Episode_Reward/lin_vel_z_l2: -0.0410
      Episode_Reward/ang_vel_xy_l2: -0.1261
     Episode_Reward/dof_torques_l2: -0.0894
         Episode_Reward/dof_acc_l2: -0.1858
     Episode_Reward/action_rate_l2: -0.1741
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9377
Metrics/base_velocity/error_vel_xy: 0.4095
Metrics/base_velocity/error_vel_yaw: 0.5012
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 37980000
                    Iteration time: 1.10s
                      Time elapsed: 00:19:35
                               ETA: 00:08:17

################################################################################
                     [1m Learning iteration 1055/1500 [0m                     

                       Computation: 33759 steps/s (collection: 1.014s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0238
               Mean surrogate loss: -0.0155
                 Mean entropy loss: 13.6510
                       Mean reward: 17.72
               Mean episode length: 868.96
Episode_Reward/track_lin_vel_xy_exp: 1.0044
Episode_Reward/track_ang_vel_z_exp: 0.4730
       Episode_Reward/lin_vel_z_l2: -0.0468
      Episode_Reward/ang_vel_xy_l2: -0.1322
     Episode_Reward/dof_torques_l2: -0.0921
         Episode_Reward/dof_acc_l2: -0.1984
     Episode_Reward/action_rate_l2: -0.1804
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9470
Metrics/base_velocity/error_vel_xy: 0.5096
Metrics/base_velocity/error_vel_yaw: 0.5382
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 38016000
                    Iteration time: 1.07s
                      Time elapsed: 00:19:36
                               ETA: 00:08:15

################################################################################
                     [1m Learning iteration 1056/1500 [0m                     

                       Computation: 32509 steps/s (collection: 1.055s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0273
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 13.6474
                       Mean reward: 18.09
               Mean episode length: 879.47
Episode_Reward/track_lin_vel_xy_exp: 1.0313
Episode_Reward/track_ang_vel_z_exp: 0.4778
       Episode_Reward/lin_vel_z_l2: -0.0448
      Episode_Reward/ang_vel_xy_l2: -0.1289
     Episode_Reward/dof_torques_l2: -0.0854
         Episode_Reward/dof_acc_l2: -0.1957
     Episode_Reward/action_rate_l2: -0.1762
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9520
Metrics/base_velocity/error_vel_xy: 0.4389
Metrics/base_velocity/error_vel_yaw: 0.5015
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 38052000
                    Iteration time: 1.11s
                      Time elapsed: 00:19:38
                               ETA: 00:08:14

################################################################################
                     [1m Learning iteration 1057/1500 [0m                     

                       Computation: 33098 steps/s (collection: 1.036s, learning 0.051s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0277
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 13.6462
                       Mean reward: 17.85
               Mean episode length: 871.01
Episode_Reward/track_lin_vel_xy_exp: 1.0640
Episode_Reward/track_ang_vel_z_exp: 0.4749
       Episode_Reward/lin_vel_z_l2: -0.0403
      Episode_Reward/ang_vel_xy_l2: -0.1265
     Episode_Reward/dof_torques_l2: -0.0848
         Episode_Reward/dof_acc_l2: -0.1787
     Episode_Reward/action_rate_l2: -0.1707
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9528
Metrics/base_velocity/error_vel_xy: 0.3742
Metrics/base_velocity/error_vel_yaw: 0.4793
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 38088000
                    Iteration time: 1.09s
                      Time elapsed: 00:19:39
                               ETA: 00:08:13

################################################################################
                     [1m Learning iteration 1058/1500 [0m                     

                       Computation: 31051 steps/s (collection: 1.107s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0226
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 13.6373
                       Mean reward: 17.61
               Mean episode length: 850.98
Episode_Reward/track_lin_vel_xy_exp: 1.0386
Episode_Reward/track_ang_vel_z_exp: 0.4709
       Episode_Reward/lin_vel_z_l2: -0.0415
      Episode_Reward/ang_vel_xy_l2: -0.1228
     Episode_Reward/dof_torques_l2: -0.0845
         Episode_Reward/dof_acc_l2: -0.1759
     Episode_Reward/action_rate_l2: -0.1685
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9488
Metrics/base_velocity/error_vel_xy: 0.4018
Metrics/base_velocity/error_vel_yaw: 0.4678
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 38124000
                    Iteration time: 1.16s
                      Time elapsed: 00:19:40
                               ETA: 00:08:12

################################################################################
                     [1m Learning iteration 1059/1500 [0m                     

                       Computation: 31069 steps/s (collection: 1.106s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0237
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 13.6297
                       Mean reward: 16.99
               Mean episode length: 841.21
Episode_Reward/track_lin_vel_xy_exp: 0.9831
Episode_Reward/track_ang_vel_z_exp: 0.4515
       Episode_Reward/lin_vel_z_l2: -0.0442
      Episode_Reward/ang_vel_xy_l2: -0.1289
     Episode_Reward/dof_torques_l2: -0.0897
         Episode_Reward/dof_acc_l2: -0.1958
     Episode_Reward/action_rate_l2: -0.1749
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9387
Metrics/base_velocity/error_vel_xy: 0.4605
Metrics/base_velocity/error_vel_yaw: 0.5267
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 38160000
                    Iteration time: 1.16s
                      Time elapsed: 00:19:41
                               ETA: 00:08:11

################################################################################
                     [1m Learning iteration 1060/1500 [0m                     

                       Computation: 31981 steps/s (collection: 1.073s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0244
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 13.6257
                       Mean reward: 16.77
               Mean episode length: 844.96
Episode_Reward/track_lin_vel_xy_exp: 1.0109
Episode_Reward/track_ang_vel_z_exp: 0.4584
       Episode_Reward/lin_vel_z_l2: -0.0421
      Episode_Reward/ang_vel_xy_l2: -0.1245
     Episode_Reward/dof_torques_l2: -0.0809
         Episode_Reward/dof_acc_l2: -0.1693
     Episode_Reward/action_rate_l2: -0.1647
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9311
Metrics/base_velocity/error_vel_xy: 0.3981
Metrics/base_velocity/error_vel_yaw: 0.4751
      Episode_Termination/time_out: 1.1667
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 38196000
                    Iteration time: 1.13s
                      Time elapsed: 00:19:42
                               ETA: 00:08:10

################################################################################
                     [1m Learning iteration 1061/1500 [0m                     

                       Computation: 30752 steps/s (collection: 1.117s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0254
               Mean surrogate loss: -0.0149
                 Mean entropy loss: 13.6202
                       Mean reward: 17.54
               Mean episode length: 844.34
Episode_Reward/track_lin_vel_xy_exp: 0.9247
Episode_Reward/track_ang_vel_z_exp: 0.4160
       Episode_Reward/lin_vel_z_l2: -0.0408
      Episode_Reward/ang_vel_xy_l2: -0.1160
     Episode_Reward/dof_torques_l2: -0.0761
         Episode_Reward/dof_acc_l2: -0.1664
     Episode_Reward/action_rate_l2: -0.1538
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9246
Metrics/base_velocity/error_vel_xy: 0.3491
Metrics/base_velocity/error_vel_yaw: 0.4399
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 38232000
                    Iteration time: 1.17s
                      Time elapsed: 00:19:43
                               ETA: 00:08:09

################################################################################
                     [1m Learning iteration 1062/1500 [0m                     

                       Computation: 31377 steps/s (collection: 1.095s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0239
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 13.6256
                       Mean reward: 17.92
               Mean episode length: 860.62
Episode_Reward/track_lin_vel_xy_exp: 1.0806
Episode_Reward/track_ang_vel_z_exp: 0.4873
       Episode_Reward/lin_vel_z_l2: -0.0446
      Episode_Reward/ang_vel_xy_l2: -0.1292
     Episode_Reward/dof_torques_l2: -0.0909
         Episode_Reward/dof_acc_l2: -0.1921
     Episode_Reward/action_rate_l2: -0.1780
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9338
Metrics/base_velocity/error_vel_xy: 0.3979
Metrics/base_velocity/error_vel_yaw: 0.4912
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 38268000
                    Iteration time: 1.15s
                      Time elapsed: 00:19:44
                               ETA: 00:08:08

################################################################################
                     [1m Learning iteration 1063/1500 [0m                     

                       Computation: 31991 steps/s (collection: 1.073s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0225
               Mean surrogate loss: -0.0159
                 Mean entropy loss: 13.6210
                       Mean reward: 18.59
               Mean episode length: 890.94
Episode_Reward/track_lin_vel_xy_exp: 1.0137
Episode_Reward/track_ang_vel_z_exp: 0.4695
       Episode_Reward/lin_vel_z_l2: -0.0427
      Episode_Reward/ang_vel_xy_l2: -0.1257
     Episode_Reward/dof_torques_l2: -0.0906
         Episode_Reward/dof_acc_l2: -0.1842
     Episode_Reward/action_rate_l2: -0.1731
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9424
Metrics/base_velocity/error_vel_xy: 0.4287
Metrics/base_velocity/error_vel_yaw: 0.4925
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 38304000
                    Iteration time: 1.13s
                      Time elapsed: 00:19:46
                               ETA: 00:08:07

################################################################################
                     [1m Learning iteration 1064/1500 [0m                     

                       Computation: 30767 steps/s (collection: 1.118s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0225
               Mean surrogate loss: -0.0150
                 Mean entropy loss: 13.6152
                       Mean reward: 18.10
               Mean episode length: 884.25
Episode_Reward/track_lin_vel_xy_exp: 1.0805
Episode_Reward/track_ang_vel_z_exp: 0.4971
       Episode_Reward/lin_vel_z_l2: -0.0458
      Episode_Reward/ang_vel_xy_l2: -0.1353
     Episode_Reward/dof_torques_l2: -0.0942
         Episode_Reward/dof_acc_l2: -0.1983
     Episode_Reward/action_rate_l2: -0.1830
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9541
Metrics/base_velocity/error_vel_xy: 0.4698
Metrics/base_velocity/error_vel_yaw: 0.5291
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 38340000
                    Iteration time: 1.17s
                      Time elapsed: 00:19:47
                               ETA: 00:08:06

################################################################################
                     [1m Learning iteration 1065/1500 [0m                     

                       Computation: 33940 steps/s (collection: 1.007s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0244
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 13.6122
                       Mean reward: 18.32
               Mean episode length: 886.98
Episode_Reward/track_lin_vel_xy_exp: 1.0463
Episode_Reward/track_ang_vel_z_exp: 0.4699
       Episode_Reward/lin_vel_z_l2: -0.0483
      Episode_Reward/ang_vel_xy_l2: -0.1273
     Episode_Reward/dof_torques_l2: -0.0851
         Episode_Reward/dof_acc_l2: -0.1807
     Episode_Reward/action_rate_l2: -0.1676
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9518
Metrics/base_velocity/error_vel_xy: 0.3756
Metrics/base_velocity/error_vel_yaw: 0.4704
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 38376000
                    Iteration time: 1.06s
                      Time elapsed: 00:19:48
                               ETA: 00:08:04

################################################################################
                     [1m Learning iteration 1066/1500 [0m                     

                       Computation: 33043 steps/s (collection: 1.037s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0230
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 13.6086
                       Mean reward: 17.44
               Mean episode length: 835.41
Episode_Reward/track_lin_vel_xy_exp: 0.9582
Episode_Reward/track_ang_vel_z_exp: 0.4347
       Episode_Reward/lin_vel_z_l2: -0.0450
      Episode_Reward/ang_vel_xy_l2: -0.1156
     Episode_Reward/dof_torques_l2: -0.0772
         Episode_Reward/dof_acc_l2: -0.1662
     Episode_Reward/action_rate_l2: -0.1566
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9422
Metrics/base_velocity/error_vel_xy: 0.3739
Metrics/base_velocity/error_vel_yaw: 0.4496
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 38412000
                    Iteration time: 1.09s
                      Time elapsed: 00:19:49
                               ETA: 00:08:03

################################################################################
                     [1m Learning iteration 1067/1500 [0m                     

                       Computation: 31033 steps/s (collection: 1.108s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0224
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 13.6091
                       Mean reward: 17.53
               Mean episode length: 866.79
Episode_Reward/track_lin_vel_xy_exp: 1.0019
Episode_Reward/track_ang_vel_z_exp: 0.4591
       Episode_Reward/lin_vel_z_l2: -0.0440
      Episode_Reward/ang_vel_xy_l2: -0.1270
     Episode_Reward/dof_torques_l2: -0.0857
         Episode_Reward/dof_acc_l2: -0.1844
     Episode_Reward/action_rate_l2: -0.1675
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9418
Metrics/base_velocity/error_vel_xy: 0.4195
Metrics/base_velocity/error_vel_yaw: 0.4928
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 38448000
                    Iteration time: 1.16s
                      Time elapsed: 00:19:50
                               ETA: 00:08:02

################################################################################
                     [1m Learning iteration 1068/1500 [0m                     

                       Computation: 30752 steps/s (collection: 1.117s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0256
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 13.6110
                       Mean reward: 18.92
               Mean episode length: 931.17
Episode_Reward/track_lin_vel_xy_exp: 1.1406
Episode_Reward/track_ang_vel_z_exp: 0.5116
       Episode_Reward/lin_vel_z_l2: -0.0448
      Episode_Reward/ang_vel_xy_l2: -0.1348
     Episode_Reward/dof_torques_l2: -0.0916
         Episode_Reward/dof_acc_l2: -0.2045
     Episode_Reward/action_rate_l2: -0.1847
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9549
Metrics/base_velocity/error_vel_xy: 0.4023
Metrics/base_velocity/error_vel_yaw: 0.5039
      Episode_Termination/time_out: 2.2083
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 38484000
                    Iteration time: 1.17s
                      Time elapsed: 00:19:51
                               ETA: 00:08:01

################################################################################
                     [1m Learning iteration 1069/1500 [0m                     

                       Computation: 30111 steps/s (collection: 1.144s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0270
               Mean surrogate loss: -0.0151
                 Mean entropy loss: 13.6105
                       Mean reward: 18.65
               Mean episode length: 922.70
Episode_Reward/track_lin_vel_xy_exp: 1.1139
Episode_Reward/track_ang_vel_z_exp: 0.5072
       Episode_Reward/lin_vel_z_l2: -0.0467
      Episode_Reward/ang_vel_xy_l2: -0.1335
     Episode_Reward/dof_torques_l2: -0.0941
         Episode_Reward/dof_acc_l2: -0.1952
     Episode_Reward/action_rate_l2: -0.1826
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9706
Metrics/base_velocity/error_vel_xy: 0.4291
Metrics/base_velocity/error_vel_yaw: 0.5034
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 38520000
                    Iteration time: 1.20s
                      Time elapsed: 00:19:52
                               ETA: 00:08:00

################################################################################
                     [1m Learning iteration 1070/1500 [0m                     

                       Computation: 30948 steps/s (collection: 1.109s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0276
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 13.6184
                       Mean reward: 18.46
               Mean episode length: 902.99
Episode_Reward/track_lin_vel_xy_exp: 1.0942
Episode_Reward/track_ang_vel_z_exp: 0.4965
       Episode_Reward/lin_vel_z_l2: -0.0486
      Episode_Reward/ang_vel_xy_l2: -0.1359
     Episode_Reward/dof_torques_l2: -0.0919
         Episode_Reward/dof_acc_l2: -0.2032
     Episode_Reward/action_rate_l2: -0.1829
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9788
Metrics/base_velocity/error_vel_xy: 0.4268
Metrics/base_velocity/error_vel_yaw: 0.5136
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 38556000
                    Iteration time: 1.16s
                      Time elapsed: 00:19:54
                               ETA: 00:07:59

################################################################################
                     [1m Learning iteration 1071/1500 [0m                     

                       Computation: 32593 steps/s (collection: 1.051s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0227
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 13.6142
                       Mean reward: 17.98
               Mean episode length: 905.24
Episode_Reward/track_lin_vel_xy_exp: 0.9855
Episode_Reward/track_ang_vel_z_exp: 0.4555
       Episode_Reward/lin_vel_z_l2: -0.0496
      Episode_Reward/ang_vel_xy_l2: -0.1339
     Episode_Reward/dof_torques_l2: -0.0849
         Episode_Reward/dof_acc_l2: -0.1998
     Episode_Reward/action_rate_l2: -0.1737
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9897
Metrics/base_velocity/error_vel_xy: 0.4569
Metrics/base_velocity/error_vel_yaw: 0.5247
      Episode_Termination/time_out: 2.0000
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 38592000
                    Iteration time: 1.10s
                      Time elapsed: 00:19:55
                               ETA: 00:07:58

################################################################################
                     [1m Learning iteration 1072/1500 [0m                     

                       Computation: 31935 steps/s (collection: 1.077s, learning 0.050s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0222
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.6103
                       Mean reward: 16.92
               Mean episode length: 848.41
Episode_Reward/track_lin_vel_xy_exp: 0.9574
Episode_Reward/track_ang_vel_z_exp: 0.4218
       Episode_Reward/lin_vel_z_l2: -0.0369
      Episode_Reward/ang_vel_xy_l2: -0.1109
     Episode_Reward/dof_torques_l2: -0.0761
         Episode_Reward/dof_acc_l2: -0.1596
     Episode_Reward/action_rate_l2: -0.1513
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 4.9912
Metrics/base_velocity/error_vel_xy: 0.3145
Metrics/base_velocity/error_vel_yaw: 0.4222
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38628000
                    Iteration time: 1.13s
                      Time elapsed: 00:19:56
                               ETA: 00:07:57

################################################################################
                     [1m Learning iteration 1073/1500 [0m                     

                       Computation: 31552 steps/s (collection: 1.089s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0219
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 13.6039
                       Mean reward: 17.47
               Mean episode length: 855.80
Episode_Reward/track_lin_vel_xy_exp: 1.0675
Episode_Reward/track_ang_vel_z_exp: 0.4918
       Episode_Reward/lin_vel_z_l2: -0.0462
      Episode_Reward/ang_vel_xy_l2: -0.1322
     Episode_Reward/dof_torques_l2: -0.0906
         Episode_Reward/dof_acc_l2: -0.1889
     Episode_Reward/action_rate_l2: -0.1780
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.0006
Metrics/base_velocity/error_vel_xy: 0.4453
Metrics/base_velocity/error_vel_yaw: 0.5098
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 38664000
                    Iteration time: 1.14s
                      Time elapsed: 00:19:57
                               ETA: 00:07:56

################################################################################
                     [1m Learning iteration 1074/1500 [0m                     

                       Computation: 32322 steps/s (collection: 1.062s, learning 0.051s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0228
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 13.5987
                       Mean reward: 18.42
               Mean episode length: 919.21
Episode_Reward/track_lin_vel_xy_exp: 1.0696
Episode_Reward/track_ang_vel_z_exp: 0.4892
       Episode_Reward/lin_vel_z_l2: -0.0460
      Episode_Reward/ang_vel_xy_l2: -0.1331
     Episode_Reward/dof_torques_l2: -0.0942
         Episode_Reward/dof_acc_l2: -0.1962
     Episode_Reward/action_rate_l2: -0.1819
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.0017
Metrics/base_velocity/error_vel_xy: 0.4443
Metrics/base_velocity/error_vel_yaw: 0.5204
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 38700000
                    Iteration time: 1.11s
                      Time elapsed: 00:19:58
                               ETA: 00:07:54

################################################################################
                     [1m Learning iteration 1075/1500 [0m                     

                       Computation: 33293 steps/s (collection: 1.030s, learning 0.051s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0283
               Mean surrogate loss: -0.0151
                 Mean entropy loss: 13.6018
                       Mean reward: 18.07
               Mean episode length: 908.77
Episode_Reward/track_lin_vel_xy_exp: 1.0822
Episode_Reward/track_ang_vel_z_exp: 0.4869
       Episode_Reward/lin_vel_z_l2: -0.0423
      Episode_Reward/ang_vel_xy_l2: -0.1283
     Episode_Reward/dof_torques_l2: -0.0897
         Episode_Reward/dof_acc_l2: -0.1790
     Episode_Reward/action_rate_l2: -0.1734
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.0089
Metrics/base_velocity/error_vel_xy: 0.3771
Metrics/base_velocity/error_vel_yaw: 0.4724
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 38736000
                    Iteration time: 1.08s
                      Time elapsed: 00:19:59
                               ETA: 00:07:53

################################################################################
                     [1m Learning iteration 1076/1500 [0m                     

                       Computation: 31400 steps/s (collection: 1.077s, learning 0.069s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0250
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 13.6111
                       Mean reward: 18.11
               Mean episode length: 894.29
Episode_Reward/track_lin_vel_xy_exp: 1.0114
Episode_Reward/track_ang_vel_z_exp: 0.4625
       Episode_Reward/lin_vel_z_l2: -0.0420
      Episode_Reward/ang_vel_xy_l2: -0.1272
     Episode_Reward/dof_torques_l2: -0.0893
         Episode_Reward/dof_acc_l2: -0.1998
     Episode_Reward/action_rate_l2: -0.1732
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.0150
Metrics/base_velocity/error_vel_xy: 0.4256
Metrics/base_velocity/error_vel_yaw: 0.4939
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 38772000
                    Iteration time: 1.15s
                      Time elapsed: 00:20:00
                               ETA: 00:07:52

################################################################################
                     [1m Learning iteration 1077/1500 [0m                     

                       Computation: 30430 steps/s (collection: 1.129s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0238
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 13.6107
                       Mean reward: 17.63
               Mean episode length: 875.01
Episode_Reward/track_lin_vel_xy_exp: 1.0214
Episode_Reward/track_ang_vel_z_exp: 0.4713
       Episode_Reward/lin_vel_z_l2: -0.0539
      Episode_Reward/ang_vel_xy_l2: -0.1367
     Episode_Reward/dof_torques_l2: -0.0865
         Episode_Reward/dof_acc_l2: -0.1969
     Episode_Reward/action_rate_l2: -0.1746
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.0196
Metrics/base_velocity/error_vel_xy: 0.4470
Metrics/base_velocity/error_vel_yaw: 0.5083
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 38808000
                    Iteration time: 1.18s
                      Time elapsed: 00:20:01
                               ETA: 00:07:51

################################################################################
                     [1m Learning iteration 1078/1500 [0m                     

                       Computation: 30458 steps/s (collection: 1.128s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0217
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 13.6109
                       Mean reward: 16.69
               Mean episode length: 852.17
Episode_Reward/track_lin_vel_xy_exp: 1.0168
Episode_Reward/track_ang_vel_z_exp: 0.4542
       Episode_Reward/lin_vel_z_l2: -0.0490
      Episode_Reward/ang_vel_xy_l2: -0.1254
     Episode_Reward/dof_torques_l2: -0.0827
         Episode_Reward/dof_acc_l2: -0.1908
     Episode_Reward/action_rate_l2: -0.1674
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.0268
Metrics/base_velocity/error_vel_xy: 0.3721
Metrics/base_velocity/error_vel_yaw: 0.4699
      Episode_Termination/time_out: 1.1667
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 38844000
                    Iteration time: 1.18s
                      Time elapsed: 00:20:03
                               ETA: 00:07:50

################################################################################
                     [1m Learning iteration 1079/1500 [0m                     

                       Computation: 30542 steps/s (collection: 1.121s, learning 0.058s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0276
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 13.6156
                       Mean reward: 16.52
               Mean episode length: 838.48
Episode_Reward/track_lin_vel_xy_exp: 0.9517
Episode_Reward/track_ang_vel_z_exp: 0.4415
       Episode_Reward/lin_vel_z_l2: -0.0421
      Episode_Reward/ang_vel_xy_l2: -0.1192
     Episode_Reward/dof_torques_l2: -0.0840
         Episode_Reward/dof_acc_l2: -0.1726
     Episode_Reward/action_rate_l2: -0.1610
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.0240
Metrics/base_velocity/error_vel_xy: 0.4125
Metrics/base_velocity/error_vel_yaw: 0.4589
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 38880000
                    Iteration time: 1.18s
                      Time elapsed: 00:20:04
                               ETA: 00:07:49

################################################################################
                     [1m Learning iteration 1080/1500 [0m                     

                       Computation: 30768 steps/s (collection: 1.118s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0222
               Mean surrogate loss: -0.0151
                 Mean entropy loss: 13.6211
                       Mean reward: 16.83
               Mean episode length: 840.12
Episode_Reward/track_lin_vel_xy_exp: 1.0823
Episode_Reward/track_ang_vel_z_exp: 0.4956
       Episode_Reward/lin_vel_z_l2: -0.0481
      Episode_Reward/ang_vel_xy_l2: -0.1378
     Episode_Reward/dof_torques_l2: -0.0892
         Episode_Reward/dof_acc_l2: -0.2024
     Episode_Reward/action_rate_l2: -0.1789
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.0293
Metrics/base_velocity/error_vel_xy: 0.4319
Metrics/base_velocity/error_vel_yaw: 0.5169
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 38916000
                    Iteration time: 1.17s
                      Time elapsed: 00:20:05
                               ETA: 00:07:48

################################################################################
                     [1m Learning iteration 1081/1500 [0m                     

                       Computation: 31392 steps/s (collection: 1.094s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0276
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 13.6246
                       Mean reward: 16.85
               Mean episode length: 867.20
Episode_Reward/track_lin_vel_xy_exp: 0.9154
Episode_Reward/track_ang_vel_z_exp: 0.4268
       Episode_Reward/lin_vel_z_l2: -0.0511
      Episode_Reward/ang_vel_xy_l2: -0.1273
     Episode_Reward/dof_torques_l2: -0.0793
         Episode_Reward/dof_acc_l2: -0.1928
     Episode_Reward/action_rate_l2: -0.1622
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.0348
Metrics/base_velocity/error_vel_xy: 0.4322
Metrics/base_velocity/error_vel_yaw: 0.4830
      Episode_Termination/time_out: 2.0000
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 38952000
                    Iteration time: 1.15s
                      Time elapsed: 00:20:06
                               ETA: 00:07:47

################################################################################
                     [1m Learning iteration 1082/1500 [0m                     

                       Computation: 31075 steps/s (collection: 1.103s, learning 0.055s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0266
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 13.6314
                       Mean reward: 16.93
               Mean episode length: 870.63
Episode_Reward/track_lin_vel_xy_exp: 1.0983
Episode_Reward/track_ang_vel_z_exp: 0.4933
       Episode_Reward/lin_vel_z_l2: -0.0447
      Episode_Reward/ang_vel_xy_l2: -0.1362
     Episode_Reward/dof_torques_l2: -0.0872
         Episode_Reward/dof_acc_l2: -0.1914
     Episode_Reward/action_rate_l2: -0.1772
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.0474
Metrics/base_velocity/error_vel_xy: 0.3931
Metrics/base_velocity/error_vel_yaw: 0.4909
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 38988000
                    Iteration time: 1.16s
                      Time elapsed: 00:20:07
                               ETA: 00:07:46

################################################################################
                     [1m Learning iteration 1083/1500 [0m                     

                       Computation: 31270 steps/s (collection: 1.095s, learning 0.056s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0255
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 13.6395
                       Mean reward: 18.03
               Mean episode length: 890.77
Episode_Reward/track_lin_vel_xy_exp: 1.0523
Episode_Reward/track_ang_vel_z_exp: 0.4863
       Episode_Reward/lin_vel_z_l2: -0.0439
      Episode_Reward/ang_vel_xy_l2: -0.1349
     Episode_Reward/dof_torques_l2: -0.0885
         Episode_Reward/dof_acc_l2: -0.1985
     Episode_Reward/action_rate_l2: -0.1781
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.0569
Metrics/base_velocity/error_vel_xy: 0.4556
Metrics/base_velocity/error_vel_yaw: 0.5137
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 39024000
                    Iteration time: 1.15s
                      Time elapsed: 00:20:08
                               ETA: 00:07:45

################################################################################
                     [1m Learning iteration 1084/1500 [0m                     

                       Computation: 31165 steps/s (collection: 1.103s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0272
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 13.6520
                       Mean reward: 17.27
               Mean episode length: 874.73
Episode_Reward/track_lin_vel_xy_exp: 1.0412
Episode_Reward/track_ang_vel_z_exp: 0.4781
       Episode_Reward/lin_vel_z_l2: -0.0459
      Episode_Reward/ang_vel_xy_l2: -0.1303
     Episode_Reward/dof_torques_l2: -0.0898
         Episode_Reward/dof_acc_l2: -0.1915
     Episode_Reward/action_rate_l2: -0.1739
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.0590
Metrics/base_velocity/error_vel_xy: 0.4340
Metrics/base_velocity/error_vel_yaw: 0.5030
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 39060000
                    Iteration time: 1.16s
                      Time elapsed: 00:20:10
                               ETA: 00:07:43

################################################################################
                     [1m Learning iteration 1085/1500 [0m                     

                       Computation: 31459 steps/s (collection: 1.088s, learning 0.057s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0230
               Mean surrogate loss: -0.0150
                 Mean entropy loss: 13.6609
                       Mean reward: 16.35
               Mean episode length: 849.11
Episode_Reward/track_lin_vel_xy_exp: 0.8699
Episode_Reward/track_ang_vel_z_exp: 0.4095
       Episode_Reward/lin_vel_z_l2: -0.0435
      Episode_Reward/ang_vel_xy_l2: -0.1183
     Episode_Reward/dof_torques_l2: -0.0751
         Episode_Reward/dof_acc_l2: -0.1732
     Episode_Reward/action_rate_l2: -0.1524
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.0548
Metrics/base_velocity/error_vel_xy: 0.4146
Metrics/base_velocity/error_vel_yaw: 0.4548
      Episode_Termination/time_out: 1.0417
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 39096000
                    Iteration time: 1.14s
                      Time elapsed: 00:20:11
                               ETA: 00:07:42

################################################################################
                     [1m Learning iteration 1086/1500 [0m                     

                       Computation: 31617 steps/s (collection: 1.087s, learning 0.052s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0274
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 13.6598
                       Mean reward: 16.58
               Mean episode length: 850.42
Episode_Reward/track_lin_vel_xy_exp: 1.0172
Episode_Reward/track_ang_vel_z_exp: 0.4651
       Episode_Reward/lin_vel_z_l2: -0.0486
      Episode_Reward/ang_vel_xy_l2: -0.1362
     Episode_Reward/dof_torques_l2: -0.0882
         Episode_Reward/dof_acc_l2: -0.1952
     Episode_Reward/action_rate_l2: -0.1739
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.0609
Metrics/base_velocity/error_vel_xy: 0.4296
Metrics/base_velocity/error_vel_yaw: 0.4925
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 39132000
                    Iteration time: 1.14s
                      Time elapsed: 00:20:12
                               ETA: 00:07:41

################################################################################
                     [1m Learning iteration 1087/1500 [0m                     

                       Computation: 30609 steps/s (collection: 1.123s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0259
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 13.6544
                       Mean reward: 17.19
               Mean episode length: 837.80
Episode_Reward/track_lin_vel_xy_exp: 0.8780
Episode_Reward/track_ang_vel_z_exp: 0.4007
       Episode_Reward/lin_vel_z_l2: -0.0369
      Episode_Reward/ang_vel_xy_l2: -0.1079
     Episode_Reward/dof_torques_l2: -0.0738
         Episode_Reward/dof_acc_l2: -0.1545
     Episode_Reward/action_rate_l2: -0.1460
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.0633
Metrics/base_velocity/error_vel_xy: 0.3500
Metrics/base_velocity/error_vel_yaw: 0.4151
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 39168000
                    Iteration time: 1.18s
                      Time elapsed: 00:20:13
                               ETA: 00:07:40

################################################################################
                     [1m Learning iteration 1088/1500 [0m                     

                       Computation: 31834 steps/s (collection: 1.077s, learning 0.054s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0258
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 13.6613
                       Mean reward: 16.53
               Mean episode length: 824.30
Episode_Reward/track_lin_vel_xy_exp: 1.0354
Episode_Reward/track_ang_vel_z_exp: 0.4785
       Episode_Reward/lin_vel_z_l2: -0.0449
      Episode_Reward/ang_vel_xy_l2: -0.1335
     Episode_Reward/dof_torques_l2: -0.0900
         Episode_Reward/dof_acc_l2: -0.1903
     Episode_Reward/action_rate_l2: -0.1749
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.0621
Metrics/base_velocity/error_vel_xy: 0.4527
Metrics/base_velocity/error_vel_yaw: 0.5094
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 39204000
                    Iteration time: 1.13s
                      Time elapsed: 00:20:14
                               ETA: 00:07:39

################################################################################
                     [1m Learning iteration 1089/1500 [0m                     

                       Computation: 31387 steps/s (collection: 1.093s, learning 0.054s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0211
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.6682
                       Mean reward: 16.09
               Mean episode length: 812.46
Episode_Reward/track_lin_vel_xy_exp: 0.7784
Episode_Reward/track_ang_vel_z_exp: 0.3614
       Episode_Reward/lin_vel_z_l2: -0.0375
      Episode_Reward/ang_vel_xy_l2: -0.1033
     Episode_Reward/dof_torques_l2: -0.0676
         Episode_Reward/dof_acc_l2: -0.1473
     Episode_Reward/action_rate_l2: -0.1349
      Episode_Reward/feet_air_time: -0.0009
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.0609
Metrics/base_velocity/error_vel_xy: 0.3424
Metrics/base_velocity/error_vel_yaw: 0.3910
      Episode_Termination/time_out: 0.9583
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 39240000
                    Iteration time: 1.15s
                      Time elapsed: 00:20:15
                               ETA: 00:07:38

################################################################################
                     [1m Learning iteration 1090/1500 [0m                     

                       Computation: 32053 steps/s (collection: 1.069s, learning 0.054s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0229
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 13.6741
                       Mean reward: 17.85
               Mean episode length: 875.70
Episode_Reward/track_lin_vel_xy_exp: 1.1366
Episode_Reward/track_ang_vel_z_exp: 0.5066
       Episode_Reward/lin_vel_z_l2: -0.0448
      Episode_Reward/ang_vel_xy_l2: -0.1353
     Episode_Reward/dof_torques_l2: -0.0933
         Episode_Reward/dof_acc_l2: -0.1974
     Episode_Reward/action_rate_l2: -0.1829
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.0564
Metrics/base_velocity/error_vel_xy: 0.4014
Metrics/base_velocity/error_vel_yaw: 0.5085
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 39276000
                    Iteration time: 1.12s
                      Time elapsed: 00:20:16
                               ETA: 00:07:37

################################################################################
                     [1m Learning iteration 1091/1500 [0m                     

                       Computation: 31239 steps/s (collection: 1.100s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0240
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 13.6858
                       Mean reward: 18.92
               Mean episode length: 909.30
Episode_Reward/track_lin_vel_xy_exp: 1.1194
Episode_Reward/track_ang_vel_z_exp: 0.5054
       Episode_Reward/lin_vel_z_l2: -0.0478
      Episode_Reward/ang_vel_xy_l2: -0.1382
     Episode_Reward/dof_torques_l2: -0.0889
         Episode_Reward/dof_acc_l2: -0.2004
     Episode_Reward/action_rate_l2: -0.1837
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.0470
Metrics/base_velocity/error_vel_xy: 0.4156
Metrics/base_velocity/error_vel_yaw: 0.5052
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 39312000
                    Iteration time: 1.15s
                      Time elapsed: 00:20:18
                               ETA: 00:07:36

################################################################################
                     [1m Learning iteration 1092/1500 [0m                     

                       Computation: 31546 steps/s (collection: 1.088s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0220
               Mean surrogate loss: -0.0149
                 Mean entropy loss: 13.6940
                       Mean reward: 19.16
               Mean episode length: 913.31
Episode_Reward/track_lin_vel_xy_exp: 1.0284
Episode_Reward/track_ang_vel_z_exp: 0.4509
       Episode_Reward/lin_vel_z_l2: -0.0368
      Episode_Reward/ang_vel_xy_l2: -0.1170
     Episode_Reward/dof_torques_l2: -0.0810
         Episode_Reward/dof_acc_l2: -0.1594
     Episode_Reward/action_rate_l2: -0.1586
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.0428
Metrics/base_velocity/error_vel_xy: 0.3296
Metrics/base_velocity/error_vel_yaw: 0.4479
      Episode_Termination/time_out: 1.0833
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 39348000
                    Iteration time: 1.14s
                      Time elapsed: 00:20:19
                               ETA: 00:07:35

################################################################################
                     [1m Learning iteration 1093/1500 [0m                     

                       Computation: 31628 steps/s (collection: 1.084s, learning 0.054s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0244
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 13.6903
                       Mean reward: 18.16
               Mean episode length: 882.26
Episode_Reward/track_lin_vel_xy_exp: 0.9868
Episode_Reward/track_ang_vel_z_exp: 0.4525
       Episode_Reward/lin_vel_z_l2: -0.0470
      Episode_Reward/ang_vel_xy_l2: -0.1279
     Episode_Reward/dof_torques_l2: -0.0827
         Episode_Reward/dof_acc_l2: -0.1856
     Episode_Reward/action_rate_l2: -0.1686
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.0461
Metrics/base_velocity/error_vel_xy: 0.4204
Metrics/base_velocity/error_vel_yaw: 0.4920
      Episode_Termination/time_out: 1.0833
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 39384000
                    Iteration time: 1.14s
                      Time elapsed: 00:20:20
                               ETA: 00:07:34

################################################################################
                     [1m Learning iteration 1094/1500 [0m                     

                       Computation: 32338 steps/s (collection: 1.059s, learning 0.054s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0242
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 13.6846
                       Mean reward: 16.73
               Mean episode length: 846.52
Episode_Reward/track_lin_vel_xy_exp: 0.7930
Episode_Reward/track_ang_vel_z_exp: 0.3885
       Episode_Reward/lin_vel_z_l2: -0.0474
      Episode_Reward/ang_vel_xy_l2: -0.1160
     Episode_Reward/dof_torques_l2: -0.0782
         Episode_Reward/dof_acc_l2: -0.1662
     Episode_Reward/action_rate_l2: -0.1470
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.0416
Metrics/base_velocity/error_vel_xy: 0.4808
Metrics/base_velocity/error_vel_yaw: 0.4970
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 39420000
                    Iteration time: 1.11s
                      Time elapsed: 00:20:21
                               ETA: 00:07:32

################################################################################
                     [1m Learning iteration 1095/1500 [0m                     

                       Computation: 31616 steps/s (collection: 1.085s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0221
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 13.6796
                       Mean reward: 16.23
               Mean episode length: 847.58
Episode_Reward/track_lin_vel_xy_exp: 0.9829
Episode_Reward/track_ang_vel_z_exp: 0.4537
       Episode_Reward/lin_vel_z_l2: -0.0466
      Episode_Reward/ang_vel_xy_l2: -0.1305
     Episode_Reward/dof_torques_l2: -0.0858
         Episode_Reward/dof_acc_l2: -0.1819
     Episode_Reward/action_rate_l2: -0.1682
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.0422
Metrics/base_velocity/error_vel_xy: 0.4307
Metrics/base_velocity/error_vel_yaw: 0.4946
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 39456000
                    Iteration time: 1.14s
                      Time elapsed: 00:20:22
                               ETA: 00:07:31

################################################################################
                     [1m Learning iteration 1096/1500 [0m                     

                       Computation: 30985 steps/s (collection: 1.108s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0264
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 13.6799
                       Mean reward: 17.69
               Mean episode length: 882.34
Episode_Reward/track_lin_vel_xy_exp: 1.1765
Episode_Reward/track_ang_vel_z_exp: 0.5241
       Episode_Reward/lin_vel_z_l2: -0.0427
      Episode_Reward/ang_vel_xy_l2: -0.1376
     Episode_Reward/dof_torques_l2: -0.0947
         Episode_Reward/dof_acc_l2: -0.1995
     Episode_Reward/action_rate_l2: -0.1867
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.0539
Metrics/base_velocity/error_vel_xy: 0.3954
Metrics/base_velocity/error_vel_yaw: 0.5072
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 39492000
                    Iteration time: 1.16s
                      Time elapsed: 00:20:23
                               ETA: 00:07:30

################################################################################
                     [1m Learning iteration 1097/1500 [0m                     

                       Computation: 31030 steps/s (collection: 1.107s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0217
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 13.6804
                       Mean reward: 17.54
               Mean episode length: 861.47
Episode_Reward/track_lin_vel_xy_exp: 0.8983
Episode_Reward/track_ang_vel_z_exp: 0.4185
       Episode_Reward/lin_vel_z_l2: -0.0453
      Episode_Reward/ang_vel_xy_l2: -0.1244
     Episode_Reward/dof_torques_l2: -0.0765
         Episode_Reward/dof_acc_l2: -0.1737
     Episode_Reward/action_rate_l2: -0.1545
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.0597
Metrics/base_velocity/error_vel_xy: 0.3953
Metrics/base_velocity/error_vel_yaw: 0.4441
      Episode_Termination/time_out: 1.0833
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 39528000
                    Iteration time: 1.16s
                      Time elapsed: 00:20:24
                               ETA: 00:07:29

################################################################################
                     [1m Learning iteration 1098/1500 [0m                     

                       Computation: 31001 steps/s (collection: 1.107s, learning 0.054s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0252
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 13.6877
                       Mean reward: 17.60
               Mean episode length: 849.53
Episode_Reward/track_lin_vel_xy_exp: 1.0723
Episode_Reward/track_ang_vel_z_exp: 0.4795
       Episode_Reward/lin_vel_z_l2: -0.0425
      Episode_Reward/ang_vel_xy_l2: -0.1327
     Episode_Reward/dof_torques_l2: -0.0817
         Episode_Reward/dof_acc_l2: -0.1793
     Episode_Reward/action_rate_l2: -0.1709
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.0606
Metrics/base_velocity/error_vel_xy: 0.3754
Metrics/base_velocity/error_vel_yaw: 0.4747
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 39564000
                    Iteration time: 1.16s
                      Time elapsed: 00:20:26
                               ETA: 00:07:28

################################################################################
                     [1m Learning iteration 1099/1500 [0m                     

                       Computation: 30724 steps/s (collection: 1.119s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0235
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 13.6859
                       Mean reward: 16.64
               Mean episode length: 849.00
Episode_Reward/track_lin_vel_xy_exp: 0.9695
Episode_Reward/track_ang_vel_z_exp: 0.4831
       Episode_Reward/lin_vel_z_l2: -0.0524
      Episode_Reward/ang_vel_xy_l2: -0.1391
     Episode_Reward/dof_torques_l2: -0.0965
         Episode_Reward/dof_acc_l2: -0.1893
     Episode_Reward/action_rate_l2: -0.1776
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.0607
Metrics/base_velocity/error_vel_xy: 0.5638
Metrics/base_velocity/error_vel_yaw: 0.5382
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 39600000
                    Iteration time: 1.17s
                      Time elapsed: 00:20:27
                               ETA: 00:07:27

################################################################################
                     [1m Learning iteration 1100/1500 [0m                     

                       Computation: 31288 steps/s (collection: 1.099s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0222
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 13.6808
                       Mean reward: 17.77
               Mean episode length: 898.71
Episode_Reward/track_lin_vel_xy_exp: 1.0533
Episode_Reward/track_ang_vel_z_exp: 0.4775
       Episode_Reward/lin_vel_z_l2: -0.0431
      Episode_Reward/ang_vel_xy_l2: -0.1350
     Episode_Reward/dof_torques_l2: -0.0866
         Episode_Reward/dof_acc_l2: -0.1953
     Episode_Reward/action_rate_l2: -0.1762
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.0700
Metrics/base_velocity/error_vel_xy: 0.4171
Metrics/base_velocity/error_vel_yaw: 0.4889
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 39636000
                    Iteration time: 1.15s
                      Time elapsed: 00:20:28
                               ETA: 00:07:26

################################################################################
                     [1m Learning iteration 1101/1500 [0m                     

                       Computation: 31636 steps/s (collection: 1.086s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0275
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 13.6835
                       Mean reward: 17.96
               Mean episode length: 911.74
Episode_Reward/track_lin_vel_xy_exp: 1.0921
Episode_Reward/track_ang_vel_z_exp: 0.5062
       Episode_Reward/lin_vel_z_l2: -0.0463
      Episode_Reward/ang_vel_xy_l2: -0.1437
     Episode_Reward/dof_torques_l2: -0.0925
         Episode_Reward/dof_acc_l2: -0.2140
     Episode_Reward/action_rate_l2: -0.1867
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.0794
Metrics/base_velocity/error_vel_xy: 0.4696
Metrics/base_velocity/error_vel_yaw: 0.5229
      Episode_Termination/time_out: 1.9167
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 39672000
                    Iteration time: 1.14s
                      Time elapsed: 00:20:29
                               ETA: 00:07:25

################################################################################
                     [1m Learning iteration 1102/1500 [0m                     

                       Computation: 32460 steps/s (collection: 1.056s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0253
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 13.6923
                       Mean reward: 16.91
               Mean episode length: 841.49
Episode_Reward/track_lin_vel_xy_exp: 0.9462
Episode_Reward/track_ang_vel_z_exp: 0.4257
       Episode_Reward/lin_vel_z_l2: -0.0377
      Episode_Reward/ang_vel_xy_l2: -0.1180
     Episode_Reward/dof_torques_l2: -0.0772
         Episode_Reward/dof_acc_l2: -0.1701
     Episode_Reward/action_rate_l2: -0.1551
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.0880
Metrics/base_velocity/error_vel_xy: 0.3551
Metrics/base_velocity/error_vel_yaw: 0.4424
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 39708000
                    Iteration time: 1.11s
                      Time elapsed: 00:20:30
                               ETA: 00:07:24

################################################################################
                     [1m Learning iteration 1103/1500 [0m                     

                       Computation: 32556 steps/s (collection: 1.056s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0246
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.6948
                       Mean reward: 17.35
               Mean episode length: 853.03
Episode_Reward/track_lin_vel_xy_exp: 1.1843
Episode_Reward/track_ang_vel_z_exp: 0.5255
       Episode_Reward/lin_vel_z_l2: -0.0460
      Episode_Reward/ang_vel_xy_l2: -0.1388
     Episode_Reward/dof_torques_l2: -0.0962
         Episode_Reward/dof_acc_l2: -0.2081
     Episode_Reward/action_rate_l2: -0.1886
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.0943
Metrics/base_velocity/error_vel_xy: 0.3926
Metrics/base_velocity/error_vel_yaw: 0.5094
      Episode_Termination/time_out: 2.2500
  Episode_Termination/base_contact: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 39744000
                    Iteration time: 1.11s
                      Time elapsed: 00:20:31
                               ETA: 00:07:22

################################################################################
                     [1m Learning iteration 1104/1500 [0m                     

                       Computation: 31983 steps/s (collection: 1.072s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0244
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 13.6942
                       Mean reward: 18.64
               Mean episode length: 909.01
Episode_Reward/track_lin_vel_xy_exp: 1.1185
Episode_Reward/track_ang_vel_z_exp: 0.5067
       Episode_Reward/lin_vel_z_l2: -0.0459
      Episode_Reward/ang_vel_xy_l2: -0.1408
     Episode_Reward/dof_torques_l2: -0.0910
         Episode_Reward/dof_acc_l2: -0.1940
     Episode_Reward/action_rate_l2: -0.1822
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.0949
Metrics/base_velocity/error_vel_xy: 0.4320
Metrics/base_velocity/error_vel_yaw: 0.5201
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 39780000
                    Iteration time: 1.13s
                      Time elapsed: 00:20:32
                               ETA: 00:07:21

################################################################################
                     [1m Learning iteration 1105/1500 [0m                     

                       Computation: 31669 steps/s (collection: 1.083s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0283
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 13.6863
                       Mean reward: 18.47
               Mean episode length: 918.13
Episode_Reward/track_lin_vel_xy_exp: 1.0331
Episode_Reward/track_ang_vel_z_exp: 0.4721
       Episode_Reward/lin_vel_z_l2: -0.0455
      Episode_Reward/ang_vel_xy_l2: -0.1312
     Episode_Reward/dof_torques_l2: -0.0879
         Episode_Reward/dof_acc_l2: -0.1892
     Episode_Reward/action_rate_l2: -0.1726
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.0908
Metrics/base_velocity/error_vel_xy: 0.4211
Metrics/base_velocity/error_vel_yaw: 0.4937
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 39816000
                    Iteration time: 1.14s
                      Time elapsed: 00:20:34
                               ETA: 00:07:20

################################################################################
                     [1m Learning iteration 1106/1500 [0m                     

                       Computation: 31353 steps/s (collection: 1.095s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0268
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 13.6818
                       Mean reward: 17.50
               Mean episode length: 893.20
Episode_Reward/track_lin_vel_xy_exp: 1.0804
Episode_Reward/track_ang_vel_z_exp: 0.4961
       Episode_Reward/lin_vel_z_l2: -0.0439
      Episode_Reward/ang_vel_xy_l2: -0.1334
     Episode_Reward/dof_torques_l2: -0.0916
         Episode_Reward/dof_acc_l2: -0.1901
     Episode_Reward/action_rate_l2: -0.1780
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.0964
Metrics/base_velocity/error_vel_xy: 0.4439
Metrics/base_velocity/error_vel_yaw: 0.5238
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 39852000
                    Iteration time: 1.15s
                      Time elapsed: 00:20:35
                               ETA: 00:07:19

################################################################################
                     [1m Learning iteration 1107/1500 [0m                     

                       Computation: 29431 steps/s (collection: 1.171s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0225
               Mean surrogate loss: -0.0151
                 Mean entropy loss: 13.6894
                       Mean reward: 17.49
               Mean episode length: 889.34
Episode_Reward/track_lin_vel_xy_exp: 1.1063
Episode_Reward/track_ang_vel_z_exp: 0.4998
       Episode_Reward/lin_vel_z_l2: -0.0423
      Episode_Reward/ang_vel_xy_l2: -0.1319
     Episode_Reward/dof_torques_l2: -0.0955
         Episode_Reward/dof_acc_l2: -0.1954
     Episode_Reward/action_rate_l2: -0.1831
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.1033
Metrics/base_velocity/error_vel_xy: 0.4156
Metrics/base_velocity/error_vel_yaw: 0.5017
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 39888000
                    Iteration time: 1.22s
                      Time elapsed: 00:20:36
                               ETA: 00:07:18

################################################################################
                     [1m Learning iteration 1108/1500 [0m                     

                       Computation: 30760 steps/s (collection: 1.116s, learning 0.054s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0243
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 13.6873
                       Mean reward: 17.81
               Mean episode length: 877.27
Episode_Reward/track_lin_vel_xy_exp: 0.9746
Episode_Reward/track_ang_vel_z_exp: 0.4401
       Episode_Reward/lin_vel_z_l2: -0.0380
      Episode_Reward/ang_vel_xy_l2: -0.1204
     Episode_Reward/dof_torques_l2: -0.0762
         Episode_Reward/dof_acc_l2: -0.1705
     Episode_Reward/action_rate_l2: -0.1567
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.1074
Metrics/base_velocity/error_vel_xy: 0.3523
Metrics/base_velocity/error_vel_yaw: 0.4360
      Episode_Termination/time_out: 1.1667
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 39924000
                    Iteration time: 1.17s
                      Time elapsed: 00:20:37
                               ETA: 00:07:17

################################################################################
                     [1m Learning iteration 1109/1500 [0m                     

                       Computation: 30902 steps/s (collection: 1.113s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0266
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 13.6804
                       Mean reward: 16.74
               Mean episode length: 830.30
Episode_Reward/track_lin_vel_xy_exp: 0.9679
Episode_Reward/track_ang_vel_z_exp: 0.4396
       Episode_Reward/lin_vel_z_l2: -0.0437
      Episode_Reward/ang_vel_xy_l2: -0.1259
     Episode_Reward/dof_torques_l2: -0.0785
         Episode_Reward/dof_acc_l2: -0.1762
     Episode_Reward/action_rate_l2: -0.1626
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.1121
Metrics/base_velocity/error_vel_xy: 0.3962
Metrics/base_velocity/error_vel_yaw: 0.4725
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 39960000
                    Iteration time: 1.16s
                      Time elapsed: 00:20:38
                               ETA: 00:07:16

################################################################################
                     [1m Learning iteration 1110/1500 [0m                     

                       Computation: 31957 steps/s (collection: 1.071s, learning 0.056s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0244
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 13.6837
                       Mean reward: 17.13
               Mean episode length: 819.72
Episode_Reward/track_lin_vel_xy_exp: 1.0268
Episode_Reward/track_ang_vel_z_exp: 0.4538
       Episode_Reward/lin_vel_z_l2: -0.0388
      Episode_Reward/ang_vel_xy_l2: -0.1208
     Episode_Reward/dof_torques_l2: -0.0788
         Episode_Reward/dof_acc_l2: -0.1590
     Episode_Reward/action_rate_l2: -0.1574
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.1231
Metrics/base_velocity/error_vel_xy: 0.3298
Metrics/base_velocity/error_vel_yaw: 0.4359
      Episode_Termination/time_out: 1.9583
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 39996000
                    Iteration time: 1.13s
                      Time elapsed: 00:20:39
                               ETA: 00:07:15

################################################################################
                     [1m Learning iteration 1111/1500 [0m                     

                       Computation: 33506 steps/s (collection: 1.020s, learning 0.054s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0254
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 13.6819
                       Mean reward: 18.10
               Mean episode length: 841.32
Episode_Reward/track_lin_vel_xy_exp: 1.0351
Episode_Reward/track_ang_vel_z_exp: 0.4839
       Episode_Reward/lin_vel_z_l2: -0.0424
      Episode_Reward/ang_vel_xy_l2: -0.1302
     Episode_Reward/dof_torques_l2: -0.0880
         Episode_Reward/dof_acc_l2: -0.1862
     Episode_Reward/action_rate_l2: -0.1743
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.1198
Metrics/base_velocity/error_vel_xy: 0.4412
Metrics/base_velocity/error_vel_yaw: 0.4875
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 40032000
                    Iteration time: 1.07s
                      Time elapsed: 00:20:40
                               ETA: 00:07:14

################################################################################
                     [1m Learning iteration 1112/1500 [0m                     

                       Computation: 32925 steps/s (collection: 1.040s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0246
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 13.6763
                       Mean reward: 17.35
               Mean episode length: 866.27
Episode_Reward/track_lin_vel_xy_exp: 0.9860
Episode_Reward/track_ang_vel_z_exp: 0.4738
       Episode_Reward/lin_vel_z_l2: -0.0505
      Episode_Reward/ang_vel_xy_l2: -0.1381
     Episode_Reward/dof_torques_l2: -0.0917
         Episode_Reward/dof_acc_l2: -0.2024
     Episode_Reward/action_rate_l2: -0.1771
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.1251
Metrics/base_velocity/error_vel_xy: 0.5002
Metrics/base_velocity/error_vel_yaw: 0.5272
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 40068000
                    Iteration time: 1.09s
                      Time elapsed: 00:20:42
                               ETA: 00:07:12

################################################################################
                     [1m Learning iteration 1113/1500 [0m                     

                       Computation: 32600 steps/s (collection: 1.051s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0238
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 13.6664
                       Mean reward: 16.58
               Mean episode length: 876.36
Episode_Reward/track_lin_vel_xy_exp: 1.0107
Episode_Reward/track_ang_vel_z_exp: 0.4746
       Episode_Reward/lin_vel_z_l2: -0.0494
      Episode_Reward/ang_vel_xy_l2: -0.1356
     Episode_Reward/dof_torques_l2: -0.0879
         Episode_Reward/dof_acc_l2: -0.2002
     Episode_Reward/action_rate_l2: -0.1790
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.1371
Metrics/base_velocity/error_vel_xy: 0.4623
Metrics/base_velocity/error_vel_yaw: 0.5008
      Episode_Termination/time_out: 1.9167
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 40104000
                    Iteration time: 1.10s
                      Time elapsed: 00:20:43
                               ETA: 00:07:11

################################################################################
                     [1m Learning iteration 1114/1500 [0m                     

                       Computation: 32212 steps/s (collection: 1.061s, learning 0.057s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0248
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 13.6604
                       Mean reward: 17.47
               Mean episode length: 893.06
Episode_Reward/track_lin_vel_xy_exp: 1.0138
Episode_Reward/track_ang_vel_z_exp: 0.4586
       Episode_Reward/lin_vel_z_l2: -0.0473
      Episode_Reward/ang_vel_xy_l2: -0.1321
     Episode_Reward/dof_torques_l2: -0.0812
         Episode_Reward/dof_acc_l2: -0.1905
     Episode_Reward/action_rate_l2: -0.1694
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.1458
Metrics/base_velocity/error_vel_xy: 0.3954
Metrics/base_velocity/error_vel_yaw: 0.4774
      Episode_Termination/time_out: 1.9583
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 40140000
                    Iteration time: 1.12s
                      Time elapsed: 00:20:44
                               ETA: 00:07:10

################################################################################
                     [1m Learning iteration 1115/1500 [0m                     

                       Computation: 31575 steps/s (collection: 1.087s, learning 0.053s)
             Mean action noise std: 0.76
          Mean value_function loss: 0.0262
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 13.6607
                       Mean reward: 19.23
               Mean episode length: 913.08
Episode_Reward/track_lin_vel_xy_exp: 1.1169
Episode_Reward/track_ang_vel_z_exp: 0.4940
       Episode_Reward/lin_vel_z_l2: -0.0426
      Episode_Reward/ang_vel_xy_l2: -0.1325
     Episode_Reward/dof_torques_l2: -0.0901
         Episode_Reward/dof_acc_l2: -0.1849
     Episode_Reward/action_rate_l2: -0.1772
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.1514
Metrics/base_velocity/error_vel_xy: 0.3776
Metrics/base_velocity/error_vel_yaw: 0.4963
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 40176000
                    Iteration time: 1.14s
                      Time elapsed: 00:20:45
                               ETA: 00:07:09

################################################################################
                     [1m Learning iteration 1116/1500 [0m                     

                       Computation: 33909 steps/s (collection: 1.011s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0253
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 13.6682
                       Mean reward: 19.48
               Mean episode length: 918.48
Episode_Reward/track_lin_vel_xy_exp: 1.1604
Episode_Reward/track_ang_vel_z_exp: 0.5301
       Episode_Reward/lin_vel_z_l2: -0.0447
      Episode_Reward/ang_vel_xy_l2: -0.1428
     Episode_Reward/dof_torques_l2: -0.1017
         Episode_Reward/dof_acc_l2: -0.2056
     Episode_Reward/action_rate_l2: -0.1941
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.1548
Metrics/base_velocity/error_vel_xy: 0.4607
Metrics/base_velocity/error_vel_yaw: 0.5514
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 40212000
                    Iteration time: 1.06s
                      Time elapsed: 00:20:46
                               ETA: 00:07:08

################################################################################
                     [1m Learning iteration 1117/1500 [0m                     

                       Computation: 32465 steps/s (collection: 1.058s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0235
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 13.6787
                       Mean reward: 18.57
               Mean episode length: 872.62
Episode_Reward/track_lin_vel_xy_exp: 0.8755
Episode_Reward/track_ang_vel_z_exp: 0.3903
       Episode_Reward/lin_vel_z_l2: -0.0364
      Episode_Reward/ang_vel_xy_l2: -0.1054
     Episode_Reward/dof_torques_l2: -0.0746
         Episode_Reward/dof_acc_l2: -0.1476
     Episode_Reward/action_rate_l2: -0.1427
      Episode_Reward/feet_air_time: -0.0009
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.1484
Metrics/base_velocity/error_vel_xy: 0.3101
Metrics/base_velocity/error_vel_yaw: 0.3973
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 40248000
                    Iteration time: 1.11s
                      Time elapsed: 00:20:47
                               ETA: 00:07:07

################################################################################
                     [1m Learning iteration 1118/1500 [0m                     

                       Computation: 33926 steps/s (collection: 1.011s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0263
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 13.6851
                       Mean reward: 16.94
               Mean episode length: 821.97
Episode_Reward/track_lin_vel_xy_exp: 0.9463
Episode_Reward/track_ang_vel_z_exp: 0.4307
       Episode_Reward/lin_vel_z_l2: -0.0374
      Episode_Reward/ang_vel_xy_l2: -0.1169
     Episode_Reward/dof_torques_l2: -0.0811
         Episode_Reward/dof_acc_l2: -0.1690
     Episode_Reward/action_rate_l2: -0.1571
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.1474
Metrics/base_velocity/error_vel_xy: 0.3675
Metrics/base_velocity/error_vel_yaw: 0.4416
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 40284000
                    Iteration time: 1.06s
                      Time elapsed: 00:20:48
                               ETA: 00:07:06

################################################################################
                     [1m Learning iteration 1119/1500 [0m                     

                       Computation: 34166 steps/s (collection: 1.001s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0232
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 13.6860
                       Mean reward: 16.80
               Mean episode length: 815.04
Episode_Reward/track_lin_vel_xy_exp: 0.9896
Episode_Reward/track_ang_vel_z_exp: 0.4297
       Episode_Reward/lin_vel_z_l2: -0.0384
      Episode_Reward/ang_vel_xy_l2: -0.1185
     Episode_Reward/dof_torques_l2: -0.0786
         Episode_Reward/dof_acc_l2: -0.1738
     Episode_Reward/action_rate_l2: -0.1588
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.1509
Metrics/base_velocity/error_vel_xy: 0.3056
Metrics/base_velocity/error_vel_yaw: 0.4324
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 40320000
                    Iteration time: 1.05s
                      Time elapsed: 00:20:49
                               ETA: 00:07:05

################################################################################
                     [1m Learning iteration 1120/1500 [0m                     

                       Computation: 33089 steps/s (collection: 1.035s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0282
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 13.6815
                       Mean reward: 15.37
               Mean episode length: 818.41
Episode_Reward/track_lin_vel_xy_exp: 0.9028
Episode_Reward/track_ang_vel_z_exp: 0.4392
       Episode_Reward/lin_vel_z_l2: -0.0460
      Episode_Reward/ang_vel_xy_l2: -0.1290
     Episode_Reward/dof_torques_l2: -0.0859
         Episode_Reward/dof_acc_l2: -0.1901
     Episode_Reward/action_rate_l2: -0.1677
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.1463
Metrics/base_velocity/error_vel_xy: 0.5014
Metrics/base_velocity/error_vel_yaw: 0.5073
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 40356000
                    Iteration time: 1.09s
                      Time elapsed: 00:20:50
                               ETA: 00:07:04

################################################################################
                     [1m Learning iteration 1121/1500 [0m                     

                       Computation: 33577 steps/s (collection: 1.020s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0274
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 13.6792
                       Mean reward: 16.07
               Mean episode length: 833.83
Episode_Reward/track_lin_vel_xy_exp: 0.9987
Episode_Reward/track_ang_vel_z_exp: 0.4415
       Episode_Reward/lin_vel_z_l2: -0.0524
      Episode_Reward/ang_vel_xy_l2: -0.1327
     Episode_Reward/dof_torques_l2: -0.0795
         Episode_Reward/dof_acc_l2: -0.1873
     Episode_Reward/action_rate_l2: -0.1651
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.1554
Metrics/base_velocity/error_vel_xy: 0.3573
Metrics/base_velocity/error_vel_yaw: 0.4725
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 40392000
                    Iteration time: 1.07s
                      Time elapsed: 00:20:51
                               ETA: 00:07:02

################################################################################
                     [1m Learning iteration 1122/1500 [0m                     

                       Computation: 32902 steps/s (collection: 1.042s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0267
               Mean surrogate loss: -0.0105
                 Mean entropy loss: 13.6836
                       Mean reward: 16.48
               Mean episode length: 847.76
Episode_Reward/track_lin_vel_xy_exp: 0.8879
Episode_Reward/track_ang_vel_z_exp: 0.4303
       Episode_Reward/lin_vel_z_l2: -0.0453
      Episode_Reward/ang_vel_xy_l2: -0.1247
     Episode_Reward/dof_torques_l2: -0.0804
         Episode_Reward/dof_acc_l2: -0.1737
     Episode_Reward/action_rate_l2: -0.1614
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.1502
Metrics/base_velocity/error_vel_xy: 0.4650
Metrics/base_velocity/error_vel_yaw: 0.4713
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 40428000
                    Iteration time: 1.09s
                      Time elapsed: 00:20:52
                               ETA: 00:07:01

################################################################################
                     [1m Learning iteration 1123/1500 [0m                     

                       Computation: 34162 steps/s (collection: 1.003s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0250
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 13.6803
                       Mean reward: 17.05
               Mean episode length: 871.03
Episode_Reward/track_lin_vel_xy_exp: 1.0867
Episode_Reward/track_ang_vel_z_exp: 0.4919
       Episode_Reward/lin_vel_z_l2: -0.0400
      Episode_Reward/ang_vel_xy_l2: -0.1311
     Episode_Reward/dof_torques_l2: -0.0878
         Episode_Reward/dof_acc_l2: -0.1837
     Episode_Reward/action_rate_l2: -0.1768
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.1546
Metrics/base_velocity/error_vel_xy: 0.4054
Metrics/base_velocity/error_vel_yaw: 0.4875
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 40464000
                    Iteration time: 1.05s
                      Time elapsed: 00:20:54
                               ETA: 00:07:00

################################################################################
                     [1m Learning iteration 1124/1500 [0m                     

                       Computation: 33313 steps/s (collection: 1.029s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0231
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 13.6888
                       Mean reward: 17.22
               Mean episode length: 893.86
Episode_Reward/track_lin_vel_xy_exp: 0.9474
Episode_Reward/track_ang_vel_z_exp: 0.4559
       Episode_Reward/lin_vel_z_l2: -0.0442
      Episode_Reward/ang_vel_xy_l2: -0.1322
     Episode_Reward/dof_torques_l2: -0.0871
         Episode_Reward/dof_acc_l2: -0.1930
     Episode_Reward/action_rate_l2: -0.1726
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.1580
Metrics/base_velocity/error_vel_xy: 0.5126
Metrics/base_velocity/error_vel_yaw: 0.5236
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 40500000
                    Iteration time: 1.08s
                      Time elapsed: 00:20:55
                               ETA: 00:06:59

################################################################################
                     [1m Learning iteration 1125/1500 [0m                     

                       Computation: 33711 steps/s (collection: 1.016s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0265
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 13.6997
                       Mean reward: 17.04
               Mean episode length: 863.86
Episode_Reward/track_lin_vel_xy_exp: 1.0645
Episode_Reward/track_ang_vel_z_exp: 0.4867
       Episode_Reward/lin_vel_z_l2: -0.0443
      Episode_Reward/ang_vel_xy_l2: -0.1319
     Episode_Reward/dof_torques_l2: -0.0875
         Episode_Reward/dof_acc_l2: -0.1797
     Episode_Reward/action_rate_l2: -0.1760
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.1588
Metrics/base_velocity/error_vel_xy: 0.4308
Metrics/base_velocity/error_vel_yaw: 0.4977
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 40536000
                    Iteration time: 1.07s
                      Time elapsed: 00:20:56
                               ETA: 00:06:58

################################################################################
                     [1m Learning iteration 1126/1500 [0m                     

                       Computation: 33275 steps/s (collection: 1.028s, learning 0.054s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0262
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 13.7121
                       Mean reward: 16.71
               Mean episode length: 865.81
Episode_Reward/track_lin_vel_xy_exp: 1.0927
Episode_Reward/track_ang_vel_z_exp: 0.4972
       Episode_Reward/lin_vel_z_l2: -0.0494
      Episode_Reward/ang_vel_xy_l2: -0.1378
     Episode_Reward/dof_torques_l2: -0.0989
         Episode_Reward/dof_acc_l2: -0.1993
     Episode_Reward/action_rate_l2: -0.1884
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.1709
Metrics/base_velocity/error_vel_xy: 0.4624
Metrics/base_velocity/error_vel_yaw: 0.5439
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 40572000
                    Iteration time: 1.08s
                      Time elapsed: 00:20:57
                               ETA: 00:06:57

################################################################################
                     [1m Learning iteration 1127/1500 [0m                     

                       Computation: 32865 steps/s (collection: 1.043s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0272
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 13.7040
                       Mean reward: 16.51
               Mean episode length: 868.56
Episode_Reward/track_lin_vel_xy_exp: 0.9934
Episode_Reward/track_ang_vel_z_exp: 0.4566
       Episode_Reward/lin_vel_z_l2: -0.0532
      Episode_Reward/ang_vel_xy_l2: -0.1346
     Episode_Reward/dof_torques_l2: -0.0865
         Episode_Reward/dof_acc_l2: -0.1947
     Episode_Reward/action_rate_l2: -0.1733
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.1814
Metrics/base_velocity/error_vel_xy: 0.4457
Metrics/base_velocity/error_vel_yaw: 0.5126
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 40608000
                    Iteration time: 1.10s
                      Time elapsed: 00:20:58
                               ETA: 00:06:56

################################################################################
                     [1m Learning iteration 1128/1500 [0m                     

                       Computation: 33136 steps/s (collection: 1.035s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0287
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 13.6928
                       Mean reward: 17.76
               Mean episode length: 892.77
Episode_Reward/track_lin_vel_xy_exp: 1.1301
Episode_Reward/track_ang_vel_z_exp: 0.5080
       Episode_Reward/lin_vel_z_l2: -0.0446
      Episode_Reward/ang_vel_xy_l2: -0.1406
     Episode_Reward/dof_torques_l2: -0.0928
         Episode_Reward/dof_acc_l2: -0.1924
     Episode_Reward/action_rate_l2: -0.1849
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.1815
Metrics/base_velocity/error_vel_xy: 0.4097
Metrics/base_velocity/error_vel_yaw: 0.5085
      Episode_Termination/time_out: 1.9167
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 40644000
                    Iteration time: 1.09s
                      Time elapsed: 00:20:59
                               ETA: 00:06:54

################################################################################
                     [1m Learning iteration 1129/1500 [0m                     

                       Computation: 32966 steps/s (collection: 1.041s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0255
               Mean surrogate loss: -0.0153
                 Mean entropy loss: 13.6995
                       Mean reward: 18.63
               Mean episode length: 912.73
Episode_Reward/track_lin_vel_xy_exp: 1.0226
Episode_Reward/track_ang_vel_z_exp: 0.4798
       Episode_Reward/lin_vel_z_l2: -0.0455
      Episode_Reward/ang_vel_xy_l2: -0.1342
     Episode_Reward/dof_torques_l2: -0.0919
         Episode_Reward/dof_acc_l2: -0.1949
     Episode_Reward/action_rate_l2: -0.1784
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.1911
Metrics/base_velocity/error_vel_xy: 0.4598
Metrics/base_velocity/error_vel_yaw: 0.5103
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 40680000
                    Iteration time: 1.09s
                      Time elapsed: 00:21:00
                               ETA: 00:06:53

################################################################################
                     [1m Learning iteration 1130/1500 [0m                     

                       Computation: 32926 steps/s (collection: 1.042s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0241
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 13.6962
                       Mean reward: 17.13
               Mean episode length: 870.70
Episode_Reward/track_lin_vel_xy_exp: 0.9976
Episode_Reward/track_ang_vel_z_exp: 0.4561
       Episode_Reward/lin_vel_z_l2: -0.0425
      Episode_Reward/ang_vel_xy_l2: -0.1232
     Episode_Reward/dof_torques_l2: -0.0838
         Episode_Reward/dof_acc_l2: -0.1698
     Episode_Reward/action_rate_l2: -0.1635
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.1945
Metrics/base_velocity/error_vel_xy: 0.3946
Metrics/base_velocity/error_vel_yaw: 0.4623
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 40716000
                    Iteration time: 1.09s
                      Time elapsed: 00:21:01
                               ETA: 00:06:52

################################################################################
                     [1m Learning iteration 1131/1500 [0m                     

                       Computation: 33533 steps/s (collection: 1.023s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0228
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 13.6908
                       Mean reward: 17.18
               Mean episode length: 845.62
Episode_Reward/track_lin_vel_xy_exp: 0.9515
Episode_Reward/track_ang_vel_z_exp: 0.4286
       Episode_Reward/lin_vel_z_l2: -0.0388
      Episode_Reward/ang_vel_xy_l2: -0.1186
     Episode_Reward/dof_torques_l2: -0.0777
         Episode_Reward/dof_acc_l2: -0.1606
     Episode_Reward/action_rate_l2: -0.1561
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.1923
Metrics/base_velocity/error_vel_xy: 0.3640
Metrics/base_velocity/error_vel_yaw: 0.4513
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 40752000
                    Iteration time: 1.07s
                      Time elapsed: 00:21:02
                               ETA: 00:06:51

################################################################################
                     [1m Learning iteration 1132/1500 [0m                     

                       Computation: 32913 steps/s (collection: 1.042s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0224
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 13.6927
                       Mean reward: 16.93
               Mean episode length: 828.35
Episode_Reward/track_lin_vel_xy_exp: 1.0272
Episode_Reward/track_ang_vel_z_exp: 0.4610
       Episode_Reward/lin_vel_z_l2: -0.0463
      Episode_Reward/ang_vel_xy_l2: -0.1252
     Episode_Reward/dof_torques_l2: -0.0868
         Episode_Reward/dof_acc_l2: -0.1845
     Episode_Reward/action_rate_l2: -0.1718
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.1873
Metrics/base_velocity/error_vel_xy: 0.3817
Metrics/base_velocity/error_vel_yaw: 0.4706
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 40788000
                    Iteration time: 1.09s
                      Time elapsed: 00:21:03
                               ETA: 00:06:50

################################################################################
                     [1m Learning iteration 1133/1500 [0m                     

                       Computation: 33472 steps/s (collection: 1.023s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0285
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 13.7017
                       Mean reward: 17.58
               Mean episode length: 846.03
Episode_Reward/track_lin_vel_xy_exp: 1.0681
Episode_Reward/track_ang_vel_z_exp: 0.4808
       Episode_Reward/lin_vel_z_l2: -0.0402
      Episode_Reward/ang_vel_xy_l2: -0.1307
     Episode_Reward/dof_torques_l2: -0.0860
         Episode_Reward/dof_acc_l2: -0.1842
     Episode_Reward/action_rate_l2: -0.1752
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.1785
Metrics/base_velocity/error_vel_xy: 0.4074
Metrics/base_velocity/error_vel_yaw: 0.5017
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 40824000
                    Iteration time: 1.08s
                      Time elapsed: 00:21:04
                               ETA: 00:06:49

################################################################################
                     [1m Learning iteration 1134/1500 [0m                     

                       Computation: 33716 steps/s (collection: 1.018s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0228
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 13.7064
                       Mean reward: 16.45
               Mean episode length: 816.85
Episode_Reward/track_lin_vel_xy_exp: 0.9721
Episode_Reward/track_ang_vel_z_exp: 0.4466
       Episode_Reward/lin_vel_z_l2: -0.0415
      Episode_Reward/ang_vel_xy_l2: -0.1239
     Episode_Reward/dof_torques_l2: -0.0840
         Episode_Reward/dof_acc_l2: -0.1824
     Episode_Reward/action_rate_l2: -0.1669
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.1786
Metrics/base_velocity/error_vel_xy: 0.4076
Metrics/base_velocity/error_vel_yaw: 0.4785
      Episode_Termination/time_out: 1.0000
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 40860000
                    Iteration time: 1.07s
                      Time elapsed: 00:21:05
                               ETA: 00:06:48

################################################################################
                     [1m Learning iteration 1135/1500 [0m                     

                       Computation: 33673 steps/s (collection: 1.019s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0243
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 13.7045
                       Mean reward: 16.57
               Mean episode length: 841.82
Episode_Reward/track_lin_vel_xy_exp: 0.9878
Episode_Reward/track_ang_vel_z_exp: 0.4773
       Episode_Reward/lin_vel_z_l2: -0.0422
      Episode_Reward/ang_vel_xy_l2: -0.1327
     Episode_Reward/dof_torques_l2: -0.0895
         Episode_Reward/dof_acc_l2: -0.1797
     Episode_Reward/action_rate_l2: -0.1777
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.1786
Metrics/base_velocity/error_vel_xy: 0.5005
Metrics/base_velocity/error_vel_yaw: 0.5306
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 40896000
                    Iteration time: 1.07s
                      Time elapsed: 00:21:07
                               ETA: 00:06:47

################################################################################
                     [1m Learning iteration 1136/1500 [0m                     

                       Computation: 33514 steps/s (collection: 1.021s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0298
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 13.7008
                       Mean reward: 16.22
               Mean episode length: 856.61
Episode_Reward/track_lin_vel_xy_exp: 1.0053
Episode_Reward/track_ang_vel_z_exp: 0.4731
       Episode_Reward/lin_vel_z_l2: -0.0446
      Episode_Reward/ang_vel_xy_l2: -0.1338
     Episode_Reward/dof_torques_l2: -0.0936
         Episode_Reward/dof_acc_l2: -0.2019
     Episode_Reward/action_rate_l2: -0.1784
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.1750
Metrics/base_velocity/error_vel_xy: 0.4808
Metrics/base_velocity/error_vel_yaw: 0.5147
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 40932000
                    Iteration time: 1.07s
                      Time elapsed: 00:21:08
                               ETA: 00:06:45

################################################################################
                     [1m Learning iteration 1137/1500 [0m                     

                       Computation: 33098 steps/s (collection: 1.036s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0266
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 13.6963
                       Mean reward: 16.66
               Mean episode length: 876.61
Episode_Reward/track_lin_vel_xy_exp: 0.9895
Episode_Reward/track_ang_vel_z_exp: 0.4342
       Episode_Reward/lin_vel_z_l2: -0.0464
      Episode_Reward/ang_vel_xy_l2: -0.1233
     Episode_Reward/dof_torques_l2: -0.0786
         Episode_Reward/dof_acc_l2: -0.1745
     Episode_Reward/action_rate_l2: -0.1589
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.1796
Metrics/base_velocity/error_vel_xy: 0.3423
Metrics/base_velocity/error_vel_yaw: 0.4466
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 40968000
                    Iteration time: 1.09s
                      Time elapsed: 00:21:09
                               ETA: 00:06:44

################################################################################
                     [1m Learning iteration 1138/1500 [0m                     

                       Computation: 33248 steps/s (collection: 1.031s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0250
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 13.6994
                       Mean reward: 16.33
               Mean episode length: 824.23
Episode_Reward/track_lin_vel_xy_exp: 0.8703
Episode_Reward/track_ang_vel_z_exp: 0.3927
       Episode_Reward/lin_vel_z_l2: -0.0346
      Episode_Reward/ang_vel_xy_l2: -0.1043
     Episode_Reward/dof_torques_l2: -0.0730
         Episode_Reward/dof_acc_l2: -0.1434
     Episode_Reward/action_rate_l2: -0.1413
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.1797
Metrics/base_velocity/error_vel_xy: 0.3215
Metrics/base_velocity/error_vel_yaw: 0.3911
      Episode_Termination/time_out: 1.1250
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 41004000
                    Iteration time: 1.08s
                      Time elapsed: 00:21:10
                               ETA: 00:06:43

################################################################################
                     [1m Learning iteration 1139/1500 [0m                     

                       Computation: 32727 steps/s (collection: 1.049s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0273
               Mean surrogate loss: -0.0149
                 Mean entropy loss: 13.6999
                       Mean reward: 17.07
               Mean episode length: 813.67
Episode_Reward/track_lin_vel_xy_exp: 1.0319
Episode_Reward/track_ang_vel_z_exp: 0.4513
       Episode_Reward/lin_vel_z_l2: -0.0426
      Episode_Reward/ang_vel_xy_l2: -0.1251
     Episode_Reward/dof_torques_l2: -0.0786
         Episode_Reward/dof_acc_l2: -0.1817
     Episode_Reward/action_rate_l2: -0.1663
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.1819
Metrics/base_velocity/error_vel_xy: 0.3448
Metrics/base_velocity/error_vel_yaw: 0.4695
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 41040000
                    Iteration time: 1.10s
                      Time elapsed: 00:21:11
                               ETA: 00:06:42

################################################################################
                     [1m Learning iteration 1140/1500 [0m                     

                       Computation: 33818 steps/s (collection: 1.014s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0236
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 13.6992
                       Mean reward: 16.72
               Mean episode length: 831.79
Episode_Reward/track_lin_vel_xy_exp: 0.9671
Episode_Reward/track_ang_vel_z_exp: 0.4525
       Episode_Reward/lin_vel_z_l2: -0.0472
      Episode_Reward/ang_vel_xy_l2: -0.1324
     Episode_Reward/dof_torques_l2: -0.0865
         Episode_Reward/dof_acc_l2: -0.1934
     Episode_Reward/action_rate_l2: -0.1743
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.1845
Metrics/base_velocity/error_vel_xy: 0.4524
Metrics/base_velocity/error_vel_yaw: 0.5005
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 41076000
                    Iteration time: 1.06s
                      Time elapsed: 00:21:12
                               ETA: 00:06:41

################################################################################
                     [1m Learning iteration 1141/1500 [0m                     

                       Computation: 33302 steps/s (collection: 1.028s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0259
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 13.6988
                       Mean reward: 15.88
               Mean episode length: 837.64
Episode_Reward/track_lin_vel_xy_exp: 0.9317
Episode_Reward/track_ang_vel_z_exp: 0.4466
       Episode_Reward/lin_vel_z_l2: -0.0457
      Episode_Reward/ang_vel_xy_l2: -0.1271
     Episode_Reward/dof_torques_l2: -0.0868
         Episode_Reward/dof_acc_l2: -0.1877
     Episode_Reward/action_rate_l2: -0.1691
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.1926
Metrics/base_velocity/error_vel_xy: 0.4663
Metrics/base_velocity/error_vel_yaw: 0.4882
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 41112000
                    Iteration time: 1.08s
                      Time elapsed: 00:21:13
                               ETA: 00:06:40

################################################################################
                     [1m Learning iteration 1142/1500 [0m                     

                       Computation: 33662 steps/s (collection: 1.019s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0220
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 13.6968
                       Mean reward: 16.34
               Mean episode length: 862.07
Episode_Reward/track_lin_vel_xy_exp: 1.0199
Episode_Reward/track_ang_vel_z_exp: 0.4578
       Episode_Reward/lin_vel_z_l2: -0.0449
      Episode_Reward/ang_vel_xy_l2: -0.1292
     Episode_Reward/dof_torques_l2: -0.0854
         Episode_Reward/dof_acc_l2: -0.1833
     Episode_Reward/action_rate_l2: -0.1707
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2053
Metrics/base_velocity/error_vel_xy: 0.4026
Metrics/base_velocity/error_vel_yaw: 0.5009
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 41148000
                    Iteration time: 1.07s
                      Time elapsed: 00:21:14
                               ETA: 00:06:39

################################################################################
                     [1m Learning iteration 1143/1500 [0m                     

                       Computation: 32674 steps/s (collection: 1.046s, learning 0.056s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0259
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 13.6887
                       Mean reward: 16.79
               Mean episode length: 855.04
Episode_Reward/track_lin_vel_xy_exp: 0.9758
Episode_Reward/track_ang_vel_z_exp: 0.4444
       Episode_Reward/lin_vel_z_l2: -0.0432
      Episode_Reward/ang_vel_xy_l2: -0.1215
     Episode_Reward/dof_torques_l2: -0.0817
         Episode_Reward/dof_acc_l2: -0.1759
     Episode_Reward/action_rate_l2: -0.1633
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2075
Metrics/base_velocity/error_vel_xy: 0.3821
Metrics/base_velocity/error_vel_yaw: 0.4542
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 41184000
                    Iteration time: 1.10s
                      Time elapsed: 00:21:15
                               ETA: 00:06:38

################################################################################
                     [1m Learning iteration 1144/1500 [0m                     

                       Computation: 33527 steps/s (collection: 1.023s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0256
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 13.6996
                       Mean reward: 16.93
               Mean episode length: 854.48
Episode_Reward/track_lin_vel_xy_exp: 1.0762
Episode_Reward/track_ang_vel_z_exp: 0.4881
       Episode_Reward/lin_vel_z_l2: -0.0448
      Episode_Reward/ang_vel_xy_l2: -0.1316
     Episode_Reward/dof_torques_l2: -0.0921
         Episode_Reward/dof_acc_l2: -0.1944
     Episode_Reward/action_rate_l2: -0.1804
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2076
Metrics/base_velocity/error_vel_xy: 0.4183
Metrics/base_velocity/error_vel_yaw: 0.5059
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 41220000
                    Iteration time: 1.07s
                      Time elapsed: 00:21:16
                               ETA: 00:06:36

################################################################################
                     [1m Learning iteration 1145/1500 [0m                     

                       Computation: 33176 steps/s (collection: 1.033s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0244
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 13.7053
                       Mean reward: 17.48
               Mean episode length: 889.52
Episode_Reward/track_lin_vel_xy_exp: 1.0620
Episode_Reward/track_ang_vel_z_exp: 0.4788
       Episode_Reward/lin_vel_z_l2: -0.0462
      Episode_Reward/ang_vel_xy_l2: -0.1325
     Episode_Reward/dof_torques_l2: -0.0915
         Episode_Reward/dof_acc_l2: -0.1932
     Episode_Reward/action_rate_l2: -0.1802
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2136
Metrics/base_velocity/error_vel_xy: 0.4198
Metrics/base_velocity/error_vel_yaw: 0.5102
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 41256000
                    Iteration time: 1.09s
                      Time elapsed: 00:21:17
                               ETA: 00:06:35

################################################################################
                     [1m Learning iteration 1146/1500 [0m                     

                       Computation: 33598 steps/s (collection: 1.018s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0224
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 13.7097
                       Mean reward: 17.59
               Mean episode length: 867.08
Episode_Reward/track_lin_vel_xy_exp: 0.9649
Episode_Reward/track_ang_vel_z_exp: 0.4350
       Episode_Reward/lin_vel_z_l2: -0.0393
      Episode_Reward/ang_vel_xy_l2: -0.1194
     Episode_Reward/dof_torques_l2: -0.0837
         Episode_Reward/dof_acc_l2: -0.1798
     Episode_Reward/action_rate_l2: -0.1636
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2156
Metrics/base_velocity/error_vel_xy: 0.3739
Metrics/base_velocity/error_vel_yaw: 0.4502
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 41292000
                    Iteration time: 1.07s
                      Time elapsed: 00:21:18
                               ETA: 00:06:34

################################################################################
                     [1m Learning iteration 1147/1500 [0m                     

                       Computation: 33510 steps/s (collection: 1.024s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0247
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 13.7062
                       Mean reward: 17.08
               Mean episode length: 854.09
Episode_Reward/track_lin_vel_xy_exp: 1.0913
Episode_Reward/track_ang_vel_z_exp: 0.4809
       Episode_Reward/lin_vel_z_l2: -0.0425
      Episode_Reward/ang_vel_xy_l2: -0.1287
     Episode_Reward/dof_torques_l2: -0.0899
         Episode_Reward/dof_acc_l2: -0.1960
     Episode_Reward/action_rate_l2: -0.1794
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2190
Metrics/base_velocity/error_vel_xy: 0.3701
Metrics/base_velocity/error_vel_yaw: 0.4808
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 41328000
                    Iteration time: 1.07s
                      Time elapsed: 00:21:19
                               ETA: 00:06:33

################################################################################
                     [1m Learning iteration 1148/1500 [0m                     

                       Computation: 33742 steps/s (collection: 1.015s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0258
               Mean surrogate loss: -0.0158
                 Mean entropy loss: 13.6967
                       Mean reward: 17.66
               Mean episode length: 853.50
Episode_Reward/track_lin_vel_xy_exp: 1.1389
Episode_Reward/track_ang_vel_z_exp: 0.5062
       Episode_Reward/lin_vel_z_l2: -0.0417
      Episode_Reward/ang_vel_xy_l2: -0.1369
     Episode_Reward/dof_torques_l2: -0.0965
         Episode_Reward/dof_acc_l2: -0.1891
     Episode_Reward/action_rate_l2: -0.1855
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2216
Metrics/base_velocity/error_vel_xy: 0.3910
Metrics/base_velocity/error_vel_yaw: 0.4946
      Episode_Termination/time_out: 2.0000
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 41364000
                    Iteration time: 1.07s
                      Time elapsed: 00:21:21
                               ETA: 00:06:32

################################################################################
                     [1m Learning iteration 1149/1500 [0m                     

                       Computation: 33303 steps/s (collection: 1.031s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0282
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 13.7055
                       Mean reward: 18.55
               Mean episode length: 895.39
Episode_Reward/track_lin_vel_xy_exp: 1.0971
Episode_Reward/track_ang_vel_z_exp: 0.4802
       Episode_Reward/lin_vel_z_l2: -0.0435
      Episode_Reward/ang_vel_xy_l2: -0.1287
     Episode_Reward/dof_torques_l2: -0.0876
         Episode_Reward/dof_acc_l2: -0.1868
     Episode_Reward/action_rate_l2: -0.1749
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2173
Metrics/base_velocity/error_vel_xy: 0.3575
Metrics/base_velocity/error_vel_yaw: 0.4806
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 41400000
                    Iteration time: 1.08s
                      Time elapsed: 00:21:22
                               ETA: 00:06:31

################################################################################
                     [1m Learning iteration 1150/1500 [0m                     

                       Computation: 33652 steps/s (collection: 1.017s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0285
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 13.7115
                       Mean reward: 18.34
               Mean episode length: 874.59
Episode_Reward/track_lin_vel_xy_exp: 0.9822
Episode_Reward/track_ang_vel_z_exp: 0.4467
       Episode_Reward/lin_vel_z_l2: -0.0446
      Episode_Reward/ang_vel_xy_l2: -0.1255
     Episode_Reward/dof_torques_l2: -0.0836
         Episode_Reward/dof_acc_l2: -0.1872
     Episode_Reward/action_rate_l2: -0.1696
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2221
Metrics/base_velocity/error_vel_xy: 0.3815
Metrics/base_velocity/error_vel_yaw: 0.4613
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 41436000
                    Iteration time: 1.07s
                      Time elapsed: 00:21:23
                               ETA: 00:06:30

################################################################################
                     [1m Learning iteration 1151/1500 [0m                     

                       Computation: 33064 steps/s (collection: 1.038s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0305
               Mean surrogate loss: -0.0153
                 Mean entropy loss: 13.7179
                       Mean reward: 17.16
               Mean episode length: 863.06
Episode_Reward/track_lin_vel_xy_exp: 1.0285
Episode_Reward/track_ang_vel_z_exp: 0.4861
       Episode_Reward/lin_vel_z_l2: -0.0492
      Episode_Reward/ang_vel_xy_l2: -0.1336
     Episode_Reward/dof_torques_l2: -0.0930
         Episode_Reward/dof_acc_l2: -0.1920
     Episode_Reward/action_rate_l2: -0.1782
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2198
Metrics/base_velocity/error_vel_xy: 0.4570
Metrics/base_velocity/error_vel_yaw: 0.4976
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 41472000
                    Iteration time: 1.09s
                      Time elapsed: 00:21:24
                               ETA: 00:06:29

################################################################################
                     [1m Learning iteration 1152/1500 [0m                     

                       Computation: 32256 steps/s (collection: 1.065s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0283
               Mean surrogate loss: -0.0158
                 Mean entropy loss: 13.7261
                       Mean reward: 17.98
               Mean episode length: 879.68
Episode_Reward/track_lin_vel_xy_exp: 1.0149
Episode_Reward/track_ang_vel_z_exp: 0.4504
       Episode_Reward/lin_vel_z_l2: -0.0482
      Episode_Reward/ang_vel_xy_l2: -0.1237
     Episode_Reward/dof_torques_l2: -0.0819
         Episode_Reward/dof_acc_l2: -0.1776
     Episode_Reward/action_rate_l2: -0.1640
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2307
Metrics/base_velocity/error_vel_xy: 0.3479
Metrics/base_velocity/error_vel_yaw: 0.4585
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 41508000
                    Iteration time: 1.12s
                      Time elapsed: 00:21:25
                               ETA: 00:06:27

################################################################################
                     [1m Learning iteration 1153/1500 [0m                     

                       Computation: 33197 steps/s (collection: 1.035s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0282
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 13.7311
                       Mean reward: 17.79
               Mean episode length: 855.49
Episode_Reward/track_lin_vel_xy_exp: 1.0434
Episode_Reward/track_ang_vel_z_exp: 0.4602
       Episode_Reward/lin_vel_z_l2: -0.0442
      Episode_Reward/ang_vel_xy_l2: -0.1291
     Episode_Reward/dof_torques_l2: -0.0825
         Episode_Reward/dof_acc_l2: -0.1832
     Episode_Reward/action_rate_l2: -0.1682
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2400
Metrics/base_velocity/error_vel_xy: 0.3597
Metrics/base_velocity/error_vel_yaw: 0.4786
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 41544000
                    Iteration time: 1.08s
                      Time elapsed: 00:21:26
                               ETA: 00:06:26

################################################################################
                     [1m Learning iteration 1154/1500 [0m                     

                       Computation: 33316 steps/s (collection: 1.027s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0247
               Mean surrogate loss: -0.0152
                 Mean entropy loss: 13.7281
                       Mean reward: 17.89
               Mean episode length: 870.22
Episode_Reward/track_lin_vel_xy_exp: 1.1284
Episode_Reward/track_ang_vel_z_exp: 0.5025
       Episode_Reward/lin_vel_z_l2: -0.0468
      Episode_Reward/ang_vel_xy_l2: -0.1384
     Episode_Reward/dof_torques_l2: -0.0897
         Episode_Reward/dof_acc_l2: -0.1959
     Episode_Reward/action_rate_l2: -0.1836
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2403
Metrics/base_velocity/error_vel_xy: 0.4075
Metrics/base_velocity/error_vel_yaw: 0.5086
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 41580000
                    Iteration time: 1.08s
                      Time elapsed: 00:21:27
                               ETA: 00:06:25

################################################################################
                     [1m Learning iteration 1155/1500 [0m                     

                       Computation: 33467 steps/s (collection: 1.023s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0224
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 13.7238
                       Mean reward: 17.70
               Mean episode length: 859.71
Episode_Reward/track_lin_vel_xy_exp: 1.0124
Episode_Reward/track_ang_vel_z_exp: 0.4434
       Episode_Reward/lin_vel_z_l2: -0.0409
      Episode_Reward/ang_vel_xy_l2: -0.1200
     Episode_Reward/dof_torques_l2: -0.0812
         Episode_Reward/dof_acc_l2: -0.1758
     Episode_Reward/action_rate_l2: -0.1627
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2339
Metrics/base_velocity/error_vel_xy: 0.3476
Metrics/base_velocity/error_vel_yaw: 0.4703
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 41616000
                    Iteration time: 1.08s
                      Time elapsed: 00:21:28
                               ETA: 00:06:24

################################################################################
                     [1m Learning iteration 1156/1500 [0m                     

                       Computation: 33999 steps/s (collection: 1.007s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0198
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 13.7254
                       Mean reward: 17.98
               Mean episode length: 867.02
Episode_Reward/track_lin_vel_xy_exp: 1.0131
Episode_Reward/track_ang_vel_z_exp: 0.4629
       Episode_Reward/lin_vel_z_l2: -0.0449
      Episode_Reward/ang_vel_xy_l2: -0.1298
     Episode_Reward/dof_torques_l2: -0.0838
         Episode_Reward/dof_acc_l2: -0.1862
     Episode_Reward/action_rate_l2: -0.1698
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2338
Metrics/base_velocity/error_vel_xy: 0.4036
Metrics/base_velocity/error_vel_yaw: 0.4728
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 41652000
                    Iteration time: 1.06s
                      Time elapsed: 00:21:29
                               ETA: 00:06:23

################################################################################
                     [1m Learning iteration 1157/1500 [0m                     

                       Computation: 33427 steps/s (collection: 1.027s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0247
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 13.7241
                       Mean reward: 18.67
               Mean episode length: 907.24
Episode_Reward/track_lin_vel_xy_exp: 1.1378
Episode_Reward/track_ang_vel_z_exp: 0.5006
       Episode_Reward/lin_vel_z_l2: -0.0444
      Episode_Reward/ang_vel_xy_l2: -0.1347
     Episode_Reward/dof_torques_l2: -0.0970
         Episode_Reward/dof_acc_l2: -0.1996
     Episode_Reward/action_rate_l2: -0.1890
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2364
Metrics/base_velocity/error_vel_xy: 0.3928
Metrics/base_velocity/error_vel_yaw: 0.5149
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 41688000
                    Iteration time: 1.08s
                      Time elapsed: 00:21:30
                               ETA: 00:06:22

################################################################################
                     [1m Learning iteration 1158/1500 [0m                     

                       Computation: 33901 steps/s (collection: 1.012s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0254
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 13.7262
                       Mean reward: 18.27
               Mean episode length: 897.69
Episode_Reward/track_lin_vel_xy_exp: 0.9251
Episode_Reward/track_ang_vel_z_exp: 0.4308
       Episode_Reward/lin_vel_z_l2: -0.0409
      Episode_Reward/ang_vel_xy_l2: -0.1201
     Episode_Reward/dof_torques_l2: -0.0841
         Episode_Reward/dof_acc_l2: -0.1769
     Episode_Reward/action_rate_l2: -0.1630
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2410
Metrics/base_velocity/error_vel_xy: 0.4306
Metrics/base_velocity/error_vel_yaw: 0.4747
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 41724000
                    Iteration time: 1.06s
                      Time elapsed: 00:21:31
                               ETA: 00:06:21

################################################################################
                     [1m Learning iteration 1159/1500 [0m                     

                       Computation: 32732 steps/s (collection: 1.049s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0250
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 13.7305
                       Mean reward: 17.24
               Mean episode length: 865.02
Episode_Reward/track_lin_vel_xy_exp: 1.1369
Episode_Reward/track_ang_vel_z_exp: 0.5055
       Episode_Reward/lin_vel_z_l2: -0.0497
      Episode_Reward/ang_vel_xy_l2: -0.1391
     Episode_Reward/dof_torques_l2: -0.0958
         Episode_Reward/dof_acc_l2: -0.2021
     Episode_Reward/action_rate_l2: -0.1878
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2523
Metrics/base_velocity/error_vel_xy: 0.4086
Metrics/base_velocity/error_vel_yaw: 0.5166
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 41760000
                    Iteration time: 1.10s
                      Time elapsed: 00:21:32
                               ETA: 00:06:20

################################################################################
                     [1m Learning iteration 1160/1500 [0m                     

                       Computation: 33445 steps/s (collection: 1.025s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0251
               Mean surrogate loss: -0.0153
                 Mean entropy loss: 13.7397
                       Mean reward: 16.65
               Mean episode length: 863.58
Episode_Reward/track_lin_vel_xy_exp: 0.9615
Episode_Reward/track_ang_vel_z_exp: 0.4510
       Episode_Reward/lin_vel_z_l2: -0.0507
      Episode_Reward/ang_vel_xy_l2: -0.1322
     Episode_Reward/dof_torques_l2: -0.0882
         Episode_Reward/dof_acc_l2: -0.1976
     Episode_Reward/action_rate_l2: -0.1730
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2573
Metrics/base_velocity/error_vel_xy: 0.4551
Metrics/base_velocity/error_vel_yaw: 0.4961
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 41796000
                    Iteration time: 1.08s
                      Time elapsed: 00:21:34
                               ETA: 00:06:18

################################################################################
                     [1m Learning iteration 1161/1500 [0m                     

                       Computation: 33829 steps/s (collection: 1.013s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0238
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 13.7537
                       Mean reward: 16.72
               Mean episode length: 834.02
Episode_Reward/track_lin_vel_xy_exp: 0.8872
Episode_Reward/track_ang_vel_z_exp: 0.3891
       Episode_Reward/lin_vel_z_l2: -0.0373
      Episode_Reward/ang_vel_xy_l2: -0.1051
     Episode_Reward/dof_torques_l2: -0.0687
         Episode_Reward/dof_acc_l2: -0.1514
     Episode_Reward/action_rate_l2: -0.1425
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2597
Metrics/base_velocity/error_vel_xy: 0.2880
Metrics/base_velocity/error_vel_yaw: 0.3862
      Episode_Termination/time_out: 1.1250
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 41832000
                    Iteration time: 1.06s
                      Time elapsed: 00:21:35
                               ETA: 00:06:17

################################################################################
                     [1m Learning iteration 1162/1500 [0m                     

                       Computation: 33501 steps/s (collection: 1.020s, learning 0.054s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0247
               Mean surrogate loss: -0.0155
                 Mean entropy loss: 13.7613
                       Mean reward: 16.20
               Mean episode length: 817.11
Episode_Reward/track_lin_vel_xy_exp: 0.9185
Episode_Reward/track_ang_vel_z_exp: 0.4258
       Episode_Reward/lin_vel_z_l2: -0.0452
      Episode_Reward/ang_vel_xy_l2: -0.1247
     Episode_Reward/dof_torques_l2: -0.0804
         Episode_Reward/dof_acc_l2: -0.1813
     Episode_Reward/action_rate_l2: -0.1623
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2593
Metrics/base_velocity/error_vel_xy: 0.4253
Metrics/base_velocity/error_vel_yaw: 0.4661
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 41868000
                    Iteration time: 1.07s
                      Time elapsed: 00:21:36
                               ETA: 00:06:16

################################################################################
                     [1m Learning iteration 1163/1500 [0m                     

                       Computation: 33823 steps/s (collection: 1.013s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0208
               Mean surrogate loss: -0.0157
                 Mean entropy loss: 13.7531
                       Mean reward: 16.42
               Mean episode length: 838.11
Episode_Reward/track_lin_vel_xy_exp: 0.9847
Episode_Reward/track_ang_vel_z_exp: 0.4627
       Episode_Reward/lin_vel_z_l2: -0.0418
      Episode_Reward/ang_vel_xy_l2: -0.1277
     Episode_Reward/dof_torques_l2: -0.0866
         Episode_Reward/dof_acc_l2: -0.1856
     Episode_Reward/action_rate_l2: -0.1710
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2446
Metrics/base_velocity/error_vel_xy: 0.4434
Metrics/base_velocity/error_vel_yaw: 0.4804
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 41904000
                    Iteration time: 1.06s
                      Time elapsed: 00:21:37
                               ETA: 00:06:15

################################################################################
                     [1m Learning iteration 1164/1500 [0m                     

                       Computation: 33529 steps/s (collection: 1.023s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0244
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 13.7319
                       Mean reward: 18.31
               Mean episode length: 894.80
Episode_Reward/track_lin_vel_xy_exp: 1.0986
Episode_Reward/track_ang_vel_z_exp: 0.5008
       Episode_Reward/lin_vel_z_l2: -0.0465
      Episode_Reward/ang_vel_xy_l2: -0.1388
     Episode_Reward/dof_torques_l2: -0.0900
         Episode_Reward/dof_acc_l2: -0.1903
     Episode_Reward/action_rate_l2: -0.1847
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2439
Metrics/base_velocity/error_vel_xy: 0.4315
Metrics/base_velocity/error_vel_yaw: 0.5163
      Episode_Termination/time_out: 2.0417
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 41940000
                    Iteration time: 1.07s
                      Time elapsed: 00:21:38
                               ETA: 00:06:14

################################################################################
                     [1m Learning iteration 1165/1500 [0m                     

                       Computation: 33138 steps/s (collection: 1.037s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0249
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 13.7196
                       Mean reward: 19.25
               Mean episode length: 923.39
Episode_Reward/track_lin_vel_xy_exp: 1.0711
Episode_Reward/track_ang_vel_z_exp: 0.4764
       Episode_Reward/lin_vel_z_l2: -0.0397
      Episode_Reward/ang_vel_xy_l2: -0.1277
     Episode_Reward/dof_torques_l2: -0.0869
         Episode_Reward/dof_acc_l2: -0.1800
     Episode_Reward/action_rate_l2: -0.1755
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2494
Metrics/base_velocity/error_vel_xy: 0.4008
Metrics/base_velocity/error_vel_yaw: 0.4909
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 41976000
                    Iteration time: 1.09s
                      Time elapsed: 00:21:39
                               ETA: 00:06:13

################################################################################
                     [1m Learning iteration 1166/1500 [0m                     

                       Computation: 33227 steps/s (collection: 1.032s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0254
               Mean surrogate loss: -0.0149
                 Mean entropy loss: 13.7281
                       Mean reward: 17.95
               Mean episode length: 864.22
Episode_Reward/track_lin_vel_xy_exp: 1.0291
Episode_Reward/track_ang_vel_z_exp: 0.4666
       Episode_Reward/lin_vel_z_l2: -0.0435
      Episode_Reward/ang_vel_xy_l2: -0.1283
     Episode_Reward/dof_torques_l2: -0.0830
         Episode_Reward/dof_acc_l2: -0.1803
     Episode_Reward/action_rate_l2: -0.1678
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2601
Metrics/base_velocity/error_vel_xy: 0.3786
Metrics/base_velocity/error_vel_yaw: 0.4598
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 42012000
                    Iteration time: 1.08s
                      Time elapsed: 00:21:40
                               ETA: 00:06:12

################################################################################
                     [1m Learning iteration 1167/1500 [0m                     

                       Computation: 33706 steps/s (collection: 1.017s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0262
               Mean surrogate loss: -0.0156
                 Mean entropy loss: 13.7303
                       Mean reward: 17.08
               Mean episode length: 835.19
Episode_Reward/track_lin_vel_xy_exp: 1.0055
Episode_Reward/track_ang_vel_z_exp: 0.4455
       Episode_Reward/lin_vel_z_l2: -0.0419
      Episode_Reward/ang_vel_xy_l2: -0.1254
     Episode_Reward/dof_torques_l2: -0.0785
         Episode_Reward/dof_acc_l2: -0.1761
     Episode_Reward/action_rate_l2: -0.1633
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2681
Metrics/base_velocity/error_vel_xy: 0.3626
Metrics/base_velocity/error_vel_yaw: 0.4614
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 42048000
                    Iteration time: 1.07s
                      Time elapsed: 00:21:41
                               ETA: 00:06:11

################################################################################
                     [1m Learning iteration 1168/1500 [0m                     

                       Computation: 33867 steps/s (collection: 1.012s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0255
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 13.7273
                       Mean reward: 16.74
               Mean episode length: 829.51
Episode_Reward/track_lin_vel_xy_exp: 1.0361
Episode_Reward/track_ang_vel_z_exp: 0.4730
       Episode_Reward/lin_vel_z_l2: -0.0435
      Episode_Reward/ang_vel_xy_l2: -0.1305
     Episode_Reward/dof_torques_l2: -0.0890
         Episode_Reward/dof_acc_l2: -0.1884
     Episode_Reward/action_rate_l2: -0.1758
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2723
Metrics/base_velocity/error_vel_xy: 0.4354
Metrics/base_velocity/error_vel_yaw: 0.5156
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 42084000
                    Iteration time: 1.06s
                      Time elapsed: 00:21:42
                               ETA: 00:06:09

################################################################################
                     [1m Learning iteration 1169/1500 [0m                     

                       Computation: 32979 steps/s (collection: 1.041s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0275
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 13.7285
                       Mean reward: 16.68
               Mean episode length: 855.23
Episode_Reward/track_lin_vel_xy_exp: 0.9685
Episode_Reward/track_ang_vel_z_exp: 0.4488
       Episode_Reward/lin_vel_z_l2: -0.0431
      Episode_Reward/ang_vel_xy_l2: -0.1267
     Episode_Reward/dof_torques_l2: -0.0857
         Episode_Reward/dof_acc_l2: -0.1875
     Episode_Reward/action_rate_l2: -0.1712
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2723
Metrics/base_velocity/error_vel_xy: 0.4408
Metrics/base_velocity/error_vel_yaw: 0.5047
      Episode_Termination/time_out: 2.1250
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 42120000
                    Iteration time: 1.09s
                      Time elapsed: 00:21:43
                               ETA: 00:06:08

################################################################################
                     [1m Learning iteration 1170/1500 [0m                     

                       Computation: 33380 steps/s (collection: 1.027s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0239
               Mean surrogate loss: -0.0151
                 Mean entropy loss: 13.7266
                       Mean reward: 16.39
               Mean episode length: 883.82
Episode_Reward/track_lin_vel_xy_exp: 1.0536
Episode_Reward/track_ang_vel_z_exp: 0.4897
       Episode_Reward/lin_vel_z_l2: -0.0516
      Episode_Reward/ang_vel_xy_l2: -0.1438
     Episode_Reward/dof_torques_l2: -0.0938
         Episode_Reward/dof_acc_l2: -0.2095
     Episode_Reward/action_rate_l2: -0.1883
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2818
Metrics/base_velocity/error_vel_xy: 0.4893
Metrics/base_velocity/error_vel_yaw: 0.5587
      Episode_Termination/time_out: 1.9167
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 42156000
                    Iteration time: 1.08s
                      Time elapsed: 00:21:44
                               ETA: 00:06:07

################################################################################
                     [1m Learning iteration 1171/1500 [0m                     

                       Computation: 33437 steps/s (collection: 1.024s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0267
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 13.7205
                       Mean reward: 16.52
               Mean episode length: 880.45
Episode_Reward/track_lin_vel_xy_exp: 1.0545
Episode_Reward/track_ang_vel_z_exp: 0.4787
       Episode_Reward/lin_vel_z_l2: -0.0424
      Episode_Reward/ang_vel_xy_l2: -0.1309
     Episode_Reward/dof_torques_l2: -0.0932
         Episode_Reward/dof_acc_l2: -0.1914
     Episode_Reward/action_rate_l2: -0.1786
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2848
Metrics/base_velocity/error_vel_xy: 0.4251
Metrics/base_velocity/error_vel_yaw: 0.5157
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 42192000
                    Iteration time: 1.08s
                      Time elapsed: 00:21:45
                               ETA: 00:06:06

################################################################################
                     [1m Learning iteration 1172/1500 [0m                     

                       Computation: 33173 steps/s (collection: 1.032s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0255
               Mean surrogate loss: -0.0156
                 Mean entropy loss: 13.7322
                       Mean reward: 17.96
               Mean episode length: 891.79
Episode_Reward/track_lin_vel_xy_exp: 1.0483
Episode_Reward/track_ang_vel_z_exp: 0.4693
       Episode_Reward/lin_vel_z_l2: -0.0399
      Episode_Reward/ang_vel_xy_l2: -0.1264
     Episode_Reward/dof_torques_l2: -0.0822
         Episode_Reward/dof_acc_l2: -0.1702
     Episode_Reward/action_rate_l2: -0.1673
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2868
Metrics/base_velocity/error_vel_xy: 0.3572
Metrics/base_velocity/error_vel_yaw: 0.4612
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 42228000
                    Iteration time: 1.09s
                      Time elapsed: 00:21:46
                               ETA: 00:06:05

################################################################################
                     [1m Learning iteration 1173/1500 [0m                     

                       Computation: 33521 steps/s (collection: 1.023s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0270
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 13.7465
                       Mean reward: 18.75
               Mean episode length: 907.91
Episode_Reward/track_lin_vel_xy_exp: 1.0068
Episode_Reward/track_ang_vel_z_exp: 0.4573
       Episode_Reward/lin_vel_z_l2: -0.0400
      Episode_Reward/ang_vel_xy_l2: -0.1283
     Episode_Reward/dof_torques_l2: -0.0888
         Episode_Reward/dof_acc_l2: -0.1787
     Episode_Reward/action_rate_l2: -0.1712
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2834
Metrics/base_velocity/error_vel_xy: 0.4083
Metrics/base_velocity/error_vel_yaw: 0.4854
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 42264000
                    Iteration time: 1.07s
                      Time elapsed: 00:21:47
                               ETA: 00:06:04

################################################################################
                     [1m Learning iteration 1174/1500 [0m                     

                       Computation: 33467 steps/s (collection: 1.023s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0274
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 13.7448
                       Mean reward: 18.98
               Mean episode length: 905.35
Episode_Reward/track_lin_vel_xy_exp: 1.1815
Episode_Reward/track_ang_vel_z_exp: 0.5125
       Episode_Reward/lin_vel_z_l2: -0.0442
      Episode_Reward/ang_vel_xy_l2: -0.1374
     Episode_Reward/dof_torques_l2: -0.0869
         Episode_Reward/dof_acc_l2: -0.1968
     Episode_Reward/action_rate_l2: -0.1867
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2848
Metrics/base_velocity/error_vel_xy: 0.3572
Metrics/base_velocity/error_vel_yaw: 0.5045
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 42300000
                    Iteration time: 1.08s
                      Time elapsed: 00:21:49
                               ETA: 00:06:03

################################################################################
                     [1m Learning iteration 1175/1500 [0m                     

                       Computation: 33796 steps/s (collection: 1.013s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0275
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 13.7437
                       Mean reward: 18.68
               Mean episode length: 884.22
Episode_Reward/track_lin_vel_xy_exp: 1.0520
Episode_Reward/track_ang_vel_z_exp: 0.4754
       Episode_Reward/lin_vel_z_l2: -0.0465
      Episode_Reward/ang_vel_xy_l2: -0.1322
     Episode_Reward/dof_torques_l2: -0.0850
         Episode_Reward/dof_acc_l2: -0.1824
     Episode_Reward/action_rate_l2: -0.1739
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2902
Metrics/base_velocity/error_vel_xy: 0.4063
Metrics/base_velocity/error_vel_yaw: 0.4782
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 42336000
                    Iteration time: 1.07s
                      Time elapsed: 00:21:50
                               ETA: 00:06:02

################################################################################
                     [1m Learning iteration 1176/1500 [0m                     

                       Computation: 33053 steps/s (collection: 1.038s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0266
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 13.7467
                       Mean reward: 17.15
               Mean episode length: 822.59
Episode_Reward/track_lin_vel_xy_exp: 0.9440
Episode_Reward/track_ang_vel_z_exp: 0.4312
       Episode_Reward/lin_vel_z_l2: -0.0413
      Episode_Reward/ang_vel_xy_l2: -0.1171
     Episode_Reward/dof_torques_l2: -0.0786
         Episode_Reward/dof_acc_l2: -0.1640
     Episode_Reward/action_rate_l2: -0.1577
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2883
Metrics/base_velocity/error_vel_xy: 0.3724
Metrics/base_velocity/error_vel_yaw: 0.4384
      Episode_Termination/time_out: 1.1250
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42372000
                    Iteration time: 1.09s
                      Time elapsed: 00:21:51
                               ETA: 00:06:00

################################################################################
                     [1m Learning iteration 1177/1500 [0m                     

                       Computation: 33025 steps/s (collection: 1.040s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0264
               Mean surrogate loss: -0.0149
                 Mean entropy loss: 13.7463
                       Mean reward: 14.74
               Mean episode length: 763.89
Episode_Reward/track_lin_vel_xy_exp: 0.8172
Episode_Reward/track_ang_vel_z_exp: 0.3767
       Episode_Reward/lin_vel_z_l2: -0.0419
      Episode_Reward/ang_vel_xy_l2: -0.1112
     Episode_Reward/dof_torques_l2: -0.0730
         Episode_Reward/dof_acc_l2: -0.1480
     Episode_Reward/action_rate_l2: -0.1394
      Episode_Reward/feet_air_time: -0.0009
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2903
Metrics/base_velocity/error_vel_xy: 0.3450
Metrics/base_velocity/error_vel_yaw: 0.4001
      Episode_Termination/time_out: 0.9583
  Episode_Termination/base_contact: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 42408000
                    Iteration time: 1.09s
                      Time elapsed: 00:21:52
                               ETA: 00:05:59

################################################################################
                     [1m Learning iteration 1178/1500 [0m                     

                       Computation: 31977 steps/s (collection: 1.075s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0249
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 13.7460
                       Mean reward: 14.31
               Mean episode length: 740.72
Episode_Reward/track_lin_vel_xy_exp: 0.8769
Episode_Reward/track_ang_vel_z_exp: 0.3886
       Episode_Reward/lin_vel_z_l2: -0.0448
      Episode_Reward/ang_vel_xy_l2: -0.1141
     Episode_Reward/dof_torques_l2: -0.0754
         Episode_Reward/dof_acc_l2: -0.1618
     Episode_Reward/action_rate_l2: -0.1454
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2904
Metrics/base_velocity/error_vel_xy: 0.3261
Metrics/base_velocity/error_vel_yaw: 0.4149
      Episode_Termination/time_out: 1.0833
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 42444000
                    Iteration time: 1.13s
                      Time elapsed: 00:21:53
                               ETA: 00:05:58

################################################################################
                     [1m Learning iteration 1179/1500 [0m                     

                       Computation: 33283 steps/s (collection: 1.030s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0226
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 13.7359
                       Mean reward: 15.17
               Mean episode length: 783.27
Episode_Reward/track_lin_vel_xy_exp: 1.1725
Episode_Reward/track_ang_vel_z_exp: 0.5161
       Episode_Reward/lin_vel_z_l2: -0.0480
      Episode_Reward/ang_vel_xy_l2: -0.1400
     Episode_Reward/dof_torques_l2: -0.0959
         Episode_Reward/dof_acc_l2: -0.2114
     Episode_Reward/action_rate_l2: -0.1949
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2871
Metrics/base_velocity/error_vel_xy: 0.3914
Metrics/base_velocity/error_vel_yaw: 0.5125
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 42480000
                    Iteration time: 1.08s
                      Time elapsed: 00:21:54
                               ETA: 00:05:57

################################################################################
                     [1m Learning iteration 1180/1500 [0m                     

                       Computation: 33042 steps/s (collection: 1.036s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0239
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 13.7361
                       Mean reward: 16.99
               Mean episode length: 836.43
Episode_Reward/track_lin_vel_xy_exp: 1.0983
Episode_Reward/track_ang_vel_z_exp: 0.4883
       Episode_Reward/lin_vel_z_l2: -0.0424
      Episode_Reward/ang_vel_xy_l2: -0.1338
     Episode_Reward/dof_torques_l2: -0.0885
         Episode_Reward/dof_acc_l2: -0.1953
     Episode_Reward/action_rate_l2: -0.1804
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2803
Metrics/base_velocity/error_vel_xy: 0.3896
Metrics/base_velocity/error_vel_yaw: 0.5041
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 42516000
                    Iteration time: 1.09s
                      Time elapsed: 00:21:55
                               ETA: 00:05:56

################################################################################
                     [1m Learning iteration 1181/1500 [0m                     

                       Computation: 33201 steps/s (collection: 1.031s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0248
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 13.7348
                       Mean reward: 18.62
               Mean episode length: 909.15
Episode_Reward/track_lin_vel_xy_exp: 1.1528
Episode_Reward/track_ang_vel_z_exp: 0.5238
       Episode_Reward/lin_vel_z_l2: -0.0456
      Episode_Reward/ang_vel_xy_l2: -0.1427
     Episode_Reward/dof_torques_l2: -0.0967
         Episode_Reward/dof_acc_l2: -0.2065
     Episode_Reward/action_rate_l2: -0.1950
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2769
Metrics/base_velocity/error_vel_xy: 0.4513
Metrics/base_velocity/error_vel_yaw: 0.5351
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 42552000
                    Iteration time: 1.08s
                      Time elapsed: 00:21:56
                               ETA: 00:05:55

################################################################################
                     [1m Learning iteration 1182/1500 [0m                     

                       Computation: 32570 steps/s (collection: 1.054s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0231
               Mean surrogate loss: -0.0152
                 Mean entropy loss: 13.7396
                       Mean reward: 17.56
               Mean episode length: 886.51
Episode_Reward/track_lin_vel_xy_exp: 0.8658
Episode_Reward/track_ang_vel_z_exp: 0.4015
       Episode_Reward/lin_vel_z_l2: -0.0461
      Episode_Reward/ang_vel_xy_l2: -0.1229
     Episode_Reward/dof_torques_l2: -0.0760
         Episode_Reward/dof_acc_l2: -0.1848
     Episode_Reward/action_rate_l2: -0.1570
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2696
Metrics/base_velocity/error_vel_xy: 0.4266
Metrics/base_velocity/error_vel_yaw: 0.4837
      Episode_Termination/time_out: 1.1667
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 42588000
                    Iteration time: 1.11s
                      Time elapsed: 00:21:57
                               ETA: 00:05:54

################################################################################
                     [1m Learning iteration 1183/1500 [0m                     

                       Computation: 32825 steps/s (collection: 1.045s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0263
               Mean surrogate loss: -0.0151
                 Mean entropy loss: 13.7318
                       Mean reward: 17.70
               Mean episode length: 885.24
Episode_Reward/track_lin_vel_xy_exp: 1.1150
Episode_Reward/track_ang_vel_z_exp: 0.4990
       Episode_Reward/lin_vel_z_l2: -0.0480
      Episode_Reward/ang_vel_xy_l2: -0.1403
     Episode_Reward/dof_torques_l2: -0.0918
         Episode_Reward/dof_acc_l2: -0.2025
     Episode_Reward/action_rate_l2: -0.1859
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2697
Metrics/base_velocity/error_vel_xy: 0.4142
Metrics/base_velocity/error_vel_yaw: 0.5253
      Episode_Termination/time_out: 2.0000
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 42624000
                    Iteration time: 1.10s
                      Time elapsed: 00:21:58
                               ETA: 00:05:53

################################################################################
                     [1m Learning iteration 1184/1500 [0m                     

                       Computation: 33445 steps/s (collection: 1.025s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0253
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 13.7290
                       Mean reward: 17.49
               Mean episode length: 875.69
Episode_Reward/track_lin_vel_xy_exp: 0.8975
Episode_Reward/track_ang_vel_z_exp: 0.4014
       Episode_Reward/lin_vel_z_l2: -0.0388
      Episode_Reward/ang_vel_xy_l2: -0.1131
     Episode_Reward/dof_torques_l2: -0.0784
         Episode_Reward/dof_acc_l2: -0.1748
     Episode_Reward/action_rate_l2: -0.1537
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2752
Metrics/base_velocity/error_vel_xy: 0.3535
Metrics/base_velocity/error_vel_yaw: 0.4377
      Episode_Termination/time_out: 1.1667
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 42660000
                    Iteration time: 1.08s
                      Time elapsed: 00:21:59
                               ETA: 00:05:51

################################################################################
                     [1m Learning iteration 1185/1500 [0m                     

                       Computation: 33276 steps/s (collection: 1.029s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0265
               Mean surrogate loss: -0.0157
                 Mean entropy loss: 13.7178
                       Mean reward: 16.87
               Mean episode length: 868.31
Episode_Reward/track_lin_vel_xy_exp: 0.9578
Episode_Reward/track_ang_vel_z_exp: 0.4681
       Episode_Reward/lin_vel_z_l2: -0.0475
      Episode_Reward/ang_vel_xy_l2: -0.1364
     Episode_Reward/dof_torques_l2: -0.0886
         Episode_Reward/dof_acc_l2: -0.2024
     Episode_Reward/action_rate_l2: -0.1774
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2710
Metrics/base_velocity/error_vel_xy: 0.5285
Metrics/base_velocity/error_vel_yaw: 0.5241
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 42696000
                    Iteration time: 1.08s
                      Time elapsed: 00:22:01
                               ETA: 00:05:50

################################################################################
                     [1m Learning iteration 1186/1500 [0m                     

                       Computation: 33224 steps/s (collection: 1.032s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0240
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 13.7175
                       Mean reward: 17.01
               Mean episode length: 865.30
Episode_Reward/track_lin_vel_xy_exp: 1.0392
Episode_Reward/track_ang_vel_z_exp: 0.4664
       Episode_Reward/lin_vel_z_l2: -0.0427
      Episode_Reward/ang_vel_xy_l2: -0.1267
     Episode_Reward/dof_torques_l2: -0.0870
         Episode_Reward/dof_acc_l2: -0.1830
     Episode_Reward/action_rate_l2: -0.1707
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2733
Metrics/base_velocity/error_vel_xy: 0.3793
Metrics/base_velocity/error_vel_yaw: 0.4702
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 42732000
                    Iteration time: 1.08s
                      Time elapsed: 00:22:02
                               ETA: 00:05:49

################################################################################
                     [1m Learning iteration 1187/1500 [0m                     

                       Computation: 33372 steps/s (collection: 1.026s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0236
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.7204
                       Mean reward: 16.55
               Mean episode length: 834.68
Episode_Reward/track_lin_vel_xy_exp: 0.9931
Episode_Reward/track_ang_vel_z_exp: 0.4368
       Episode_Reward/lin_vel_z_l2: -0.0460
      Episode_Reward/ang_vel_xy_l2: -0.1258
     Episode_Reward/dof_torques_l2: -0.0819
         Episode_Reward/dof_acc_l2: -0.1690
     Episode_Reward/action_rate_l2: -0.1629
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2682
Metrics/base_velocity/error_vel_xy: 0.3512
Metrics/base_velocity/error_vel_yaw: 0.4678
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 42768000
                    Iteration time: 1.08s
                      Time elapsed: 00:22:03
                               ETA: 00:05:48

################################################################################
                     [1m Learning iteration 1188/1500 [0m                     

                       Computation: 33456 steps/s (collection: 1.024s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0223
               Mean surrogate loss: -0.0159
                 Mean entropy loss: 13.7143
                       Mean reward: 16.26
               Mean episode length: 827.35
Episode_Reward/track_lin_vel_xy_exp: 0.8976
Episode_Reward/track_ang_vel_z_exp: 0.4267
       Episode_Reward/lin_vel_z_l2: -0.0436
      Episode_Reward/ang_vel_xy_l2: -0.1232
     Episode_Reward/dof_torques_l2: -0.0808
         Episode_Reward/dof_acc_l2: -0.1822
     Episode_Reward/action_rate_l2: -0.1616
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2654
Metrics/base_velocity/error_vel_xy: 0.4473
Metrics/base_velocity/error_vel_yaw: 0.4620
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 42804000
                    Iteration time: 1.08s
                      Time elapsed: 00:22:04
                               ETA: 00:05:47

################################################################################
                     [1m Learning iteration 1189/1500 [0m                     

                       Computation: 32718 steps/s (collection: 1.050s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0252
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 13.7116
                       Mean reward: 16.09
               Mean episode length: 834.87
Episode_Reward/track_lin_vel_xy_exp: 1.0941
Episode_Reward/track_ang_vel_z_exp: 0.4847
       Episode_Reward/lin_vel_z_l2: -0.0415
      Episode_Reward/ang_vel_xy_l2: -0.1316
     Episode_Reward/dof_torques_l2: -0.0900
         Episode_Reward/dof_acc_l2: -0.1883
     Episode_Reward/action_rate_l2: -0.1784
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2713
Metrics/base_velocity/error_vel_xy: 0.3866
Metrics/base_velocity/error_vel_yaw: 0.4846
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 42840000
                    Iteration time: 1.10s
                      Time elapsed: 00:22:05
                               ETA: 00:05:46

################################################################################
                     [1m Learning iteration 1190/1500 [0m                     

                       Computation: 33364 steps/s (collection: 1.028s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0291
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 13.7166
                       Mean reward: 18.18
               Mean episode length: 870.83
Episode_Reward/track_lin_vel_xy_exp: 1.1565
Episode_Reward/track_ang_vel_z_exp: 0.5034
       Episode_Reward/lin_vel_z_l2: -0.0384
      Episode_Reward/ang_vel_xy_l2: -0.1270
     Episode_Reward/dof_torques_l2: -0.0933
         Episode_Reward/dof_acc_l2: -0.1773
     Episode_Reward/action_rate_l2: -0.1788
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2700
Metrics/base_velocity/error_vel_xy: 0.3548
Metrics/base_velocity/error_vel_yaw: 0.4939
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 42876000
                    Iteration time: 1.08s
                      Time elapsed: 00:22:06
                               ETA: 00:05:45

################################################################################
                     [1m Learning iteration 1191/1500 [0m                     

                       Computation: 33525 steps/s (collection: 1.023s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0269
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 13.7204
                       Mean reward: 18.90
               Mean episode length: 878.70
Episode_Reward/track_lin_vel_xy_exp: 1.0620
Episode_Reward/track_ang_vel_z_exp: 0.4712
       Episode_Reward/lin_vel_z_l2: -0.0480
      Episode_Reward/ang_vel_xy_l2: -0.1329
     Episode_Reward/dof_torques_l2: -0.0845
         Episode_Reward/dof_acc_l2: -0.1857
     Episode_Reward/action_rate_l2: -0.1746
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2687
Metrics/base_velocity/error_vel_xy: 0.3657
Metrics/base_velocity/error_vel_yaw: 0.4796
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 42912000
                    Iteration time: 1.07s
                      Time elapsed: 00:22:07
                               ETA: 00:05:44

################################################################################
                     [1m Learning iteration 1192/1500 [0m                     

                       Computation: 33509 steps/s (collection: 1.022s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0236
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 13.7203
                       Mean reward: 18.43
               Mean episode length: 883.77
Episode_Reward/track_lin_vel_xy_exp: 1.0378
Episode_Reward/track_ang_vel_z_exp: 0.4946
       Episode_Reward/lin_vel_z_l2: -0.0478
      Episode_Reward/ang_vel_xy_l2: -0.1444
     Episode_Reward/dof_torques_l2: -0.0953
         Episode_Reward/dof_acc_l2: -0.2061
     Episode_Reward/action_rate_l2: -0.1854
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2748
Metrics/base_velocity/error_vel_xy: 0.5278
Metrics/base_velocity/error_vel_yaw: 0.5421
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 42948000
                    Iteration time: 1.07s
                      Time elapsed: 00:22:08
                               ETA: 00:05:43

################################################################################
                     [1m Learning iteration 1193/1500 [0m                     

                       Computation: 33252 steps/s (collection: 1.032s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0223
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 13.7205
                       Mean reward: 17.74
               Mean episode length: 894.10
Episode_Reward/track_lin_vel_xy_exp: 1.0498
Episode_Reward/track_ang_vel_z_exp: 0.4723
       Episode_Reward/lin_vel_z_l2: -0.0446
      Episode_Reward/ang_vel_xy_l2: -0.1300
     Episode_Reward/dof_torques_l2: -0.0900
         Episode_Reward/dof_acc_l2: -0.1875
     Episode_Reward/action_rate_l2: -0.1773
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2810
Metrics/base_velocity/error_vel_xy: 0.4005
Metrics/base_velocity/error_vel_yaw: 0.4887
      Episode_Termination/time_out: 1.9167
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 42984000
                    Iteration time: 1.08s
                      Time elapsed: 00:22:09
                               ETA: 00:05:41

################################################################################
                     [1m Learning iteration 1194/1500 [0m                     

                       Computation: 33815 steps/s (collection: 1.014s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0273
               Mean surrogate loss: -0.0149
                 Mean entropy loss: 13.7211
                       Mean reward: 17.70
               Mean episode length: 878.27
Episode_Reward/track_lin_vel_xy_exp: 1.0657
Episode_Reward/track_ang_vel_z_exp: 0.4720
       Episode_Reward/lin_vel_z_l2: -0.0410
      Episode_Reward/ang_vel_xy_l2: -0.1309
     Episode_Reward/dof_torques_l2: -0.0888
         Episode_Reward/dof_acc_l2: -0.1849
     Episode_Reward/action_rate_l2: -0.1750
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2869
Metrics/base_velocity/error_vel_xy: 0.3772
Metrics/base_velocity/error_vel_yaw: 0.4940
      Episode_Termination/time_out: 1.9167
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 43020000
                    Iteration time: 1.06s
                      Time elapsed: 00:22:10
                               ETA: 00:05:40

################################################################################
                     [1m Learning iteration 1195/1500 [0m                     

                       Computation: 33386 steps/s (collection: 1.028s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0252
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 13.7191
                       Mean reward: 17.98
               Mean episode length: 857.65
Episode_Reward/track_lin_vel_xy_exp: 1.0503
Episode_Reward/track_ang_vel_z_exp: 0.4609
       Episode_Reward/lin_vel_z_l2: -0.0455
      Episode_Reward/ang_vel_xy_l2: -0.1269
     Episode_Reward/dof_torques_l2: -0.0847
         Episode_Reward/dof_acc_l2: -0.1847
     Episode_Reward/action_rate_l2: -0.1730
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2840
Metrics/base_velocity/error_vel_xy: 0.3619
Metrics/base_velocity/error_vel_yaw: 0.4790
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 43056000
                    Iteration time: 1.08s
                      Time elapsed: 00:22:11
                               ETA: 00:05:39

################################################################################
                     [1m Learning iteration 1196/1500 [0m                     

                       Computation: 33901 steps/s (collection: 1.011s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0282
               Mean surrogate loss: -0.0163
                 Mean entropy loss: 13.7164
                       Mean reward: 18.27
               Mean episode length: 880.74
Episode_Reward/track_lin_vel_xy_exp: 1.1239
Episode_Reward/track_ang_vel_z_exp: 0.4984
       Episode_Reward/lin_vel_z_l2: -0.0482
      Episode_Reward/ang_vel_xy_l2: -0.1401
     Episode_Reward/dof_torques_l2: -0.0913
         Episode_Reward/dof_acc_l2: -0.2092
     Episode_Reward/action_rate_l2: -0.1891
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2867
Metrics/base_velocity/error_vel_xy: 0.4180
Metrics/base_velocity/error_vel_yaw: 0.5302
      Episode_Termination/time_out: 2.0417
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 43092000
                    Iteration time: 1.06s
                      Time elapsed: 00:22:12
                               ETA: 00:05:38

################################################################################
                     [1m Learning iteration 1197/1500 [0m                     

                       Computation: 34102 steps/s (collection: 1.004s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0235
               Mean surrogate loss: -0.0158
                 Mean entropy loss: 13.7152
                       Mean reward: 18.26
               Mean episode length: 892.33
Episode_Reward/track_lin_vel_xy_exp: 0.9576
Episode_Reward/track_ang_vel_z_exp: 0.4211
       Episode_Reward/lin_vel_z_l2: -0.0502
      Episode_Reward/ang_vel_xy_l2: -0.1229
     Episode_Reward/dof_torques_l2: -0.0781
         Episode_Reward/dof_acc_l2: -0.1598
     Episode_Reward/action_rate_l2: -0.1558
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2900
Metrics/base_velocity/error_vel_xy: 0.3315
Metrics/base_velocity/error_vel_yaw: 0.4521
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 43128000
                    Iteration time: 1.06s
                      Time elapsed: 00:22:13
                               ETA: 00:05:37

################################################################################
                     [1m Learning iteration 1198/1500 [0m                     

                       Computation: 33687 steps/s (collection: 1.017s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0234
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 13.7177
                       Mean reward: 18.12
               Mean episode length: 884.41
Episode_Reward/track_lin_vel_xy_exp: 1.0941
Episode_Reward/track_ang_vel_z_exp: 0.4850
       Episode_Reward/lin_vel_z_l2: -0.0536
      Episode_Reward/ang_vel_xy_l2: -0.1404
     Episode_Reward/dof_torques_l2: -0.0858
         Episode_Reward/dof_acc_l2: -0.2026
     Episode_Reward/action_rate_l2: -0.1821
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2939
Metrics/base_velocity/error_vel_xy: 0.3956
Metrics/base_velocity/error_vel_yaw: 0.5065
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 43164000
                    Iteration time: 1.07s
                      Time elapsed: 00:22:15
                               ETA: 00:05:36

################################################################################
                     [1m Learning iteration 1199/1500 [0m                     

                       Computation: 33316 steps/s (collection: 1.028s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0214
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 13.7186
                       Mean reward: 18.43
               Mean episode length: 903.07
Episode_Reward/track_lin_vel_xy_exp: 1.1286
Episode_Reward/track_ang_vel_z_exp: 0.4971
       Episode_Reward/lin_vel_z_l2: -0.0481
      Episode_Reward/ang_vel_xy_l2: -0.1356
     Episode_Reward/dof_torques_l2: -0.0935
         Episode_Reward/dof_acc_l2: -0.1912
     Episode_Reward/action_rate_l2: -0.1836
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2883
Metrics/base_velocity/error_vel_xy: 0.3848
Metrics/base_velocity/error_vel_yaw: 0.5116
      Episode_Termination/time_out: 1.9167
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 43200000
                    Iteration time: 1.08s
                      Time elapsed: 00:22:16
                               ETA: 00:05:35

################################################################################
                     [1m Learning iteration 1200/1500 [0m                     

                       Computation: 33639 steps/s (collection: 1.017s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0257
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 13.7047
                       Mean reward: 18.39
               Mean episode length: 916.15
Episode_Reward/track_lin_vel_xy_exp: 1.0266
Episode_Reward/track_ang_vel_z_exp: 0.4791
       Episode_Reward/lin_vel_z_l2: -0.0437
      Episode_Reward/ang_vel_xy_l2: -0.1369
     Episode_Reward/dof_torques_l2: -0.0940
         Episode_Reward/dof_acc_l2: -0.2004
     Episode_Reward/action_rate_l2: -0.1830
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2902
Metrics/base_velocity/error_vel_xy: 0.4762
Metrics/base_velocity/error_vel_yaw: 0.5211
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 43236000
                    Iteration time: 1.07s
                      Time elapsed: 00:22:17
                               ETA: 00:05:34

################################################################################
                     [1m Learning iteration 1201/1500 [0m                     

                       Computation: 33783 steps/s (collection: 1.016s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0226
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 13.7021
                       Mean reward: 18.88
               Mean episode length: 923.11
Episode_Reward/track_lin_vel_xy_exp: 1.1024
Episode_Reward/track_ang_vel_z_exp: 0.4820
       Episode_Reward/lin_vel_z_l2: -0.0439
      Episode_Reward/ang_vel_xy_l2: -0.1342
     Episode_Reward/dof_torques_l2: -0.0871
         Episode_Reward/dof_acc_l2: -0.1946
     Episode_Reward/action_rate_l2: -0.1782
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2902
Metrics/base_velocity/error_vel_xy: 0.3734
Metrics/base_velocity/error_vel_yaw: 0.4865
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 43272000
                    Iteration time: 1.07s
                      Time elapsed: 00:22:18
                               ETA: 00:05:32

################################################################################
                     [1m Learning iteration 1202/1500 [0m                     

                       Computation: 34071 steps/s (collection: 1.007s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0221
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 13.7066
                       Mean reward: 18.27
               Mean episode length: 917.70
Episode_Reward/track_lin_vel_xy_exp: 1.0553
Episode_Reward/track_ang_vel_z_exp: 0.4769
       Episode_Reward/lin_vel_z_l2: -0.0483
      Episode_Reward/ang_vel_xy_l2: -0.1423
     Episode_Reward/dof_torques_l2: -0.0910
         Episode_Reward/dof_acc_l2: -0.2022
     Episode_Reward/action_rate_l2: -0.1838
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.2920
Metrics/base_velocity/error_vel_xy: 0.4437
Metrics/base_velocity/error_vel_yaw: 0.5306
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 43308000
                    Iteration time: 1.06s
                      Time elapsed: 00:22:19
                               ETA: 00:05:31

################################################################################
                     [1m Learning iteration 1203/1500 [0m                     

                       Computation: 33846 steps/s (collection: 1.012s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0263
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 13.7067
                       Mean reward: 17.06
               Mean episode length: 879.87
Episode_Reward/track_lin_vel_xy_exp: 0.9055
Episode_Reward/track_ang_vel_z_exp: 0.4249
       Episode_Reward/lin_vel_z_l2: -0.0466
      Episode_Reward/ang_vel_xy_l2: -0.1303
     Episode_Reward/dof_torques_l2: -0.0813
         Episode_Reward/dof_acc_l2: -0.1868
     Episode_Reward/action_rate_l2: -0.1621
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3018
Metrics/base_velocity/error_vel_xy: 0.4514
Metrics/base_velocity/error_vel_yaw: 0.4887
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 43344000
                    Iteration time: 1.06s
                      Time elapsed: 00:22:20
                               ETA: 00:05:30

################################################################################
                     [1m Learning iteration 1204/1500 [0m                     

                       Computation: 33359 steps/s (collection: 1.026s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0260
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 13.7136
                       Mean reward: 15.85
               Mean episode length: 857.39
Episode_Reward/track_lin_vel_xy_exp: 0.9962
Episode_Reward/track_ang_vel_z_exp: 0.4644
       Episode_Reward/lin_vel_z_l2: -0.0497
      Episode_Reward/ang_vel_xy_l2: -0.1355
     Episode_Reward/dof_torques_l2: -0.0894
         Episode_Reward/dof_acc_l2: -0.1970
     Episode_Reward/action_rate_l2: -0.1765
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3043
Metrics/base_velocity/error_vel_xy: 0.4596
Metrics/base_velocity/error_vel_yaw: 0.5110
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 43380000
                    Iteration time: 1.08s
                      Time elapsed: 00:22:21
                               ETA: 00:05:29

################################################################################
                     [1m Learning iteration 1205/1500 [0m                     

                       Computation: 33773 steps/s (collection: 1.014s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0207
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 13.7136
                       Mean reward: 15.67
               Mean episode length: 834.52
Episode_Reward/track_lin_vel_xy_exp: 0.9205
Episode_Reward/track_ang_vel_z_exp: 0.4118
       Episode_Reward/lin_vel_z_l2: -0.0406
      Episode_Reward/ang_vel_xy_l2: -0.1184
     Episode_Reward/dof_torques_l2: -0.0742
         Episode_Reward/dof_acc_l2: -0.1569
     Episode_Reward/action_rate_l2: -0.1504
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3051
Metrics/base_velocity/error_vel_xy: 0.3648
Metrics/base_velocity/error_vel_yaw: 0.4514
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 43416000
                    Iteration time: 1.07s
                      Time elapsed: 00:22:22
                               ETA: 00:05:28

################################################################################
                     [1m Learning iteration 1206/1500 [0m                     

                       Computation: 33865 steps/s (collection: 1.013s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0251
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 13.7119
                       Mean reward: 17.45
               Mean episode length: 872.34
Episode_Reward/track_lin_vel_xy_exp: 1.1046
Episode_Reward/track_ang_vel_z_exp: 0.5021
       Episode_Reward/lin_vel_z_l2: -0.0452
      Episode_Reward/ang_vel_xy_l2: -0.1408
     Episode_Reward/dof_torques_l2: -0.0949
         Episode_Reward/dof_acc_l2: -0.2084
     Episode_Reward/action_rate_l2: -0.1871
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3081
Metrics/base_velocity/error_vel_xy: 0.4292
Metrics/base_velocity/error_vel_yaw: 0.5185
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 43452000
                    Iteration time: 1.06s
                      Time elapsed: 00:22:23
                               ETA: 00:05:27

################################################################################
                     [1m Learning iteration 1207/1500 [0m                     

                       Computation: 34124 steps/s (collection: 1.005s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0212
               Mean surrogate loss: -0.0152
                 Mean entropy loss: 13.7106
                       Mean reward: 19.66
               Mean episode length: 906.37
Episode_Reward/track_lin_vel_xy_exp: 1.2224
Episode_Reward/track_ang_vel_z_exp: 0.5187
       Episode_Reward/lin_vel_z_l2: -0.0413
      Episode_Reward/ang_vel_xy_l2: -0.1346
     Episode_Reward/dof_torques_l2: -0.0882
         Episode_Reward/dof_acc_l2: -0.1846
     Episode_Reward/action_rate_l2: -0.1843
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3115
Metrics/base_velocity/error_vel_xy: 0.3066
Metrics/base_velocity/error_vel_yaw: 0.4838
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 43488000
                    Iteration time: 1.05s
                      Time elapsed: 00:22:24
                               ETA: 00:05:26

################################################################################
                     [1m Learning iteration 1208/1500 [0m                     

                       Computation: 34126 steps/s (collection: 1.004s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0234
               Mean surrogate loss: -0.0151
                 Mean entropy loss: 13.7037
                       Mean reward: 19.69
               Mean episode length: 926.11
Episode_Reward/track_lin_vel_xy_exp: 1.1480
Episode_Reward/track_ang_vel_z_exp: 0.5160
       Episode_Reward/lin_vel_z_l2: -0.0449
      Episode_Reward/ang_vel_xy_l2: -0.1441
     Episode_Reward/dof_torques_l2: -0.0904
         Episode_Reward/dof_acc_l2: -0.1979
     Episode_Reward/action_rate_l2: -0.1890
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3095
Metrics/base_velocity/error_vel_xy: 0.4305
Metrics/base_velocity/error_vel_yaw: 0.5312
      Episode_Termination/time_out: 1.9583
  Episode_Termination/base_contact: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 43524000
                    Iteration time: 1.05s
                      Time elapsed: 00:22:25
                               ETA: 00:05:25

################################################################################
                     [1m Learning iteration 1209/1500 [0m                     

                       Computation: 32913 steps/s (collection: 1.042s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0225
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 13.6998
                       Mean reward: 18.96
               Mean episode length: 910.20
Episode_Reward/track_lin_vel_xy_exp: 1.0554
Episode_Reward/track_ang_vel_z_exp: 0.4692
       Episode_Reward/lin_vel_z_l2: -0.0386
      Episode_Reward/ang_vel_xy_l2: -0.1328
     Episode_Reward/dof_torques_l2: -0.0882
         Episode_Reward/dof_acc_l2: -0.1817
     Episode_Reward/action_rate_l2: -0.1722
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3085
Metrics/base_velocity/error_vel_xy: 0.4036
Metrics/base_velocity/error_vel_yaw: 0.5164
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 43560000
                    Iteration time: 1.09s
                      Time elapsed: 00:22:26
                               ETA: 00:05:23

################################################################################
                     [1m Learning iteration 1210/1500 [0m                     

                       Computation: 33196 steps/s (collection: 1.035s, learning 0.049s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0261
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 13.6974
                       Mean reward: 17.59
               Mean episode length: 897.53
Episode_Reward/track_lin_vel_xy_exp: 0.8902
Episode_Reward/track_ang_vel_z_exp: 0.4603
       Episode_Reward/lin_vel_z_l2: -0.0488
      Episode_Reward/ang_vel_xy_l2: -0.1400
     Episode_Reward/dof_torques_l2: -0.0912
         Episode_Reward/dof_acc_l2: -0.1982
     Episode_Reward/action_rate_l2: -0.1758
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3119
Metrics/base_velocity/error_vel_xy: 0.6137
Metrics/base_velocity/error_vel_yaw: 0.5301
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 43596000
                    Iteration time: 1.08s
                      Time elapsed: 00:22:27
                               ETA: 00:05:22

################################################################################
                     [1m Learning iteration 1211/1500 [0m                     

                       Computation: 33716 steps/s (collection: 1.018s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0287
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 13.6974
                       Mean reward: 16.38
               Mean episode length: 853.95
Episode_Reward/track_lin_vel_xy_exp: 0.9812
Episode_Reward/track_ang_vel_z_exp: 0.4459
       Episode_Reward/lin_vel_z_l2: -0.0401
      Episode_Reward/ang_vel_xy_l2: -0.1249
     Episode_Reward/dof_torques_l2: -0.0857
         Episode_Reward/dof_acc_l2: -0.1754
     Episode_Reward/action_rate_l2: -0.1656
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3137
Metrics/base_velocity/error_vel_xy: 0.3960
Metrics/base_velocity/error_vel_yaw: 0.4774
      Episode_Termination/time_out: 1.9167
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 43632000
                    Iteration time: 1.07s
                      Time elapsed: 00:22:28
                               ETA: 00:05:21

################################################################################
                     [1m Learning iteration 1212/1500 [0m                     

                       Computation: 33726 steps/s (collection: 1.015s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0242
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 13.7000
                       Mean reward: 16.03
               Mean episode length: 838.57
Episode_Reward/track_lin_vel_xy_exp: 1.0286
Episode_Reward/track_ang_vel_z_exp: 0.4759
       Episode_Reward/lin_vel_z_l2: -0.0476
      Episode_Reward/ang_vel_xy_l2: -0.1377
     Episode_Reward/dof_torques_l2: -0.0897
         Episode_Reward/dof_acc_l2: -0.2009
     Episode_Reward/action_rate_l2: -0.1808
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3092
Metrics/base_velocity/error_vel_xy: 0.4728
Metrics/base_velocity/error_vel_yaw: 0.5427
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 43668000
                    Iteration time: 1.07s
                      Time elapsed: 00:22:29
                               ETA: 00:05:20

################################################################################
                     [1m Learning iteration 1213/1500 [0m                     

                       Computation: 33474 steps/s (collection: 1.024s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0243
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 13.6984
                       Mean reward: 16.82
               Mean episode length: 886.37
Episode_Reward/track_lin_vel_xy_exp: 1.0736
Episode_Reward/track_ang_vel_z_exp: 0.4973
       Episode_Reward/lin_vel_z_l2: -0.0496
      Episode_Reward/ang_vel_xy_l2: -0.1415
     Episode_Reward/dof_torques_l2: -0.0986
         Episode_Reward/dof_acc_l2: -0.2126
     Episode_Reward/action_rate_l2: -0.1904
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3122
Metrics/base_velocity/error_vel_xy: 0.4570
Metrics/base_velocity/error_vel_yaw: 0.5266
      Episode_Termination/time_out: 2.0417
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 43704000
                    Iteration time: 1.08s
                      Time elapsed: 00:22:31
                               ETA: 00:05:19

################################################################################
                     [1m Learning iteration 1214/1500 [0m                     

                       Computation: 33448 steps/s (collection: 1.024s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0235
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 13.7046
                       Mean reward: 17.10
               Mean episode length: 893.41
Episode_Reward/track_lin_vel_xy_exp: 0.9669
Episode_Reward/track_ang_vel_z_exp: 0.4370
       Episode_Reward/lin_vel_z_l2: -0.0497
      Episode_Reward/ang_vel_xy_l2: -0.1356
     Episode_Reward/dof_torques_l2: -0.0813
         Episode_Reward/dof_acc_l2: -0.1877
     Episode_Reward/action_rate_l2: -0.1660
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3127
Metrics/base_velocity/error_vel_xy: 0.4126
Metrics/base_velocity/error_vel_yaw: 0.4993
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 43740000
                    Iteration time: 1.08s
                      Time elapsed: 00:22:32
                               ETA: 00:05:18

################################################################################
                     [1m Learning iteration 1215/1500 [0m                     

                       Computation: 33694 steps/s (collection: 1.015s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0266
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 13.7048
                       Mean reward: 17.84
               Mean episode length: 891.94
Episode_Reward/track_lin_vel_xy_exp: 1.1408
Episode_Reward/track_ang_vel_z_exp: 0.5030
       Episode_Reward/lin_vel_z_l2: -0.0432
      Episode_Reward/ang_vel_xy_l2: -0.1351
     Episode_Reward/dof_torques_l2: -0.0929
         Episode_Reward/dof_acc_l2: -0.1978
     Episode_Reward/action_rate_l2: -0.1853
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3119
Metrics/base_velocity/error_vel_xy: 0.3912
Metrics/base_velocity/error_vel_yaw: 0.5107
      Episode_Termination/time_out: 2.2083
  Episode_Termination/base_contact: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 43776000
                    Iteration time: 1.07s
                      Time elapsed: 00:22:33
                               ETA: 00:05:17

################################################################################
                     [1m Learning iteration 1216/1500 [0m                     

                       Computation: 33830 steps/s (collection: 1.012s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0236
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 13.6972
                       Mean reward: 19.08
               Mean episode length: 918.74
Episode_Reward/track_lin_vel_xy_exp: 1.0961
Episode_Reward/track_ang_vel_z_exp: 0.4909
       Episode_Reward/lin_vel_z_l2: -0.0405
      Episode_Reward/ang_vel_xy_l2: -0.1349
     Episode_Reward/dof_torques_l2: -0.0864
         Episode_Reward/dof_acc_l2: -0.1844
     Episode_Reward/action_rate_l2: -0.1792
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3160
Metrics/base_velocity/error_vel_xy: 0.4076
Metrics/base_velocity/error_vel_yaw: 0.5038
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 43812000
                    Iteration time: 1.06s
                      Time elapsed: 00:22:34
                               ETA: 00:05:16

################################################################################
                     [1m Learning iteration 1217/1500 [0m                     

                       Computation: 33420 steps/s (collection: 1.026s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0243
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.6992
                       Mean reward: 18.31
               Mean episode length: 890.46
Episode_Reward/track_lin_vel_xy_exp: 1.0954
Episode_Reward/track_ang_vel_z_exp: 0.4906
       Episode_Reward/lin_vel_z_l2: -0.0483
      Episode_Reward/ang_vel_xy_l2: -0.1377
     Episode_Reward/dof_torques_l2: -0.0923
         Episode_Reward/dof_acc_l2: -0.1941
     Episode_Reward/action_rate_l2: -0.1813
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3176
Metrics/base_velocity/error_vel_xy: 0.4234
Metrics/base_velocity/error_vel_yaw: 0.5222
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 43848000
                    Iteration time: 1.08s
                      Time elapsed: 00:22:35
                               ETA: 00:05:14

################################################################################
                     [1m Learning iteration 1218/1500 [0m                     

                       Computation: 33484 steps/s (collection: 1.023s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0251
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 13.7039
                       Mean reward: 16.48
               Mean episode length: 852.91
Episode_Reward/track_lin_vel_xy_exp: 0.9981
Episode_Reward/track_ang_vel_z_exp: 0.4524
       Episode_Reward/lin_vel_z_l2: -0.0427
      Episode_Reward/ang_vel_xy_l2: -0.1307
     Episode_Reward/dof_torques_l2: -0.0859
         Episode_Reward/dof_acc_l2: -0.1957
     Episode_Reward/action_rate_l2: -0.1735
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3274
Metrics/base_velocity/error_vel_xy: 0.4121
Metrics/base_velocity/error_vel_yaw: 0.5003
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 43884000
                    Iteration time: 1.08s
                      Time elapsed: 00:22:36
                               ETA: 00:05:13

################################################################################
                     [1m Learning iteration 1219/1500 [0m                     

                       Computation: 33095 steps/s (collection: 1.037s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0244
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 13.7036
                       Mean reward: 16.62
               Mean episode length: 856.83
Episode_Reward/track_lin_vel_xy_exp: 1.0684
Episode_Reward/track_ang_vel_z_exp: 0.4841
       Episode_Reward/lin_vel_z_l2: -0.0479
      Episode_Reward/ang_vel_xy_l2: -0.1397
     Episode_Reward/dof_torques_l2: -0.0921
         Episode_Reward/dof_acc_l2: -0.1916
     Episode_Reward/action_rate_l2: -0.1821
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3340
Metrics/base_velocity/error_vel_xy: 0.4481
Metrics/base_velocity/error_vel_yaw: 0.5275
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 43920000
                    Iteration time: 1.09s
                      Time elapsed: 00:22:37
                               ETA: 00:05:12

################################################################################
                     [1m Learning iteration 1220/1500 [0m                     

                       Computation: 33393 steps/s (collection: 1.026s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0275
               Mean surrogate loss: -0.0150
                 Mean entropy loss: 13.7045
                       Mean reward: 17.07
               Mean episode length: 864.75
Episode_Reward/track_lin_vel_xy_exp: 1.0020
Episode_Reward/track_ang_vel_z_exp: 0.4562
       Episode_Reward/lin_vel_z_l2: -0.0477
      Episode_Reward/ang_vel_xy_l2: -0.1274
     Episode_Reward/dof_torques_l2: -0.0890
         Episode_Reward/dof_acc_l2: -0.1712
     Episode_Reward/action_rate_l2: -0.1683
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3242
Metrics/base_velocity/error_vel_xy: 0.4035
Metrics/base_velocity/error_vel_yaw: 0.4879
      Episode_Termination/time_out: 1.0833
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 43956000
                    Iteration time: 1.08s
                      Time elapsed: 00:22:38
                               ETA: 00:05:11

################################################################################
                     [1m Learning iteration 1221/1500 [0m                     

                       Computation: 33205 steps/s (collection: 1.032s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0219
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 13.7081
                       Mean reward: 18.10
               Mean episode length: 886.81
Episode_Reward/track_lin_vel_xy_exp: 1.1368
Episode_Reward/track_ang_vel_z_exp: 0.4950
       Episode_Reward/lin_vel_z_l2: -0.0420
      Episode_Reward/ang_vel_xy_l2: -0.1324
     Episode_Reward/dof_torques_l2: -0.0907
         Episode_Reward/dof_acc_l2: -0.1876
     Episode_Reward/action_rate_l2: -0.1805
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3230
Metrics/base_velocity/error_vel_xy: 0.3503
Metrics/base_velocity/error_vel_yaw: 0.4869
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 43992000
                    Iteration time: 1.08s
                      Time elapsed: 00:22:39
                               ETA: 00:05:10

################################################################################
                     [1m Learning iteration 1222/1500 [0m                     

                       Computation: 33623 steps/s (collection: 1.021s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0237
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 13.6988
                       Mean reward: 18.97
               Mean episode length: 899.33
Episode_Reward/track_lin_vel_xy_exp: 1.0501
Episode_Reward/track_ang_vel_z_exp: 0.4704
       Episode_Reward/lin_vel_z_l2: -0.0462
      Episode_Reward/ang_vel_xy_l2: -0.1366
     Episode_Reward/dof_torques_l2: -0.0905
         Episode_Reward/dof_acc_l2: -0.2058
     Episode_Reward/action_rate_l2: -0.1806
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3149
Metrics/base_velocity/error_vel_xy: 0.4147
Metrics/base_velocity/error_vel_yaw: 0.5188
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 44028000
                    Iteration time: 1.07s
                      Time elapsed: 00:22:40
                               ETA: 00:05:09

################################################################################
                     [1m Learning iteration 1223/1500 [0m                     

                       Computation: 32975 steps/s (collection: 1.040s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0210
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 13.6849
                       Mean reward: 18.30
               Mean episode length: 874.07
Episode_Reward/track_lin_vel_xy_exp: 1.0490
Episode_Reward/track_ang_vel_z_exp: 0.4601
       Episode_Reward/lin_vel_z_l2: -0.0452
      Episode_Reward/ang_vel_xy_l2: -0.1294
     Episode_Reward/dof_torques_l2: -0.0805
         Episode_Reward/dof_acc_l2: -0.1851
     Episode_Reward/action_rate_l2: -0.1713
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3232
Metrics/base_velocity/error_vel_xy: 0.3591
Metrics/base_velocity/error_vel_yaw: 0.4813
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 44064000
                    Iteration time: 1.09s
                      Time elapsed: 00:22:41
                               ETA: 00:05:08

################################################################################
                     [1m Learning iteration 1224/1500 [0m                     

                       Computation: 33297 steps/s (collection: 1.028s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0223
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 13.6738
                       Mean reward: 17.18
               Mean episode length: 841.96
Episode_Reward/track_lin_vel_xy_exp: 0.8992
Episode_Reward/track_ang_vel_z_exp: 0.4217
       Episode_Reward/lin_vel_z_l2: -0.0422
      Episode_Reward/ang_vel_xy_l2: -0.1235
     Episode_Reward/dof_torques_l2: -0.0847
         Episode_Reward/dof_acc_l2: -0.1797
     Episode_Reward/action_rate_l2: -0.1611
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3264
Metrics/base_velocity/error_vel_xy: 0.4374
Metrics/base_velocity/error_vel_yaw: 0.4820
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 44100000
                    Iteration time: 1.08s
                      Time elapsed: 00:22:42
                               ETA: 00:05:07

################################################################################
                     [1m Learning iteration 1225/1500 [0m                     

                       Computation: 33751 steps/s (collection: 1.016s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0259
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 13.6752
                       Mean reward: 16.52
               Mean episode length: 823.53
Episode_Reward/track_lin_vel_xy_exp: 0.9081
Episode_Reward/track_ang_vel_z_exp: 0.4121
       Episode_Reward/lin_vel_z_l2: -0.0384
      Episode_Reward/ang_vel_xy_l2: -0.1128
     Episode_Reward/dof_torques_l2: -0.0722
         Episode_Reward/dof_acc_l2: -0.1618
     Episode_Reward/action_rate_l2: -0.1494
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3157
Metrics/base_velocity/error_vel_xy: 0.3528
Metrics/base_velocity/error_vel_yaw: 0.4202
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 44136000
                    Iteration time: 1.07s
                      Time elapsed: 00:22:43
                               ETA: 00:05:05

################################################################################
                     [1m Learning iteration 1226/1500 [0m                     

                       Computation: 32787 steps/s (collection: 1.046s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0216
               Mean surrogate loss: -0.0152
                 Mean entropy loss: 13.6745
                       Mean reward: 17.42
               Mean episode length: 843.94
Episode_Reward/track_lin_vel_xy_exp: 1.0596
Episode_Reward/track_ang_vel_z_exp: 0.4782
       Episode_Reward/lin_vel_z_l2: -0.0447
      Episode_Reward/ang_vel_xy_l2: -0.1350
     Episode_Reward/dof_torques_l2: -0.0888
         Episode_Reward/dof_acc_l2: -0.1980
     Episode_Reward/action_rate_l2: -0.1783
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3127
Metrics/base_velocity/error_vel_xy: 0.4193
Metrics/base_velocity/error_vel_yaw: 0.5136
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 44172000
                    Iteration time: 1.10s
                      Time elapsed: 00:22:45
                               ETA: 00:05:04

################################################################################
                     [1m Learning iteration 1227/1500 [0m                     

                       Computation: 33159 steps/s (collection: 1.034s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0215
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.6756
                       Mean reward: 17.95
               Mean episode length: 884.85
Episode_Reward/track_lin_vel_xy_exp: 1.0827
Episode_Reward/track_ang_vel_z_exp: 0.5035
       Episode_Reward/lin_vel_z_l2: -0.0460
      Episode_Reward/ang_vel_xy_l2: -0.1440
     Episode_Reward/dof_torques_l2: -0.0936
         Episode_Reward/dof_acc_l2: -0.2030
     Episode_Reward/action_rate_l2: -0.1865
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3134
Metrics/base_velocity/error_vel_xy: 0.4866
Metrics/base_velocity/error_vel_yaw: 0.5452
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 44208000
                    Iteration time: 1.09s
                      Time elapsed: 00:22:46
                               ETA: 00:05:03

################################################################################
                     [1m Learning iteration 1228/1500 [0m                     

                       Computation: 33712 steps/s (collection: 1.017s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0215
               Mean surrogate loss: -0.0123
                 Mean entropy loss: 13.6770
                       Mean reward: 18.50
               Mean episode length: 899.34
Episode_Reward/track_lin_vel_xy_exp: 1.1437
Episode_Reward/track_ang_vel_z_exp: 0.4998
       Episode_Reward/lin_vel_z_l2: -0.0397
      Episode_Reward/ang_vel_xy_l2: -0.1267
     Episode_Reward/dof_torques_l2: -0.0925
         Episode_Reward/dof_acc_l2: -0.1797
     Episode_Reward/action_rate_l2: -0.1785
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3151
Metrics/base_velocity/error_vel_xy: 0.3327
Metrics/base_velocity/error_vel_yaw: 0.4669
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 44244000
                    Iteration time: 1.07s
                      Time elapsed: 00:22:47
                               ETA: 00:05:02

################################################################################
                     [1m Learning iteration 1229/1500 [0m                     

                       Computation: 33797 steps/s (collection: 1.015s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0245
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 13.6814
                       Mean reward: 17.87
               Mean episode length: 882.51
Episode_Reward/track_lin_vel_xy_exp: 0.9126
Episode_Reward/track_ang_vel_z_exp: 0.4201
       Episode_Reward/lin_vel_z_l2: -0.0510
      Episode_Reward/ang_vel_xy_l2: -0.1309
     Episode_Reward/dof_torques_l2: -0.0817
         Episode_Reward/dof_acc_l2: -0.1922
     Episode_Reward/action_rate_l2: -0.1638
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3140
Metrics/base_velocity/error_vel_xy: 0.4273
Metrics/base_velocity/error_vel_yaw: 0.4909
      Episode_Termination/time_out: 1.1250
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 44280000
                    Iteration time: 1.07s
                      Time elapsed: 00:22:48
                               ETA: 00:05:01

################################################################################
                     [1m Learning iteration 1230/1500 [0m                     

                       Computation: 33361 steps/s (collection: 1.028s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0243
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 13.6995
                       Mean reward: 16.74
               Mean episode length: 843.24
Episode_Reward/track_lin_vel_xy_exp: 1.0436
Episode_Reward/track_ang_vel_z_exp: 0.4704
       Episode_Reward/lin_vel_z_l2: -0.0446
      Episode_Reward/ang_vel_xy_l2: -0.1299
     Episode_Reward/dof_torques_l2: -0.0896
         Episode_Reward/dof_acc_l2: -0.1900
     Episode_Reward/action_rate_l2: -0.1765
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3186
Metrics/base_velocity/error_vel_xy: 0.3983
Metrics/base_velocity/error_vel_yaw: 0.4843
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 44316000
                    Iteration time: 1.08s
                      Time elapsed: 00:22:49
                               ETA: 00:05:00

################################################################################
                     [1m Learning iteration 1231/1500 [0m                     

                       Computation: 33548 steps/s (collection: 1.023s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0245
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 13.7044
                       Mean reward: 17.71
               Mean episode length: 876.40
Episode_Reward/track_lin_vel_xy_exp: 1.1811
Episode_Reward/track_ang_vel_z_exp: 0.5227
       Episode_Reward/lin_vel_z_l2: -0.0409
      Episode_Reward/ang_vel_xy_l2: -0.1388
     Episode_Reward/dof_torques_l2: -0.0946
         Episode_Reward/dof_acc_l2: -0.1917
     Episode_Reward/action_rate_l2: -0.1882
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3114
Metrics/base_velocity/error_vel_xy: 0.3974
Metrics/base_velocity/error_vel_yaw: 0.5143
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 44352000
                    Iteration time: 1.07s
                      Time elapsed: 00:22:50
                               ETA: 00:04:59

################################################################################
                     [1m Learning iteration 1232/1500 [0m                     

                       Computation: 33518 steps/s (collection: 1.023s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0273
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 13.7031
                       Mean reward: 18.48
               Mean episode length: 869.13
Episode_Reward/track_lin_vel_xy_exp: 1.0465
Episode_Reward/track_ang_vel_z_exp: 0.4653
       Episode_Reward/lin_vel_z_l2: -0.0436
      Episode_Reward/ang_vel_xy_l2: -0.1266
     Episode_Reward/dof_torques_l2: -0.0889
         Episode_Reward/dof_acc_l2: -0.1748
     Episode_Reward/action_rate_l2: -0.1709
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3078
Metrics/base_velocity/error_vel_xy: 0.3632
Metrics/base_velocity/error_vel_yaw: 0.4696
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 44388000
                    Iteration time: 1.07s
                      Time elapsed: 00:22:51
                               ETA: 00:04:58

################################################################################
                     [1m Learning iteration 1233/1500 [0m                     

                       Computation: 33296 steps/s (collection: 1.029s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0205
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 13.7015
                       Mean reward: 18.08
               Mean episode length: 853.57
Episode_Reward/track_lin_vel_xy_exp: 0.9730
Episode_Reward/track_ang_vel_z_exp: 0.4269
       Episode_Reward/lin_vel_z_l2: -0.0456
      Episode_Reward/ang_vel_xy_l2: -0.1255
     Episode_Reward/dof_torques_l2: -0.0784
         Episode_Reward/dof_acc_l2: -0.1761
     Episode_Reward/action_rate_l2: -0.1595
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3084
Metrics/base_velocity/error_vel_xy: 0.3492
Metrics/base_velocity/error_vel_yaw: 0.4760
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 44424000
                    Iteration time: 1.08s
                      Time elapsed: 00:22:52
                               ETA: 00:04:56

################################################################################
                     [1m Learning iteration 1234/1500 [0m                     

                       Computation: 33664 steps/s (collection: 1.018s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0189
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 13.6946
                       Mean reward: 17.83
               Mean episode length: 860.98
Episode_Reward/track_lin_vel_xy_exp: 1.0160
Episode_Reward/track_ang_vel_z_exp: 0.4724
       Episode_Reward/lin_vel_z_l2: -0.0429
      Episode_Reward/ang_vel_xy_l2: -0.1312
     Episode_Reward/dof_torques_l2: -0.0906
         Episode_Reward/dof_acc_l2: -0.1858
     Episode_Reward/action_rate_l2: -0.1751
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3101
Metrics/base_velocity/error_vel_xy: 0.4322
Metrics/base_velocity/error_vel_yaw: 0.5046
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 44460000
                    Iteration time: 1.07s
                      Time elapsed: 00:22:53
                               ETA: 00:04:55

################################################################################
                     [1m Learning iteration 1235/1500 [0m                     

                       Computation: 33392 steps/s (collection: 1.025s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0223
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 13.6946
                       Mean reward: 18.51
               Mean episode length: 873.50
Episode_Reward/track_lin_vel_xy_exp: 1.0521
Episode_Reward/track_ang_vel_z_exp: 0.4677
       Episode_Reward/lin_vel_z_l2: -0.0400
      Episode_Reward/ang_vel_xy_l2: -0.1268
     Episode_Reward/dof_torques_l2: -0.0852
         Episode_Reward/dof_acc_l2: -0.1819
     Episode_Reward/action_rate_l2: -0.1713
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3118
Metrics/base_velocity/error_vel_xy: 0.3676
Metrics/base_velocity/error_vel_yaw: 0.4796
      Episode_Termination/time_out: 1.9167
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 44496000
                    Iteration time: 1.08s
                      Time elapsed: 00:22:54
                               ETA: 00:04:54

################################################################################
                     [1m Learning iteration 1236/1500 [0m                     

                       Computation: 33472 steps/s (collection: 1.023s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0224
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 13.6921
                       Mean reward: 19.35
               Mean episode length: 898.50
Episode_Reward/track_lin_vel_xy_exp: 1.1776
Episode_Reward/track_ang_vel_z_exp: 0.5192
       Episode_Reward/lin_vel_z_l2: -0.0426
      Episode_Reward/ang_vel_xy_l2: -0.1346
     Episode_Reward/dof_torques_l2: -0.0935
         Episode_Reward/dof_acc_l2: -0.1927
     Episode_Reward/action_rate_l2: -0.1868
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3109
Metrics/base_velocity/error_vel_xy: 0.3881
Metrics/base_velocity/error_vel_yaw: 0.5074
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 44532000
                    Iteration time: 1.08s
                      Time elapsed: 00:22:55
                               ETA: 00:04:53

################################################################################
                     [1m Learning iteration 1237/1500 [0m                     

                       Computation: 33514 steps/s (collection: 1.023s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0239
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 13.6837
                       Mean reward: 18.65
               Mean episode length: 894.27
Episode_Reward/track_lin_vel_xy_exp: 1.1191
Episode_Reward/track_ang_vel_z_exp: 0.4954
       Episode_Reward/lin_vel_z_l2: -0.0432
      Episode_Reward/ang_vel_xy_l2: -0.1358
     Episode_Reward/dof_torques_l2: -0.0896
         Episode_Reward/dof_acc_l2: -0.1957
     Episode_Reward/action_rate_l2: -0.1815
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3165
Metrics/base_velocity/error_vel_xy: 0.3716
Metrics/base_velocity/error_vel_yaw: 0.4859
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 44568000
                    Iteration time: 1.07s
                      Time elapsed: 00:22:56
                               ETA: 00:04:52

################################################################################
                     [1m Learning iteration 1238/1500 [0m                     

                       Computation: 33220 steps/s (collection: 1.032s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0227
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 13.6834
                       Mean reward: 18.46
               Mean episode length: 900.87
Episode_Reward/track_lin_vel_xy_exp: 1.1379
Episode_Reward/track_ang_vel_z_exp: 0.4975
       Episode_Reward/lin_vel_z_l2: -0.0460
      Episode_Reward/ang_vel_xy_l2: -0.1358
     Episode_Reward/dof_torques_l2: -0.0900
         Episode_Reward/dof_acc_l2: -0.2044
     Episode_Reward/action_rate_l2: -0.1852
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3234
Metrics/base_velocity/error_vel_xy: 0.3787
Metrics/base_velocity/error_vel_yaw: 0.5147
      Episode_Termination/time_out: 2.3333
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 44604000
                    Iteration time: 1.08s
                      Time elapsed: 00:22:57
                               ETA: 00:04:51

################################################################################
                     [1m Learning iteration 1239/1500 [0m                     

                       Computation: 33744 steps/s (collection: 1.015s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0223
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 13.6800
                       Mean reward: 18.97
               Mean episode length: 905.72
Episode_Reward/track_lin_vel_xy_exp: 1.1132
Episode_Reward/track_ang_vel_z_exp: 0.4944
       Episode_Reward/lin_vel_z_l2: -0.0377
      Episode_Reward/ang_vel_xy_l2: -0.1285
     Episode_Reward/dof_torques_l2: -0.0895
         Episode_Reward/dof_acc_l2: -0.1737
     Episode_Reward/action_rate_l2: -0.1762
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3248
Metrics/base_velocity/error_vel_xy: 0.3655
Metrics/base_velocity/error_vel_yaw: 0.4699
      Episode_Termination/time_out: 2.1667
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 44640000
                    Iteration time: 1.07s
                      Time elapsed: 00:22:59
                               ETA: 00:04:50

################################################################################
                     [1m Learning iteration 1240/1500 [0m                     

                       Computation: 33300 steps/s (collection: 1.029s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0235
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 13.6765
                       Mean reward: 19.37
               Mean episode length: 934.91
Episode_Reward/track_lin_vel_xy_exp: 1.1311
Episode_Reward/track_ang_vel_z_exp: 0.5182
       Episode_Reward/lin_vel_z_l2: -0.0435
      Episode_Reward/ang_vel_xy_l2: -0.1422
     Episode_Reward/dof_torques_l2: -0.0961
         Episode_Reward/dof_acc_l2: -0.2031
     Episode_Reward/action_rate_l2: -0.1894
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3309
Metrics/base_velocity/error_vel_xy: 0.4492
Metrics/base_velocity/error_vel_yaw: 0.5294
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 44676000
                    Iteration time: 1.08s
                      Time elapsed: 00:23:00
                               ETA: 00:04:49

################################################################################
                     [1m Learning iteration 1241/1500 [0m                     

                       Computation: 33286 steps/s (collection: 1.029s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0246
               Mean surrogate loss: -0.0155
                 Mean entropy loss: 13.6796
                       Mean reward: 19.51
               Mean episode length: 926.57
Episode_Reward/track_lin_vel_xy_exp: 1.1076
Episode_Reward/track_ang_vel_z_exp: 0.4946
       Episode_Reward/lin_vel_z_l2: -0.0426
      Episode_Reward/ang_vel_xy_l2: -0.1320
     Episode_Reward/dof_torques_l2: -0.0923
         Episode_Reward/dof_acc_l2: -0.1799
     Episode_Reward/action_rate_l2: -0.1774
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3306
Metrics/base_velocity/error_vel_xy: 0.3833
Metrics/base_velocity/error_vel_yaw: 0.5000
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 44712000
                    Iteration time: 1.08s
                      Time elapsed: 00:23:01
                               ETA: 00:04:48

################################################################################
                     [1m Learning iteration 1242/1500 [0m                     

                       Computation: 33852 steps/s (collection: 1.012s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0232
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 13.6772
                       Mean reward: 19.34
               Mean episode length: 928.88
Episode_Reward/track_lin_vel_xy_exp: 1.1638
Episode_Reward/track_ang_vel_z_exp: 0.5295
       Episode_Reward/lin_vel_z_l2: -0.0467
      Episode_Reward/ang_vel_xy_l2: -0.1453
     Episode_Reward/dof_torques_l2: -0.0959
         Episode_Reward/dof_acc_l2: -0.2167
     Episode_Reward/action_rate_l2: -0.1955
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3324
Metrics/base_velocity/error_vel_xy: 0.4402
Metrics/base_velocity/error_vel_yaw: 0.5257
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 44748000
                    Iteration time: 1.06s
                      Time elapsed: 00:23:02
                               ETA: 00:04:46

################################################################################
                     [1m Learning iteration 1243/1500 [0m                     

                       Computation: 33940 steps/s (collection: 1.010s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0237
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 13.6771
                       Mean reward: 18.97
               Mean episode length: 908.84
Episode_Reward/track_lin_vel_xy_exp: 1.0201
Episode_Reward/track_ang_vel_z_exp: 0.4728
       Episode_Reward/lin_vel_z_l2: -0.0463
      Episode_Reward/ang_vel_xy_l2: -0.1376
     Episode_Reward/dof_torques_l2: -0.0937
         Episode_Reward/dof_acc_l2: -0.1927
     Episode_Reward/action_rate_l2: -0.1790
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3368
Metrics/base_velocity/error_vel_xy: 0.4671
Metrics/base_velocity/error_vel_yaw: 0.5199
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 44784000
                    Iteration time: 1.06s
                      Time elapsed: 00:23:03
                               ETA: 00:04:45

################################################################################
                     [1m Learning iteration 1244/1500 [0m                     

                       Computation: 33489 steps/s (collection: 1.021s, learning 0.054s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0240
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 13.6853
                       Mean reward: 17.31
               Mean episode length: 851.54
Episode_Reward/track_lin_vel_xy_exp: 0.9041
Episode_Reward/track_ang_vel_z_exp: 0.4077
       Episode_Reward/lin_vel_z_l2: -0.0422
      Episode_Reward/ang_vel_xy_l2: -0.1202
     Episode_Reward/dof_torques_l2: -0.0756
         Episode_Reward/dof_acc_l2: -0.1740
     Episode_Reward/action_rate_l2: -0.1530
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3512
Metrics/base_velocity/error_vel_xy: 0.3730
Metrics/base_velocity/error_vel_yaw: 0.4516
      Episode_Termination/time_out: 0.9167
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 44820000
                    Iteration time: 1.07s
                      Time elapsed: 00:23:04
                               ETA: 00:04:44

################################################################################
                     [1m Learning iteration 1245/1500 [0m                     

                       Computation: 33654 steps/s (collection: 1.017s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0257
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 13.6865
                       Mean reward: 16.94
               Mean episode length: 859.93
Episode_Reward/track_lin_vel_xy_exp: 1.0918
Episode_Reward/track_ang_vel_z_exp: 0.4973
       Episode_Reward/lin_vel_z_l2: -0.0448
      Episode_Reward/ang_vel_xy_l2: -0.1365
     Episode_Reward/dof_torques_l2: -0.0955
         Episode_Reward/dof_acc_l2: -0.1956
     Episode_Reward/action_rate_l2: -0.1837
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3523
Metrics/base_velocity/error_vel_xy: 0.4393
Metrics/base_velocity/error_vel_yaw: 0.5270
      Episode_Termination/time_out: 2.0833
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 44856000
                    Iteration time: 1.07s
                      Time elapsed: 00:23:05
                               ETA: 00:04:43

################################################################################
                     [1m Learning iteration 1246/1500 [0m                     

                       Computation: 33444 steps/s (collection: 1.024s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0258
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 13.6855
                       Mean reward: 16.46
               Mean episode length: 857.72
Episode_Reward/track_lin_vel_xy_exp: 1.0262
Episode_Reward/track_ang_vel_z_exp: 0.4754
       Episode_Reward/lin_vel_z_l2: -0.0433
      Episode_Reward/ang_vel_xy_l2: -0.1340
     Episode_Reward/dof_torques_l2: -0.0853
         Episode_Reward/dof_acc_l2: -0.2023
     Episode_Reward/action_rate_l2: -0.1766
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3556
Metrics/base_velocity/error_vel_xy: 0.4439
Metrics/base_velocity/error_vel_yaw: 0.4977
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 44892000
                    Iteration time: 1.08s
                      Time elapsed: 00:23:06
                               ETA: 00:04:42

################################################################################
                     [1m Learning iteration 1247/1500 [0m                     

                       Computation: 33535 steps/s (collection: 1.022s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0221
               Mean surrogate loss: -0.0150
                 Mean entropy loss: 13.6926
                       Mean reward: 17.51
               Mean episode length: 878.74
Episode_Reward/track_lin_vel_xy_exp: 1.0633
Episode_Reward/track_ang_vel_z_exp: 0.4870
       Episode_Reward/lin_vel_z_l2: -0.0411
      Episode_Reward/ang_vel_xy_l2: -0.1328
     Episode_Reward/dof_torques_l2: -0.0919
         Episode_Reward/dof_acc_l2: -0.1809
     Episode_Reward/action_rate_l2: -0.1780
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3577
Metrics/base_velocity/error_vel_xy: 0.4289
Metrics/base_velocity/error_vel_yaw: 0.5008
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 44928000
                    Iteration time: 1.07s
                      Time elapsed: 00:23:07
                               ETA: 00:04:41

################################################################################
                     [1m Learning iteration 1248/1500 [0m                     

                       Computation: 33504 steps/s (collection: 1.021s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0225
               Mean surrogate loss: -0.0158
                 Mean entropy loss: 13.6971
                       Mean reward: 17.28
               Mean episode length: 850.20
Episode_Reward/track_lin_vel_xy_exp: 1.0029
Episode_Reward/track_ang_vel_z_exp: 0.4518
       Episode_Reward/lin_vel_z_l2: -0.0376
      Episode_Reward/ang_vel_xy_l2: -0.1262
     Episode_Reward/dof_torques_l2: -0.0878
         Episode_Reward/dof_acc_l2: -0.1863
     Episode_Reward/action_rate_l2: -0.1694
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3537
Metrics/base_velocity/error_vel_xy: 0.3891
Metrics/base_velocity/error_vel_yaw: 0.4773
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 44964000
                    Iteration time: 1.07s
                      Time elapsed: 00:23:08
                               ETA: 00:04:40

################################################################################
                     [1m Learning iteration 1249/1500 [0m                     

                       Computation: 33493 steps/s (collection: 1.024s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0229
               Mean surrogate loss: -0.0160
                 Mean entropy loss: 13.6954
                       Mean reward: 16.58
               Mean episode length: 839.38
Episode_Reward/track_lin_vel_xy_exp: 0.9562
Episode_Reward/track_ang_vel_z_exp: 0.4288
       Episode_Reward/lin_vel_z_l2: -0.0457
      Episode_Reward/ang_vel_xy_l2: -0.1270
     Episode_Reward/dof_torques_l2: -0.0786
         Episode_Reward/dof_acc_l2: -0.1902
     Episode_Reward/action_rate_l2: -0.1668
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3512
Metrics/base_velocity/error_vel_xy: 0.3968
Metrics/base_velocity/error_vel_yaw: 0.4879
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 45000000
                    Iteration time: 1.07s
                      Time elapsed: 00:23:09
                               ETA: 00:04:39

################################################################################
                     [1m Learning iteration 1250/1500 [0m                     

                       Computation: 33839 steps/s (collection: 1.014s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0197
               Mean surrogate loss: -0.0161
                 Mean entropy loss: 13.7022
                       Mean reward: 17.56
               Mean episode length: 844.57
Episode_Reward/track_lin_vel_xy_exp: 1.1299
Episode_Reward/track_ang_vel_z_exp: 0.4948
       Episode_Reward/lin_vel_z_l2: -0.0421
      Episode_Reward/ang_vel_xy_l2: -0.1351
     Episode_Reward/dof_torques_l2: -0.0872
         Episode_Reward/dof_acc_l2: -0.1882
     Episode_Reward/action_rate_l2: -0.1791
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3546
Metrics/base_velocity/error_vel_xy: 0.3704
Metrics/base_velocity/error_vel_yaw: 0.5093
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 45036000
                    Iteration time: 1.06s
                      Time elapsed: 00:23:10
                               ETA: 00:04:37

################################################################################
                     [1m Learning iteration 1251/1500 [0m                     

                       Computation: 33158 steps/s (collection: 1.034s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0232
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 13.6991
                       Mean reward: 17.71
               Mean episode length: 873.57
Episode_Reward/track_lin_vel_xy_exp: 1.0949
Episode_Reward/track_ang_vel_z_exp: 0.4885
       Episode_Reward/lin_vel_z_l2: -0.0455
      Episode_Reward/ang_vel_xy_l2: -0.1349
     Episode_Reward/dof_torques_l2: -0.0960
         Episode_Reward/dof_acc_l2: -0.2005
     Episode_Reward/action_rate_l2: -0.1825
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3552
Metrics/base_velocity/error_vel_xy: 0.4048
Metrics/base_velocity/error_vel_yaw: 0.5081
      Episode_Termination/time_out: 1.1667
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 45072000
                    Iteration time: 1.09s
                      Time elapsed: 00:23:11
                               ETA: 00:04:36

################################################################################
                     [1m Learning iteration 1252/1500 [0m                     

                       Computation: 33490 steps/s (collection: 1.023s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0242
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 13.6988
                       Mean reward: 18.17
               Mean episode length: 880.23
Episode_Reward/track_lin_vel_xy_exp: 0.9258
Episode_Reward/track_ang_vel_z_exp: 0.4168
       Episode_Reward/lin_vel_z_l2: -0.0490
      Episode_Reward/ang_vel_xy_l2: -0.1276
     Episode_Reward/dof_torques_l2: -0.0775
         Episode_Reward/dof_acc_l2: -0.1706
     Episode_Reward/action_rate_l2: -0.1540
      Episode_Reward/feet_air_time: -0.0009
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3546
Metrics/base_velocity/error_vel_xy: 0.3587
Metrics/base_velocity/error_vel_yaw: 0.4476
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 45108000
                    Iteration time: 1.07s
                      Time elapsed: 00:23:13
                               ETA: 00:04:35

################################################################################
                     [1m Learning iteration 1253/1500 [0m                     

                       Computation: 33546 steps/s (collection: 1.023s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0252
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 13.7090
                       Mean reward: 18.49
               Mean episode length: 903.46
Episode_Reward/track_lin_vel_xy_exp: 1.0954
Episode_Reward/track_ang_vel_z_exp: 0.4923
       Episode_Reward/lin_vel_z_l2: -0.0441
      Episode_Reward/ang_vel_xy_l2: -0.1335
     Episode_Reward/dof_torques_l2: -0.0897
         Episode_Reward/dof_acc_l2: -0.1904
     Episode_Reward/action_rate_l2: -0.1819
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3605
Metrics/base_velocity/error_vel_xy: 0.3970
Metrics/base_velocity/error_vel_yaw: 0.5020
      Episode_Termination/time_out: 1.9583
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 45144000
                    Iteration time: 1.07s
                      Time elapsed: 00:23:14
                               ETA: 00:04:34

################################################################################
                     [1m Learning iteration 1254/1500 [0m                     

                       Computation: 33802 steps/s (collection: 1.012s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0259
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 13.7132
                       Mean reward: 17.73
               Mean episode length: 855.71
Episode_Reward/track_lin_vel_xy_exp: 0.9114
Episode_Reward/track_ang_vel_z_exp: 0.4228
       Episode_Reward/lin_vel_z_l2: -0.0392
      Episode_Reward/ang_vel_xy_l2: -0.1179
     Episode_Reward/dof_torques_l2: -0.0811
         Episode_Reward/dof_acc_l2: -0.1639
     Episode_Reward/action_rate_l2: -0.1554
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3596
Metrics/base_velocity/error_vel_xy: 0.3897
Metrics/base_velocity/error_vel_yaw: 0.4445
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 45180000
                    Iteration time: 1.07s
                      Time elapsed: 00:23:15
                               ETA: 00:04:33

################################################################################
                     [1m Learning iteration 1255/1500 [0m                     

                       Computation: 32720 steps/s (collection: 1.049s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0224
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 13.7047
                       Mean reward: 17.63
               Mean episode length: 863.61
Episode_Reward/track_lin_vel_xy_exp: 1.1480
Episode_Reward/track_ang_vel_z_exp: 0.5113
       Episode_Reward/lin_vel_z_l2: -0.0520
      Episode_Reward/ang_vel_xy_l2: -0.1397
     Episode_Reward/dof_torques_l2: -0.0971
         Episode_Reward/dof_acc_l2: -0.2071
     Episode_Reward/action_rate_l2: -0.1907
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3566
Metrics/base_velocity/error_vel_xy: 0.4085
Metrics/base_velocity/error_vel_yaw: 0.5165
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 45216000
                    Iteration time: 1.10s
                      Time elapsed: 00:23:16
                               ETA: 00:04:32

################################################################################
                     [1m Learning iteration 1256/1500 [0m                     

                       Computation: 33552 steps/s (collection: 1.021s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0231
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 13.7019
                       Mean reward: 18.04
               Mean episode length: 871.14
Episode_Reward/track_lin_vel_xy_exp: 1.0407
Episode_Reward/track_ang_vel_z_exp: 0.4677
       Episode_Reward/lin_vel_z_l2: -0.0408
      Episode_Reward/ang_vel_xy_l2: -0.1277
     Episode_Reward/dof_torques_l2: -0.0837
         Episode_Reward/dof_acc_l2: -0.1789
     Episode_Reward/action_rate_l2: -0.1683
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3666
Metrics/base_velocity/error_vel_xy: 0.3753
Metrics/base_velocity/error_vel_yaw: 0.4713
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 45252000
                    Iteration time: 1.07s
                      Time elapsed: 00:23:17
                               ETA: 00:04:31

################################################################################
                     [1m Learning iteration 1257/1500 [0m                     

                       Computation: 33008 steps/s (collection: 1.038s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0216
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 13.7082
                       Mean reward: 18.12
               Mean episode length: 869.05
Episode_Reward/track_lin_vel_xy_exp: 1.0948
Episode_Reward/track_ang_vel_z_exp: 0.4849
       Episode_Reward/lin_vel_z_l2: -0.0425
      Episode_Reward/ang_vel_xy_l2: -0.1302
     Episode_Reward/dof_torques_l2: -0.0902
         Episode_Reward/dof_acc_l2: -0.1874
     Episode_Reward/action_rate_l2: -0.1786
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3786
Metrics/base_velocity/error_vel_xy: 0.3780
Metrics/base_velocity/error_vel_yaw: 0.4871
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 45288000
                    Iteration time: 1.09s
                      Time elapsed: 00:23:18
                               ETA: 00:04:30

################################################################################
                     [1m Learning iteration 1258/1500 [0m                     

                       Computation: 33361 steps/s (collection: 1.029s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0269
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 13.7123
                       Mean reward: 19.26
               Mean episode length: 896.83
Episode_Reward/track_lin_vel_xy_exp: 1.1387
Episode_Reward/track_ang_vel_z_exp: 0.4941
       Episode_Reward/lin_vel_z_l2: -0.0451
      Episode_Reward/ang_vel_xy_l2: -0.1301
     Episode_Reward/dof_torques_l2: -0.0898
         Episode_Reward/dof_acc_l2: -0.2009
     Episode_Reward/action_rate_l2: -0.1813
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3755
Metrics/base_velocity/error_vel_xy: 0.3254
Metrics/base_velocity/error_vel_yaw: 0.4662
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 45324000
                    Iteration time: 1.08s
                      Time elapsed: 00:23:19
                               ETA: 00:04:29

################################################################################
                     [1m Learning iteration 1259/1500 [0m                     

                       Computation: 33770 steps/s (collection: 1.015s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0236
               Mean surrogate loss: -0.0161
                 Mean entropy loss: 13.7251
                       Mean reward: 18.43
               Mean episode length: 876.51
Episode_Reward/track_lin_vel_xy_exp: 1.0709
Episode_Reward/track_ang_vel_z_exp: 0.4785
       Episode_Reward/lin_vel_z_l2: -0.0458
      Episode_Reward/ang_vel_xy_l2: -0.1334
     Episode_Reward/dof_torques_l2: -0.0893
         Episode_Reward/dof_acc_l2: -0.1867
     Episode_Reward/action_rate_l2: -0.1779
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3803
Metrics/base_velocity/error_vel_xy: 0.3941
Metrics/base_velocity/error_vel_yaw: 0.5068
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 45360000
                    Iteration time: 1.07s
                      Time elapsed: 00:23:20
                               ETA: 00:04:27

################################################################################
                     [1m Learning iteration 1260/1500 [0m                     

                       Computation: 32738 steps/s (collection: 1.049s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0257
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 13.7374
                       Mean reward: 17.67
               Mean episode length: 849.91
Episode_Reward/track_lin_vel_xy_exp: 0.9289
Episode_Reward/track_ang_vel_z_exp: 0.4317
       Episode_Reward/lin_vel_z_l2: -0.0457
      Episode_Reward/ang_vel_xy_l2: -0.1270
     Episode_Reward/dof_torques_l2: -0.0814
         Episode_Reward/dof_acc_l2: -0.1673
     Episode_Reward/action_rate_l2: -0.1596
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3713
Metrics/base_velocity/error_vel_xy: 0.4159
Metrics/base_velocity/error_vel_yaw: 0.4724
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 45396000
                    Iteration time: 1.10s
                      Time elapsed: 00:23:21
                               ETA: 00:04:26

################################################################################
                     [1m Learning iteration 1261/1500 [0m                     

                       Computation: 33174 steps/s (collection: 1.033s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0251
               Mean surrogate loss: -0.0152
                 Mean entropy loss: 13.7502
                       Mean reward: 16.57
               Mean episode length: 828.46
Episode_Reward/track_lin_vel_xy_exp: 1.0641
Episode_Reward/track_ang_vel_z_exp: 0.4775
       Episode_Reward/lin_vel_z_l2: -0.0456
      Episode_Reward/ang_vel_xy_l2: -0.1356
     Episode_Reward/dof_torques_l2: -0.0853
         Episode_Reward/dof_acc_l2: -0.1850
     Episode_Reward/action_rate_l2: -0.1737
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3718
Metrics/base_velocity/error_vel_xy: 0.3997
Metrics/base_velocity/error_vel_yaw: 0.5007
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 45432000
                    Iteration time: 1.09s
                      Time elapsed: 00:23:22
                               ETA: 00:04:25

################################################################################
                     [1m Learning iteration 1262/1500 [0m                     

                       Computation: 32979 steps/s (collection: 1.040s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0239
               Mean surrogate loss: -0.0157
                 Mean entropy loss: 13.7527
                       Mean reward: 16.64
               Mean episode length: 830.43
Episode_Reward/track_lin_vel_xy_exp: 1.0247
Episode_Reward/track_ang_vel_z_exp: 0.4665
       Episode_Reward/lin_vel_z_l2: -0.0440
      Episode_Reward/ang_vel_xy_l2: -0.1303
     Episode_Reward/dof_torques_l2: -0.0902
         Episode_Reward/dof_acc_l2: -0.1843
     Episode_Reward/action_rate_l2: -0.1711
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3689
Metrics/base_velocity/error_vel_xy: 0.4063
Metrics/base_velocity/error_vel_yaw: 0.4887
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 45468000
                    Iteration time: 1.09s
                      Time elapsed: 00:23:23
                               ETA: 00:04:24

################################################################################
                     [1m Learning iteration 1263/1500 [0m                     

                       Computation: 33340 steps/s (collection: 1.030s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0263
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 13.7488
                       Mean reward: 17.72
               Mean episode length: 839.03
Episode_Reward/track_lin_vel_xy_exp: 1.1278
Episode_Reward/track_ang_vel_z_exp: 0.4913
       Episode_Reward/lin_vel_z_l2: -0.0455
      Episode_Reward/ang_vel_xy_l2: -0.1393
     Episode_Reward/dof_torques_l2: -0.0881
         Episode_Reward/dof_acc_l2: -0.1909
     Episode_Reward/action_rate_l2: -0.1792
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3704
Metrics/base_velocity/error_vel_xy: 0.3423
Metrics/base_velocity/error_vel_yaw: 0.4856
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 45504000
                    Iteration time: 1.08s
                      Time elapsed: 00:23:24
                               ETA: 00:04:23

################################################################################
                     [1m Learning iteration 1264/1500 [0m                     

                       Computation: 33636 steps/s (collection: 1.019s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0229
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 13.7445
                       Mean reward: 18.30
               Mean episode length: 846.38
Episode_Reward/track_lin_vel_xy_exp: 0.9906
Episode_Reward/track_ang_vel_z_exp: 0.4492
       Episode_Reward/lin_vel_z_l2: -0.0461
      Episode_Reward/ang_vel_xy_l2: -0.1326
     Episode_Reward/dof_torques_l2: -0.0856
         Episode_Reward/dof_acc_l2: -0.1915
     Episode_Reward/action_rate_l2: -0.1718
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3542
Metrics/base_velocity/error_vel_xy: 0.4151
Metrics/base_velocity/error_vel_yaw: 0.4987
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 45540000
                    Iteration time: 1.07s
                      Time elapsed: 00:23:25
                               ETA: 00:04:22

################################################################################
                     [1m Learning iteration 1265/1500 [0m                     

                       Computation: 34015 steps/s (collection: 1.007s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0244
               Mean surrogate loss: -0.0151
                 Mean entropy loss: 13.7451
                       Mean reward: 18.03
               Mean episode length: 868.48
Episode_Reward/track_lin_vel_xy_exp: 1.0260
Episode_Reward/track_ang_vel_z_exp: 0.4442
       Episode_Reward/lin_vel_z_l2: -0.0443
      Episode_Reward/ang_vel_xy_l2: -0.1268
     Episode_Reward/dof_torques_l2: -0.0799
         Episode_Reward/dof_acc_l2: -0.1849
     Episode_Reward/action_rate_l2: -0.1663
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3333
Metrics/base_velocity/error_vel_xy: 0.3445
Metrics/base_velocity/error_vel_yaw: 0.4734
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 45576000
                    Iteration time: 1.06s
                      Time elapsed: 00:23:27
                               ETA: 00:04:21

################################################################################
                     [1m Learning iteration 1266/1500 [0m                     

                       Computation: 33385 steps/s (collection: 1.026s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0256
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 13.7500
                       Mean reward: 17.20
               Mean episode length: 829.94
Episode_Reward/track_lin_vel_xy_exp: 1.0283
Episode_Reward/track_ang_vel_z_exp: 0.4479
       Episode_Reward/lin_vel_z_l2: -0.0370
      Episode_Reward/ang_vel_xy_l2: -0.1219
     Episode_Reward/dof_torques_l2: -0.0793
         Episode_Reward/dof_acc_l2: -0.1639
     Episode_Reward/action_rate_l2: -0.1632
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3231
Metrics/base_velocity/error_vel_xy: 0.3343
Metrics/base_velocity/error_vel_yaw: 0.4587
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 45612000
                    Iteration time: 1.08s
                      Time elapsed: 00:23:28
                               ETA: 00:04:20

################################################################################
                     [1m Learning iteration 1267/1500 [0m                     

                       Computation: 33294 steps/s (collection: 1.029s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0255
               Mean surrogate loss: -0.0155
                 Mean entropy loss: 13.7577
                       Mean reward: 16.44
               Mean episode length: 798.70
Episode_Reward/track_lin_vel_xy_exp: 0.8715
Episode_Reward/track_ang_vel_z_exp: 0.4037
       Episode_Reward/lin_vel_z_l2: -0.0447
      Episode_Reward/ang_vel_xy_l2: -0.1190
     Episode_Reward/dof_torques_l2: -0.0759
         Episode_Reward/dof_acc_l2: -0.1681
     Episode_Reward/action_rate_l2: -0.1533
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3200
Metrics/base_velocity/error_vel_xy: 0.4436
Metrics/base_velocity/error_vel_yaw: 0.4829
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 45648000
                    Iteration time: 1.08s
                      Time elapsed: 00:23:29
                               ETA: 00:04:18

################################################################################
                     [1m Learning iteration 1268/1500 [0m                     

                       Computation: 33357 steps/s (collection: 1.026s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0216
               Mean surrogate loss: -0.0166
                 Mean entropy loss: 13.7664
                       Mean reward: 16.25
               Mean episode length: 817.06
Episode_Reward/track_lin_vel_xy_exp: 0.9527
Episode_Reward/track_ang_vel_z_exp: 0.4373
       Episode_Reward/lin_vel_z_l2: -0.0463
      Episode_Reward/ang_vel_xy_l2: -0.1308
     Episode_Reward/dof_torques_l2: -0.0818
         Episode_Reward/dof_acc_l2: -0.1832
     Episode_Reward/action_rate_l2: -0.1652
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3124
Metrics/base_velocity/error_vel_xy: 0.4078
Metrics/base_velocity/error_vel_yaw: 0.4767
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 45684000
                    Iteration time: 1.08s
                      Time elapsed: 00:23:30
                               ETA: 00:04:17

################################################################################
                     [1m Learning iteration 1269/1500 [0m                     

                       Computation: 33548 steps/s (collection: 1.020s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0249
               Mean surrogate loss: -0.0158
                 Mean entropy loss: 13.7835
                       Mean reward: 16.95
               Mean episode length: 856.46
Episode_Reward/track_lin_vel_xy_exp: 1.0533
Episode_Reward/track_ang_vel_z_exp: 0.4772
       Episode_Reward/lin_vel_z_l2: -0.0403
      Episode_Reward/ang_vel_xy_l2: -0.1347
     Episode_Reward/dof_torques_l2: -0.0864
         Episode_Reward/dof_acc_l2: -0.1980
     Episode_Reward/action_rate_l2: -0.1771
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3150
Metrics/base_velocity/error_vel_xy: 0.3968
Metrics/base_velocity/error_vel_yaw: 0.4871
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 45720000
                    Iteration time: 1.07s
                      Time elapsed: 00:23:31
                               ETA: 00:04:16

################################################################################
                     [1m Learning iteration 1270/1500 [0m                     

                       Computation: 33250 steps/s (collection: 1.032s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0284
               Mean surrogate loss: -0.0155
                 Mean entropy loss: 13.7916
                       Mean reward: 18.13
               Mean episode length: 885.19
Episode_Reward/track_lin_vel_xy_exp: 1.1123
Episode_Reward/track_ang_vel_z_exp: 0.4829
       Episode_Reward/lin_vel_z_l2: -0.0430
      Episode_Reward/ang_vel_xy_l2: -0.1314
     Episode_Reward/dof_torques_l2: -0.0900
         Episode_Reward/dof_acc_l2: -0.1921
     Episode_Reward/action_rate_l2: -0.1798
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3216
Metrics/base_velocity/error_vel_xy: 0.3394
Metrics/base_velocity/error_vel_yaw: 0.4781
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 45756000
                    Iteration time: 1.08s
                      Time elapsed: 00:23:32
                               ETA: 00:04:15

################################################################################
                     [1m Learning iteration 1271/1500 [0m                     

                       Computation: 33090 steps/s (collection: 1.035s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0247
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 13.7952
                       Mean reward: 17.52
               Mean episode length: 835.92
Episode_Reward/track_lin_vel_xy_exp: 0.9903
Episode_Reward/track_ang_vel_z_exp: 0.4415
       Episode_Reward/lin_vel_z_l2: -0.0422
      Episode_Reward/ang_vel_xy_l2: -0.1286
     Episode_Reward/dof_torques_l2: -0.0823
         Episode_Reward/dof_acc_l2: -0.1865
     Episode_Reward/action_rate_l2: -0.1666
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3307
Metrics/base_velocity/error_vel_xy: 0.3724
Metrics/base_velocity/error_vel_yaw: 0.4713
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 45792000
                    Iteration time: 1.09s
                      Time elapsed: 00:23:33
                               ETA: 00:04:14

################################################################################
                     [1m Learning iteration 1272/1500 [0m                     

                       Computation: 33360 steps/s (collection: 1.028s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0213
               Mean surrogate loss: -0.0149
                 Mean entropy loss: 13.7961
                       Mean reward: 16.28
               Mean episode length: 815.12
Episode_Reward/track_lin_vel_xy_exp: 1.0543
Episode_Reward/track_ang_vel_z_exp: 0.4715
       Episode_Reward/lin_vel_z_l2: -0.0446
      Episode_Reward/ang_vel_xy_l2: -0.1301
     Episode_Reward/dof_torques_l2: -0.0863
         Episode_Reward/dof_acc_l2: -0.1839
     Episode_Reward/action_rate_l2: -0.1725
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3229
Metrics/base_velocity/error_vel_xy: 0.3882
Metrics/base_velocity/error_vel_yaw: 0.4883
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 45828000
                    Iteration time: 1.08s
                      Time elapsed: 00:23:34
                               ETA: 00:04:13

################################################################################
                     [1m Learning iteration 1273/1500 [0m                     

                       Computation: 33385 steps/s (collection: 1.026s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0248
               Mean surrogate loss: -0.0149
                 Mean entropy loss: 13.7955
                       Mean reward: 17.11
               Mean episode length: 839.31
Episode_Reward/track_lin_vel_xy_exp: 1.0332
Episode_Reward/track_ang_vel_z_exp: 0.4603
       Episode_Reward/lin_vel_z_l2: -0.0443
      Episode_Reward/ang_vel_xy_l2: -0.1283
     Episode_Reward/dof_torques_l2: -0.0881
         Episode_Reward/dof_acc_l2: -0.1783
     Episode_Reward/action_rate_l2: -0.1701
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3283
Metrics/base_velocity/error_vel_xy: 0.3720
Metrics/base_velocity/error_vel_yaw: 0.4729
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 45864000
                    Iteration time: 1.08s
                      Time elapsed: 00:23:35
                               ETA: 00:04:12

################################################################################
                     [1m Learning iteration 1274/1500 [0m                     

                       Computation: 33588 steps/s (collection: 1.020s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0239
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 13.7941
                       Mean reward: 18.35
               Mean episode length: 864.80
Episode_Reward/track_lin_vel_xy_exp: 1.0633
Episode_Reward/track_ang_vel_z_exp: 0.4693
       Episode_Reward/lin_vel_z_l2: -0.0437
      Episode_Reward/ang_vel_xy_l2: -0.1340
     Episode_Reward/dof_torques_l2: -0.0848
         Episode_Reward/dof_acc_l2: -0.1852
     Episode_Reward/action_rate_l2: -0.1726
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3289
Metrics/base_velocity/error_vel_xy: 0.3834
Metrics/base_velocity/error_vel_yaw: 0.4928
      Episode_Termination/time_out: 1.9583
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 45900000
                    Iteration time: 1.07s
                      Time elapsed: 00:23:36
                               ETA: 00:04:11

################################################################################
                     [1m Learning iteration 1275/1500 [0m                     

                       Computation: 33842 steps/s (collection: 1.013s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0249
               Mean surrogate loss: -0.0152
                 Mean entropy loss: 13.8053
                       Mean reward: 17.52
               Mean episode length: 864.04
Episode_Reward/track_lin_vel_xy_exp: 1.0435
Episode_Reward/track_ang_vel_z_exp: 0.4713
       Episode_Reward/lin_vel_z_l2: -0.0481
      Episode_Reward/ang_vel_xy_l2: -0.1390
     Episode_Reward/dof_torques_l2: -0.0898
         Episode_Reward/dof_acc_l2: -0.1994
     Episode_Reward/action_rate_l2: -0.1797
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3359
Metrics/base_velocity/error_vel_xy: 0.4427
Metrics/base_velocity/error_vel_yaw: 0.5223
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 45936000
                    Iteration time: 1.06s
                      Time elapsed: 00:23:37
                               ETA: 00:04:10

################################################################################
                     [1m Learning iteration 1276/1500 [0m                     

                       Computation: 33459 steps/s (collection: 1.023s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0226
               Mean surrogate loss: -0.0151
                 Mean entropy loss: 13.8230
                       Mean reward: 16.51
               Mean episode length: 874.73
Episode_Reward/track_lin_vel_xy_exp: 1.0579
Episode_Reward/track_ang_vel_z_exp: 0.4764
       Episode_Reward/lin_vel_z_l2: -0.0502
      Episode_Reward/ang_vel_xy_l2: -0.1355
     Episode_Reward/dof_torques_l2: -0.0922
         Episode_Reward/dof_acc_l2: -0.1919
     Episode_Reward/action_rate_l2: -0.1808
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3427
Metrics/base_velocity/error_vel_xy: 0.4031
Metrics/base_velocity/error_vel_yaw: 0.4947
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 45972000
                    Iteration time: 1.08s
                      Time elapsed: 00:23:38
                               ETA: 00:04:08

################################################################################
                     [1m Learning iteration 1277/1500 [0m                     

                       Computation: 33271 steps/s (collection: 1.030s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0286
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 13.8220
                       Mean reward: 18.60
               Mean episode length: 900.79
Episode_Reward/track_lin_vel_xy_exp: 1.1450
Episode_Reward/track_ang_vel_z_exp: 0.5106
       Episode_Reward/lin_vel_z_l2: -0.0442
      Episode_Reward/ang_vel_xy_l2: -0.1402
     Episode_Reward/dof_torques_l2: -0.0938
         Episode_Reward/dof_acc_l2: -0.1821
     Episode_Reward/action_rate_l2: -0.1835
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3504
Metrics/base_velocity/error_vel_xy: 0.3900
Metrics/base_velocity/error_vel_yaw: 0.5044
      Episode_Termination/time_out: 2.1250
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 46008000
                    Iteration time: 1.08s
                      Time elapsed: 00:23:39
                               ETA: 00:04:07

################################################################################
                     [1m Learning iteration 1278/1500 [0m                     

                       Computation: 33661 steps/s (collection: 1.018s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0240
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 13.8142
                       Mean reward: 18.78
               Mean episode length: 885.36
Episode_Reward/track_lin_vel_xy_exp: 1.0609
Episode_Reward/track_ang_vel_z_exp: 0.4673
       Episode_Reward/lin_vel_z_l2: -0.0459
      Episode_Reward/ang_vel_xy_l2: -0.1358
     Episode_Reward/dof_torques_l2: -0.0860
         Episode_Reward/dof_acc_l2: -0.1922
     Episode_Reward/action_rate_l2: -0.1737
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3541
Metrics/base_velocity/error_vel_xy: 0.3577
Metrics/base_velocity/error_vel_yaw: 0.4772
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 46044000
                    Iteration time: 1.07s
                      Time elapsed: 00:23:41
                               ETA: 00:04:06

################################################################################
                     [1m Learning iteration 1279/1500 [0m                     

                       Computation: 33391 steps/s (collection: 1.027s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0267
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.8113
                       Mean reward: 19.47
               Mean episode length: 901.80
Episode_Reward/track_lin_vel_xy_exp: 1.1805
Episode_Reward/track_ang_vel_z_exp: 0.5155
       Episode_Reward/lin_vel_z_l2: -0.0464
      Episode_Reward/ang_vel_xy_l2: -0.1424
     Episode_Reward/dof_torques_l2: -0.0933
         Episode_Reward/dof_acc_l2: -0.1986
     Episode_Reward/action_rate_l2: -0.1902
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3590
Metrics/base_velocity/error_vel_xy: 0.3741
Metrics/base_velocity/error_vel_yaw: 0.5138
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 46080000
                    Iteration time: 1.08s
                      Time elapsed: 00:23:42
                               ETA: 00:04:05

################################################################################
                     [1m Learning iteration 1280/1500 [0m                     

                       Computation: 33087 steps/s (collection: 1.038s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0230
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 13.8152
                       Mean reward: 18.33
               Mean episode length: 895.70
Episode_Reward/track_lin_vel_xy_exp: 0.9118
Episode_Reward/track_ang_vel_z_exp: 0.4229
       Episode_Reward/lin_vel_z_l2: -0.0490
      Episode_Reward/ang_vel_xy_l2: -0.1307
     Episode_Reward/dof_torques_l2: -0.0862
         Episode_Reward/dof_acc_l2: -0.1881
     Episode_Reward/action_rate_l2: -0.1642
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3658
Metrics/base_velocity/error_vel_xy: 0.4401
Metrics/base_velocity/error_vel_yaw: 0.5153
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 46116000
                    Iteration time: 1.09s
                      Time elapsed: 00:23:43
                               ETA: 00:04:04

################################################################################
                     [1m Learning iteration 1281/1500 [0m                     

                       Computation: 34052 steps/s (collection: 1.007s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0262
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.8114
                       Mean reward: 17.42
               Mean episode length: 856.12
Episode_Reward/track_lin_vel_xy_exp: 0.9787
Episode_Reward/track_ang_vel_z_exp: 0.4385
       Episode_Reward/lin_vel_z_l2: -0.0415
      Episode_Reward/ang_vel_xy_l2: -0.1200
     Episode_Reward/dof_torques_l2: -0.0855
         Episode_Reward/dof_acc_l2: -0.1815
     Episode_Reward/action_rate_l2: -0.1667
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3696
Metrics/base_velocity/error_vel_xy: 0.3695
Metrics/base_velocity/error_vel_yaw: 0.4570
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 46152000
                    Iteration time: 1.06s
                      Time elapsed: 00:23:44
                               ETA: 00:04:03

################################################################################
                     [1m Learning iteration 1282/1500 [0m                     

                       Computation: 33190 steps/s (collection: 1.033s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0259
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 13.8115
                       Mean reward: 18.26
               Mean episode length: 853.12
Episode_Reward/track_lin_vel_xy_exp: 1.0773
Episode_Reward/track_ang_vel_z_exp: 0.4783
       Episode_Reward/lin_vel_z_l2: -0.0377
      Episode_Reward/ang_vel_xy_l2: -0.1277
     Episode_Reward/dof_torques_l2: -0.0882
         Episode_Reward/dof_acc_l2: -0.1748
     Episode_Reward/action_rate_l2: -0.1734
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3777
Metrics/base_velocity/error_vel_xy: 0.3617
Metrics/base_velocity/error_vel_yaw: 0.4724
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 46188000
                    Iteration time: 1.08s
                      Time elapsed: 00:23:45
                               ETA: 00:04:02

################################################################################
                     [1m Learning iteration 1283/1500 [0m                     

                       Computation: 33942 steps/s (collection: 1.008s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0213
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 13.8038
                       Mean reward: 18.37
               Mean episode length: 876.04
Episode_Reward/track_lin_vel_xy_exp: 1.0812
Episode_Reward/track_ang_vel_z_exp: 0.4829
       Episode_Reward/lin_vel_z_l2: -0.0445
      Episode_Reward/ang_vel_xy_l2: -0.1376
     Episode_Reward/dof_torques_l2: -0.0907
         Episode_Reward/dof_acc_l2: -0.1994
     Episode_Reward/action_rate_l2: -0.1821
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3832
Metrics/base_velocity/error_vel_xy: 0.4203
Metrics/base_velocity/error_vel_yaw: 0.5165
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 46224000
                    Iteration time: 1.06s
                      Time elapsed: 00:23:46
                               ETA: 00:04:01

################################################################################
                     [1m Learning iteration 1284/1500 [0m                     

                       Computation: 34008 steps/s (collection: 1.007s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0246
               Mean surrogate loss: -0.0151
                 Mean entropy loss: 13.7974
                       Mean reward: 17.84
               Mean episode length: 868.40
Episode_Reward/track_lin_vel_xy_exp: 0.9138
Episode_Reward/track_ang_vel_z_exp: 0.3980
       Episode_Reward/lin_vel_z_l2: -0.0408
      Episode_Reward/ang_vel_xy_l2: -0.1162
     Episode_Reward/dof_torques_l2: -0.0743
         Episode_Reward/dof_acc_l2: -0.1615
     Episode_Reward/action_rate_l2: -0.1488
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3829
Metrics/base_velocity/error_vel_xy: 0.3177
Metrics/base_velocity/error_vel_yaw: 0.4339
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 46260000
                    Iteration time: 1.06s
                      Time elapsed: 00:23:47
                               ETA: 00:03:59

################################################################################
                     [1m Learning iteration 1285/1500 [0m                     

                       Computation: 33757 steps/s (collection: 1.015s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0211
               Mean surrogate loss: -0.0156
                 Mean entropy loss: 13.7920
                       Mean reward: 17.86
               Mean episode length: 881.42
Episode_Reward/track_lin_vel_xy_exp: 1.1884
Episode_Reward/track_ang_vel_z_exp: 0.5219
       Episode_Reward/lin_vel_z_l2: -0.0479
      Episode_Reward/ang_vel_xy_l2: -0.1492
     Episode_Reward/dof_torques_l2: -0.1013
         Episode_Reward/dof_acc_l2: -0.2169
     Episode_Reward/action_rate_l2: -0.2000
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3830
Metrics/base_velocity/error_vel_xy: 0.4225
Metrics/base_velocity/error_vel_yaw: 0.5507
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 46296000
                    Iteration time: 1.07s
                      Time elapsed: 00:23:48
                               ETA: 00:03:58

################################################################################
                     [1m Learning iteration 1286/1500 [0m                     

                       Computation: 33853 steps/s (collection: 1.012s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0211
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.7858
                       Mean reward: 17.41
               Mean episode length: 832.08
Episode_Reward/track_lin_vel_xy_exp: 0.8997
Episode_Reward/track_ang_vel_z_exp: 0.3956
       Episode_Reward/lin_vel_z_l2: -0.0375
      Episode_Reward/ang_vel_xy_l2: -0.1107
     Episode_Reward/dof_torques_l2: -0.0729
         Episode_Reward/dof_acc_l2: -0.1441
     Episode_Reward/action_rate_l2: -0.1447
      Episode_Reward/feet_air_time: -0.0009
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3789
Metrics/base_velocity/error_vel_xy: 0.2982
Metrics/base_velocity/error_vel_yaw: 0.4015
      Episode_Termination/time_out: 1.0417
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 46332000
                    Iteration time: 1.06s
                      Time elapsed: 00:23:49
                               ETA: 00:03:57

################################################################################
                     [1m Learning iteration 1287/1500 [0m                     

                       Computation: 33567 steps/s (collection: 1.021s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0235
               Mean surrogate loss: -0.0115
                 Mean entropy loss: 13.7934
                       Mean reward: 17.44
               Mean episode length: 860.73
Episode_Reward/track_lin_vel_xy_exp: 1.0024
Episode_Reward/track_ang_vel_z_exp: 0.4767
       Episode_Reward/lin_vel_z_l2: -0.0518
      Episode_Reward/ang_vel_xy_l2: -0.1488
     Episode_Reward/dof_torques_l2: -0.0969
         Episode_Reward/dof_acc_l2: -0.2168
     Episode_Reward/action_rate_l2: -0.1897
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3757
Metrics/base_velocity/error_vel_xy: 0.5548
Metrics/base_velocity/error_vel_yaw: 0.5752
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 46368000
                    Iteration time: 1.07s
                      Time elapsed: 00:23:50
                               ETA: 00:03:56

################################################################################
                     [1m Learning iteration 1288/1500 [0m                     

                       Computation: 33150 steps/s (collection: 1.033s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0236
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 13.8072
                       Mean reward: 17.83
               Mean episode length: 855.91
Episode_Reward/track_lin_vel_xy_exp: 1.1039
Episode_Reward/track_ang_vel_z_exp: 0.4784
       Episode_Reward/lin_vel_z_l2: -0.0383
      Episode_Reward/ang_vel_xy_l2: -0.1281
     Episode_Reward/dof_torques_l2: -0.0839
         Episode_Reward/dof_acc_l2: -0.1758
     Episode_Reward/action_rate_l2: -0.1717
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3809
Metrics/base_velocity/error_vel_xy: 0.3259
Metrics/base_velocity/error_vel_yaw: 0.4664
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 46404000
                    Iteration time: 1.09s
                      Time elapsed: 00:23:51
                               ETA: 00:03:55

################################################################################
                     [1m Learning iteration 1289/1500 [0m                     

                       Computation: 33206 steps/s (collection: 1.034s, learning 0.050s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0265
               Mean surrogate loss: -0.0158
                 Mean entropy loss: 13.8253
                       Mean reward: 18.06
               Mean episode length: 863.13
Episode_Reward/track_lin_vel_xy_exp: 1.0716
Episode_Reward/track_ang_vel_z_exp: 0.4658
       Episode_Reward/lin_vel_z_l2: -0.0442
      Episode_Reward/ang_vel_xy_l2: -0.1309
     Episode_Reward/dof_torques_l2: -0.0871
         Episode_Reward/dof_acc_l2: -0.1876
     Episode_Reward/action_rate_l2: -0.1746
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3911
Metrics/base_velocity/error_vel_xy: 0.3486
Metrics/base_velocity/error_vel_yaw: 0.4773
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 46440000
                    Iteration time: 1.08s
                      Time elapsed: 00:23:52
                               ETA: 00:03:54

################################################################################
                     [1m Learning iteration 1290/1500 [0m                     

                       Computation: 33591 steps/s (collection: 1.020s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0248
               Mean surrogate loss: -0.0153
                 Mean entropy loss: 13.8379
                       Mean reward: 18.13
               Mean episode length: 867.55
Episode_Reward/track_lin_vel_xy_exp: 1.1313
Episode_Reward/track_ang_vel_z_exp: 0.4919
       Episode_Reward/lin_vel_z_l2: -0.0420
      Episode_Reward/ang_vel_xy_l2: -0.1355
     Episode_Reward/dof_torques_l2: -0.0880
         Episode_Reward/dof_acc_l2: -0.1912
     Episode_Reward/action_rate_l2: -0.1806
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3945
Metrics/base_velocity/error_vel_xy: 0.3581
Metrics/base_velocity/error_vel_yaw: 0.4949
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 46476000
                    Iteration time: 1.07s
                      Time elapsed: 00:23:53
                               ETA: 00:03:53

################################################################################
                     [1m Learning iteration 1291/1500 [0m                     

                       Computation: 33528 steps/s (collection: 1.021s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0250
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 13.8314
                       Mean reward: 17.85
               Mean episode length: 862.22
Episode_Reward/track_lin_vel_xy_exp: 1.0191
Episode_Reward/track_ang_vel_z_exp: 0.4513
       Episode_Reward/lin_vel_z_l2: -0.0490
      Episode_Reward/ang_vel_xy_l2: -0.1332
     Episode_Reward/dof_torques_l2: -0.0858
         Episode_Reward/dof_acc_l2: -0.1980
     Episode_Reward/action_rate_l2: -0.1735
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3931
Metrics/base_velocity/error_vel_xy: 0.3953
Metrics/base_velocity/error_vel_yaw: 0.4916
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 46512000
                    Iteration time: 1.07s
                      Time elapsed: 00:23:55
                               ETA: 00:03:52

################################################################################
                     [1m Learning iteration 1292/1500 [0m                     

                       Computation: 33514 steps/s (collection: 1.023s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0223
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 13.8222
                       Mean reward: 18.06
               Mean episode length: 883.95
Episode_Reward/track_lin_vel_xy_exp: 1.1208
Episode_Reward/track_ang_vel_z_exp: 0.4980
       Episode_Reward/lin_vel_z_l2: -0.0485
      Episode_Reward/ang_vel_xy_l2: -0.1470
     Episode_Reward/dof_torques_l2: -0.0890
         Episode_Reward/dof_acc_l2: -0.2075
     Episode_Reward/action_rate_l2: -0.1883
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4017
Metrics/base_velocity/error_vel_xy: 0.4127
Metrics/base_velocity/error_vel_yaw: 0.5310
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 46548000
                    Iteration time: 1.07s
                      Time elapsed: 00:23:56
                               ETA: 00:03:51

################################################################################
                     [1m Learning iteration 1293/1500 [0m                     

                       Computation: 33338 steps/s (collection: 1.028s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0218
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 13.8211
                       Mean reward: 17.38
               Mean episode length: 867.18
Episode_Reward/track_lin_vel_xy_exp: 0.9688
Episode_Reward/track_ang_vel_z_exp: 0.4542
       Episode_Reward/lin_vel_z_l2: -0.0465
      Episode_Reward/ang_vel_xy_l2: -0.1362
     Episode_Reward/dof_torques_l2: -0.0919
         Episode_Reward/dof_acc_l2: -0.1867
     Episode_Reward/action_rate_l2: -0.1753
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4020
Metrics/base_velocity/error_vel_xy: 0.4754
Metrics/base_velocity/error_vel_yaw: 0.5160
      Episode_Termination/time_out: 1.1667
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 46584000
                    Iteration time: 1.08s
                      Time elapsed: 00:23:57
                               ETA: 00:03:49

################################################################################
                     [1m Learning iteration 1294/1500 [0m                     

                       Computation: 33151 steps/s (collection: 1.033s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0230
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 13.8209
                       Mean reward: 17.57
               Mean episode length: 873.66
Episode_Reward/track_lin_vel_xy_exp: 1.0438
Episode_Reward/track_ang_vel_z_exp: 0.4639
       Episode_Reward/lin_vel_z_l2: -0.0455
      Episode_Reward/ang_vel_xy_l2: -0.1346
     Episode_Reward/dof_torques_l2: -0.0876
         Episode_Reward/dof_acc_l2: -0.1962
     Episode_Reward/action_rate_l2: -0.1772
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4025
Metrics/base_velocity/error_vel_xy: 0.3889
Metrics/base_velocity/error_vel_yaw: 0.4919
      Episode_Termination/time_out: 2.0000
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 46620000
                    Iteration time: 1.09s
                      Time elapsed: 00:23:58
                               ETA: 00:03:48

################################################################################
                     [1m Learning iteration 1295/1500 [0m                     

                       Computation: 32482 steps/s (collection: 1.057s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0247
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 13.8272
                       Mean reward: 18.00
               Mean episode length: 899.34
Episode_Reward/track_lin_vel_xy_exp: 1.1409
Episode_Reward/track_ang_vel_z_exp: 0.5102
       Episode_Reward/lin_vel_z_l2: -0.0462
      Episode_Reward/ang_vel_xy_l2: -0.1440
     Episode_Reward/dof_torques_l2: -0.0946
         Episode_Reward/dof_acc_l2: -0.2001
     Episode_Reward/action_rate_l2: -0.1889
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3941
Metrics/base_velocity/error_vel_xy: 0.4164
Metrics/base_velocity/error_vel_yaw: 0.5319
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 46656000
                    Iteration time: 1.11s
                      Time elapsed: 00:23:59
                               ETA: 00:03:47

################################################################################
                     [1m Learning iteration 1296/1500 [0m                     

                       Computation: 33629 steps/s (collection: 1.020s, learning 0.050s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0258
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 13.8293
                       Mean reward: 17.07
               Mean episode length: 886.93
Episode_Reward/track_lin_vel_xy_exp: 0.9706
Episode_Reward/track_ang_vel_z_exp: 0.4570
       Episode_Reward/lin_vel_z_l2: -0.0517
      Episode_Reward/ang_vel_xy_l2: -0.1360
     Episode_Reward/dof_torques_l2: -0.0968
         Episode_Reward/dof_acc_l2: -0.2044
     Episode_Reward/action_rate_l2: -0.1821
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3968
Metrics/base_velocity/error_vel_xy: 0.4949
Metrics/base_velocity/error_vel_yaw: 0.5254
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 46692000
                    Iteration time: 1.07s
                      Time elapsed: 00:24:00
                               ETA: 00:03:46

################################################################################
                     [1m Learning iteration 1297/1500 [0m                     

                       Computation: 33080 steps/s (collection: 1.037s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0271
               Mean surrogate loss: -0.0157
                 Mean entropy loss: 13.8260
                       Mean reward: 16.58
               Mean episode length: 865.73
Episode_Reward/track_lin_vel_xy_exp: 1.1074
Episode_Reward/track_ang_vel_z_exp: 0.4893
       Episode_Reward/lin_vel_z_l2: -0.0493
      Episode_Reward/ang_vel_xy_l2: -0.1384
     Episode_Reward/dof_torques_l2: -0.0934
         Episode_Reward/dof_acc_l2: -0.2071
     Episode_Reward/action_rate_l2: -0.1890
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3858
Metrics/base_velocity/error_vel_xy: 0.4095
Metrics/base_velocity/error_vel_yaw: 0.5245
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 46728000
                    Iteration time: 1.09s
                      Time elapsed: 00:24:01
                               ETA: 00:03:45

################################################################################
                     [1m Learning iteration 1298/1500 [0m                     

                       Computation: 33342 steps/s (collection: 1.027s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0244
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 13.8227
                       Mean reward: 16.94
               Mean episode length: 867.45
Episode_Reward/track_lin_vel_xy_exp: 1.0203
Episode_Reward/track_ang_vel_z_exp: 0.4631
       Episode_Reward/lin_vel_z_l2: -0.0496
      Episode_Reward/ang_vel_xy_l2: -0.1340
     Episode_Reward/dof_torques_l2: -0.0906
         Episode_Reward/dof_acc_l2: -0.1919
     Episode_Reward/action_rate_l2: -0.1786
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3962
Metrics/base_velocity/error_vel_xy: 0.4240
Metrics/base_velocity/error_vel_yaw: 0.5080
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 46764000
                    Iteration time: 1.08s
                      Time elapsed: 00:24:02
                               ETA: 00:03:44

################################################################################
                     [1m Learning iteration 1299/1500 [0m                     

                       Computation: 33442 steps/s (collection: 1.024s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0215
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 13.8129
                       Mean reward: 17.55
               Mean episode length: 877.42
Episode_Reward/track_lin_vel_xy_exp: 1.1376
Episode_Reward/track_ang_vel_z_exp: 0.5056
       Episode_Reward/lin_vel_z_l2: -0.0453
      Episode_Reward/ang_vel_xy_l2: -0.1413
     Episode_Reward/dof_torques_l2: -0.0880
         Episode_Reward/dof_acc_l2: -0.2007
     Episode_Reward/action_rate_l2: -0.1862
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3953
Metrics/base_velocity/error_vel_xy: 0.3962
Metrics/base_velocity/error_vel_yaw: 0.5087
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 46800000
                    Iteration time: 1.08s
                      Time elapsed: 00:24:03
                               ETA: 00:03:43

################################################################################
                     [1m Learning iteration 1300/1500 [0m                     

                       Computation: 33489 steps/s (collection: 1.024s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0220
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 13.8118
                       Mean reward: 18.18
               Mean episode length: 878.65
Episode_Reward/track_lin_vel_xy_exp: 1.0069
Episode_Reward/track_ang_vel_z_exp: 0.4475
       Episode_Reward/lin_vel_z_l2: -0.0438
      Episode_Reward/ang_vel_xy_l2: -0.1306
     Episode_Reward/dof_torques_l2: -0.0803
         Episode_Reward/dof_acc_l2: -0.1750
     Episode_Reward/action_rate_l2: -0.1641
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3926
Metrics/base_velocity/error_vel_xy: 0.3532
Metrics/base_velocity/error_vel_yaw: 0.4586
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 46836000
                    Iteration time: 1.07s
                      Time elapsed: 00:24:04
                               ETA: 00:03:42

################################################################################
                     [1m Learning iteration 1301/1500 [0m                     

                       Computation: 33062 steps/s (collection: 1.036s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0230
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 13.8121
                       Mean reward: 18.54
               Mean episode length: 870.91
Episode_Reward/track_lin_vel_xy_exp: 1.1144
Episode_Reward/track_ang_vel_z_exp: 0.4818
       Episode_Reward/lin_vel_z_l2: -0.0476
      Episode_Reward/ang_vel_xy_l2: -0.1349
     Episode_Reward/dof_torques_l2: -0.0870
         Episode_Reward/dof_acc_l2: -0.1836
     Episode_Reward/action_rate_l2: -0.1763
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3797
Metrics/base_velocity/error_vel_xy: 0.3388
Metrics/base_velocity/error_vel_yaw: 0.4779
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 46872000
                    Iteration time: 1.09s
                      Time elapsed: 00:24:05
                               ETA: 00:03:40

################################################################################
                     [1m Learning iteration 1302/1500 [0m                     

                       Computation: 32454 steps/s (collection: 1.058s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0241
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 13.8104
                       Mean reward: 17.21
               Mean episode length: 849.97
Episode_Reward/track_lin_vel_xy_exp: 0.9688
Episode_Reward/track_ang_vel_z_exp: 0.4450
       Episode_Reward/lin_vel_z_l2: -0.0468
      Episode_Reward/ang_vel_xy_l2: -0.1309
     Episode_Reward/dof_torques_l2: -0.0887
         Episode_Reward/dof_acc_l2: -0.1932
     Episode_Reward/action_rate_l2: -0.1726
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3851
Metrics/base_velocity/error_vel_xy: 0.4384
Metrics/base_velocity/error_vel_yaw: 0.5128
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 46908000
                    Iteration time: 1.11s
                      Time elapsed: 00:24:06
                               ETA: 00:03:39

################################################################################
                     [1m Learning iteration 1303/1500 [0m                     

                       Computation: 32852 steps/s (collection: 1.046s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0252
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 13.8104
                       Mean reward: 16.76
               Mean episode length: 842.22
Episode_Reward/track_lin_vel_xy_exp: 1.0726
Episode_Reward/track_ang_vel_z_exp: 0.4729
       Episode_Reward/lin_vel_z_l2: -0.0386
      Episode_Reward/ang_vel_xy_l2: -0.1306
     Episode_Reward/dof_torques_l2: -0.0886
         Episode_Reward/dof_acc_l2: -0.1645
     Episode_Reward/action_rate_l2: -0.1709
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3809
Metrics/base_velocity/error_vel_xy: 0.3707
Metrics/base_velocity/error_vel_yaw: 0.4893
      Episode_Termination/time_out: 1.1250
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 46944000
                    Iteration time: 1.10s
                      Time elapsed: 00:24:08
                               ETA: 00:03:38

################################################################################
                     [1m Learning iteration 1304/1500 [0m                     

                       Computation: 33108 steps/s (collection: 1.035s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0253
               Mean surrogate loss: -0.0150
                 Mean entropy loss: 13.8080
                       Mean reward: 16.91
               Mean episode length: 839.92
Episode_Reward/track_lin_vel_xy_exp: 1.0949
Episode_Reward/track_ang_vel_z_exp: 0.4737
       Episode_Reward/lin_vel_z_l2: -0.0453
      Episode_Reward/ang_vel_xy_l2: -0.1381
     Episode_Reward/dof_torques_l2: -0.0914
         Episode_Reward/dof_acc_l2: -0.1957
     Episode_Reward/action_rate_l2: -0.1800
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3819
Metrics/base_velocity/error_vel_xy: 0.3595
Metrics/base_velocity/error_vel_yaw: 0.5069
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 46980000
                    Iteration time: 1.09s
                      Time elapsed: 00:24:09
                               ETA: 00:03:37

################################################################################
                     [1m Learning iteration 1305/1500 [0m                     

                       Computation: 33386 steps/s (collection: 1.027s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0245
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 13.8057
                       Mean reward: 17.86
               Mean episode length: 850.24
Episode_Reward/track_lin_vel_xy_exp: 1.0366
Episode_Reward/track_ang_vel_z_exp: 0.4452
       Episode_Reward/lin_vel_z_l2: -0.0399
      Episode_Reward/ang_vel_xy_l2: -0.1220
     Episode_Reward/dof_torques_l2: -0.0809
         Episode_Reward/dof_acc_l2: -0.1661
     Episode_Reward/action_rate_l2: -0.1631
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3870
Metrics/base_velocity/error_vel_xy: 0.2973
Metrics/base_velocity/error_vel_yaw: 0.4364
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 47016000
                    Iteration time: 1.08s
                      Time elapsed: 00:24:10
                               ETA: 00:03:36

################################################################################
                     [1m Learning iteration 1306/1500 [0m                     

                       Computation: 33420 steps/s (collection: 1.026s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0206
               Mean surrogate loss: -0.0151
                 Mean entropy loss: 13.8029
                       Mean reward: 17.92
               Mean episode length: 840.55
Episode_Reward/track_lin_vel_xy_exp: 1.0670
Episode_Reward/track_ang_vel_z_exp: 0.4747
       Episode_Reward/lin_vel_z_l2: -0.0464
      Episode_Reward/ang_vel_xy_l2: -0.1329
     Episode_Reward/dof_torques_l2: -0.0854
         Episode_Reward/dof_acc_l2: -0.1885
     Episode_Reward/action_rate_l2: -0.1765
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3866
Metrics/base_velocity/error_vel_xy: 0.3838
Metrics/base_velocity/error_vel_yaw: 0.4896
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 47052000
                    Iteration time: 1.08s
                      Time elapsed: 00:24:11
                               ETA: 00:03:35

################################################################################
                     [1m Learning iteration 1307/1500 [0m                     

                       Computation: 32746 steps/s (collection: 1.048s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0206
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.7958
                       Mean reward: 18.59
               Mean episode length: 876.44
Episode_Reward/track_lin_vel_xy_exp: 1.1027
Episode_Reward/track_ang_vel_z_exp: 0.4853
       Episode_Reward/lin_vel_z_l2: -0.0449
      Episode_Reward/ang_vel_xy_l2: -0.1350
     Episode_Reward/dof_torques_l2: -0.0876
         Episode_Reward/dof_acc_l2: -0.1923
     Episode_Reward/action_rate_l2: -0.1772
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3970
Metrics/base_velocity/error_vel_xy: 0.3601
Metrics/base_velocity/error_vel_yaw: 0.4926
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 47088000
                    Iteration time: 1.10s
                      Time elapsed: 00:24:12
                               ETA: 00:03:34

################################################################################
                     [1m Learning iteration 1308/1500 [0m                     

                       Computation: 33334 steps/s (collection: 1.030s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0212
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 13.7933
                       Mean reward: 18.83
               Mean episode length: 901.00
Episode_Reward/track_lin_vel_xy_exp: 1.1729
Episode_Reward/track_ang_vel_z_exp: 0.5109
       Episode_Reward/lin_vel_z_l2: -0.0495
      Episode_Reward/ang_vel_xy_l2: -0.1415
     Episode_Reward/dof_torques_l2: -0.0950
         Episode_Reward/dof_acc_l2: -0.2088
     Episode_Reward/action_rate_l2: -0.1928
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4069
Metrics/base_velocity/error_vel_xy: 0.3812
Metrics/base_velocity/error_vel_yaw: 0.5267
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 47124000
                    Iteration time: 1.08s
                      Time elapsed: 00:24:13
                               ETA: 00:03:33

################################################################################
                     [1m Learning iteration 1309/1500 [0m                     

                       Computation: 33589 steps/s (collection: 1.021s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0218
               Mean surrogate loss: -0.0150
                 Mean entropy loss: 13.8056
                       Mean reward: 18.17
               Mean episode length: 883.65
Episode_Reward/track_lin_vel_xy_exp: 1.0545
Episode_Reward/track_ang_vel_z_exp: 0.4645
       Episode_Reward/lin_vel_z_l2: -0.0458
      Episode_Reward/ang_vel_xy_l2: -0.1354
     Episode_Reward/dof_torques_l2: -0.0867
         Episode_Reward/dof_acc_l2: -0.1952
     Episode_Reward/action_rate_l2: -0.1776
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4123
Metrics/base_velocity/error_vel_xy: 0.3966
Metrics/base_velocity/error_vel_yaw: 0.5303
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 47160000
                    Iteration time: 1.07s
                      Time elapsed: 00:24:14
                               ETA: 00:03:32

################################################################################
                     [1m Learning iteration 1310/1500 [0m                     

                       Computation: 33399 steps/s (collection: 1.026s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0228
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 13.8077
                       Mean reward: 18.17
               Mean episode length: 901.18
Episode_Reward/track_lin_vel_xy_exp: 1.0589
Episode_Reward/track_ang_vel_z_exp: 0.4756
       Episode_Reward/lin_vel_z_l2: -0.0453
      Episode_Reward/ang_vel_xy_l2: -0.1368
     Episode_Reward/dof_torques_l2: -0.0892
         Episode_Reward/dof_acc_l2: -0.1957
     Episode_Reward/action_rate_l2: -0.1804
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4143
Metrics/base_velocity/error_vel_xy: 0.4207
Metrics/base_velocity/error_vel_yaw: 0.5178
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 47196000
                    Iteration time: 1.08s
                      Time elapsed: 00:24:15
                               ETA: 00:03:30

################################################################################
                     [1m Learning iteration 1311/1500 [0m                     

                       Computation: 32275 steps/s (collection: 1.065s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0218
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 13.8066
                       Mean reward: 19.02
               Mean episode length: 898.79
Episode_Reward/track_lin_vel_xy_exp: 1.1583
Episode_Reward/track_ang_vel_z_exp: 0.5043
       Episode_Reward/lin_vel_z_l2: -0.0383
      Episode_Reward/ang_vel_xy_l2: -0.1262
     Episode_Reward/dof_torques_l2: -0.0895
         Episode_Reward/dof_acc_l2: -0.1741
     Episode_Reward/action_rate_l2: -0.1766
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4156
Metrics/base_velocity/error_vel_xy: 0.3216
Metrics/base_velocity/error_vel_yaw: 0.4644
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 47232000
                    Iteration time: 1.12s
                      Time elapsed: 00:24:16
                               ETA: 00:03:29

################################################################################
                     [1m Learning iteration 1312/1500 [0m                     

                       Computation: 32904 steps/s (collection: 1.042s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0225
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 13.8076
                       Mean reward: 19.48
               Mean episode length: 895.04
Episode_Reward/track_lin_vel_xy_exp: 1.0762
Episode_Reward/track_ang_vel_z_exp: 0.4729
       Episode_Reward/lin_vel_z_l2: -0.0447
      Episode_Reward/ang_vel_xy_l2: -0.1318
     Episode_Reward/dof_torques_l2: -0.0864
         Episode_Reward/dof_acc_l2: -0.1911
     Episode_Reward/action_rate_l2: -0.1779
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4168
Metrics/base_velocity/error_vel_xy: 0.3808
Metrics/base_velocity/error_vel_yaw: 0.4973
      Episode_Termination/time_out: 1.9167
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 47268000
                    Iteration time: 1.09s
                      Time elapsed: 00:24:17
                               ETA: 00:03:28

################################################################################
                     [1m Learning iteration 1313/1500 [0m                     

                       Computation: 33464 steps/s (collection: 1.023s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0250
               Mean surrogate loss: -0.0154
                 Mean entropy loss: 13.7992
                       Mean reward: 18.58
               Mean episode length: 884.41
Episode_Reward/track_lin_vel_xy_exp: 1.0910
Episode_Reward/track_ang_vel_z_exp: 0.4867
       Episode_Reward/lin_vel_z_l2: -0.0472
      Episode_Reward/ang_vel_xy_l2: -0.1359
     Episode_Reward/dof_torques_l2: -0.0922
         Episode_Reward/dof_acc_l2: -0.1923
     Episode_Reward/action_rate_l2: -0.1822
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4245
Metrics/base_velocity/error_vel_xy: 0.4070
Metrics/base_velocity/error_vel_yaw: 0.5140
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 47304000
                    Iteration time: 1.08s
                      Time elapsed: 00:24:18
                               ETA: 00:03:27

################################################################################
                     [1m Learning iteration 1314/1500 [0m                     

                       Computation: 33290 steps/s (collection: 1.031s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0254
               Mean surrogate loss: -0.0150
                 Mean entropy loss: 13.7921
                       Mean reward: 18.21
               Mean episode length: 867.59
Episode_Reward/track_lin_vel_xy_exp: 1.0028
Episode_Reward/track_ang_vel_z_exp: 0.4387
       Episode_Reward/lin_vel_z_l2: -0.0413
      Episode_Reward/ang_vel_xy_l2: -0.1206
     Episode_Reward/dof_torques_l2: -0.0814
         Episode_Reward/dof_acc_l2: -0.1662
     Episode_Reward/action_rate_l2: -0.1618
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4284
Metrics/base_velocity/error_vel_xy: 0.3341
Metrics/base_velocity/error_vel_yaw: 0.4539
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 47340000
                    Iteration time: 1.08s
                      Time elapsed: 00:24:19
                               ETA: 00:03:26

################################################################################
                     [1m Learning iteration 1315/1500 [0m                     

                       Computation: 33271 steps/s (collection: 1.030s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0255
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 13.7896
                       Mean reward: 18.26
               Mean episode length: 847.10
Episode_Reward/track_lin_vel_xy_exp: 1.0638
Episode_Reward/track_ang_vel_z_exp: 0.4706
       Episode_Reward/lin_vel_z_l2: -0.0421
      Episode_Reward/ang_vel_xy_l2: -0.1272
     Episode_Reward/dof_torques_l2: -0.0907
         Episode_Reward/dof_acc_l2: -0.1716
     Episode_Reward/action_rate_l2: -0.1740
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4325
Metrics/base_velocity/error_vel_xy: 0.3777
Metrics/base_velocity/error_vel_yaw: 0.4989
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 47376000
                    Iteration time: 1.08s
                      Time elapsed: 00:24:21
                               ETA: 00:03:25

################################################################################
                     [1m Learning iteration 1316/1500 [0m                     

                       Computation: 32934 steps/s (collection: 1.041s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0256
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 13.7890
                       Mean reward: 18.19
               Mean episode length: 870.05
Episode_Reward/track_lin_vel_xy_exp: 1.0951
Episode_Reward/track_ang_vel_z_exp: 0.4824
       Episode_Reward/lin_vel_z_l2: -0.0436
      Episode_Reward/ang_vel_xy_l2: -0.1372
     Episode_Reward/dof_torques_l2: -0.0927
         Episode_Reward/dof_acc_l2: -0.1910
     Episode_Reward/action_rate_l2: -0.1824
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4320
Metrics/base_velocity/error_vel_xy: 0.4058
Metrics/base_velocity/error_vel_yaw: 0.5370
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 47412000
                    Iteration time: 1.09s
                      Time elapsed: 00:24:22
                               ETA: 00:03:24

################################################################################
                     [1m Learning iteration 1317/1500 [0m                     

                       Computation: 33257 steps/s (collection: 1.033s, learning 0.049s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0221
               Mean surrogate loss: -0.0154
                 Mean entropy loss: 13.7825
                       Mean reward: 17.96
               Mean episode length: 897.70
Episode_Reward/track_lin_vel_xy_exp: 1.0998
Episode_Reward/track_ang_vel_z_exp: 0.4932
       Episode_Reward/lin_vel_z_l2: -0.0495
      Episode_Reward/ang_vel_xy_l2: -0.1402
     Episode_Reward/dof_torques_l2: -0.0955
         Episode_Reward/dof_acc_l2: -0.2117
     Episode_Reward/action_rate_l2: -0.1887
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4444
Metrics/base_velocity/error_vel_xy: 0.4259
Metrics/base_velocity/error_vel_yaw: 0.5368
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 47448000
                    Iteration time: 1.08s
                      Time elapsed: 00:24:23
                               ETA: 00:03:23

################################################################################
                     [1m Learning iteration 1318/1500 [0m                     

                       Computation: 33427 steps/s (collection: 1.027s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0230
               Mean surrogate loss: -0.0153
                 Mean entropy loss: 13.7647
                       Mean reward: 17.88
               Mean episode length: 908.67
Episode_Reward/track_lin_vel_xy_exp: 1.0245
Episode_Reward/track_ang_vel_z_exp: 0.4554
       Episode_Reward/lin_vel_z_l2: -0.0457
      Episode_Reward/ang_vel_xy_l2: -0.1348
     Episode_Reward/dof_torques_l2: -0.0830
         Episode_Reward/dof_acc_l2: -0.1784
     Episode_Reward/action_rate_l2: -0.1700
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4543
Metrics/base_velocity/error_vel_xy: 0.3808
Metrics/base_velocity/error_vel_yaw: 0.4875
      Episode_Termination/time_out: 2.0417
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 47484000
                    Iteration time: 1.08s
                      Time elapsed: 00:24:24
                               ETA: 00:03:22

################################################################################
                     [1m Learning iteration 1319/1500 [0m                     

                       Computation: 33038 steps/s (collection: 1.040s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0250
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 13.7575
                       Mean reward: 19.18
               Mean episode length: 907.03
Episode_Reward/track_lin_vel_xy_exp: 1.2272
Episode_Reward/track_ang_vel_z_exp: 0.5270
       Episode_Reward/lin_vel_z_l2: -0.0442
      Episode_Reward/ang_vel_xy_l2: -0.1395
     Episode_Reward/dof_torques_l2: -0.0956
         Episode_Reward/dof_acc_l2: -0.1960
     Episode_Reward/action_rate_l2: -0.1930
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4446
Metrics/base_velocity/error_vel_xy: 0.3503
Metrics/base_velocity/error_vel_yaw: 0.5166
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 47520000
                    Iteration time: 1.09s
                      Time elapsed: 00:24:25
                               ETA: 00:03:20

################################################################################
                     [1m Learning iteration 1320/1500 [0m                     

                       Computation: 33372 steps/s (collection: 1.028s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0258
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.7582
                       Mean reward: 18.22
               Mean episode length: 875.18
Episode_Reward/track_lin_vel_xy_exp: 1.0975
Episode_Reward/track_ang_vel_z_exp: 0.4791
       Episode_Reward/lin_vel_z_l2: -0.0508
      Episode_Reward/ang_vel_xy_l2: -0.1380
     Episode_Reward/dof_torques_l2: -0.0859
         Episode_Reward/dof_acc_l2: -0.2006
     Episode_Reward/action_rate_l2: -0.1825
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4435
Metrics/base_velocity/error_vel_xy: 0.3695
Metrics/base_velocity/error_vel_yaw: 0.4989
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 47556000
                    Iteration time: 1.08s
                      Time elapsed: 00:24:26
                               ETA: 00:03:19

################################################################################
                     [1m Learning iteration 1321/1500 [0m                     

                       Computation: 33989 steps/s (collection: 1.007s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0235
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 13.7663
                       Mean reward: 16.89
               Mean episode length: 826.43
Episode_Reward/track_lin_vel_xy_exp: 0.9552
Episode_Reward/track_ang_vel_z_exp: 0.4192
       Episode_Reward/lin_vel_z_l2: -0.0415
      Episode_Reward/ang_vel_xy_l2: -0.1201
     Episode_Reward/dof_torques_l2: -0.0794
         Episode_Reward/dof_acc_l2: -0.1705
     Episode_Reward/action_rate_l2: -0.1588
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4358
Metrics/base_velocity/error_vel_xy: 0.3381
Metrics/base_velocity/error_vel_yaw: 0.4543
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 47592000
                    Iteration time: 1.06s
                      Time elapsed: 00:24:27
                               ETA: 00:03:18

################################################################################
                     [1m Learning iteration 1322/1500 [0m                     

                       Computation: 33868 steps/s (collection: 1.013s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0255
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 13.7638
                       Mean reward: 15.94
               Mean episode length: 800.41
Episode_Reward/track_lin_vel_xy_exp: 0.7589
Episode_Reward/track_ang_vel_z_exp: 0.3450
       Episode_Reward/lin_vel_z_l2: -0.0362
      Episode_Reward/ang_vel_xy_l2: -0.0981
     Episode_Reward/dof_torques_l2: -0.0672
         Episode_Reward/dof_acc_l2: -0.1388
     Episode_Reward/action_rate_l2: -0.1296
      Episode_Reward/feet_air_time: -0.0009
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4355
Metrics/base_velocity/error_vel_xy: 0.3168
Metrics/base_velocity/error_vel_yaw: 0.3739
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 47628000
                    Iteration time: 1.06s
                      Time elapsed: 00:24:28
                               ETA: 00:03:17

################################################################################
                     [1m Learning iteration 1323/1500 [0m                     

                       Computation: 33184 steps/s (collection: 1.033s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0281
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 13.7658
                       Mean reward: 16.47
               Mean episode length: 821.52
Episode_Reward/track_lin_vel_xy_exp: 1.0591
Episode_Reward/track_ang_vel_z_exp: 0.4733
       Episode_Reward/lin_vel_z_l2: -0.0470
      Episode_Reward/ang_vel_xy_l2: -0.1332
     Episode_Reward/dof_torques_l2: -0.0865
         Episode_Reward/dof_acc_l2: -0.1941
     Episode_Reward/action_rate_l2: -0.1790
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4324
Metrics/base_velocity/error_vel_xy: 0.3818
Metrics/base_velocity/error_vel_yaw: 0.4805
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 47664000
                    Iteration time: 1.08s
                      Time elapsed: 00:24:29
                               ETA: 00:03:16

################################################################################
                     [1m Learning iteration 1324/1500 [0m                     

                       Computation: 33532 steps/s (collection: 1.022s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0240
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 13.7779
                       Mean reward: 17.61
               Mean episode length: 877.77
Episode_Reward/track_lin_vel_xy_exp: 1.0875
Episode_Reward/track_ang_vel_z_exp: 0.4894
       Episode_Reward/lin_vel_z_l2: -0.0440
      Episode_Reward/ang_vel_xy_l2: -0.1322
     Episode_Reward/dof_torques_l2: -0.0943
         Episode_Reward/dof_acc_l2: -0.1853
     Episode_Reward/action_rate_l2: -0.1800
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4306
Metrics/base_velocity/error_vel_xy: 0.4136
Metrics/base_velocity/error_vel_yaw: 0.5019
      Episode_Termination/time_out: 2.0417
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 47700000
                    Iteration time: 1.07s
                      Time elapsed: 00:24:30
                               ETA: 00:03:15

################################################################################
                     [1m Learning iteration 1325/1500 [0m                     

                       Computation: 33644 steps/s (collection: 1.017s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0229
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 13.7756
                       Mean reward: 18.56
               Mean episode length: 925.25
Episode_Reward/track_lin_vel_xy_exp: 1.0566
Episode_Reward/track_ang_vel_z_exp: 0.4846
       Episode_Reward/lin_vel_z_l2: -0.0535
      Episode_Reward/ang_vel_xy_l2: -0.1518
     Episode_Reward/dof_torques_l2: -0.0978
         Episode_Reward/dof_acc_l2: -0.2320
     Episode_Reward/action_rate_l2: -0.1953
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4298
Metrics/base_velocity/error_vel_xy: 0.5163
Metrics/base_velocity/error_vel_yaw: 0.5943
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 47736000
                    Iteration time: 1.07s
                      Time elapsed: 00:24:31
                               ETA: 00:03:14

################################################################################
                     [1m Learning iteration 1326/1500 [0m                     

                       Computation: 33700 steps/s (collection: 1.015s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0236
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 13.7747
                       Mean reward: 18.45
               Mean episode length: 907.92
Episode_Reward/track_lin_vel_xy_exp: 1.0618
Episode_Reward/track_ang_vel_z_exp: 0.4690
       Episode_Reward/lin_vel_z_l2: -0.0427
      Episode_Reward/ang_vel_xy_l2: -0.1315
     Episode_Reward/dof_torques_l2: -0.0910
         Episode_Reward/dof_acc_l2: -0.1859
     Episode_Reward/action_rate_l2: -0.1758
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4266
Metrics/base_velocity/error_vel_xy: 0.3715
Metrics/base_velocity/error_vel_yaw: 0.4913
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 47772000
                    Iteration time: 1.07s
                      Time elapsed: 00:24:32
                               ETA: 00:03:13

################################################################################
                     [1m Learning iteration 1327/1500 [0m                     

                       Computation: 33388 steps/s (collection: 1.027s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0234
               Mean surrogate loss: -0.0151
                 Mean entropy loss: 13.7879
                       Mean reward: 18.58
               Mean episode length: 892.99
Episode_Reward/track_lin_vel_xy_exp: 1.0754
Episode_Reward/track_ang_vel_z_exp: 0.4649
       Episode_Reward/lin_vel_z_l2: -0.0428
      Episode_Reward/ang_vel_xy_l2: -0.1282
     Episode_Reward/dof_torques_l2: -0.0889
         Episode_Reward/dof_acc_l2: -0.1788
     Episode_Reward/action_rate_l2: -0.1737
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4265
Metrics/base_velocity/error_vel_xy: 0.3373
Metrics/base_velocity/error_vel_yaw: 0.4734
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 47808000
                    Iteration time: 1.08s
                      Time elapsed: 00:24:33
                               ETA: 00:03:12

################################################################################
                     [1m Learning iteration 1328/1500 [0m                     

                       Computation: 33338 steps/s (collection: 1.028s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0262
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 13.8007
                       Mean reward: 16.31
               Mean episode length: 815.97
Episode_Reward/track_lin_vel_xy_exp: 0.8859
Episode_Reward/track_ang_vel_z_exp: 0.4168
       Episode_Reward/lin_vel_z_l2: -0.0475
      Episode_Reward/ang_vel_xy_l2: -0.1280
     Episode_Reward/dof_torques_l2: -0.0801
         Episode_Reward/dof_acc_l2: -0.1822
     Episode_Reward/action_rate_l2: -0.1637
      Episode_Reward/feet_air_time: -0.0009
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4151
Metrics/base_velocity/error_vel_xy: 0.4688
Metrics/base_velocity/error_vel_yaw: 0.5002
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 47844000
                    Iteration time: 1.08s
                      Time elapsed: 00:24:35
                               ETA: 00:03:10

################################################################################
                     [1m Learning iteration 1329/1500 [0m                     

                       Computation: 33185 steps/s (collection: 1.030s, learning 0.054s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0253
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 13.8086
                       Mean reward: 15.13
               Mean episode length: 786.77
Episode_Reward/track_lin_vel_xy_exp: 0.9475
Episode_Reward/track_ang_vel_z_exp: 0.4168
       Episode_Reward/lin_vel_z_l2: -0.0427
      Episode_Reward/ang_vel_xy_l2: -0.1200
     Episode_Reward/dof_torques_l2: -0.0815
         Episode_Reward/dof_acc_l2: -0.1732
     Episode_Reward/action_rate_l2: -0.1613
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4037
Metrics/base_velocity/error_vel_xy: 0.3414
Metrics/base_velocity/error_vel_yaw: 0.4524
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 47880000
                    Iteration time: 1.08s
                      Time elapsed: 00:24:36
                               ETA: 00:03:09

################################################################################
                     [1m Learning iteration 1330/1500 [0m                     

                       Computation: 33384 steps/s (collection: 1.026s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0266
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 13.8095
                       Mean reward: 15.82
               Mean episode length: 823.22
Episode_Reward/track_lin_vel_xy_exp: 0.9711
Episode_Reward/track_ang_vel_z_exp: 0.4310
       Episode_Reward/lin_vel_z_l2: -0.0469
      Episode_Reward/ang_vel_xy_l2: -0.1287
     Episode_Reward/dof_torques_l2: -0.0781
         Episode_Reward/dof_acc_l2: -0.1881
     Episode_Reward/action_rate_l2: -0.1656
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3989
Metrics/base_velocity/error_vel_xy: 0.3710
Metrics/base_velocity/error_vel_yaw: 0.4646
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 47916000
                    Iteration time: 1.08s
                      Time elapsed: 00:24:37
                               ETA: 00:03:08

################################################################################
                     [1m Learning iteration 1331/1500 [0m                     

                       Computation: 33325 steps/s (collection: 1.027s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0214
               Mean surrogate loss: -0.0152
                 Mean entropy loss: 13.8072
                       Mean reward: 16.99
               Mean episode length: 832.88
Episode_Reward/track_lin_vel_xy_exp: 1.0123
Episode_Reward/track_ang_vel_z_exp: 0.4519
       Episode_Reward/lin_vel_z_l2: -0.0406
      Episode_Reward/ang_vel_xy_l2: -0.1251
     Episode_Reward/dof_torques_l2: -0.0857
         Episode_Reward/dof_acc_l2: -0.1805
     Episode_Reward/action_rate_l2: -0.1670
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4108
Metrics/base_velocity/error_vel_xy: 0.3721
Metrics/base_velocity/error_vel_yaw: 0.4726
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 47952000
                    Iteration time: 1.08s
                      Time elapsed: 00:24:38
                               ETA: 00:03:07

################################################################################
                     [1m Learning iteration 1332/1500 [0m                     

                       Computation: 33704 steps/s (collection: 1.016s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0233
               Mean surrogate loss: -0.0151
                 Mean entropy loss: 13.8067
                       Mean reward: 18.25
               Mean episode length: 863.09
Episode_Reward/track_lin_vel_xy_exp: 1.0612
Episode_Reward/track_ang_vel_z_exp: 0.4524
       Episode_Reward/lin_vel_z_l2: -0.0392
      Episode_Reward/ang_vel_xy_l2: -0.1239
     Episode_Reward/dof_torques_l2: -0.0791
         Episode_Reward/dof_acc_l2: -0.1737
     Episode_Reward/action_rate_l2: -0.1651
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4162
Metrics/base_velocity/error_vel_xy: 0.2911
Metrics/base_velocity/error_vel_yaw: 0.4432
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 47988000
                    Iteration time: 1.07s
                      Time elapsed: 00:24:39
                               ETA: 00:03:06

################################################################################
                     [1m Learning iteration 1333/1500 [0m                     

                       Computation: 33663 steps/s (collection: 1.018s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0217
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 13.8080
                       Mean reward: 18.46
               Mean episode length: 844.15
Episode_Reward/track_lin_vel_xy_exp: 1.1139
Episode_Reward/track_ang_vel_z_exp: 0.4830
       Episode_Reward/lin_vel_z_l2: -0.0431
      Episode_Reward/ang_vel_xy_l2: -0.1298
     Episode_Reward/dof_torques_l2: -0.0834
         Episode_Reward/dof_acc_l2: -0.1804
     Episode_Reward/action_rate_l2: -0.1747
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4165
Metrics/base_velocity/error_vel_xy: 0.3297
Metrics/base_velocity/error_vel_yaw: 0.4734
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 48024000
                    Iteration time: 1.07s
                      Time elapsed: 00:24:40
                               ETA: 00:03:05

################################################################################
                     [1m Learning iteration 1334/1500 [0m                     

                       Computation: 33384 steps/s (collection: 1.025s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0234
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 13.8068
                       Mean reward: 17.69
               Mean episode length: 851.87
Episode_Reward/track_lin_vel_xy_exp: 0.9835
Episode_Reward/track_ang_vel_z_exp: 0.4535
       Episode_Reward/lin_vel_z_l2: -0.0469
      Episode_Reward/ang_vel_xy_l2: -0.1358
     Episode_Reward/dof_torques_l2: -0.0883
         Episode_Reward/dof_acc_l2: -0.2063
     Episode_Reward/action_rate_l2: -0.1795
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4081
Metrics/base_velocity/error_vel_xy: 0.4650
Metrics/base_velocity/error_vel_yaw: 0.5236
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 48060000
                    Iteration time: 1.08s
                      Time elapsed: 00:24:41
                               ETA: 00:03:04

################################################################################
                     [1m Learning iteration 1335/1500 [0m                     

                       Computation: 33538 steps/s (collection: 1.020s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0211
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 13.8043
                       Mean reward: 17.02
               Mean episode length: 851.26
Episode_Reward/track_lin_vel_xy_exp: 0.9981
Episode_Reward/track_ang_vel_z_exp: 0.4406
       Episode_Reward/lin_vel_z_l2: -0.0412
      Episode_Reward/ang_vel_xy_l2: -0.1215
     Episode_Reward/dof_torques_l2: -0.0829
         Episode_Reward/dof_acc_l2: -0.1827
     Episode_Reward/action_rate_l2: -0.1667
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4086
Metrics/base_velocity/error_vel_xy: 0.3714
Metrics/base_velocity/error_vel_yaw: 0.4786
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 48096000
                    Iteration time: 1.07s
                      Time elapsed: 00:24:42
                               ETA: 00:03:03

################################################################################
                     [1m Learning iteration 1336/1500 [0m                     

                       Computation: 33338 steps/s (collection: 1.029s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0241
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 13.8017
                       Mean reward: 17.09
               Mean episode length: 844.42
Episode_Reward/track_lin_vel_xy_exp: 1.0630
Episode_Reward/track_ang_vel_z_exp: 0.4671
       Episode_Reward/lin_vel_z_l2: -0.0433
      Episode_Reward/ang_vel_xy_l2: -0.1297
     Episode_Reward/dof_torques_l2: -0.0848
         Episode_Reward/dof_acc_l2: -0.1833
     Episode_Reward/action_rate_l2: -0.1736
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3983
Metrics/base_velocity/error_vel_xy: 0.3647
Metrics/base_velocity/error_vel_yaw: 0.4883
      Episode_Termination/time_out: 2.0000
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 48132000
                    Iteration time: 1.08s
                      Time elapsed: 00:24:43
                               ETA: 00:03:01

################################################################################
                     [1m Learning iteration 1337/1500 [0m                     

                       Computation: 33206 steps/s (collection: 1.032s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0222
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 13.8036
                       Mean reward: 16.74
               Mean episode length: 844.37
Episode_Reward/track_lin_vel_xy_exp: 0.8887
Episode_Reward/track_ang_vel_z_exp: 0.4107
       Episode_Reward/lin_vel_z_l2: -0.0450
      Episode_Reward/ang_vel_xy_l2: -0.1227
     Episode_Reward/dof_torques_l2: -0.0828
         Episode_Reward/dof_acc_l2: -0.1842
     Episode_Reward/action_rate_l2: -0.1628
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.3965
Metrics/base_velocity/error_vel_xy: 0.4351
Metrics/base_velocity/error_vel_yaw: 0.4860
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 48168000
                    Iteration time: 1.08s
                      Time elapsed: 00:24:44
                               ETA: 00:03:00

################################################################################
                     [1m Learning iteration 1338/1500 [0m                     

                       Computation: 33390 steps/s (collection: 1.028s, learning 0.050s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0216
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 13.8109
                       Mean reward: 16.43
               Mean episode length: 847.92
Episode_Reward/track_lin_vel_xy_exp: 1.0175
Episode_Reward/track_ang_vel_z_exp: 0.4616
       Episode_Reward/lin_vel_z_l2: -0.0417
      Episode_Reward/ang_vel_xy_l2: -0.1282
     Episode_Reward/dof_torques_l2: -0.0915
         Episode_Reward/dof_acc_l2: -0.1824
     Episode_Reward/action_rate_l2: -0.1745
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4081
Metrics/base_velocity/error_vel_xy: 0.4161
Metrics/base_velocity/error_vel_yaw: 0.5045
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 48204000
                    Iteration time: 1.08s
                      Time elapsed: 00:24:45
                               ETA: 00:02:59

################################################################################
                     [1m Learning iteration 1339/1500 [0m                     

                       Computation: 32872 steps/s (collection: 1.042s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0256
               Mean surrogate loss: -0.0166
                 Mean entropy loss: 13.8122
                       Mean reward: 17.28
               Mean episode length: 867.63
Episode_Reward/track_lin_vel_xy_exp: 1.0787
Episode_Reward/track_ang_vel_z_exp: 0.4849
       Episode_Reward/lin_vel_z_l2: -0.0492
      Episode_Reward/ang_vel_xy_l2: -0.1459
     Episode_Reward/dof_torques_l2: -0.0902
         Episode_Reward/dof_acc_l2: -0.2116
     Episode_Reward/action_rate_l2: -0.1880
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4174
Metrics/base_velocity/error_vel_xy: 0.4408
Metrics/base_velocity/error_vel_yaw: 0.5366
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 48240000
                    Iteration time: 1.10s
                      Time elapsed: 00:24:46
                               ETA: 00:02:58

################################################################################
                     [1m Learning iteration 1340/1500 [0m                     

                       Computation: 32744 steps/s (collection: 1.047s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0284
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 13.8079
                       Mean reward: 17.32
               Mean episode length: 866.15
Episode_Reward/track_lin_vel_xy_exp: 0.9797
Episode_Reward/track_ang_vel_z_exp: 0.4226
       Episode_Reward/lin_vel_z_l2: -0.0430
      Episode_Reward/ang_vel_xy_l2: -0.1197
     Episode_Reward/dof_torques_l2: -0.0817
         Episode_Reward/dof_acc_l2: -0.1715
     Episode_Reward/action_rate_l2: -0.1610
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4187
Metrics/base_velocity/error_vel_xy: 0.3297
Metrics/base_velocity/error_vel_yaw: 0.4633
      Episode_Termination/time_out: 2.1667
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 48276000
                    Iteration time: 1.10s
                      Time elapsed: 00:24:48
                               ETA: 00:02:57

################################################################################
                     [1m Learning iteration 1341/1500 [0m                     

                       Computation: 32569 steps/s (collection: 1.053s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0258
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 13.8085
                       Mean reward: 17.91
               Mean episode length: 874.93
Episode_Reward/track_lin_vel_xy_exp: 1.1340
Episode_Reward/track_ang_vel_z_exp: 0.5069
       Episode_Reward/lin_vel_z_l2: -0.0451
      Episode_Reward/ang_vel_xy_l2: -0.1398
     Episode_Reward/dof_torques_l2: -0.0886
         Episode_Reward/dof_acc_l2: -0.1979
     Episode_Reward/action_rate_l2: -0.1878
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4171
Metrics/base_velocity/error_vel_xy: 0.3904
Metrics/base_velocity/error_vel_yaw: 0.4972
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 48312000
                    Iteration time: 1.11s
                      Time elapsed: 00:24:49
                               ETA: 00:02:56

################################################################################
                     [1m Learning iteration 1342/1500 [0m                     

                       Computation: 32645 steps/s (collection: 1.051s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0255
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 13.7981
                       Mean reward: 17.67
               Mean episode length: 867.16
Episode_Reward/track_lin_vel_xy_exp: 0.8942
Episode_Reward/track_ang_vel_z_exp: 0.4010
       Episode_Reward/lin_vel_z_l2: -0.0400
      Episode_Reward/ang_vel_xy_l2: -0.1166
     Episode_Reward/dof_torques_l2: -0.0791
         Episode_Reward/dof_acc_l2: -0.1768
     Episode_Reward/action_rate_l2: -0.1569
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4298
Metrics/base_velocity/error_vel_xy: 0.3862
Metrics/base_velocity/error_vel_yaw: 0.4693
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 48348000
                    Iteration time: 1.10s
                      Time elapsed: 00:24:50
                               ETA: 00:02:55

################################################################################
                     [1m Learning iteration 1343/1500 [0m                     

                       Computation: 33678 steps/s (collection: 1.018s, learning 0.051s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0254
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 13.7927
                       Mean reward: 16.21
               Mean episode length: 814.13
Episode_Reward/track_lin_vel_xy_exp: 0.9644
Episode_Reward/track_ang_vel_z_exp: 0.4216
       Episode_Reward/lin_vel_z_l2: -0.0421
      Episode_Reward/ang_vel_xy_l2: -0.1224
     Episode_Reward/dof_torques_l2: -0.0791
         Episode_Reward/dof_acc_l2: -0.1783
     Episode_Reward/action_rate_l2: -0.1618
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4333
Metrics/base_velocity/error_vel_xy: 0.3603
Metrics/base_velocity/error_vel_yaw: 0.4799
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 48384000
                    Iteration time: 1.07s
                      Time elapsed: 00:24:51
                               ETA: 00:02:54

################################################################################
                     [1m Learning iteration 1344/1500 [0m                     

                       Computation: 33240 steps/s (collection: 1.030s, learning 0.053s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0260
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 13.7954
                       Mean reward: 15.58
               Mean episode length: 809.11
Episode_Reward/track_lin_vel_xy_exp: 0.9245
Episode_Reward/track_ang_vel_z_exp: 0.4090
       Episode_Reward/lin_vel_z_l2: -0.0415
      Episode_Reward/ang_vel_xy_l2: -0.1172
     Episode_Reward/dof_torques_l2: -0.0836
         Episode_Reward/dof_acc_l2: -0.1787
     Episode_Reward/action_rate_l2: -0.1615
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4390
Metrics/base_velocity/error_vel_xy: 0.3644
Metrics/base_velocity/error_vel_yaw: 0.4729
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 48420000
                    Iteration time: 1.08s
                      Time elapsed: 00:24:52
                               ETA: 00:02:53

################################################################################
                     [1m Learning iteration 1345/1500 [0m                     

                       Computation: 33544 steps/s (collection: 1.021s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0213
               Mean surrogate loss: -0.0160
                 Mean entropy loss: 13.7966
                       Mean reward: 16.23
               Mean episode length: 829.41
Episode_Reward/track_lin_vel_xy_exp: 1.0161
Episode_Reward/track_ang_vel_z_exp: 0.4455
       Episode_Reward/lin_vel_z_l2: -0.0431
      Episode_Reward/ang_vel_xy_l2: -0.1238
     Episode_Reward/dof_torques_l2: -0.0829
         Episode_Reward/dof_acc_l2: -0.1649
     Episode_Reward/action_rate_l2: -0.1649
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4441
Metrics/base_velocity/error_vel_xy: 0.3470
Metrics/base_velocity/error_vel_yaw: 0.4773
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 48456000
                    Iteration time: 1.07s
                      Time elapsed: 00:24:53
                               ETA: 00:02:51

################################################################################
                     [1m Learning iteration 1346/1500 [0m                     

                       Computation: 34154 steps/s (collection: 1.002s, learning 0.052s)
             Mean action noise std: 0.77
          Mean value_function loss: 0.0267
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 13.8045
                       Mean reward: 17.29
               Mean episode length: 830.45
Episode_Reward/track_lin_vel_xy_exp: 1.0580
Episode_Reward/track_ang_vel_z_exp: 0.4636
       Episode_Reward/lin_vel_z_l2: -0.0432
      Episode_Reward/ang_vel_xy_l2: -0.1262
     Episode_Reward/dof_torques_l2: -0.0873
         Episode_Reward/dof_acc_l2: -0.1866
     Episode_Reward/action_rate_l2: -0.1760
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4504
Metrics/base_velocity/error_vel_xy: 0.3595
Metrics/base_velocity/error_vel_yaw: 0.4807
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 48492000
                    Iteration time: 1.05s
                      Time elapsed: 00:24:54
                               ETA: 00:02:50

################################################################################
                     [1m Learning iteration 1347/1500 [0m                     

                       Computation: 34085 steps/s (collection: 1.004s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0271
               Mean surrogate loss: -0.0156
                 Mean entropy loss: 13.8175
                       Mean reward: 17.86
               Mean episode length: 837.88
Episode_Reward/track_lin_vel_xy_exp: 0.8803
Episode_Reward/track_ang_vel_z_exp: 0.3941
       Episode_Reward/lin_vel_z_l2: -0.0394
      Episode_Reward/ang_vel_xy_l2: -0.1128
     Episode_Reward/dof_torques_l2: -0.0769
         Episode_Reward/dof_acc_l2: -0.1660
     Episode_Reward/action_rate_l2: -0.1497
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4477
Metrics/base_velocity/error_vel_xy: 0.3569
Metrics/base_velocity/error_vel_yaw: 0.4403
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 48528000
                    Iteration time: 1.06s
                      Time elapsed: 00:24:55
                               ETA: 00:02:49

################################################################################
                     [1m Learning iteration 1348/1500 [0m                     

                       Computation: 33582 steps/s (collection: 1.021s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0245
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 13.8235
                       Mean reward: 17.32
               Mean episode length: 808.73
Episode_Reward/track_lin_vel_xy_exp: 1.0944
Episode_Reward/track_ang_vel_z_exp: 0.4667
       Episode_Reward/lin_vel_z_l2: -0.0416
      Episode_Reward/ang_vel_xy_l2: -0.1279
     Episode_Reward/dof_torques_l2: -0.0840
         Episode_Reward/dof_acc_l2: -0.1913
     Episode_Reward/action_rate_l2: -0.1753
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4540
Metrics/base_velocity/error_vel_xy: 0.3219
Metrics/base_velocity/error_vel_yaw: 0.4776
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 48564000
                    Iteration time: 1.07s
                      Time elapsed: 00:24:56
                               ETA: 00:02:48

################################################################################
                     [1m Learning iteration 1349/1500 [0m                     

                       Computation: 33056 steps/s (collection: 1.036s, learning 0.053s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0263
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 13.8254
                       Mean reward: 17.24
               Mean episode length: 824.45
Episode_Reward/track_lin_vel_xy_exp: 1.0191
Episode_Reward/track_ang_vel_z_exp: 0.4640
       Episode_Reward/lin_vel_z_l2: -0.0452
      Episode_Reward/ang_vel_xy_l2: -0.1310
     Episode_Reward/dof_torques_l2: -0.0887
         Episode_Reward/dof_acc_l2: -0.1893
     Episode_Reward/action_rate_l2: -0.1741
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4578
Metrics/base_velocity/error_vel_xy: 0.4263
Metrics/base_velocity/error_vel_yaw: 0.5085
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 48600000
                    Iteration time: 1.09s
                      Time elapsed: 00:24:57
                               ETA: 00:02:47

################################################################################
                     [1m Learning iteration 1350/1500 [0m                     

                       Computation: 33125 steps/s (collection: 1.034s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0327
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 13.8241
                       Mean reward: 17.09
               Mean episode length: 818.64
Episode_Reward/track_lin_vel_xy_exp: 1.0009
Episode_Reward/track_ang_vel_z_exp: 0.4332
       Episode_Reward/lin_vel_z_l2: -0.0476
      Episode_Reward/ang_vel_xy_l2: -0.1221
     Episode_Reward/dof_torques_l2: -0.0753
         Episode_Reward/dof_acc_l2: -0.1631
     Episode_Reward/action_rate_l2: -0.1583
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4618
Metrics/base_velocity/error_vel_xy: 0.3193
Metrics/base_velocity/error_vel_yaw: 0.4499
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 48636000
                    Iteration time: 1.09s
                      Time elapsed: 00:24:58
                               ETA: 00:02:46

################################################################################
                     [1m Learning iteration 1351/1500 [0m                     

                       Computation: 33448 steps/s (collection: 1.024s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0250
               Mean surrogate loss: -0.0153
                 Mean entropy loss: 13.8399
                       Mean reward: 17.20
               Mean episode length: 852.21
Episode_Reward/track_lin_vel_xy_exp: 1.0807
Episode_Reward/track_ang_vel_z_exp: 0.4784
       Episode_Reward/lin_vel_z_l2: -0.0457
      Episode_Reward/ang_vel_xy_l2: -0.1353
     Episode_Reward/dof_torques_l2: -0.0875
         Episode_Reward/dof_acc_l2: -0.1939
     Episode_Reward/action_rate_l2: -0.1789
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4689
Metrics/base_velocity/error_vel_xy: 0.4055
Metrics/base_velocity/error_vel_yaw: 0.5231
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 48672000
                    Iteration time: 1.08s
                      Time elapsed: 00:24:59
                               ETA: 00:02:45

################################################################################
                     [1m Learning iteration 1352/1500 [0m                     

                       Computation: 33294 steps/s (collection: 1.031s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0242
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 13.8442
                       Mean reward: 16.76
               Mean episode length: 848.36
Episode_Reward/track_lin_vel_xy_exp: 1.0872
Episode_Reward/track_ang_vel_z_exp: 0.4785
       Episode_Reward/lin_vel_z_l2: -0.0473
      Episode_Reward/ang_vel_xy_l2: -0.1306
     Episode_Reward/dof_torques_l2: -0.0881
         Episode_Reward/dof_acc_l2: -0.1973
     Episode_Reward/action_rate_l2: -0.1789
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4780
Metrics/base_velocity/error_vel_xy: 0.3654
Metrics/base_velocity/error_vel_yaw: 0.4838
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 48708000
                    Iteration time: 1.08s
                      Time elapsed: 00:25:00
                               ETA: 00:02:44

################################################################################
                     [1m Learning iteration 1353/1500 [0m                     

                       Computation: 33876 steps/s (collection: 1.011s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0215
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 13.8413
                       Mean reward: 16.66
               Mean episode length: 829.75
Episode_Reward/track_lin_vel_xy_exp: 1.0205
Episode_Reward/track_ang_vel_z_exp: 0.4368
       Episode_Reward/lin_vel_z_l2: -0.0445
      Episode_Reward/ang_vel_xy_l2: -0.1220
     Episode_Reward/dof_torques_l2: -0.0834
         Episode_Reward/dof_acc_l2: -0.1824
     Episode_Reward/action_rate_l2: -0.1675
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4785
Metrics/base_velocity/error_vel_xy: 0.3061
Metrics/base_velocity/error_vel_yaw: 0.4491
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 48744000
                    Iteration time: 1.06s
                      Time elapsed: 00:25:02
                               ETA: 00:02:43

################################################################################
                     [1m Learning iteration 1354/1500 [0m                     

                       Computation: 33660 steps/s (collection: 1.018s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0218
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 13.8411
                       Mean reward: 17.04
               Mean episode length: 836.55
Episode_Reward/track_lin_vel_xy_exp: 1.0788
Episode_Reward/track_ang_vel_z_exp: 0.4818
       Episode_Reward/lin_vel_z_l2: -0.0419
      Episode_Reward/ang_vel_xy_l2: -0.1305
     Episode_Reward/dof_torques_l2: -0.0898
         Episode_Reward/dof_acc_l2: -0.1893
     Episode_Reward/action_rate_l2: -0.1781
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4880
Metrics/base_velocity/error_vel_xy: 0.3918
Metrics/base_velocity/error_vel_yaw: 0.4961
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 48780000
                    Iteration time: 1.07s
                      Time elapsed: 00:25:03
                               ETA: 00:02:41

################################################################################
                     [1m Learning iteration 1355/1500 [0m                     

                       Computation: 33509 steps/s (collection: 1.025s, learning 0.050s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0239
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 13.8543
                       Mean reward: 16.61
               Mean episode length: 818.86
Episode_Reward/track_lin_vel_xy_exp: 0.9991
Episode_Reward/track_ang_vel_z_exp: 0.4498
       Episode_Reward/lin_vel_z_l2: -0.0416
      Episode_Reward/ang_vel_xy_l2: -0.1233
     Episode_Reward/dof_torques_l2: -0.0833
         Episode_Reward/dof_acc_l2: -0.1865
     Episode_Reward/action_rate_l2: -0.1697
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4917
Metrics/base_velocity/error_vel_xy: 0.3739
Metrics/base_velocity/error_vel_yaw: 0.4564
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 48816000
                    Iteration time: 1.07s
                      Time elapsed: 00:25:04
                               ETA: 00:02:40

################################################################################
                     [1m Learning iteration 1356/1500 [0m                     

                       Computation: 33423 steps/s (collection: 1.026s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0219
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 13.8625
                       Mean reward: 17.35
               Mean episode length: 825.02
Episode_Reward/track_lin_vel_xy_exp: 1.0368
Episode_Reward/track_ang_vel_z_exp: 0.4531
       Episode_Reward/lin_vel_z_l2: -0.0440
      Episode_Reward/ang_vel_xy_l2: -0.1280
     Episode_Reward/dof_torques_l2: -0.0878
         Episode_Reward/dof_acc_l2: -0.1882
     Episode_Reward/action_rate_l2: -0.1768
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4842
Metrics/base_velocity/error_vel_xy: 0.3605
Metrics/base_velocity/error_vel_yaw: 0.4846
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 48852000
                    Iteration time: 1.08s
                      Time elapsed: 00:25:05
                               ETA: 00:02:39

################################################################################
                     [1m Learning iteration 1357/1500 [0m                     

                       Computation: 32927 steps/s (collection: 1.041s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0250
               Mean surrogate loss: -0.0150
                 Mean entropy loss: 13.8633
                       Mean reward: 17.96
               Mean episode length: 833.74
Episode_Reward/track_lin_vel_xy_exp: 0.9687
Episode_Reward/track_ang_vel_z_exp: 0.4226
       Episode_Reward/lin_vel_z_l2: -0.0398
      Episode_Reward/ang_vel_xy_l2: -0.1151
     Episode_Reward/dof_torques_l2: -0.0807
         Episode_Reward/dof_acc_l2: -0.1659
     Episode_Reward/action_rate_l2: -0.1580
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4816
Metrics/base_velocity/error_vel_xy: 0.3120
Metrics/base_velocity/error_vel_yaw: 0.4314
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 48888000
                    Iteration time: 1.09s
                      Time elapsed: 00:25:06
                               ETA: 00:02:38

################################################################################
                     [1m Learning iteration 1358/1500 [0m                     

                       Computation: 33502 steps/s (collection: 1.023s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0211
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 13.8695
                       Mean reward: 18.34
               Mean episode length: 869.71
Episode_Reward/track_lin_vel_xy_exp: 1.1649
Episode_Reward/track_ang_vel_z_exp: 0.5159
       Episode_Reward/lin_vel_z_l2: -0.0484
      Episode_Reward/ang_vel_xy_l2: -0.1432
     Episode_Reward/dof_torques_l2: -0.0988
         Episode_Reward/dof_acc_l2: -0.2150
     Episode_Reward/action_rate_l2: -0.1965
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4716
Metrics/base_velocity/error_vel_xy: 0.4037
Metrics/base_velocity/error_vel_yaw: 0.5363
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 48924000
                    Iteration time: 1.07s
                      Time elapsed: 00:25:07
                               ETA: 00:02:37

################################################################################
                     [1m Learning iteration 1359/1500 [0m                     

                       Computation: 33404 steps/s (collection: 1.026s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0280
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 13.8701
                       Mean reward: 16.89
               Mean episode length: 839.55
Episode_Reward/track_lin_vel_xy_exp: 0.9018
Episode_Reward/track_ang_vel_z_exp: 0.4197
       Episode_Reward/lin_vel_z_l2: -0.0467
      Episode_Reward/ang_vel_xy_l2: -0.1250
     Episode_Reward/dof_torques_l2: -0.0794
         Episode_Reward/dof_acc_l2: -0.1826
     Episode_Reward/action_rate_l2: -0.1629
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4640
Metrics/base_velocity/error_vel_xy: 0.4229
Metrics/base_velocity/error_vel_yaw: 0.4841
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 48960000
                    Iteration time: 1.08s
                      Time elapsed: 00:25:08
                               ETA: 00:02:36

################################################################################
                     [1m Learning iteration 1360/1500 [0m                     

                       Computation: 33253 steps/s (collection: 1.030s, learning 0.053s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0250
               Mean surrogate loss: -0.0149
                 Mean entropy loss: 13.8572
                       Mean reward: 16.31
               Mean episode length: 814.19
Episode_Reward/track_lin_vel_xy_exp: 1.0543
Episode_Reward/track_ang_vel_z_exp: 0.4670
       Episode_Reward/lin_vel_z_l2: -0.0432
      Episode_Reward/ang_vel_xy_l2: -0.1275
     Episode_Reward/dof_torques_l2: -0.0869
         Episode_Reward/dof_acc_l2: -0.1860
     Episode_Reward/action_rate_l2: -0.1735
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4543
Metrics/base_velocity/error_vel_xy: 0.3867
Metrics/base_velocity/error_vel_yaw: 0.4864
      Episode_Termination/time_out: 2.0417
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 48996000
                    Iteration time: 1.08s
                      Time elapsed: 00:25:09
                               ETA: 00:02:35

################################################################################
                     [1m Learning iteration 1361/1500 [0m                     

                       Computation: 33631 steps/s (collection: 1.018s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0249
               Mean surrogate loss: -0.0158
                 Mean entropy loss: 13.8475
                       Mean reward: 17.40
               Mean episode length: 832.47
Episode_Reward/track_lin_vel_xy_exp: 1.0509
Episode_Reward/track_ang_vel_z_exp: 0.4511
       Episode_Reward/lin_vel_z_l2: -0.0455
      Episode_Reward/ang_vel_xy_l2: -0.1287
     Episode_Reward/dof_torques_l2: -0.0822
         Episode_Reward/dof_acc_l2: -0.1892
     Episode_Reward/action_rate_l2: -0.1734
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4502
Metrics/base_velocity/error_vel_xy: 0.3385
Metrics/base_velocity/error_vel_yaw: 0.4866
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 49032000
                    Iteration time: 1.07s
                      Time elapsed: 00:25:10
                               ETA: 00:02:34

################################################################################
                     [1m Learning iteration 1362/1500 [0m                     

                       Computation: 33757 steps/s (collection: 1.014s, learning 0.053s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0276
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 13.8570
                       Mean reward: 16.38
               Mean episode length: 786.04
Episode_Reward/track_lin_vel_xy_exp: 0.9459
Episode_Reward/track_ang_vel_z_exp: 0.4218
       Episode_Reward/lin_vel_z_l2: -0.0432
      Episode_Reward/ang_vel_xy_l2: -0.1225
     Episode_Reward/dof_torques_l2: -0.0780
         Episode_Reward/dof_acc_l2: -0.1796
     Episode_Reward/action_rate_l2: -0.1628
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4433
Metrics/base_velocity/error_vel_xy: 0.3895
Metrics/base_velocity/error_vel_yaw: 0.4763
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 49068000
                    Iteration time: 1.07s
                      Time elapsed: 00:25:11
                               ETA: 00:02:33

################################################################################
                     [1m Learning iteration 1363/1500 [0m                     

                       Computation: 33005 steps/s (collection: 1.038s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0243
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 13.8637
                       Mean reward: 16.69
               Mean episode length: 817.65
Episode_Reward/track_lin_vel_xy_exp: 1.0319
Episode_Reward/track_ang_vel_z_exp: 0.4617
       Episode_Reward/lin_vel_z_l2: -0.0486
      Episode_Reward/ang_vel_xy_l2: -0.1317
     Episode_Reward/dof_torques_l2: -0.0897
         Episode_Reward/dof_acc_l2: -0.2066
     Episode_Reward/action_rate_l2: -0.1796
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4436
Metrics/base_velocity/error_vel_xy: 0.3978
Metrics/base_velocity/error_vel_yaw: 0.4918
      Episode_Termination/time_out: 2.0000
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 49104000
                    Iteration time: 1.09s
                      Time elapsed: 00:25:12
                               ETA: 00:02:31

################################################################################
                     [1m Learning iteration 1364/1500 [0m                     

                       Computation: 33657 steps/s (collection: 1.019s, learning 0.050s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0232
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 13.8649
                       Mean reward: 18.06
               Mean episode length: 860.91
Episode_Reward/track_lin_vel_xy_exp: 0.9994
Episode_Reward/track_ang_vel_z_exp: 0.4310
       Episode_Reward/lin_vel_z_l2: -0.0433
      Episode_Reward/ang_vel_xy_l2: -0.1188
     Episode_Reward/dof_torques_l2: -0.0808
         Episode_Reward/dof_acc_l2: -0.1766
     Episode_Reward/action_rate_l2: -0.1611
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4291
Metrics/base_velocity/error_vel_xy: 0.3156
Metrics/base_velocity/error_vel_yaw: 0.4501
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 49140000
                    Iteration time: 1.07s
                      Time elapsed: 00:25:13
                               ETA: 00:02:30

################################################################################
                     [1m Learning iteration 1365/1500 [0m                     

                       Computation: 33283 steps/s (collection: 1.031s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0252
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 13.8707
                       Mean reward: 18.08
               Mean episode length: 854.62
Episode_Reward/track_lin_vel_xy_exp: 1.0734
Episode_Reward/track_ang_vel_z_exp: 0.4663
       Episode_Reward/lin_vel_z_l2: -0.0470
      Episode_Reward/ang_vel_xy_l2: -0.1336
     Episode_Reward/dof_torques_l2: -0.0844
         Episode_Reward/dof_acc_l2: -0.1874
     Episode_Reward/action_rate_l2: -0.1755
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4352
Metrics/base_velocity/error_vel_xy: 0.3557
Metrics/base_velocity/error_vel_yaw: 0.4897
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 49176000
                    Iteration time: 1.08s
                      Time elapsed: 00:25:14
                               ETA: 00:02:29

################################################################################
                     [1m Learning iteration 1366/1500 [0m                     

                       Computation: 33296 steps/s (collection: 1.031s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0218
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 13.8752
                       Mean reward: 18.30
               Mean episode length: 866.81
Episode_Reward/track_lin_vel_xy_exp: 1.0759
Episode_Reward/track_ang_vel_z_exp: 0.4615
       Episode_Reward/lin_vel_z_l2: -0.0446
      Episode_Reward/ang_vel_xy_l2: -0.1255
     Episode_Reward/dof_torques_l2: -0.0861
         Episode_Reward/dof_acc_l2: -0.1877
     Episode_Reward/action_rate_l2: -0.1747
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4375
Metrics/base_velocity/error_vel_xy: 0.3286
Metrics/base_velocity/error_vel_yaw: 0.4808
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 49212000
                    Iteration time: 1.08s
                      Time elapsed: 00:25:16
                               ETA: 00:02:28

################################################################################
                     [1m Learning iteration 1367/1500 [0m                     

                       Computation: 33152 steps/s (collection: 1.034s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0217
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.8753
                       Mean reward: 16.53
               Mean episode length: 796.20
Episode_Reward/track_lin_vel_xy_exp: 0.9037
Episode_Reward/track_ang_vel_z_exp: 0.4071
       Episode_Reward/lin_vel_z_l2: -0.0380
      Episode_Reward/ang_vel_xy_l2: -0.1138
     Episode_Reward/dof_torques_l2: -0.0791
         Episode_Reward/dof_acc_l2: -0.1630
     Episode_Reward/action_rate_l2: -0.1545
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4331
Metrics/base_velocity/error_vel_xy: 0.3493
Metrics/base_velocity/error_vel_yaw: 0.4330
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49248000
                    Iteration time: 1.09s
                      Time elapsed: 00:25:17
                               ETA: 00:02:27

################################################################################
                     [1m Learning iteration 1368/1500 [0m                     

                       Computation: 33920 steps/s (collection: 1.010s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0226
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 13.8671
                       Mean reward: 17.56
               Mean episode length: 837.81
Episode_Reward/track_lin_vel_xy_exp: 1.1382
Episode_Reward/track_ang_vel_z_exp: 0.5127
       Episode_Reward/lin_vel_z_l2: -0.0473
      Episode_Reward/ang_vel_xy_l2: -0.1421
     Episode_Reward/dof_torques_l2: -0.0977
         Episode_Reward/dof_acc_l2: -0.2042
     Episode_Reward/action_rate_l2: -0.1936
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4416
Metrics/base_velocity/error_vel_xy: 0.4547
Metrics/base_velocity/error_vel_yaw: 0.5502
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 49284000
                    Iteration time: 1.06s
                      Time elapsed: 00:25:18
                               ETA: 00:02:26

################################################################################
                     [1m Learning iteration 1369/1500 [0m                     

                       Computation: 32989 steps/s (collection: 1.040s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0234
               Mean surrogate loss: -0.0151
                 Mean entropy loss: 13.8706
                       Mean reward: 16.61
               Mean episode length: 838.73
Episode_Reward/track_lin_vel_xy_exp: 0.9470
Episode_Reward/track_ang_vel_z_exp: 0.4252
       Episode_Reward/lin_vel_z_l2: -0.0472
      Episode_Reward/ang_vel_xy_l2: -0.1234
     Episode_Reward/dof_torques_l2: -0.0825
         Episode_Reward/dof_acc_l2: -0.1814
     Episode_Reward/action_rate_l2: -0.1637
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4482
Metrics/base_velocity/error_vel_xy: 0.3752
Metrics/base_velocity/error_vel_yaw: 0.4624
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 49320000
                    Iteration time: 1.09s
                      Time elapsed: 00:25:19
                               ETA: 00:02:25

################################################################################
                     [1m Learning iteration 1370/1500 [0m                     

                       Computation: 33643 steps/s (collection: 1.018s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0248
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 13.8852
                       Mean reward: 16.38
               Mean episode length: 838.26
Episode_Reward/track_lin_vel_xy_exp: 0.9843
Episode_Reward/track_ang_vel_z_exp: 0.4647
       Episode_Reward/lin_vel_z_l2: -0.0449
      Episode_Reward/ang_vel_xy_l2: -0.1332
     Episode_Reward/dof_torques_l2: -0.0870
         Episode_Reward/dof_acc_l2: -0.1940
     Episode_Reward/action_rate_l2: -0.1779
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4515
Metrics/base_velocity/error_vel_xy: 0.4699
Metrics/base_velocity/error_vel_yaw: 0.5054
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 49356000
                    Iteration time: 1.07s
                      Time elapsed: 00:25:20
                               ETA: 00:02:24

################################################################################
                     [1m Learning iteration 1371/1500 [0m                     

                       Computation: 33050 steps/s (collection: 1.038s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0211
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 13.8977
                       Mean reward: 17.01
               Mean episode length: 837.39
Episode_Reward/track_lin_vel_xy_exp: 0.9206
Episode_Reward/track_ang_vel_z_exp: 0.4073
       Episode_Reward/lin_vel_z_l2: -0.0402
      Episode_Reward/ang_vel_xy_l2: -0.1113
     Episode_Reward/dof_torques_l2: -0.0764
         Episode_Reward/dof_acc_l2: -0.1654
     Episode_Reward/action_rate_l2: -0.1531
      Episode_Reward/feet_air_time: -0.0009
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4530
Metrics/base_velocity/error_vel_xy: 0.3266
Metrics/base_velocity/error_vel_yaw: 0.4293
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 49392000
                    Iteration time: 1.09s
                      Time elapsed: 00:25:21
                               ETA: 00:02:23

################################################################################
                     [1m Learning iteration 1372/1500 [0m                     

                       Computation: 32474 steps/s (collection: 1.058s, learning 0.050s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0265
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 13.8997
                       Mean reward: 17.79
               Mean episode length: 864.53
Episode_Reward/track_lin_vel_xy_exp: 1.0875
Episode_Reward/track_ang_vel_z_exp: 0.4860
       Episode_Reward/lin_vel_z_l2: -0.0476
      Episode_Reward/ang_vel_xy_l2: -0.1359
     Episode_Reward/dof_torques_l2: -0.0915
         Episode_Reward/dof_acc_l2: -0.2146
     Episode_Reward/action_rate_l2: -0.1892
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4428
Metrics/base_velocity/error_vel_xy: 0.4046
Metrics/base_velocity/error_vel_yaw: 0.5100
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 49428000
                    Iteration time: 1.11s
                      Time elapsed: 00:25:22
                               ETA: 00:02:21

################################################################################
                     [1m Learning iteration 1373/1500 [0m                     

                       Computation: 33224 steps/s (collection: 1.034s, learning 0.050s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0229
               Mean surrogate loss: -0.0157
                 Mean entropy loss: 13.9009
                       Mean reward: 16.90
               Mean episode length: 842.35
Episode_Reward/track_lin_vel_xy_exp: 1.0307
Episode_Reward/track_ang_vel_z_exp: 0.4500
       Episode_Reward/lin_vel_z_l2: -0.0437
      Episode_Reward/ang_vel_xy_l2: -0.1263
     Episode_Reward/dof_torques_l2: -0.0840
         Episode_Reward/dof_acc_l2: -0.1750
     Episode_Reward/action_rate_l2: -0.1692
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4366
Metrics/base_velocity/error_vel_xy: 0.3498
Metrics/base_velocity/error_vel_yaw: 0.4803
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 49464000
                    Iteration time: 1.08s
                      Time elapsed: 00:25:23
                               ETA: 00:02:20

################################################################################
                     [1m Learning iteration 1374/1500 [0m                     

                       Computation: 33540 steps/s (collection: 1.022s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0216
               Mean surrogate loss: -0.0154
                 Mean entropy loss: 13.9041
                       Mean reward: 17.62
               Mean episode length: 843.09
Episode_Reward/track_lin_vel_xy_exp: 1.0634
Episode_Reward/track_ang_vel_z_exp: 0.4580
       Episode_Reward/lin_vel_z_l2: -0.0414
      Episode_Reward/ang_vel_xy_l2: -0.1223
     Episode_Reward/dof_torques_l2: -0.0784
         Episode_Reward/dof_acc_l2: -0.1737
     Episode_Reward/action_rate_l2: -0.1666
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4411
Metrics/base_velocity/error_vel_xy: 0.3071
Metrics/base_velocity/error_vel_yaw: 0.4470
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 49500000
                    Iteration time: 1.07s
                      Time elapsed: 00:25:24
                               ETA: 00:02:19

################################################################################
                     [1m Learning iteration 1375/1500 [0m                     

                       Computation: 33440 steps/s (collection: 1.025s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0198
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 13.9030
                       Mean reward: 17.84
               Mean episode length: 842.19
Episode_Reward/track_lin_vel_xy_exp: 1.0868
Episode_Reward/track_ang_vel_z_exp: 0.4885
       Episode_Reward/lin_vel_z_l2: -0.0449
      Episode_Reward/ang_vel_xy_l2: -0.1360
     Episode_Reward/dof_torques_l2: -0.0893
         Episode_Reward/dof_acc_l2: -0.1976
     Episode_Reward/action_rate_l2: -0.1834
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4470
Metrics/base_velocity/error_vel_xy: 0.4048
Metrics/base_velocity/error_vel_yaw: 0.4974
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 49536000
                    Iteration time: 1.08s
                      Time elapsed: 00:25:25
                               ETA: 00:02:18

################################################################################
                     [1m Learning iteration 1376/1500 [0m                     

                       Computation: 33447 steps/s (collection: 1.025s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0229
               Mean surrogate loss: -0.0150
                 Mean entropy loss: 13.9062
                       Mean reward: 17.98
               Mean episode length: 865.66
Episode_Reward/track_lin_vel_xy_exp: 0.9907
Episode_Reward/track_ang_vel_z_exp: 0.4389
       Episode_Reward/lin_vel_z_l2: -0.0445
      Episode_Reward/ang_vel_xy_l2: -0.1204
     Episode_Reward/dof_torques_l2: -0.0790
         Episode_Reward/dof_acc_l2: -0.1774
     Episode_Reward/action_rate_l2: -0.1632
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4514
Metrics/base_velocity/error_vel_xy: 0.3513
Metrics/base_velocity/error_vel_yaw: 0.4561
      Episode_Termination/time_out: 1.0833
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 49572000
                    Iteration time: 1.08s
                      Time elapsed: 00:25:26
                               ETA: 00:02:17

################################################################################
                     [1m Learning iteration 1377/1500 [0m                     

                       Computation: 33363 steps/s (collection: 1.028s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0226
               Mean surrogate loss: -0.0149
                 Mean entropy loss: 13.9081
                       Mean reward: 17.51
               Mean episode length: 877.43
Episode_Reward/track_lin_vel_xy_exp: 1.0073
Episode_Reward/track_ang_vel_z_exp: 0.4602
       Episode_Reward/lin_vel_z_l2: -0.0490
      Episode_Reward/ang_vel_xy_l2: -0.1368
     Episode_Reward/dof_torques_l2: -0.0887
         Episode_Reward/dof_acc_l2: -0.2061
     Episode_Reward/action_rate_l2: -0.1824
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4537
Metrics/base_velocity/error_vel_xy: 0.4621
Metrics/base_velocity/error_vel_yaw: 0.5420
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 49608000
                    Iteration time: 1.08s
                      Time elapsed: 00:25:27
                               ETA: 00:02:16

################################################################################
                     [1m Learning iteration 1378/1500 [0m                     

                       Computation: 33725 steps/s (collection: 1.015s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0227
               Mean surrogate loss: -0.0162
                 Mean entropy loss: 13.9097
                       Mean reward: 17.58
               Mean episode length: 883.36
Episode_Reward/track_lin_vel_xy_exp: 1.1213
Episode_Reward/track_ang_vel_z_exp: 0.5050
       Episode_Reward/lin_vel_z_l2: -0.0464
      Episode_Reward/ang_vel_xy_l2: -0.1409
     Episode_Reward/dof_torques_l2: -0.0942
         Episode_Reward/dof_acc_l2: -0.2098
     Episode_Reward/action_rate_l2: -0.1930
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4574
Metrics/base_velocity/error_vel_xy: 0.4341
Metrics/base_velocity/error_vel_yaw: 0.5305
      Episode_Termination/time_out: 2.0417
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 49644000
                    Iteration time: 1.07s
                      Time elapsed: 00:25:29
                               ETA: 00:02:15

################################################################################
                     [1m Learning iteration 1379/1500 [0m                     

                       Computation: 33208 steps/s (collection: 1.035s, learning 0.050s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0260
               Mean surrogate loss: -0.0153
                 Mean entropy loss: 13.9091
                       Mean reward: 17.52
               Mean episode length: 869.30
Episode_Reward/track_lin_vel_xy_exp: 0.9622
Episode_Reward/track_ang_vel_z_exp: 0.4318
       Episode_Reward/lin_vel_z_l2: -0.0450
      Episode_Reward/ang_vel_xy_l2: -0.1272
     Episode_Reward/dof_torques_l2: -0.0843
         Episode_Reward/dof_acc_l2: -0.1929
     Episode_Reward/action_rate_l2: -0.1695
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4567
Metrics/base_velocity/error_vel_xy: 0.3979
Metrics/base_velocity/error_vel_yaw: 0.4869
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 49680000
                    Iteration time: 1.08s
                      Time elapsed: 00:25:30
                               ETA: 00:02:14

################################################################################
                     [1m Learning iteration 1380/1500 [0m                     

                       Computation: 33318 steps/s (collection: 1.030s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0260
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 13.9103
                       Mean reward: 16.68
               Mean episode length: 835.45
Episode_Reward/track_lin_vel_xy_exp: 0.9365
Episode_Reward/track_ang_vel_z_exp: 0.4200
       Episode_Reward/lin_vel_z_l2: -0.0421
      Episode_Reward/ang_vel_xy_l2: -0.1172
     Episode_Reward/dof_torques_l2: -0.0811
         Episode_Reward/dof_acc_l2: -0.1720
     Episode_Reward/action_rate_l2: -0.1614
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4646
Metrics/base_velocity/error_vel_xy: 0.3781
Metrics/base_velocity/error_vel_yaw: 0.4602
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 49716000
                    Iteration time: 1.08s
                      Time elapsed: 00:25:31
                               ETA: 00:02:13

################################################################################
                     [1m Learning iteration 1381/1500 [0m                     

                       Computation: 32882 steps/s (collection: 1.043s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0273
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 13.9123
                       Mean reward: 16.92
               Mean episode length: 842.45
Episode_Reward/track_lin_vel_xy_exp: 1.0334
Episode_Reward/track_ang_vel_z_exp: 0.4455
       Episode_Reward/lin_vel_z_l2: -0.0528
      Episode_Reward/ang_vel_xy_l2: -0.1323
     Episode_Reward/dof_torques_l2: -0.0818
         Episode_Reward/dof_acc_l2: -0.1809
     Episode_Reward/action_rate_l2: -0.1669
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4721
Metrics/base_velocity/error_vel_xy: 0.3485
Metrics/base_velocity/error_vel_yaw: 0.4991
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 49752000
                    Iteration time: 1.09s
                      Time elapsed: 00:25:32
                               ETA: 00:02:11

################################################################################
                     [1m Learning iteration 1382/1500 [0m                     

                       Computation: 33375 steps/s (collection: 1.029s, learning 0.050s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0267
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 13.9161
                       Mean reward: 17.46
               Mean episode length: 853.63
Episode_Reward/track_lin_vel_xy_exp: 1.0173
Episode_Reward/track_ang_vel_z_exp: 0.4451
       Episode_Reward/lin_vel_z_l2: -0.0432
      Episode_Reward/ang_vel_xy_l2: -0.1226
     Episode_Reward/dof_torques_l2: -0.0815
         Episode_Reward/dof_acc_l2: -0.1790
     Episode_Reward/action_rate_l2: -0.1674
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4704
Metrics/base_velocity/error_vel_xy: 0.3442
Metrics/base_velocity/error_vel_yaw: 0.4685
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 49788000
                    Iteration time: 1.08s
                      Time elapsed: 00:25:33
                               ETA: 00:02:10

################################################################################
                     [1m Learning iteration 1383/1500 [0m                     

                       Computation: 33054 steps/s (collection: 1.037s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0249
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 13.9238
                       Mean reward: 18.13
               Mean episode length: 866.13
Episode_Reward/track_lin_vel_xy_exp: 1.1484
Episode_Reward/track_ang_vel_z_exp: 0.4988
       Episode_Reward/lin_vel_z_l2: -0.0474
      Episode_Reward/ang_vel_xy_l2: -0.1396
     Episode_Reward/dof_torques_l2: -0.0872
         Episode_Reward/dof_acc_l2: -0.2008
     Episode_Reward/action_rate_l2: -0.1865
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4751
Metrics/base_velocity/error_vel_xy: 0.3544
Metrics/base_velocity/error_vel_yaw: 0.4962
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 49824000
                    Iteration time: 1.09s
                      Time elapsed: 00:25:34
                               ETA: 00:02:09

################################################################################
                     [1m Learning iteration 1384/1500 [0m                     

                       Computation: 33461 steps/s (collection: 1.024s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0234
               Mean surrogate loss: -0.0159
                 Mean entropy loss: 13.9150
                       Mean reward: 18.56
               Mean episode length: 866.03
Episode_Reward/track_lin_vel_xy_exp: 0.9910
Episode_Reward/track_ang_vel_z_exp: 0.4347
       Episode_Reward/lin_vel_z_l2: -0.0500
      Episode_Reward/ang_vel_xy_l2: -0.1313
     Episode_Reward/dof_torques_l2: -0.0802
         Episode_Reward/dof_acc_l2: -0.1956
     Episode_Reward/action_rate_l2: -0.1693
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4853
Metrics/base_velocity/error_vel_xy: 0.3661
Metrics/base_velocity/error_vel_yaw: 0.4802
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 49860000
                    Iteration time: 1.08s
                      Time elapsed: 00:25:35
                               ETA: 00:02:08

################################################################################
                     [1m Learning iteration 1385/1500 [0m                     

                       Computation: 33049 steps/s (collection: 1.038s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0251
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 13.9002
                       Mean reward: 18.14
               Mean episode length: 869.33
Episode_Reward/track_lin_vel_xy_exp: 1.0001
Episode_Reward/track_ang_vel_z_exp: 0.4402
       Episode_Reward/lin_vel_z_l2: -0.0435
      Episode_Reward/ang_vel_xy_l2: -0.1259
     Episode_Reward/dof_torques_l2: -0.0844
         Episode_Reward/dof_acc_l2: -0.1938
     Episode_Reward/action_rate_l2: -0.1700
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4820
Metrics/base_velocity/error_vel_xy: 0.3656
Metrics/base_velocity/error_vel_yaw: 0.4778
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 49896000
                    Iteration time: 1.09s
                      Time elapsed: 00:25:36
                               ETA: 00:02:07

################################################################################
                     [1m Learning iteration 1386/1500 [0m                     

                       Computation: 32786 steps/s (collection: 1.047s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0236
               Mean surrogate loss: -0.0152
                 Mean entropy loss: 13.8997
                       Mean reward: 17.39
               Mean episode length: 864.78
Episode_Reward/track_lin_vel_xy_exp: 1.0413
Episode_Reward/track_ang_vel_z_exp: 0.4801
       Episode_Reward/lin_vel_z_l2: -0.0461
      Episode_Reward/ang_vel_xy_l2: -0.1327
     Episode_Reward/dof_torques_l2: -0.0913
         Episode_Reward/dof_acc_l2: -0.2060
     Episode_Reward/action_rate_l2: -0.1830
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4872
Metrics/base_velocity/error_vel_xy: 0.4362
Metrics/base_velocity/error_vel_yaw: 0.5120
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 49932000
                    Iteration time: 1.10s
                      Time elapsed: 00:25:37
                               ETA: 00:02:06

################################################################################
                     [1m Learning iteration 1387/1500 [0m                     

                       Computation: 31875 steps/s (collection: 1.076s, learning 0.053s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0222
               Mean surrogate loss: -0.0150
                 Mean entropy loss: 13.9075
                       Mean reward: 16.50
               Mean episode length: 853.05
Episode_Reward/track_lin_vel_xy_exp: 0.9475
Episode_Reward/track_ang_vel_z_exp: 0.4278
       Episode_Reward/lin_vel_z_l2: -0.0538
      Episode_Reward/ang_vel_xy_l2: -0.1313
     Episode_Reward/dof_torques_l2: -0.0786
         Episode_Reward/dof_acc_l2: -0.1882
     Episode_Reward/action_rate_l2: -0.1681
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4916
Metrics/base_velocity/error_vel_xy: 0.3897
Metrics/base_velocity/error_vel_yaw: 0.4765
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 49968000
                    Iteration time: 1.13s
                      Time elapsed: 00:25:38
                               ETA: 00:02:05

################################################################################
                     [1m Learning iteration 1388/1500 [0m                     

                       Computation: 33249 steps/s (collection: 1.031s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0233
               Mean surrogate loss: -0.0156
                 Mean entropy loss: 13.9202
                       Mean reward: 17.42
               Mean episode length: 866.55
Episode_Reward/track_lin_vel_xy_exp: 1.1123
Episode_Reward/track_ang_vel_z_exp: 0.4775
       Episode_Reward/lin_vel_z_l2: -0.0451
      Episode_Reward/ang_vel_xy_l2: -0.1274
     Episode_Reward/dof_torques_l2: -0.0896
         Episode_Reward/dof_acc_l2: -0.1854
     Episode_Reward/action_rate_l2: -0.1787
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4899
Metrics/base_velocity/error_vel_xy: 0.3306
Metrics/base_velocity/error_vel_yaw: 0.4800
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 50004000
                    Iteration time: 1.08s
                      Time elapsed: 00:25:39
                               ETA: 00:02:04

################################################################################
                     [1m Learning iteration 1389/1500 [0m                     

                       Computation: 32211 steps/s (collection: 1.065s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0272
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 13.9206
                       Mean reward: 18.43
               Mean episode length: 845.17
Episode_Reward/track_lin_vel_xy_exp: 1.0791
Episode_Reward/track_ang_vel_z_exp: 0.4695
       Episode_Reward/lin_vel_z_l2: -0.0415
      Episode_Reward/ang_vel_xy_l2: -0.1267
     Episode_Reward/dof_torques_l2: -0.0856
         Episode_Reward/dof_acc_l2: -0.1909
     Episode_Reward/action_rate_l2: -0.1748
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4920
Metrics/base_velocity/error_vel_xy: 0.3504
Metrics/base_velocity/error_vel_yaw: 0.4819
      Episode_Termination/time_out: 1.8750
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 50040000
                    Iteration time: 1.12s
                      Time elapsed: 00:25:41
                               ETA: 00:02:03

################################################################################
                     [1m Learning iteration 1390/1500 [0m                     

                       Computation: 32867 steps/s (collection: 1.044s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0210
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 13.9174
                       Mean reward: 18.78
               Mean episode length: 863.84
Episode_Reward/track_lin_vel_xy_exp: 1.1484
Episode_Reward/track_ang_vel_z_exp: 0.4986
       Episode_Reward/lin_vel_z_l2: -0.0481
      Episode_Reward/ang_vel_xy_l2: -0.1379
     Episode_Reward/dof_torques_l2: -0.0915
         Episode_Reward/dof_acc_l2: -0.2038
     Episode_Reward/action_rate_l2: -0.1902
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4907
Metrics/base_velocity/error_vel_xy: 0.3755
Metrics/base_velocity/error_vel_yaw: 0.5215
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 50076000
                    Iteration time: 1.10s
                      Time elapsed: 00:25:42
                               ETA: 00:02:01

################################################################################
                     [1m Learning iteration 1391/1500 [0m                     

                       Computation: 32994 steps/s (collection: 1.041s, learning 0.050s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0246
               Mean surrogate loss: -0.0149
                 Mean entropy loss: 13.9147
                       Mean reward: 18.68
               Mean episode length: 892.26
Episode_Reward/track_lin_vel_xy_exp: 1.1497
Episode_Reward/track_ang_vel_z_exp: 0.4980
       Episode_Reward/lin_vel_z_l2: -0.0452
      Episode_Reward/ang_vel_xy_l2: -0.1356
     Episode_Reward/dof_torques_l2: -0.0892
         Episode_Reward/dof_acc_l2: -0.2108
     Episode_Reward/action_rate_l2: -0.1904
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4928
Metrics/base_velocity/error_vel_xy: 0.3774
Metrics/base_velocity/error_vel_yaw: 0.5234
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 50112000
                    Iteration time: 1.09s
                      Time elapsed: 00:25:43
                               ETA: 00:02:00

################################################################################
                     [1m Learning iteration 1392/1500 [0m                     

                       Computation: 33032 steps/s (collection: 1.038s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0249
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 13.9169
                       Mean reward: 17.23
               Mean episode length: 839.29
Episode_Reward/track_lin_vel_xy_exp: 0.9727
Episode_Reward/track_ang_vel_z_exp: 0.4237
       Episode_Reward/lin_vel_z_l2: -0.0423
      Episode_Reward/ang_vel_xy_l2: -0.1191
     Episode_Reward/dof_torques_l2: -0.0758
         Episode_Reward/dof_acc_l2: -0.1691
     Episode_Reward/action_rate_l2: -0.1589
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4831
Metrics/base_velocity/error_vel_xy: 0.3294
Metrics/base_velocity/error_vel_yaw: 0.4442
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 50148000
                    Iteration time: 1.09s
                      Time elapsed: 00:25:44
                               ETA: 00:01:59

################################################################################
                     [1m Learning iteration 1393/1500 [0m                     

                       Computation: 32690 steps/s (collection: 1.049s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0269
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 13.9193
                       Mean reward: 16.88
               Mean episode length: 816.04
Episode_Reward/track_lin_vel_xy_exp: 1.1060
Episode_Reward/track_ang_vel_z_exp: 0.4770
       Episode_Reward/lin_vel_z_l2: -0.0475
      Episode_Reward/ang_vel_xy_l2: -0.1265
     Episode_Reward/dof_torques_l2: -0.0873
         Episode_Reward/dof_acc_l2: -0.1959
     Episode_Reward/action_rate_l2: -0.1797
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4803
Metrics/base_velocity/error_vel_xy: 0.3307
Metrics/base_velocity/error_vel_yaw: 0.4765
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 50184000
                    Iteration time: 1.10s
                      Time elapsed: 00:25:45
                               ETA: 00:01:58

################################################################################
                     [1m Learning iteration 1394/1500 [0m                     

                       Computation: 32574 steps/s (collection: 1.053s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0199
               Mean surrogate loss: -0.0151
                 Mean entropy loss: 13.9200
                       Mean reward: 17.41
               Mean episode length: 813.17
Episode_Reward/track_lin_vel_xy_exp: 1.0803
Episode_Reward/track_ang_vel_z_exp: 0.4636
       Episode_Reward/lin_vel_z_l2: -0.0460
      Episode_Reward/ang_vel_xy_l2: -0.1275
     Episode_Reward/dof_torques_l2: -0.0816
         Episode_Reward/dof_acc_l2: -0.1833
     Episode_Reward/action_rate_l2: -0.1711
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4871
Metrics/base_velocity/error_vel_xy: 0.3100
Metrics/base_velocity/error_vel_yaw: 0.4589
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 50220000
                    Iteration time: 1.11s
                      Time elapsed: 00:25:46
                               ETA: 00:01:57

################################################################################
                     [1m Learning iteration 1395/1500 [0m                     

                       Computation: 32846 steps/s (collection: 1.045s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0226
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 13.9126
                       Mean reward: 18.82
               Mean episode length: 881.49
Episode_Reward/track_lin_vel_xy_exp: 1.0766
Episode_Reward/track_ang_vel_z_exp: 0.4806
       Episode_Reward/lin_vel_z_l2: -0.0494
      Episode_Reward/ang_vel_xy_l2: -0.1326
     Episode_Reward/dof_torques_l2: -0.0900
         Episode_Reward/dof_acc_l2: -0.2005
     Episode_Reward/action_rate_l2: -0.1866
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4802
Metrics/base_velocity/error_vel_xy: 0.3963
Metrics/base_velocity/error_vel_yaw: 0.4980
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 50256000
                    Iteration time: 1.10s
                      Time elapsed: 00:25:47
                               ETA: 00:01:56

################################################################################
                     [1m Learning iteration 1396/1500 [0m                     

                       Computation: 33382 steps/s (collection: 1.027s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0261
               Mean surrogate loss: -0.0153
                 Mean entropy loss: 13.9038
                       Mean reward: 17.25
               Mean episode length: 847.11
Episode_Reward/track_lin_vel_xy_exp: 0.9666
Episode_Reward/track_ang_vel_z_exp: 0.4331
       Episode_Reward/lin_vel_z_l2: -0.0458
      Episode_Reward/ang_vel_xy_l2: -0.1271
     Episode_Reward/dof_torques_l2: -0.0830
         Episode_Reward/dof_acc_l2: -0.1817
     Episode_Reward/action_rate_l2: -0.1685
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4677
Metrics/base_velocity/error_vel_xy: 0.3888
Metrics/base_velocity/error_vel_yaw: 0.4819
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 50292000
                    Iteration time: 1.08s
                      Time elapsed: 00:25:48
                               ETA: 00:01:55

################################################################################
                     [1m Learning iteration 1397/1500 [0m                     

                       Computation: 33001 steps/s (collection: 1.039s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0245
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 13.8988
                       Mean reward: 16.72
               Mean episode length: 849.20
Episode_Reward/track_lin_vel_xy_exp: 1.0617
Episode_Reward/track_ang_vel_z_exp: 0.4793
       Episode_Reward/lin_vel_z_l2: -0.0469
      Episode_Reward/ang_vel_xy_l2: -0.1338
     Episode_Reward/dof_torques_l2: -0.0933
         Episode_Reward/dof_acc_l2: -0.2074
     Episode_Reward/action_rate_l2: -0.1883
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4652
Metrics/base_velocity/error_vel_xy: 0.4350
Metrics/base_velocity/error_vel_yaw: 0.5188
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 50328000
                    Iteration time: 1.09s
                      Time elapsed: 00:25:49
                               ETA: 00:01:54

################################################################################
                     [1m Learning iteration 1398/1500 [0m                     

                       Computation: 33202 steps/s (collection: 1.032s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0247
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 13.9046
                       Mean reward: 16.95
               Mean episode length: 840.56
Episode_Reward/track_lin_vel_xy_exp: 0.8464
Episode_Reward/track_ang_vel_z_exp: 0.3959
       Episode_Reward/lin_vel_z_l2: -0.0394
      Episode_Reward/ang_vel_xy_l2: -0.1080
     Episode_Reward/dof_torques_l2: -0.0743
         Episode_Reward/dof_acc_l2: -0.1571
     Episode_Reward/action_rate_l2: -0.1488
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4666
Metrics/base_velocity/error_vel_xy: 0.3755
Metrics/base_velocity/error_vel_yaw: 0.4093
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 50364000
                    Iteration time: 1.08s
                      Time elapsed: 00:25:50
                               ETA: 00:01:53

################################################################################
                     [1m Learning iteration 1399/1500 [0m                     

                       Computation: 33875 steps/s (collection: 1.011s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0270
               Mean surrogate loss: -0.0158
                 Mean entropy loss: 13.9089
                       Mean reward: 17.59
               Mean episode length: 821.15
Episode_Reward/track_lin_vel_xy_exp: 1.0058
Episode_Reward/track_ang_vel_z_exp: 0.4345
       Episode_Reward/lin_vel_z_l2: -0.0389
      Episode_Reward/ang_vel_xy_l2: -0.1187
     Episode_Reward/dof_torques_l2: -0.0751
         Episode_Reward/dof_acc_l2: -0.1708
     Episode_Reward/action_rate_l2: -0.1609
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4743
Metrics/base_velocity/error_vel_xy: 0.2977
Metrics/base_velocity/error_vel_yaw: 0.4284
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 50400000
                    Iteration time: 1.06s
                      Time elapsed: 00:25:51
                               ETA: 00:01:51

################################################################################
                     [1m Learning iteration 1400/1500 [0m                     

                       Computation: 33233 steps/s (collection: 1.031s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0245
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 13.9086
                       Mean reward: 16.87
               Mean episode length: 798.97
Episode_Reward/track_lin_vel_xy_exp: 1.0418
Episode_Reward/track_ang_vel_z_exp: 0.4540
       Episode_Reward/lin_vel_z_l2: -0.0433
      Episode_Reward/ang_vel_xy_l2: -0.1270
     Episode_Reward/dof_torques_l2: -0.0858
         Episode_Reward/dof_acc_l2: -0.1771
     Episode_Reward/action_rate_l2: -0.1725
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4639
Metrics/base_velocity/error_vel_xy: 0.3501
Metrics/base_velocity/error_vel_yaw: 0.4838
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 50436000
                    Iteration time: 1.08s
                      Time elapsed: 00:25:53
                               ETA: 00:01:50

################################################################################
                     [1m Learning iteration 1401/1500 [0m                     

                       Computation: 33241 steps/s (collection: 1.030s, learning 0.053s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0250
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 13.9073
                       Mean reward: 16.68
               Mean episode length: 823.23
Episode_Reward/track_lin_vel_xy_exp: 1.0813
Episode_Reward/track_ang_vel_z_exp: 0.4779
       Episode_Reward/lin_vel_z_l2: -0.0480
      Episode_Reward/ang_vel_xy_l2: -0.1354
     Episode_Reward/dof_torques_l2: -0.0899
         Episode_Reward/dof_acc_l2: -0.2004
     Episode_Reward/action_rate_l2: -0.1834
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4577
Metrics/base_velocity/error_vel_xy: 0.4108
Metrics/base_velocity/error_vel_yaw: 0.5266
      Episode_Termination/time_out: 2.0000
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 50472000
                    Iteration time: 1.08s
                      Time elapsed: 00:25:54
                               ETA: 00:01:49

################################################################################
                     [1m Learning iteration 1402/1500 [0m                     

                       Computation: 32618 steps/s (collection: 1.052s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0277
               Mean surrogate loss: -0.0157
                 Mean entropy loss: 13.9113
                       Mean reward: 18.13
               Mean episode length: 855.59
Episode_Reward/track_lin_vel_xy_exp: 1.0562
Episode_Reward/track_ang_vel_z_exp: 0.4557
       Episode_Reward/lin_vel_z_l2: -0.0383
      Episode_Reward/ang_vel_xy_l2: -0.1210
     Episode_Reward/dof_torques_l2: -0.0829
         Episode_Reward/dof_acc_l2: -0.1747
     Episode_Reward/action_rate_l2: -0.1695
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4653
Metrics/base_velocity/error_vel_xy: 0.3284
Metrics/base_velocity/error_vel_yaw: 0.4684
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 50508000
                    Iteration time: 1.10s
                      Time elapsed: 00:25:55
                               ETA: 00:01:48

################################################################################
                     [1m Learning iteration 1403/1500 [0m                     

                       Computation: 32529 steps/s (collection: 1.055s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0240
               Mean surrogate loss: -0.0152
                 Mean entropy loss: 13.9179
                       Mean reward: 18.20
               Mean episode length: 845.94
Episode_Reward/track_lin_vel_xy_exp: 1.0642
Episode_Reward/track_ang_vel_z_exp: 0.4698
       Episode_Reward/lin_vel_z_l2: -0.0435
      Episode_Reward/ang_vel_xy_l2: -0.1297
     Episode_Reward/dof_torques_l2: -0.0846
         Episode_Reward/dof_acc_l2: -0.1898
     Episode_Reward/action_rate_l2: -0.1777
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4588
Metrics/base_velocity/error_vel_xy: 0.3719
Metrics/base_velocity/error_vel_yaw: 0.4850
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 50544000
                    Iteration time: 1.11s
                      Time elapsed: 00:25:56
                               ETA: 00:01:47

################################################################################
                     [1m Learning iteration 1404/1500 [0m                     

                       Computation: 33530 steps/s (collection: 1.013s, learning 0.061s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0201
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 13.9216
                       Mean reward: 18.58
               Mean episode length: 881.71
Episode_Reward/track_lin_vel_xy_exp: 1.0800
Episode_Reward/track_ang_vel_z_exp: 0.4803
       Episode_Reward/lin_vel_z_l2: -0.0430
      Episode_Reward/ang_vel_xy_l2: -0.1307
     Episode_Reward/dof_torques_l2: -0.0888
         Episode_Reward/dof_acc_l2: -0.2023
     Episode_Reward/action_rate_l2: -0.1823
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4693
Metrics/base_velocity/error_vel_xy: 0.3960
Metrics/base_velocity/error_vel_yaw: 0.5132
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 50580000
                    Iteration time: 1.07s
                      Time elapsed: 00:25:57
                               ETA: 00:01:46

################################################################################
                     [1m Learning iteration 1405/1500 [0m                     

                       Computation: 33039 steps/s (collection: 1.039s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0232
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 13.9233
                       Mean reward: 18.32
               Mean episode length: 899.46
Episode_Reward/track_lin_vel_xy_exp: 1.0222
Episode_Reward/track_ang_vel_z_exp: 0.4576
       Episode_Reward/lin_vel_z_l2: -0.0443
      Episode_Reward/ang_vel_xy_l2: -0.1297
     Episode_Reward/dof_torques_l2: -0.0889
         Episode_Reward/dof_acc_l2: -0.1923
     Episode_Reward/action_rate_l2: -0.1787
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4738
Metrics/base_velocity/error_vel_xy: 0.4101
Metrics/base_velocity/error_vel_yaw: 0.5057
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 50616000
                    Iteration time: 1.09s
                      Time elapsed: 00:25:58
                               ETA: 00:01:45

################################################################################
                     [1m Learning iteration 1406/1500 [0m                     

                       Computation: 32924 steps/s (collection: 1.042s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0242
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 13.9250
                       Mean reward: 17.02
               Mean episode length: 839.09
Episode_Reward/track_lin_vel_xy_exp: 1.0411
Episode_Reward/track_ang_vel_z_exp: 0.4552
       Episode_Reward/lin_vel_z_l2: -0.0396
      Episode_Reward/ang_vel_xy_l2: -0.1223
     Episode_Reward/dof_torques_l2: -0.0828
         Episode_Reward/dof_acc_l2: -0.1805
     Episode_Reward/action_rate_l2: -0.1690
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4745
Metrics/base_velocity/error_vel_xy: 0.3383
Metrics/base_velocity/error_vel_yaw: 0.4579
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 50652000
                    Iteration time: 1.09s
                      Time elapsed: 00:25:59
                               ETA: 00:01:44

################################################################################
                     [1m Learning iteration 1407/1500 [0m                     

                       Computation: 32648 steps/s (collection: 1.048s, learning 0.054s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0258
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 13.9212
                       Mean reward: 18.23
               Mean episode length: 835.17
Episode_Reward/track_lin_vel_xy_exp: 1.0972
Episode_Reward/track_ang_vel_z_exp: 0.4773
       Episode_Reward/lin_vel_z_l2: -0.0430
      Episode_Reward/ang_vel_xy_l2: -0.1308
     Episode_Reward/dof_torques_l2: -0.0861
         Episode_Reward/dof_acc_l2: -0.1870
     Episode_Reward/action_rate_l2: -0.1768
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4794
Metrics/base_velocity/error_vel_xy: 0.3500
Metrics/base_velocity/error_vel_yaw: 0.4976
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 50688000
                    Iteration time: 1.10s
                      Time elapsed: 00:26:00
                               ETA: 00:01:43

################################################################################
                     [1m Learning iteration 1408/1500 [0m                     

                       Computation: 33378 steps/s (collection: 1.026s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0246
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 13.9227
                       Mean reward: 18.14
               Mean episode length: 854.46
Episode_Reward/track_lin_vel_xy_exp: 1.0104
Episode_Reward/track_ang_vel_z_exp: 0.4554
       Episode_Reward/lin_vel_z_l2: -0.0429
      Episode_Reward/ang_vel_xy_l2: -0.1289
     Episode_Reward/dof_torques_l2: -0.0840
         Episode_Reward/dof_acc_l2: -0.1794
     Episode_Reward/action_rate_l2: -0.1713
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4706
Metrics/base_velocity/error_vel_xy: 0.3942
Metrics/base_velocity/error_vel_yaw: 0.4958
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 50724000
                    Iteration time: 1.08s
                      Time elapsed: 00:26:01
                               ETA: 00:01:41

################################################################################
                     [1m Learning iteration 1409/1500 [0m                     

                       Computation: 32800 steps/s (collection: 1.048s, learning 0.050s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0246
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.9230
                       Mean reward: 17.81
               Mean episode length: 863.30
Episode_Reward/track_lin_vel_xy_exp: 1.1255
Episode_Reward/track_ang_vel_z_exp: 0.4984
       Episode_Reward/lin_vel_z_l2: -0.0426
      Episode_Reward/ang_vel_xy_l2: -0.1313
     Episode_Reward/dof_torques_l2: -0.0955
         Episode_Reward/dof_acc_l2: -0.1926
     Episode_Reward/action_rate_l2: -0.1872
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4689
Metrics/base_velocity/error_vel_xy: 0.3980
Metrics/base_velocity/error_vel_yaw: 0.5177
      Episode_Termination/time_out: 1.9167
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 50760000
                    Iteration time: 1.10s
                      Time elapsed: 00:26:02
                               ETA: 00:01:40

################################################################################
                     [1m Learning iteration 1410/1500 [0m                     

                       Computation: 32993 steps/s (collection: 1.040s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0229
               Mean surrogate loss: -0.0149
                 Mean entropy loss: 13.9238
                       Mean reward: 17.70
               Mean episode length: 846.57
Episode_Reward/track_lin_vel_xy_exp: 0.9104
Episode_Reward/track_ang_vel_z_exp: 0.4062
       Episode_Reward/lin_vel_z_l2: -0.0391
      Episode_Reward/ang_vel_xy_l2: -0.1151
     Episode_Reward/dof_torques_l2: -0.0788
         Episode_Reward/dof_acc_l2: -0.1685
     Episode_Reward/action_rate_l2: -0.1551
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4734
Metrics/base_velocity/error_vel_xy: 0.3381
Metrics/base_velocity/error_vel_yaw: 0.4224
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 50796000
                    Iteration time: 1.09s
                      Time elapsed: 00:26:03
                               ETA: 00:01:39

################################################################################
                     [1m Learning iteration 1411/1500 [0m                     

                       Computation: 33060 steps/s (collection: 1.038s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0266
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.9271
                       Mean reward: 17.46
               Mean episode length: 842.62
Episode_Reward/track_lin_vel_xy_exp: 1.0664
Episode_Reward/track_ang_vel_z_exp: 0.4687
       Episode_Reward/lin_vel_z_l2: -0.0409
      Episode_Reward/ang_vel_xy_l2: -0.1284
     Episode_Reward/dof_torques_l2: -0.0853
         Episode_Reward/dof_acc_l2: -0.1951
     Episode_Reward/action_rate_l2: -0.1792
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4747
Metrics/base_velocity/error_vel_xy: 0.3779
Metrics/base_velocity/error_vel_yaw: 0.4919
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 50832000
                    Iteration time: 1.09s
                      Time elapsed: 00:26:05
                               ETA: 00:01:38

################################################################################
                     [1m Learning iteration 1412/1500 [0m                     

                       Computation: 32916 steps/s (collection: 1.041s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0224
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 13.9207
                       Mean reward: 17.84
               Mean episode length: 876.25
Episode_Reward/track_lin_vel_xy_exp: 1.0304
Episode_Reward/track_ang_vel_z_exp: 0.4605
       Episode_Reward/lin_vel_z_l2: -0.0454
      Episode_Reward/ang_vel_xy_l2: -0.1337
     Episode_Reward/dof_torques_l2: -0.0874
         Episode_Reward/dof_acc_l2: -0.2023
     Episode_Reward/action_rate_l2: -0.1798
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4773
Metrics/base_velocity/error_vel_xy: 0.4153
Metrics/base_velocity/error_vel_yaw: 0.5320
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 50868000
                    Iteration time: 1.09s
                      Time elapsed: 00:26:06
                               ETA: 00:01:37

################################################################################
                     [1m Learning iteration 1413/1500 [0m                     

                       Computation: 33226 steps/s (collection: 1.034s, learning 0.049s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0223
               Mean surrogate loss: -0.0153
                 Mean entropy loss: 13.9241
                       Mean reward: 18.66
               Mean episode length: 901.47
Episode_Reward/track_lin_vel_xy_exp: 1.1345
Episode_Reward/track_ang_vel_z_exp: 0.4934
       Episode_Reward/lin_vel_z_l2: -0.0407
      Episode_Reward/ang_vel_xy_l2: -0.1320
     Episode_Reward/dof_torques_l2: -0.0901
         Episode_Reward/dof_acc_l2: -0.1943
     Episode_Reward/action_rate_l2: -0.1840
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4805
Metrics/base_velocity/error_vel_xy: 0.3564
Metrics/base_velocity/error_vel_yaw: 0.4925
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 50904000
                    Iteration time: 1.08s
                      Time elapsed: 00:26:07
                               ETA: 00:01:36

################################################################################
                     [1m Learning iteration 1414/1500 [0m                     

                       Computation: 33098 steps/s (collection: 1.038s, learning 0.049s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0252
               Mean surrogate loss: -0.0156
                 Mean entropy loss: 13.9245
                       Mean reward: 17.83
               Mean episode length: 867.80
Episode_Reward/track_lin_vel_xy_exp: 0.9796
Episode_Reward/track_ang_vel_z_exp: 0.4424
       Episode_Reward/lin_vel_z_l2: -0.0414
      Episode_Reward/ang_vel_xy_l2: -0.1244
     Episode_Reward/dof_torques_l2: -0.0860
         Episode_Reward/dof_acc_l2: -0.1856
     Episode_Reward/action_rate_l2: -0.1708
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4708
Metrics/base_velocity/error_vel_xy: 0.3989
Metrics/base_velocity/error_vel_yaw: 0.4891
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 50940000
                    Iteration time: 1.09s
                      Time elapsed: 00:26:08
                               ETA: 00:01:35

################################################################################
                     [1m Learning iteration 1415/1500 [0m                     

                       Computation: 33439 steps/s (collection: 1.025s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0263
               Mean surrogate loss: -0.0149
                 Mean entropy loss: 13.9207
                       Mean reward: 17.41
               Mean episode length: 843.86
Episode_Reward/track_lin_vel_xy_exp: 1.0867
Episode_Reward/track_ang_vel_z_exp: 0.4654
       Episode_Reward/lin_vel_z_l2: -0.0395
      Episode_Reward/ang_vel_xy_l2: -0.1244
     Episode_Reward/dof_torques_l2: -0.0835
         Episode_Reward/dof_acc_l2: -0.1867
     Episode_Reward/action_rate_l2: -0.1768
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4747
Metrics/base_velocity/error_vel_xy: 0.3648
Metrics/base_velocity/error_vel_yaw: 0.5304
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 50976000
                    Iteration time: 1.08s
                      Time elapsed: 00:26:09
                               ETA: 00:01:34

################################################################################
                     [1m Learning iteration 1416/1500 [0m                     

                       Computation: 33012 steps/s (collection: 1.036s, learning 0.055s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0240
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 13.9187
                       Mean reward: 17.48
               Mean episode length: 829.96
Episode_Reward/track_lin_vel_xy_exp: 1.0392
Episode_Reward/track_ang_vel_z_exp: 0.4523
       Episode_Reward/lin_vel_z_l2: -0.0462
      Episode_Reward/ang_vel_xy_l2: -0.1235
     Episode_Reward/dof_torques_l2: -0.0806
         Episode_Reward/dof_acc_l2: -0.1840
     Episode_Reward/action_rate_l2: -0.1714
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4774
Metrics/base_velocity/error_vel_xy: 0.3338
Metrics/base_velocity/error_vel_yaw: 0.4673
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 51012000
                    Iteration time: 1.09s
                      Time elapsed: 00:26:10
                               ETA: 00:01:33

################################################################################
                     [1m Learning iteration 1417/1500 [0m                     

                       Computation: 32595 steps/s (collection: 1.054s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0224
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 13.9112
                       Mean reward: 16.88
               Mean episode length: 827.75
Episode_Reward/track_lin_vel_xy_exp: 0.9349
Episode_Reward/track_ang_vel_z_exp: 0.4345
       Episode_Reward/lin_vel_z_l2: -0.0410
      Episode_Reward/ang_vel_xy_l2: -0.1241
     Episode_Reward/dof_torques_l2: -0.0830
         Episode_Reward/dof_acc_l2: -0.1820
     Episode_Reward/action_rate_l2: -0.1675
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4721
Metrics/base_velocity/error_vel_xy: 0.4259
Metrics/base_velocity/error_vel_yaw: 0.4872
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 51048000
                    Iteration time: 1.10s
                      Time elapsed: 00:26:11
                               ETA: 00:01:31

################################################################################
                     [1m Learning iteration 1418/1500 [0m                     

                       Computation: 33073 steps/s (collection: 1.037s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0185
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 13.8975
                       Mean reward: 15.58
               Mean episode length: 809.42
Episode_Reward/track_lin_vel_xy_exp: 0.7952
Episode_Reward/track_ang_vel_z_exp: 0.3596
       Episode_Reward/lin_vel_z_l2: -0.0393
      Episode_Reward/ang_vel_xy_l2: -0.1060
     Episode_Reward/dof_torques_l2: -0.0715
         Episode_Reward/dof_acc_l2: -0.1675
     Episode_Reward/action_rate_l2: -0.1465
      Episode_Reward/feet_air_time: -0.0009
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4714
Metrics/base_velocity/error_vel_xy: 0.3756
Metrics/base_velocity/error_vel_yaw: 0.4472
      Episode_Termination/time_out: 0.7083
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 51084000
                    Iteration time: 1.09s
                      Time elapsed: 00:26:12
                               ETA: 00:01:30

################################################################################
                     [1m Learning iteration 1419/1500 [0m                     

                       Computation: 33132 steps/s (collection: 1.035s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0208
               Mean surrogate loss: -0.0152
                 Mean entropy loss: 13.8874
                       Mean reward: 16.70
               Mean episode length: 837.46
Episode_Reward/track_lin_vel_xy_exp: 1.0226
Episode_Reward/track_ang_vel_z_exp: 0.4546
       Episode_Reward/lin_vel_z_l2: -0.0388
      Episode_Reward/ang_vel_xy_l2: -0.1225
     Episode_Reward/dof_torques_l2: -0.0829
         Episode_Reward/dof_acc_l2: -0.1771
     Episode_Reward/action_rate_l2: -0.1704
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4798
Metrics/base_velocity/error_vel_xy: 0.3676
Metrics/base_velocity/error_vel_yaw: 0.4709
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 51120000
                    Iteration time: 1.09s
                      Time elapsed: 00:26:13
                               ETA: 00:01:29

################################################################################
                     [1m Learning iteration 1420/1500 [0m                     

                       Computation: 31952 steps/s (collection: 1.074s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0204
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 13.8836
                       Mean reward: 17.74
               Mean episode length: 872.10
Episode_Reward/track_lin_vel_xy_exp: 0.9970
Episode_Reward/track_ang_vel_z_exp: 0.4621
       Episode_Reward/lin_vel_z_l2: -0.0404
      Episode_Reward/ang_vel_xy_l2: -0.1250
     Episode_Reward/dof_torques_l2: -0.0842
         Episode_Reward/dof_acc_l2: -0.1869
     Episode_Reward/action_rate_l2: -0.1723
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4871
Metrics/base_velocity/error_vel_xy: 0.4177
Metrics/base_velocity/error_vel_yaw: 0.4804
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 51156000
                    Iteration time: 1.13s
                      Time elapsed: 00:26:14
                               ETA: 00:01:28

################################################################################
                     [1m Learning iteration 1421/1500 [0m                     

                       Computation: 33326 steps/s (collection: 1.025s, learning 0.055s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0239
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 13.8842
                       Mean reward: 19.05
               Mean episode length: 907.85
Episode_Reward/track_lin_vel_xy_exp: 1.1143
Episode_Reward/track_ang_vel_z_exp: 0.4913
       Episode_Reward/lin_vel_z_l2: -0.0435
      Episode_Reward/ang_vel_xy_l2: -0.1302
     Episode_Reward/dof_torques_l2: -0.0905
         Episode_Reward/dof_acc_l2: -0.1965
     Episode_Reward/action_rate_l2: -0.1838
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4802
Metrics/base_velocity/error_vel_xy: 0.3830
Metrics/base_velocity/error_vel_yaw: 0.4992
      Episode_Termination/time_out: 2.0833
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 51192000
                    Iteration time: 1.08s
                      Time elapsed: 00:26:15
                               ETA: 00:01:27

################################################################################
                     [1m Learning iteration 1422/1500 [0m                     

                       Computation: 32458 steps/s (collection: 1.054s, learning 0.055s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0233
               Mean surrogate loss: -0.0155
                 Mean entropy loss: 13.8845
                       Mean reward: 18.24
               Mean episode length: 888.55
Episode_Reward/track_lin_vel_xy_exp: 1.1563
Episode_Reward/track_ang_vel_z_exp: 0.5059
       Episode_Reward/lin_vel_z_l2: -0.0442
      Episode_Reward/ang_vel_xy_l2: -0.1414
     Episode_Reward/dof_torques_l2: -0.0926
         Episode_Reward/dof_acc_l2: -0.1960
     Episode_Reward/action_rate_l2: -0.1903
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4807
Metrics/base_velocity/error_vel_xy: 0.3934
Metrics/base_velocity/error_vel_yaw: 0.5277
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 51228000
                    Iteration time: 1.11s
                      Time elapsed: 00:26:17
                               ETA: 00:01:26

################################################################################
                     [1m Learning iteration 1423/1500 [0m                     

                       Computation: 33162 steps/s (collection: 1.034s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0253
               Mean surrogate loss: -0.0151
                 Mean entropy loss: 13.8859
                       Mean reward: 18.14
               Mean episode length: 888.73
Episode_Reward/track_lin_vel_xy_exp: 1.0732
Episode_Reward/track_ang_vel_z_exp: 0.4682
       Episode_Reward/lin_vel_z_l2: -0.0386
      Episode_Reward/ang_vel_xy_l2: -0.1234
     Episode_Reward/dof_torques_l2: -0.0848
         Episode_Reward/dof_acc_l2: -0.1790
     Episode_Reward/action_rate_l2: -0.1737
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4895
Metrics/base_velocity/error_vel_xy: 0.3298
Metrics/base_velocity/error_vel_yaw: 0.4569
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 51264000
                    Iteration time: 1.09s
                      Time elapsed: 00:26:18
                               ETA: 00:01:25

################################################################################
                     [1m Learning iteration 1424/1500 [0m                     

                       Computation: 33362 steps/s (collection: 1.027s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0253
               Mean surrogate loss: -0.0151
                 Mean entropy loss: 13.8944
                       Mean reward: 17.80
               Mean episode length: 862.74
Episode_Reward/track_lin_vel_xy_exp: 1.1462
Episode_Reward/track_ang_vel_z_exp: 0.4990
       Episode_Reward/lin_vel_z_l2: -0.0455
      Episode_Reward/ang_vel_xy_l2: -0.1364
     Episode_Reward/dof_torques_l2: -0.0867
         Episode_Reward/dof_acc_l2: -0.2027
     Episode_Reward/action_rate_l2: -0.1872
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5016
Metrics/base_velocity/error_vel_xy: 0.3591
Metrics/base_velocity/error_vel_yaw: 0.4916
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 51300000
                    Iteration time: 1.08s
                      Time elapsed: 00:26:19
                               ETA: 00:01:24

################################################################################
                     [1m Learning iteration 1425/1500 [0m                     

                       Computation: 32961 steps/s (collection: 1.041s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0243
               Mean surrogate loss: -0.0151
                 Mean entropy loss: 13.8955
                       Mean reward: 18.05
               Mean episode length: 862.01
Episode_Reward/track_lin_vel_xy_exp: 0.9909
Episode_Reward/track_ang_vel_z_exp: 0.4566
       Episode_Reward/lin_vel_z_l2: -0.0414
      Episode_Reward/ang_vel_xy_l2: -0.1278
     Episode_Reward/dof_torques_l2: -0.0848
         Episode_Reward/dof_acc_l2: -0.1907
     Episode_Reward/action_rate_l2: -0.1734
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5117
Metrics/base_velocity/error_vel_xy: 0.4037
Metrics/base_velocity/error_vel_yaw: 0.4771
      Episode_Termination/time_out: 1.0417
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 51336000
                    Iteration time: 1.09s
                      Time elapsed: 00:26:20
                               ETA: 00:01:23

################################################################################
                     [1m Learning iteration 1426/1500 [0m                     

                       Computation: 32949 steps/s (collection: 1.043s, learning 0.049s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0245
               Mean surrogate loss: -0.0159
                 Mean entropy loss: 13.9012
                       Mean reward: 17.57
               Mean episode length: 847.59
Episode_Reward/track_lin_vel_xy_exp: 0.9915
Episode_Reward/track_ang_vel_z_exp: 0.4332
       Episode_Reward/lin_vel_z_l2: -0.0429
      Episode_Reward/ang_vel_xy_l2: -0.1241
     Episode_Reward/dof_torques_l2: -0.0771
         Episode_Reward/dof_acc_l2: -0.1852
     Episode_Reward/action_rate_l2: -0.1656
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5176
Metrics/base_velocity/error_vel_xy: 0.3405
Metrics/base_velocity/error_vel_yaw: 0.4599
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 51372000
                    Iteration time: 1.09s
                      Time elapsed: 00:26:21
                               ETA: 00:01:22

################################################################################
                     [1m Learning iteration 1427/1500 [0m                     

                       Computation: 32989 steps/s (collection: 1.041s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0264
               Mean surrogate loss: -0.0152
                 Mean entropy loss: 13.9000
                       Mean reward: 16.87
               Mean episode length: 813.75
Episode_Reward/track_lin_vel_xy_exp: 0.8874
Episode_Reward/track_ang_vel_z_exp: 0.4025
       Episode_Reward/lin_vel_z_l2: -0.0401
      Episode_Reward/ang_vel_xy_l2: -0.1171
     Episode_Reward/dof_torques_l2: -0.0817
         Episode_Reward/dof_acc_l2: -0.1723
     Episode_Reward/action_rate_l2: -0.1593
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5159
Metrics/base_velocity/error_vel_xy: 0.3886
Metrics/base_velocity/error_vel_yaw: 0.4560
      Episode_Termination/time_out: 1.0833
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 51408000
                    Iteration time: 1.09s
                      Time elapsed: 00:26:22
                               ETA: 00:01:20

################################################################################
                     [1m Learning iteration 1428/1500 [0m                     

                       Computation: 32759 steps/s (collection: 1.046s, learning 0.053s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0272
               Mean surrogate loss: -0.0157
                 Mean entropy loss: 13.8939
                       Mean reward: 15.65
               Mean episode length: 769.30
Episode_Reward/track_lin_vel_xy_exp: 0.9007
Episode_Reward/track_ang_vel_z_exp: 0.4030
       Episode_Reward/lin_vel_z_l2: -0.0436
      Episode_Reward/ang_vel_xy_l2: -0.1191
     Episode_Reward/dof_torques_l2: -0.0722
         Episode_Reward/dof_acc_l2: -0.1682
     Episode_Reward/action_rate_l2: -0.1525
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5175
Metrics/base_velocity/error_vel_xy: 0.3644
Metrics/base_velocity/error_vel_yaw: 0.4539
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 51444000
                    Iteration time: 1.10s
                      Time elapsed: 00:26:23
                               ETA: 00:01:19

################################################################################
                     [1m Learning iteration 1429/1500 [0m                     

                       Computation: 33158 steps/s (collection: 1.032s, learning 0.053s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0224
               Mean surrogate loss: -0.0154
                 Mean entropy loss: 13.8927
                       Mean reward: 15.75
               Mean episode length: 788.28
Episode_Reward/track_lin_vel_xy_exp: 1.0214
Episode_Reward/track_ang_vel_z_exp: 0.4508
       Episode_Reward/lin_vel_z_l2: -0.0426
      Episode_Reward/ang_vel_xy_l2: -0.1265
     Episode_Reward/dof_torques_l2: -0.0830
         Episode_Reward/dof_acc_l2: -0.1940
     Episode_Reward/action_rate_l2: -0.1710
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5061
Metrics/base_velocity/error_vel_xy: 0.3680
Metrics/base_velocity/error_vel_yaw: 0.4827
      Episode_Termination/time_out: 1.2083
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 51480000
                    Iteration time: 1.09s
                      Time elapsed: 00:26:24
                               ETA: 00:01:18

################################################################################
                     [1m Learning iteration 1430/1500 [0m                     

                       Computation: 33167 steps/s (collection: 1.033s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0218
               Mean surrogate loss: -0.0154
                 Mean entropy loss: 13.9004
                       Mean reward: 16.92
               Mean episode length: 852.36
Episode_Reward/track_lin_vel_xy_exp: 1.0678
Episode_Reward/track_ang_vel_z_exp: 0.4695
       Episode_Reward/lin_vel_z_l2: -0.0474
      Episode_Reward/ang_vel_xy_l2: -0.1336
     Episode_Reward/dof_torques_l2: -0.0888
         Episode_Reward/dof_acc_l2: -0.2132
     Episode_Reward/action_rate_l2: -0.1843
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5150
Metrics/base_velocity/error_vel_xy: 0.3791
Metrics/base_velocity/error_vel_yaw: 0.4930
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 51516000
                    Iteration time: 1.09s
                      Time elapsed: 00:26:25
                               ETA: 00:01:17

################################################################################
                     [1m Learning iteration 1431/1500 [0m                     

                       Computation: 33126 steps/s (collection: 1.036s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0200
               Mean surrogate loss: -0.0134
                 Mean entropy loss: 13.9047
                       Mean reward: 18.14
               Mean episode length: 881.32
Episode_Reward/track_lin_vel_xy_exp: 1.0312
Episode_Reward/track_ang_vel_z_exp: 0.4488
       Episode_Reward/lin_vel_z_l2: -0.0402
      Episode_Reward/ang_vel_xy_l2: -0.1250
     Episode_Reward/dof_torques_l2: -0.0836
         Episode_Reward/dof_acc_l2: -0.1789
     Episode_Reward/action_rate_l2: -0.1695
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5274
Metrics/base_velocity/error_vel_xy: 0.3488
Metrics/base_velocity/error_vel_yaw: 0.4747
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 51552000
                    Iteration time: 1.09s
                      Time elapsed: 00:26:26
                               ETA: 00:01:16

################################################################################
                     [1m Learning iteration 1432/1500 [0m                     

                       Computation: 31727 steps/s (collection: 1.084s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0247
               Mean surrogate loss: -0.0131
                 Mean entropy loss: 13.8962
                       Mean reward: 17.58
               Mean episode length: 858.17
Episode_Reward/track_lin_vel_xy_exp: 1.0258
Episode_Reward/track_ang_vel_z_exp: 0.4566
       Episode_Reward/lin_vel_z_l2: -0.0449
      Episode_Reward/ang_vel_xy_l2: -0.1257
     Episode_Reward/dof_torques_l2: -0.0830
         Episode_Reward/dof_acc_l2: -0.1880
     Episode_Reward/action_rate_l2: -0.1708
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5251
Metrics/base_velocity/error_vel_xy: 0.3670
Metrics/base_velocity/error_vel_yaw: 0.4732
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 51588000
                    Iteration time: 1.13s
                      Time elapsed: 00:26:27
                               ETA: 00:01:15

################################################################################
                     [1m Learning iteration 1433/1500 [0m                     

                       Computation: 31902 steps/s (collection: 1.078s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0250
               Mean surrogate loss: -0.0097
                 Mean entropy loss: 13.9044
                       Mean reward: 17.07
               Mean episode length: 838.86
Episode_Reward/track_lin_vel_xy_exp: 1.0763
Episode_Reward/track_ang_vel_z_exp: 0.4708
       Episode_Reward/lin_vel_z_l2: -0.0435
      Episode_Reward/ang_vel_xy_l2: -0.1280
     Episode_Reward/dof_torques_l2: -0.0860
         Episode_Reward/dof_acc_l2: -0.1887
     Episode_Reward/action_rate_l2: -0.1777
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5229
Metrics/base_velocity/error_vel_xy: 0.3497
Metrics/base_velocity/error_vel_yaw: 0.4735
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 51624000
                    Iteration time: 1.13s
                      Time elapsed: 00:26:29
                               ETA: 00:01:14

################################################################################
                     [1m Learning iteration 1434/1500 [0m                     

                       Computation: 31219 steps/s (collection: 1.098s, learning 0.056s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0204
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 13.9114
                       Mean reward: 16.76
               Mean episode length: 824.59
Episode_Reward/track_lin_vel_xy_exp: 1.0877
Episode_Reward/track_ang_vel_z_exp: 0.4750
       Episode_Reward/lin_vel_z_l2: -0.0444
      Episode_Reward/ang_vel_xy_l2: -0.1286
     Episode_Reward/dof_torques_l2: -0.0866
         Episode_Reward/dof_acc_l2: -0.1841
     Episode_Reward/action_rate_l2: -0.1757
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5237
Metrics/base_velocity/error_vel_xy: 0.3567
Metrics/base_velocity/error_vel_yaw: 0.4866
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 51660000
                    Iteration time: 1.15s
                      Time elapsed: 00:26:30
                               ETA: 00:01:13

################################################################################
                     [1m Learning iteration 1435/1500 [0m                     

                       Computation: 32214 steps/s (collection: 1.068s, learning 0.050s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0228
               Mean surrogate loss: -0.0150
                 Mean entropy loss: 13.9143
                       Mean reward: 16.75
               Mean episode length: 811.19
Episode_Reward/track_lin_vel_xy_exp: 0.9992
Episode_Reward/track_ang_vel_z_exp: 0.4393
       Episode_Reward/lin_vel_z_l2: -0.0398
      Episode_Reward/ang_vel_xy_l2: -0.1171
     Episode_Reward/dof_torques_l2: -0.0819
         Episode_Reward/dof_acc_l2: -0.1766
     Episode_Reward/action_rate_l2: -0.1662
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5199
Metrics/base_velocity/error_vel_xy: 0.3368
Metrics/base_velocity/error_vel_yaw: 0.4507
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51696000
                    Iteration time: 1.12s
                      Time elapsed: 00:26:31
                               ETA: 00:01:12

################################################################################
                     [1m Learning iteration 1436/1500 [0m                     

                       Computation: 32033 steps/s (collection: 1.071s, learning 0.053s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0248
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 13.9199
                       Mean reward: 18.03
               Mean episode length: 848.82
Episode_Reward/track_lin_vel_xy_exp: 1.1707
Episode_Reward/track_ang_vel_z_exp: 0.5116
       Episode_Reward/lin_vel_z_l2: -0.0463
      Episode_Reward/ang_vel_xy_l2: -0.1421
     Episode_Reward/dof_torques_l2: -0.0904
         Episode_Reward/dof_acc_l2: -0.1996
     Episode_Reward/action_rate_l2: -0.1891
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5129
Metrics/base_velocity/error_vel_xy: 0.3822
Metrics/base_velocity/error_vel_yaw: 0.5177
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 51732000
                    Iteration time: 1.12s
                      Time elapsed: 00:26:32
                               ETA: 00:01:10

################################################################################
                     [1m Learning iteration 1437/1500 [0m                     

                       Computation: 32337 steps/s (collection: 1.063s, learning 0.050s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0238
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.9290
                       Mean reward: 17.82
               Mean episode length: 847.20
Episode_Reward/track_lin_vel_xy_exp: 0.9663
Episode_Reward/track_ang_vel_z_exp: 0.4322
       Episode_Reward/lin_vel_z_l2: -0.0429
      Episode_Reward/ang_vel_xy_l2: -0.1223
     Episode_Reward/dof_torques_l2: -0.0798
         Episode_Reward/dof_acc_l2: -0.1749
     Episode_Reward/action_rate_l2: -0.1618
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5103
Metrics/base_velocity/error_vel_xy: 0.3673
Metrics/base_velocity/error_vel_yaw: 0.4582
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 51768000
                    Iteration time: 1.11s
                      Time elapsed: 00:26:33
                               ETA: 00:01:09

################################################################################
                     [1m Learning iteration 1438/1500 [0m                     

                       Computation: 33577 steps/s (collection: 1.021s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0254
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 13.9331
                       Mean reward: 17.45
               Mean episode length: 842.32
Episode_Reward/track_lin_vel_xy_exp: 0.9664
Episode_Reward/track_ang_vel_z_exp: 0.4498
       Episode_Reward/lin_vel_z_l2: -0.0430
      Episode_Reward/ang_vel_xy_l2: -0.1258
     Episode_Reward/dof_torques_l2: -0.0851
         Episode_Reward/dof_acc_l2: -0.1873
     Episode_Reward/action_rate_l2: -0.1693
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5025
Metrics/base_velocity/error_vel_xy: 0.4279
Metrics/base_velocity/error_vel_yaw: 0.4833
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 51804000
                    Iteration time: 1.07s
                      Time elapsed: 00:26:34
                               ETA: 00:01:08

################################################################################
                     [1m Learning iteration 1439/1500 [0m                     

                       Computation: 33093 steps/s (collection: 1.038s, learning 0.050s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0197
               Mean surrogate loss: -0.0155
                 Mean entropy loss: 13.9279
                       Mean reward: 17.75
               Mean episode length: 836.57
Episode_Reward/track_lin_vel_xy_exp: 1.1542
Episode_Reward/track_ang_vel_z_exp: 0.4948
       Episode_Reward/lin_vel_z_l2: -0.0374
      Episode_Reward/ang_vel_xy_l2: -0.1260
     Episode_Reward/dof_torques_l2: -0.0889
         Episode_Reward/dof_acc_l2: -0.1759
     Episode_Reward/action_rate_l2: -0.1778
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4932
Metrics/base_velocity/error_vel_xy: 0.3126
Metrics/base_velocity/error_vel_yaw: 0.4665
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 51840000
                    Iteration time: 1.09s
                      Time elapsed: 00:26:35
                               ETA: 00:01:07

################################################################################
                     [1m Learning iteration 1440/1500 [0m                     

                       Computation: 32716 steps/s (collection: 1.049s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0255
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 13.9192
                       Mean reward: 18.43
               Mean episode length: 827.90
Episode_Reward/track_lin_vel_xy_exp: 1.0142
Episode_Reward/track_ang_vel_z_exp: 0.4370
       Episode_Reward/lin_vel_z_l2: -0.0380
      Episode_Reward/ang_vel_xy_l2: -0.1154
     Episode_Reward/dof_torques_l2: -0.0816
         Episode_Reward/dof_acc_l2: -0.1597
     Episode_Reward/action_rate_l2: -0.1616
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4931
Metrics/base_velocity/error_vel_xy: 0.3127
Metrics/base_velocity/error_vel_yaw: 0.4481
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 51876000
                    Iteration time: 1.10s
                      Time elapsed: 00:26:36
                               ETA: 00:01:06

################################################################################
                     [1m Learning iteration 1441/1500 [0m                     

                       Computation: 33258 steps/s (collection: 1.029s, learning 0.053s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0255
               Mean surrogate loss: -0.0164
                 Mean entropy loss: 13.9205
                       Mean reward: 18.58
               Mean episode length: 845.66
Episode_Reward/track_lin_vel_xy_exp: 1.0704
Episode_Reward/track_ang_vel_z_exp: 0.4730
       Episode_Reward/lin_vel_z_l2: -0.0417
      Episode_Reward/ang_vel_xy_l2: -0.1305
     Episode_Reward/dof_torques_l2: -0.0844
         Episode_Reward/dof_acc_l2: -0.1875
     Episode_Reward/action_rate_l2: -0.1774
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4910
Metrics/base_velocity/error_vel_xy: 0.3830
Metrics/base_velocity/error_vel_yaw: 0.4932
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 51912000
                    Iteration time: 1.08s
                      Time elapsed: 00:26:37
                               ETA: 00:01:05

################################################################################
                     [1m Learning iteration 1442/1500 [0m                     

                       Computation: 32448 steps/s (collection: 1.056s, learning 0.053s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0233
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 13.9225
                       Mean reward: 17.78
               Mean episode length: 847.81
Episode_Reward/track_lin_vel_xy_exp: 1.0463
Episode_Reward/track_ang_vel_z_exp: 0.4682
       Episode_Reward/lin_vel_z_l2: -0.0420
      Episode_Reward/ang_vel_xy_l2: -0.1345
     Episode_Reward/dof_torques_l2: -0.0888
         Episode_Reward/dof_acc_l2: -0.1894
     Episode_Reward/action_rate_l2: -0.1772
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4954
Metrics/base_velocity/error_vel_xy: 0.4139
Metrics/base_velocity/error_vel_yaw: 0.5219
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 51948000
                    Iteration time: 1.11s
                      Time elapsed: 00:26:39
                               ETA: 00:01:04

################################################################################
                     [1m Learning iteration 1443/1500 [0m                     

                       Computation: 33105 steps/s (collection: 1.036s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0247
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 13.9141
                       Mean reward: 16.96
               Mean episode length: 859.39
Episode_Reward/track_lin_vel_xy_exp: 1.0130
Episode_Reward/track_ang_vel_z_exp: 0.4494
       Episode_Reward/lin_vel_z_l2: -0.0447
      Episode_Reward/ang_vel_xy_l2: -0.1249
     Episode_Reward/dof_torques_l2: -0.0812
         Episode_Reward/dof_acc_l2: -0.1818
     Episode_Reward/action_rate_l2: -0.1688
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5061
Metrics/base_velocity/error_vel_xy: 0.3719
Metrics/base_velocity/error_vel_yaw: 0.4815
      Episode_Termination/time_out: 1.9167
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 51984000
                    Iteration time: 1.09s
                      Time elapsed: 00:26:40
                               ETA: 00:01:03

################################################################################
                     [1m Learning iteration 1444/1500 [0m                     

                       Computation: 33592 steps/s (collection: 1.019s, learning 0.053s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0229
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 13.9112
                       Mean reward: 18.55
               Mean episode length: 863.51
Episode_Reward/track_lin_vel_xy_exp: 1.1332
Episode_Reward/track_ang_vel_z_exp: 0.4882
       Episode_Reward/lin_vel_z_l2: -0.0370
      Episode_Reward/ang_vel_xy_l2: -0.1257
     Episode_Reward/dof_torques_l2: -0.0838
         Episode_Reward/dof_acc_l2: -0.1704
     Episode_Reward/action_rate_l2: -0.1706
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5198
Metrics/base_velocity/error_vel_xy: 0.3112
Metrics/base_velocity/error_vel_yaw: 0.4635
      Episode_Termination/time_out: 2.0000
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 52020000
                    Iteration time: 1.07s
                      Time elapsed: 00:26:41
                               ETA: 00:01:02

################################################################################
                     [1m Learning iteration 1445/1500 [0m                     

                       Computation: 33614 steps/s (collection: 1.019s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0227
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 13.9098
                       Mean reward: 19.38
               Mean episode length: 869.75
Episode_Reward/track_lin_vel_xy_exp: 1.0737
Episode_Reward/track_ang_vel_z_exp: 0.4662
       Episode_Reward/lin_vel_z_l2: -0.0402
      Episode_Reward/ang_vel_xy_l2: -0.1256
     Episode_Reward/dof_torques_l2: -0.0824
         Episode_Reward/dof_acc_l2: -0.1839
     Episode_Reward/action_rate_l2: -0.1719
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5167
Metrics/base_velocity/error_vel_xy: 0.3243
Metrics/base_velocity/error_vel_yaw: 0.4542
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 52056000
                    Iteration time: 1.07s
                      Time elapsed: 00:26:42
                               ETA: 00:01:00

################################################################################
                     [1m Learning iteration 1446/1500 [0m                     

                       Computation: 33619 steps/s (collection: 1.018s, learning 0.053s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0259
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 13.8991
                       Mean reward: 18.18
               Mean episode length: 849.77
Episode_Reward/track_lin_vel_xy_exp: 1.0453
Episode_Reward/track_ang_vel_z_exp: 0.4546
       Episode_Reward/lin_vel_z_l2: -0.0406
      Episode_Reward/ang_vel_xy_l2: -0.1226
     Episode_Reward/dof_torques_l2: -0.0847
         Episode_Reward/dof_acc_l2: -0.1905
     Episode_Reward/action_rate_l2: -0.1692
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5111
Metrics/base_velocity/error_vel_xy: 0.3353
Metrics/base_velocity/error_vel_yaw: 0.4610
      Episode_Termination/time_out: 2.2500
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 52092000
                    Iteration time: 1.07s
                      Time elapsed: 00:26:43
                               ETA: 00:00:59

################################################################################
                     [1m Learning iteration 1447/1500 [0m                     

                       Computation: 33211 steps/s (collection: 1.034s, learning 0.050s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0279
               Mean surrogate loss: -0.0161
                 Mean entropy loss: 13.9017
                       Mean reward: 17.36
               Mean episode length: 848.19
Episode_Reward/track_lin_vel_xy_exp: 1.0460
Episode_Reward/track_ang_vel_z_exp: 0.4688
       Episode_Reward/lin_vel_z_l2: -0.0419
      Episode_Reward/ang_vel_xy_l2: -0.1315
     Episode_Reward/dof_torques_l2: -0.0888
         Episode_Reward/dof_acc_l2: -0.1888
     Episode_Reward/action_rate_l2: -0.1790
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5101
Metrics/base_velocity/error_vel_xy: 0.4182
Metrics/base_velocity/error_vel_yaw: 0.5214
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 52128000
                    Iteration time: 1.08s
                      Time elapsed: 00:26:44
                               ETA: 00:00:58

################################################################################
                     [1m Learning iteration 1448/1500 [0m                     

                       Computation: 32871 steps/s (collection: 1.043s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0244
               Mean surrogate loss: -0.0156
                 Mean entropy loss: 13.9122
                       Mean reward: 17.91
               Mean episode length: 868.05
Episode_Reward/track_lin_vel_xy_exp: 1.0785
Episode_Reward/track_ang_vel_z_exp: 0.4791
       Episode_Reward/lin_vel_z_l2: -0.0448
      Episode_Reward/ang_vel_xy_l2: -0.1354
     Episode_Reward/dof_torques_l2: -0.0881
         Episode_Reward/dof_acc_l2: -0.1995
     Episode_Reward/action_rate_l2: -0.1835
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5007
Metrics/base_velocity/error_vel_xy: 0.3939
Metrics/base_velocity/error_vel_yaw: 0.5003
      Episode_Termination/time_out: 1.9167
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 52164000
                    Iteration time: 1.10s
                      Time elapsed: 00:26:45
                               ETA: 00:00:57

################################################################################
                     [1m Learning iteration 1449/1500 [0m                     

                       Computation: 32638 steps/s (collection: 1.052s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0215
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 13.9069
                       Mean reward: 18.28
               Mean episode length: 897.46
Episode_Reward/track_lin_vel_xy_exp: 1.1211
Episode_Reward/track_ang_vel_z_exp: 0.4912
       Episode_Reward/lin_vel_z_l2: -0.0435
      Episode_Reward/ang_vel_xy_l2: -0.1352
     Episode_Reward/dof_torques_l2: -0.0936
         Episode_Reward/dof_acc_l2: -0.1969
     Episode_Reward/action_rate_l2: -0.1871
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4992
Metrics/base_velocity/error_vel_xy: 0.3914
Metrics/base_velocity/error_vel_yaw: 0.5340
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 52200000
                    Iteration time: 1.10s
                      Time elapsed: 00:26:46
                               ETA: 00:00:56

################################################################################
                     [1m Learning iteration 1450/1500 [0m                     

                       Computation: 32586 steps/s (collection: 1.033s, learning 0.072s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0206
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 13.8987
                       Mean reward: 18.37
               Mean episode length: 913.45
Episode_Reward/track_lin_vel_xy_exp: 1.1111
Episode_Reward/track_ang_vel_z_exp: 0.5042
       Episode_Reward/lin_vel_z_l2: -0.0447
      Episode_Reward/ang_vel_xy_l2: -0.1395
     Episode_Reward/dof_torques_l2: -0.0947
         Episode_Reward/dof_acc_l2: -0.2094
     Episode_Reward/action_rate_l2: -0.1889
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5043
Metrics/base_velocity/error_vel_xy: 0.4499
Metrics/base_velocity/error_vel_yaw: 0.5350
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 52236000
                    Iteration time: 1.10s
                      Time elapsed: 00:26:47
                               ETA: 00:00:55

################################################################################
                     [1m Learning iteration 1451/1500 [0m                     

                       Computation: 31117 steps/s (collection: 1.106s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0205
               Mean surrogate loss: -0.0155
                 Mean entropy loss: 13.8985
                       Mean reward: 17.37
               Mean episode length: 857.93
Episode_Reward/track_lin_vel_xy_exp: 1.0323
Episode_Reward/track_ang_vel_z_exp: 0.4579
       Episode_Reward/lin_vel_z_l2: -0.0389
      Episode_Reward/ang_vel_xy_l2: -0.1213
     Episode_Reward/dof_torques_l2: -0.0889
         Episode_Reward/dof_acc_l2: -0.1828
     Episode_Reward/action_rate_l2: -0.1738
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5061
Metrics/base_velocity/error_vel_xy: 0.3600
Metrics/base_velocity/error_vel_yaw: 0.4661
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 52272000
                    Iteration time: 1.16s
                      Time elapsed: 00:26:48
                               ETA: 00:00:54

################################################################################
                     [1m Learning iteration 1452/1500 [0m                     

                       Computation: 32300 steps/s (collection: 1.060s, learning 0.054s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0222
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 13.8963
                       Mean reward: 16.98
               Mean episode length: 868.91
Episode_Reward/track_lin_vel_xy_exp: 1.0081
Episode_Reward/track_ang_vel_z_exp: 0.4517
       Episode_Reward/lin_vel_z_l2: -0.0430
      Episode_Reward/ang_vel_xy_l2: -0.1310
     Episode_Reward/dof_torques_l2: -0.0887
         Episode_Reward/dof_acc_l2: -0.2008
     Episode_Reward/action_rate_l2: -0.1769
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5065
Metrics/base_velocity/error_vel_xy: 0.3965
Metrics/base_velocity/error_vel_yaw: 0.4955
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 52308000
                    Iteration time: 1.11s
                      Time elapsed: 00:26:50
                               ETA: 00:00:53

################################################################################
                     [1m Learning iteration 1453/1500 [0m                     

                       Computation: 32719 steps/s (collection: 1.048s, learning 0.053s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0240
               Mean surrogate loss: -0.0160
                 Mean entropy loss: 13.9001
                       Mean reward: 16.45
               Mean episode length: 821.19
Episode_Reward/track_lin_vel_xy_exp: 0.9117
Episode_Reward/track_ang_vel_z_exp: 0.4042
       Episode_Reward/lin_vel_z_l2: -0.0368
      Episode_Reward/ang_vel_xy_l2: -0.1113
     Episode_Reward/dof_torques_l2: -0.0747
         Episode_Reward/dof_acc_l2: -0.1648
     Episode_Reward/action_rate_l2: -0.1523
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5127
Metrics/base_velocity/error_vel_xy: 0.3135
Metrics/base_velocity/error_vel_yaw: 0.4102
      Episode_Termination/time_out: 1.1250
  Episode_Termination/base_contact: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 52344000
                    Iteration time: 1.10s
                      Time elapsed: 00:26:51
                               ETA: 00:00:52

################################################################################
                     [1m Learning iteration 1454/1500 [0m                     

                       Computation: 32356 steps/s (collection: 1.056s, learning 0.056s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0247
               Mean surrogate loss: -0.0160
                 Mean entropy loss: 13.9035
                       Mean reward: 16.87
               Mean episode length: 804.20
Episode_Reward/track_lin_vel_xy_exp: 1.0363
Episode_Reward/track_ang_vel_z_exp: 0.4598
       Episode_Reward/lin_vel_z_l2: -0.0420
      Episode_Reward/ang_vel_xy_l2: -0.1270
     Episode_Reward/dof_torques_l2: -0.0792
         Episode_Reward/dof_acc_l2: -0.1829
     Episode_Reward/action_rate_l2: -0.1674
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5124
Metrics/base_velocity/error_vel_xy: 0.3591
Metrics/base_velocity/error_vel_yaw: 0.4685
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 52380000
                    Iteration time: 1.11s
                      Time elapsed: 00:26:52
                               ETA: 00:00:50

################################################################################
                     [1m Learning iteration 1455/1500 [0m                     

                       Computation: 32867 steps/s (collection: 1.045s, learning 0.050s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0237
               Mean surrogate loss: -0.0149
                 Mean entropy loss: 13.9084
                       Mean reward: 16.98
               Mean episode length: 806.65
Episode_Reward/track_lin_vel_xy_exp: 1.0081
Episode_Reward/track_ang_vel_z_exp: 0.4479
       Episode_Reward/lin_vel_z_l2: -0.0384
      Episode_Reward/ang_vel_xy_l2: -0.1255
     Episode_Reward/dof_torques_l2: -0.0795
         Episode_Reward/dof_acc_l2: -0.1736
     Episode_Reward/action_rate_l2: -0.1657
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5125
Metrics/base_velocity/error_vel_xy: 0.3531
Metrics/base_velocity/error_vel_yaw: 0.4528
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 52416000
                    Iteration time: 1.10s
                      Time elapsed: 00:26:53
                               ETA: 00:00:49

################################################################################
                     [1m Learning iteration 1456/1500 [0m                     

                       Computation: 32754 steps/s (collection: 1.047s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0241
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 13.9126
                       Mean reward: 17.80
               Mean episode length: 847.53
Episode_Reward/track_lin_vel_xy_exp: 1.0243
Episode_Reward/track_ang_vel_z_exp: 0.4607
       Episode_Reward/lin_vel_z_l2: -0.0463
      Episode_Reward/ang_vel_xy_l2: -0.1307
     Episode_Reward/dof_torques_l2: -0.0837
         Episode_Reward/dof_acc_l2: -0.1933
     Episode_Reward/action_rate_l2: -0.1724
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5097
Metrics/base_velocity/error_vel_xy: 0.3980
Metrics/base_velocity/error_vel_yaw: 0.4996
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 52452000
                    Iteration time: 1.10s
                      Time elapsed: 00:26:54
                               ETA: 00:00:48

################################################################################
                     [1m Learning iteration 1457/1500 [0m                     

                       Computation: 32179 steps/s (collection: 1.066s, learning 0.053s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0253
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 13.9146
                       Mean reward: 17.13
               Mean episode length: 829.86
Episode_Reward/track_lin_vel_xy_exp: 1.0002
Episode_Reward/track_ang_vel_z_exp: 0.4410
       Episode_Reward/lin_vel_z_l2: -0.0458
      Episode_Reward/ang_vel_xy_l2: -0.1244
     Episode_Reward/dof_torques_l2: -0.0789
         Episode_Reward/dof_acc_l2: -0.1789
     Episode_Reward/action_rate_l2: -0.1663
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5048
Metrics/base_velocity/error_vel_xy: 0.3554
Metrics/base_velocity/error_vel_yaw: 0.4543
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 52488000
                    Iteration time: 1.12s
                      Time elapsed: 00:26:55
                               ETA: 00:00:47

################################################################################
                     [1m Learning iteration 1458/1500 [0m                     

                       Computation: 32931 steps/s (collection: 1.041s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0245
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 13.9054
                       Mean reward: 17.03
               Mean episode length: 839.12
Episode_Reward/track_lin_vel_xy_exp: 1.1270
Episode_Reward/track_ang_vel_z_exp: 0.4939
       Episode_Reward/lin_vel_z_l2: -0.0424
      Episode_Reward/ang_vel_xy_l2: -0.1342
     Episode_Reward/dof_torques_l2: -0.0857
         Episode_Reward/dof_acc_l2: -0.2049
     Episode_Reward/action_rate_l2: -0.1810
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5114
Metrics/base_velocity/error_vel_xy: 0.3666
Metrics/base_velocity/error_vel_yaw: 0.4942
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 52524000
                    Iteration time: 1.09s
                      Time elapsed: 00:26:56
                               ETA: 00:00:46

################################################################################
                     [1m Learning iteration 1459/1500 [0m                     

                       Computation: 32347 steps/s (collection: 1.060s, learning 0.053s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0278
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 13.8981
                       Mean reward: 17.87
               Mean episode length: 845.06
Episode_Reward/track_lin_vel_xy_exp: 0.9725
Episode_Reward/track_ang_vel_z_exp: 0.4285
       Episode_Reward/lin_vel_z_l2: -0.0350
      Episode_Reward/ang_vel_xy_l2: -0.1134
     Episode_Reward/dof_torques_l2: -0.0765
         Episode_Reward/dof_acc_l2: -0.1597
     Episode_Reward/action_rate_l2: -0.1526
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5176
Metrics/base_velocity/error_vel_xy: 0.3063
Metrics/base_velocity/error_vel_yaw: 0.4173
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 52560000
                    Iteration time: 1.11s
                      Time elapsed: 00:26:57
                               ETA: 00:00:45

################################################################################
                     [1m Learning iteration 1460/1500 [0m                     

                       Computation: 33129 steps/s (collection: 1.035s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0229
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 13.9077
                       Mean reward: 16.84
               Mean episode length: 804.70
Episode_Reward/track_lin_vel_xy_exp: 0.9317
Episode_Reward/track_ang_vel_z_exp: 0.4059
       Episode_Reward/lin_vel_z_l2: -0.0415
      Episode_Reward/ang_vel_xy_l2: -0.1180
     Episode_Reward/dof_torques_l2: -0.0663
         Episode_Reward/dof_acc_l2: -0.1440
     Episode_Reward/action_rate_l2: -0.1452
      Episode_Reward/feet_air_time: -0.0008
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5032
Metrics/base_velocity/error_vel_xy: 0.2963
Metrics/base_velocity/error_vel_yaw: 0.4039
      Episode_Termination/time_out: 0.6667
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 52596000
                    Iteration time: 1.09s
                      Time elapsed: 00:26:58
                               ETA: 00:00:44

################################################################################
                     [1m Learning iteration 1461/1500 [0m                     

                       Computation: 32944 steps/s (collection: 1.041s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0255
               Mean surrogate loss: -0.0154
                 Mean entropy loss: 13.9289
                       Mean reward: 17.13
               Mean episode length: 801.01
Episode_Reward/track_lin_vel_xy_exp: 1.1055
Episode_Reward/track_ang_vel_z_exp: 0.4797
       Episode_Reward/lin_vel_z_l2: -0.0437
      Episode_Reward/ang_vel_xy_l2: -0.1340
     Episode_Reward/dof_torques_l2: -0.0850
         Episode_Reward/dof_acc_l2: -0.1932
     Episode_Reward/action_rate_l2: -0.1786
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5001
Metrics/base_velocity/error_vel_xy: 0.3461
Metrics/base_velocity/error_vel_yaw: 0.4880
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 52632000
                    Iteration time: 1.09s
                      Time elapsed: 00:26:59
                               ETA: 00:00:43

################################################################################
                     [1m Learning iteration 1462/1500 [0m                     

                       Computation: 32286 steps/s (collection: 1.065s, learning 0.050s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0262
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 13.9421
                       Mean reward: 17.34
               Mean episode length: 834.33
Episode_Reward/track_lin_vel_xy_exp: 1.0166
Episode_Reward/track_ang_vel_z_exp: 0.4561
       Episode_Reward/lin_vel_z_l2: -0.0476
      Episode_Reward/ang_vel_xy_l2: -0.1346
     Episode_Reward/dof_torques_l2: -0.0859
         Episode_Reward/dof_acc_l2: -0.1984
     Episode_Reward/action_rate_l2: -0.1758
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5013
Metrics/base_velocity/error_vel_xy: 0.4103
Metrics/base_velocity/error_vel_yaw: 0.5001
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 52668000
                    Iteration time: 1.12s
                      Time elapsed: 00:27:01
                               ETA: 00:00:42

################################################################################
                     [1m Learning iteration 1463/1500 [0m                     

                       Computation: 32278 steps/s (collection: 1.062s, learning 0.053s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0257
               Mean surrogate loss: -0.0155
                 Mean entropy loss: 13.9449
                       Mean reward: 17.58
               Mean episode length: 861.73
Episode_Reward/track_lin_vel_xy_exp: 1.0388
Episode_Reward/track_ang_vel_z_exp: 0.4605
       Episode_Reward/lin_vel_z_l2: -0.0412
      Episode_Reward/ang_vel_xy_l2: -0.1301
     Episode_Reward/dof_torques_l2: -0.0868
         Episode_Reward/dof_acc_l2: -0.1915
     Episode_Reward/action_rate_l2: -0.1774
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5046
Metrics/base_velocity/error_vel_xy: 0.3861
Metrics/base_velocity/error_vel_yaw: 0.5095
      Episode_Termination/time_out: 1.7917
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 52704000
                    Iteration time: 1.12s
                      Time elapsed: 00:27:02
                               ETA: 00:00:40

################################################################################
                     [1m Learning iteration 1464/1500 [0m                     

                       Computation: 32075 steps/s (collection: 1.071s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0239
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 13.9516
                       Mean reward: 16.80
               Mean episode length: 823.84
Episode_Reward/track_lin_vel_xy_exp: 0.8669
Episode_Reward/track_ang_vel_z_exp: 0.3939
       Episode_Reward/lin_vel_z_l2: -0.0399
      Episode_Reward/ang_vel_xy_l2: -0.1153
     Episode_Reward/dof_torques_l2: -0.0720
         Episode_Reward/dof_acc_l2: -0.1679
     Episode_Reward/action_rate_l2: -0.1513
      Episode_Reward/feet_air_time: -0.0009
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4853
Metrics/base_velocity/error_vel_xy: 0.3676
Metrics/base_velocity/error_vel_yaw: 0.4340
      Episode_Termination/time_out: 0.9583
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 52740000
                    Iteration time: 1.12s
                      Time elapsed: 00:27:03
                               ETA: 00:00:39

################################################################################
                     [1m Learning iteration 1465/1500 [0m                     

                       Computation: 32425 steps/s (collection: 1.057s, learning 0.053s)
             Mean action noise std: 0.79
          Mean value_function loss: 0.0254
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 13.9594
                       Mean reward: 17.00
               Mean episode length: 816.59
Episode_Reward/track_lin_vel_xy_exp: 0.9694
Episode_Reward/track_ang_vel_z_exp: 0.4293
       Episode_Reward/lin_vel_z_l2: -0.0400
      Episode_Reward/ang_vel_xy_l2: -0.1187
     Episode_Reward/dof_torques_l2: -0.0755
         Episode_Reward/dof_acc_l2: -0.1755
     Episode_Reward/action_rate_l2: -0.1576
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4790
Metrics/base_velocity/error_vel_xy: 0.3270
Metrics/base_velocity/error_vel_yaw: 0.4345
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 52776000
                    Iteration time: 1.11s
                      Time elapsed: 00:27:04
                               ETA: 00:00:38

################################################################################
                     [1m Learning iteration 1466/1500 [0m                     

                       Computation: 32507 steps/s (collection: 1.056s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0219
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.9651
                       Mean reward: 18.18
               Mean episode length: 864.56
Episode_Reward/track_lin_vel_xy_exp: 1.0806
Episode_Reward/track_ang_vel_z_exp: 0.4753
       Episode_Reward/lin_vel_z_l2: -0.0467
      Episode_Reward/ang_vel_xy_l2: -0.1320
     Episode_Reward/dof_torques_l2: -0.0821
         Episode_Reward/dof_acc_l2: -0.1979
     Episode_Reward/action_rate_l2: -0.1785
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4768
Metrics/base_velocity/error_vel_xy: 0.3566
Metrics/base_velocity/error_vel_yaw: 0.4768
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 52812000
                    Iteration time: 1.11s
                      Time elapsed: 00:27:05
                               ETA: 00:00:37

################################################################################
                     [1m Learning iteration 1467/1500 [0m                     

                       Computation: 32240 steps/s (collection: 1.065s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0226
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 13.9574
                       Mean reward: 17.01
               Mean episode length: 828.09
Episode_Reward/track_lin_vel_xy_exp: 0.9117
Episode_Reward/track_ang_vel_z_exp: 0.4099
       Episode_Reward/lin_vel_z_l2: -0.0401
      Episode_Reward/ang_vel_xy_l2: -0.1137
     Episode_Reward/dof_torques_l2: -0.0806
         Episode_Reward/dof_acc_l2: -0.1689
     Episode_Reward/action_rate_l2: -0.1587
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4777
Metrics/base_velocity/error_vel_xy: 0.3529
Metrics/base_velocity/error_vel_yaw: 0.4392
      Episode_Termination/time_out: 0.9167
  Episode_Termination/base_contact: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 52848000
                    Iteration time: 1.12s
                      Time elapsed: 00:27:06
                               ETA: 00:00:36

################################################################################
                     [1m Learning iteration 1468/1500 [0m                     

                       Computation: 31484 steps/s (collection: 1.091s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0259
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.9582
                       Mean reward: 17.11
               Mean episode length: 820.38
Episode_Reward/track_lin_vel_xy_exp: 0.9820
Episode_Reward/track_ang_vel_z_exp: 0.4319
       Episode_Reward/lin_vel_z_l2: -0.0386
      Episode_Reward/ang_vel_xy_l2: -0.1211
     Episode_Reward/dof_torques_l2: -0.0767
         Episode_Reward/dof_acc_l2: -0.1810
     Episode_Reward/action_rate_l2: -0.1603
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4836
Metrics/base_velocity/error_vel_xy: 0.3259
Metrics/base_velocity/error_vel_yaw: 0.4376
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 52884000
                    Iteration time: 1.14s
                      Time elapsed: 00:27:07
                               ETA: 00:00:35

################################################################################
                     [1m Learning iteration 1469/1500 [0m                     

                       Computation: 31119 steps/s (collection: 1.104s, learning 0.053s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0231
               Mean surrogate loss: -0.0156
                 Mean entropy loss: 13.9464
                       Mean reward: 17.17
               Mean episode length: 843.81
Episode_Reward/track_lin_vel_xy_exp: 1.1596
Episode_Reward/track_ang_vel_z_exp: 0.5135
       Episode_Reward/lin_vel_z_l2: -0.0448
      Episode_Reward/ang_vel_xy_l2: -0.1389
     Episode_Reward/dof_torques_l2: -0.0967
         Episode_Reward/dof_acc_l2: -0.2026
     Episode_Reward/action_rate_l2: -0.1935
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4777
Metrics/base_velocity/error_vel_xy: 0.3860
Metrics/base_velocity/error_vel_yaw: 0.5041
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 52920000
                    Iteration time: 1.16s
                      Time elapsed: 00:27:08
                               ETA: 00:00:34

################################################################################
                     [1m Learning iteration 1470/1500 [0m                     

                       Computation: 31281 steps/s (collection: 1.095s, learning 0.056s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0241
               Mean surrogate loss: -0.0158
                 Mean entropy loss: 13.9499
                       Mean reward: 17.75
               Mean episode length: 836.24
Episode_Reward/track_lin_vel_xy_exp: 1.0331
Episode_Reward/track_ang_vel_z_exp: 0.4478
       Episode_Reward/lin_vel_z_l2: -0.0376
      Episode_Reward/ang_vel_xy_l2: -0.1180
     Episode_Reward/dof_torques_l2: -0.0795
         Episode_Reward/dof_acc_l2: -0.1659
     Episode_Reward/action_rate_l2: -0.1629
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4721
Metrics/base_velocity/error_vel_xy: 0.3160
Metrics/base_velocity/error_vel_yaw: 0.4471
      Episode_Termination/time_out: 1.2500
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 52956000
                    Iteration time: 1.15s
                      Time elapsed: 00:27:10
                               ETA: 00:00:33

################################################################################
                     [1m Learning iteration 1471/1500 [0m                     

                       Computation: 31012 steps/s (collection: 1.105s, learning 0.055s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0219
               Mean surrogate loss: -0.0158
                 Mean entropy loss: 13.9476
                       Mean reward: 17.57
               Mean episode length: 823.55
Episode_Reward/track_lin_vel_xy_exp: 1.0384
Episode_Reward/track_ang_vel_z_exp: 0.4484
       Episode_Reward/lin_vel_z_l2: -0.0418
      Episode_Reward/ang_vel_xy_l2: -0.1253
     Episode_Reward/dof_torques_l2: -0.0818
         Episode_Reward/dof_acc_l2: -0.1678
     Episode_Reward/action_rate_l2: -0.1669
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4732
Metrics/base_velocity/error_vel_xy: 0.3337
Metrics/base_velocity/error_vel_yaw: 0.4667
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 52992000
                    Iteration time: 1.16s
                      Time elapsed: 00:27:11
                               ETA: 00:00:32

################################################################################
                     [1m Learning iteration 1472/1500 [0m                     

                       Computation: 31668 steps/s (collection: 1.083s, learning 0.053s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0246
               Mean surrogate loss: -0.0156
                 Mean entropy loss: 13.9487
                       Mean reward: 18.62
               Mean episode length: 851.66
Episode_Reward/track_lin_vel_xy_exp: 1.0176
Episode_Reward/track_ang_vel_z_exp: 0.4579
       Episode_Reward/lin_vel_z_l2: -0.0447
      Episode_Reward/ang_vel_xy_l2: -0.1298
     Episode_Reward/dof_torques_l2: -0.0822
         Episode_Reward/dof_acc_l2: -0.1965
     Episode_Reward/action_rate_l2: -0.1740
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4764
Metrics/base_velocity/error_vel_xy: 0.3889
Metrics/base_velocity/error_vel_yaw: 0.4888
      Episode_Termination/time_out: 1.5833
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 53028000
                    Iteration time: 1.14s
                      Time elapsed: 00:27:12
                               ETA: 00:00:31

################################################################################
                     [1m Learning iteration 1473/1500 [0m                     

                       Computation: 29954 steps/s (collection: 1.148s, learning 0.054s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0265
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 13.9543
                       Mean reward: 18.50
               Mean episode length: 874.73
Episode_Reward/track_lin_vel_xy_exp: 1.0925
Episode_Reward/track_ang_vel_z_exp: 0.4899
       Episode_Reward/lin_vel_z_l2: -0.0416
      Episode_Reward/ang_vel_xy_l2: -0.1342
     Episode_Reward/dof_torques_l2: -0.0906
         Episode_Reward/dof_acc_l2: -0.1905
     Episode_Reward/action_rate_l2: -0.1841
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4793
Metrics/base_velocity/error_vel_xy: 0.3899
Metrics/base_velocity/error_vel_yaw: 0.4916
      Episode_Termination/time_out: 1.7083
  Episode_Termination/base_contact: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 53064000
                    Iteration time: 1.20s
                      Time elapsed: 00:27:13
                               ETA: 00:00:29

################################################################################
                     [1m Learning iteration 1474/1500 [0m                     

                       Computation: 30973 steps/s (collection: 1.110s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0246
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 13.9557
                       Mean reward: 18.25
               Mean episode length: 886.09
Episode_Reward/track_lin_vel_xy_exp: 1.0582
Episode_Reward/track_ang_vel_z_exp: 0.4702
       Episode_Reward/lin_vel_z_l2: -0.0391
      Episode_Reward/ang_vel_xy_l2: -0.1278
     Episode_Reward/dof_torques_l2: -0.0883
         Episode_Reward/dof_acc_l2: -0.1830
     Episode_Reward/action_rate_l2: -0.1772
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4799
Metrics/base_velocity/error_vel_xy: 0.3819
Metrics/base_velocity/error_vel_yaw: 0.4896
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 53100000
                    Iteration time: 1.16s
                      Time elapsed: 00:27:14
                               ETA: 00:00:28

################################################################################
                     [1m Learning iteration 1475/1500 [0m                     

                       Computation: 30118 steps/s (collection: 1.142s, learning 0.054s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0223
               Mean surrogate loss: -0.0164
                 Mean entropy loss: 13.9527
                       Mean reward: 17.36
               Mean episode length: 838.60
Episode_Reward/track_lin_vel_xy_exp: 0.9815
Episode_Reward/track_ang_vel_z_exp: 0.4267
       Episode_Reward/lin_vel_z_l2: -0.0411
      Episode_Reward/ang_vel_xy_l2: -0.1198
     Episode_Reward/dof_torques_l2: -0.0760
         Episode_Reward/dof_acc_l2: -0.1749
     Episode_Reward/action_rate_l2: -0.1593
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4786
Metrics/base_velocity/error_vel_xy: 0.3194
Metrics/base_velocity/error_vel_yaw: 0.4506
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 53136000
                    Iteration time: 1.20s
                      Time elapsed: 00:27:15
                               ETA: 00:00:27

################################################################################
                     [1m Learning iteration 1476/1500 [0m                     

                       Computation: 30731 steps/s (collection: 1.117s, learning 0.054s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0251
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 13.9510
                       Mean reward: 17.50
               Mean episode length: 844.32
Episode_Reward/track_lin_vel_xy_exp: 1.0010
Episode_Reward/track_ang_vel_z_exp: 0.4509
       Episode_Reward/lin_vel_z_l2: -0.0431
      Episode_Reward/ang_vel_xy_l2: -0.1288
     Episode_Reward/dof_torques_l2: -0.0826
         Episode_Reward/dof_acc_l2: -0.1852
     Episode_Reward/action_rate_l2: -0.1730
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4713
Metrics/base_velocity/error_vel_xy: 0.3815
Metrics/base_velocity/error_vel_yaw: 0.4769
      Episode_Termination/time_out: 1.3333
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 53172000
                    Iteration time: 1.17s
                      Time elapsed: 00:27:17
                               ETA: 00:00:26

################################################################################
                     [1m Learning iteration 1477/1500 [0m                     

                       Computation: 31087 steps/s (collection: 1.104s, learning 0.054s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0229
               Mean surrogate loss: -0.0154
                 Mean entropy loss: 13.9497
                       Mean reward: 17.24
               Mean episode length: 836.98
Episode_Reward/track_lin_vel_xy_exp: 1.0516
Episode_Reward/track_ang_vel_z_exp: 0.4585
       Episode_Reward/lin_vel_z_l2: -0.0392
      Episode_Reward/ang_vel_xy_l2: -0.1245
     Episode_Reward/dof_torques_l2: -0.0824
         Episode_Reward/dof_acc_l2: -0.1846
     Episode_Reward/action_rate_l2: -0.1716
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4736
Metrics/base_velocity/error_vel_xy: 0.3331
Metrics/base_velocity/error_vel_yaw: 0.4634
      Episode_Termination/time_out: 1.6250
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 53208000
                    Iteration time: 1.16s
                      Time elapsed: 00:27:18
                               ETA: 00:00:25

################################################################################
                     [1m Learning iteration 1478/1500 [0m                     

                       Computation: 32116 steps/s (collection: 1.070s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0272
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 13.9526
                       Mean reward: 17.46
               Mean episode length: 835.64
Episode_Reward/track_lin_vel_xy_exp: 0.9218
Episode_Reward/track_ang_vel_z_exp: 0.4219
       Episode_Reward/lin_vel_z_l2: -0.0388
      Episode_Reward/ang_vel_xy_l2: -0.1184
     Episode_Reward/dof_torques_l2: -0.0789
         Episode_Reward/dof_acc_l2: -0.1775
     Episode_Reward/action_rate_l2: -0.1622
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4743
Metrics/base_velocity/error_vel_xy: 0.4001
Metrics/base_velocity/error_vel_yaw: 0.4590
      Episode_Termination/time_out: 1.5000
  Episode_Termination/base_contact: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 53244000
                    Iteration time: 1.12s
                      Time elapsed: 00:27:19
                               ETA: 00:00:24

################################################################################
                     [1m Learning iteration 1479/1500 [0m                     

                       Computation: 33543 steps/s (collection: 1.020s, learning 0.053s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0238
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 13.9504
                       Mean reward: 16.89
               Mean episode length: 848.08
Episode_Reward/track_lin_vel_xy_exp: 0.9930
Episode_Reward/track_ang_vel_z_exp: 0.4695
       Episode_Reward/lin_vel_z_l2: -0.0452
      Episode_Reward/ang_vel_xy_l2: -0.1356
     Episode_Reward/dof_torques_l2: -0.0914
         Episode_Reward/dof_acc_l2: -0.1997
     Episode_Reward/action_rate_l2: -0.1852
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4822
Metrics/base_velocity/error_vel_xy: 0.5067
Metrics/base_velocity/error_vel_yaw: 0.5448
      Episode_Termination/time_out: 1.1667
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 53280000
                    Iteration time: 1.07s
                      Time elapsed: 00:27:20
                               ETA: 00:00:23

################################################################################
                     [1m Learning iteration 1480/1500 [0m                     

                       Computation: 32069 steps/s (collection: 1.070s, learning 0.052s)
             Mean action noise std: 0.79
          Mean value_function loss: 0.0215
               Mean surrogate loss: -0.0161
                 Mean entropy loss: 13.9634
                       Mean reward: 17.09
               Mean episode length: 861.66
Episode_Reward/track_lin_vel_xy_exp: 1.0150
Episode_Reward/track_ang_vel_z_exp: 0.4459
       Episode_Reward/lin_vel_z_l2: -0.0408
      Episode_Reward/ang_vel_xy_l2: -0.1235
     Episode_Reward/dof_torques_l2: -0.0807
         Episode_Reward/dof_acc_l2: -0.1819
     Episode_Reward/action_rate_l2: -0.1679
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4849
Metrics/base_velocity/error_vel_xy: 0.3441
Metrics/base_velocity/error_vel_yaw: 0.4572
      Episode_Termination/time_out: 1.4167
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 53316000
                    Iteration time: 1.12s
                      Time elapsed: 00:27:21
                               ETA: 00:00:22

################################################################################
                     [1m Learning iteration 1481/1500 [0m                     

                       Computation: 32008 steps/s (collection: 1.071s, learning 0.053s)
             Mean action noise std: 0.79
          Mean value_function loss: 0.0231
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 13.9704
                       Mean reward: 17.72
               Mean episode length: 865.22
Episode_Reward/track_lin_vel_xy_exp: 1.0692
Episode_Reward/track_ang_vel_z_exp: 0.4789
       Episode_Reward/lin_vel_z_l2: -0.0421
      Episode_Reward/ang_vel_xy_l2: -0.1298
     Episode_Reward/dof_torques_l2: -0.0878
         Episode_Reward/dof_acc_l2: -0.1989
     Episode_Reward/action_rate_l2: -0.1812
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4849
Metrics/base_velocity/error_vel_xy: 0.3825
Metrics/base_velocity/error_vel_yaw: 0.4812
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 53352000
                    Iteration time: 1.12s
                      Time elapsed: 00:27:22
                               ETA: 00:00:21

################################################################################
                     [1m Learning iteration 1482/1500 [0m                     

                       Computation: 30891 steps/s (collection: 1.113s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0205
               Mean surrogate loss: -0.0164
                 Mean entropy loss: 13.9684
                       Mean reward: 18.42
               Mean episode length: 878.41
Episode_Reward/track_lin_vel_xy_exp: 1.0002
Episode_Reward/track_ang_vel_z_exp: 0.4569
       Episode_Reward/lin_vel_z_l2: -0.0383
      Episode_Reward/ang_vel_xy_l2: -0.1240
     Episode_Reward/dof_torques_l2: -0.0819
         Episode_Reward/dof_acc_l2: -0.1774
     Episode_Reward/action_rate_l2: -0.1692
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4834
Metrics/base_velocity/error_vel_xy: 0.3913
Metrics/base_velocity/error_vel_yaw: 0.4643
      Episode_Termination/time_out: 1.3750
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 53388000
                    Iteration time: 1.17s
                      Time elapsed: 00:27:23
                               ETA: 00:00:19

################################################################################
                     [1m Learning iteration 1483/1500 [0m                     

                       Computation: 30009 steps/s (collection: 1.145s, learning 0.055s)
             Mean action noise std: 0.79
          Mean value_function loss: 0.0245
               Mean surrogate loss: -0.0155
                 Mean entropy loss: 13.9635
                       Mean reward: 17.26
               Mean episode length: 844.04
Episode_Reward/track_lin_vel_xy_exp: 0.8902
Episode_Reward/track_ang_vel_z_exp: 0.3985
       Episode_Reward/lin_vel_z_l2: -0.0380
      Episode_Reward/ang_vel_xy_l2: -0.1153
     Episode_Reward/dof_torques_l2: -0.0724
         Episode_Reward/dof_acc_l2: -0.1673
     Episode_Reward/action_rate_l2: -0.1529
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4853
Metrics/base_velocity/error_vel_xy: 0.3582
Metrics/base_velocity/error_vel_yaw: 0.4450
      Episode_Termination/time_out: 1.1250
  Episode_Termination/base_contact: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 53424000
                    Iteration time: 1.20s
                      Time elapsed: 00:27:25
                               ETA: 00:00:18

################################################################################
                     [1m Learning iteration 1484/1500 [0m                     

                       Computation: 31036 steps/s (collection: 1.107s, learning 0.053s)
             Mean action noise std: 0.79
          Mean value_function loss: 0.0238
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 13.9771
                       Mean reward: 16.90
               Mean episode length: 839.90
Episode_Reward/track_lin_vel_xy_exp: 0.9322
Episode_Reward/track_ang_vel_z_exp: 0.4387
       Episode_Reward/lin_vel_z_l2: -0.0413
      Episode_Reward/ang_vel_xy_l2: -0.1255
     Episode_Reward/dof_torques_l2: -0.0824
         Episode_Reward/dof_acc_l2: -0.1930
     Episode_Reward/action_rate_l2: -0.1703
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4868
Metrics/base_velocity/error_vel_xy: 0.4498
Metrics/base_velocity/error_vel_yaw: 0.4916
      Episode_Termination/time_out: 1.1250
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 53460000
                    Iteration time: 1.16s
                      Time elapsed: 00:27:26
                               ETA: 00:00:17

################################################################################
                     [1m Learning iteration 1485/1500 [0m                     

                       Computation: 31889 steps/s (collection: 1.076s, learning 0.053s)
             Mean action noise std: 0.79
          Mean value_function loss: 0.0240
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 13.9732
                       Mean reward: 17.97
               Mean episode length: 869.50
Episode_Reward/track_lin_vel_xy_exp: 1.0690
Episode_Reward/track_ang_vel_z_exp: 0.4722
       Episode_Reward/lin_vel_z_l2: -0.0377
      Episode_Reward/ang_vel_xy_l2: -0.1278
     Episode_Reward/dof_torques_l2: -0.0859
         Episode_Reward/dof_acc_l2: -0.1795
     Episode_Reward/action_rate_l2: -0.1763
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4848
Metrics/base_velocity/error_vel_xy: 0.3639
Metrics/base_velocity/error_vel_yaw: 0.4736
      Episode_Termination/time_out: 2.1250
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 53496000
                    Iteration time: 1.13s
                      Time elapsed: 00:27:27
                               ETA: 00:00:16

################################################################################
                     [1m Learning iteration 1486/1500 [0m                     

                       Computation: 32666 steps/s (collection: 1.051s, learning 0.051s)
             Mean action noise std: 0.79
          Mean value_function loss: 0.0224
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 13.9727
                       Mean reward: 18.76
               Mean episode length: 886.31
Episode_Reward/track_lin_vel_xy_exp: 1.0861
Episode_Reward/track_ang_vel_z_exp: 0.4750
       Episode_Reward/lin_vel_z_l2: -0.0392
      Episode_Reward/ang_vel_xy_l2: -0.1291
     Episode_Reward/dof_torques_l2: -0.0861
         Episode_Reward/dof_acc_l2: -0.1946
     Episode_Reward/action_rate_l2: -0.1788
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4859
Metrics/base_velocity/error_vel_xy: 0.3637
Metrics/base_velocity/error_vel_yaw: 0.4959
      Episode_Termination/time_out: 1.7500
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 53532000
                    Iteration time: 1.10s
                      Time elapsed: 00:27:28
                               ETA: 00:00:15

################################################################################
                     [1m Learning iteration 1487/1500 [0m                     

                       Computation: 30575 steps/s (collection: 1.122s, learning 0.055s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0222
               Mean surrogate loss: -0.0152
                 Mean entropy loss: 13.9662
                       Mean reward: 19.34
               Mean episode length: 903.49
Episode_Reward/track_lin_vel_xy_exp: 1.1421
Episode_Reward/track_ang_vel_z_exp: 0.4977
       Episode_Reward/lin_vel_z_l2: -0.0384
      Episode_Reward/ang_vel_xy_l2: -0.1318
     Episode_Reward/dof_torques_l2: -0.0877
         Episode_Reward/dof_acc_l2: -0.1938
     Episode_Reward/action_rate_l2: -0.1840
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4861
Metrics/base_velocity/error_vel_xy: 0.3633
Metrics/base_velocity/error_vel_yaw: 0.5055
      Episode_Termination/time_out: 1.9167
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 53568000
                    Iteration time: 1.18s
                      Time elapsed: 00:27:29
                               ETA: 00:00:14

################################################################################
                     [1m Learning iteration 1488/1500 [0m                     

                       Computation: 31111 steps/s (collection: 1.106s, learning 0.051s)
             Mean action noise std: 0.79
          Mean value_function loss: 0.0234
               Mean surrogate loss: -0.0154
                 Mean entropy loss: 13.9617
                       Mean reward: 19.70
               Mean episode length: 925.68
Episode_Reward/track_lin_vel_xy_exp: 1.2072
Episode_Reward/track_ang_vel_z_exp: 0.5254
       Episode_Reward/lin_vel_z_l2: -0.0442
      Episode_Reward/ang_vel_xy_l2: -0.1411
     Episode_Reward/dof_torques_l2: -0.0961
         Episode_Reward/dof_acc_l2: -0.2010
     Episode_Reward/action_rate_l2: -0.1975
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4926
Metrics/base_velocity/error_vel_xy: 0.3870
Metrics/base_velocity/error_vel_yaw: 0.5436
      Episode_Termination/time_out: 2.6250
  Episode_Termination/base_contact: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 53604000
                    Iteration time: 1.16s
                      Time elapsed: 00:27:30
                               ETA: 00:00:13

################################################################################
                     [1m Learning iteration 1489/1500 [0m                     

                       Computation: 29371 steps/s (collection: 1.173s, learning 0.053s)
             Mean action noise std: 0.79
          Mean value_function loss: 0.0196
               Mean surrogate loss: -0.0151
                 Mean entropy loss: 13.9728
                       Mean reward: 19.96
               Mean episode length: 914.71
Episode_Reward/track_lin_vel_xy_exp: 1.1053
Episode_Reward/track_ang_vel_z_exp: 0.4781
       Episode_Reward/lin_vel_z_l2: -0.0396
      Episode_Reward/ang_vel_xy_l2: -0.1279
     Episode_Reward/dof_torques_l2: -0.0840
         Episode_Reward/dof_acc_l2: -0.1865
     Episode_Reward/action_rate_l2: -0.1787
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4963
Metrics/base_velocity/error_vel_xy: 0.3349
Metrics/base_velocity/error_vel_yaw: 0.4793
      Episode_Termination/time_out: 2.0417
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 53640000
                    Iteration time: 1.23s
                      Time elapsed: 00:27:32
                               ETA: 00:00:12

################################################################################
                     [1m Learning iteration 1490/1500 [0m                     

                       Computation: 30430 steps/s (collection: 1.131s, learning 0.052s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0240
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.9643
                       Mean reward: 19.77
               Mean episode length: 923.25
Episode_Reward/track_lin_vel_xy_exp: 1.1821
Episode_Reward/track_ang_vel_z_exp: 0.5230
       Episode_Reward/lin_vel_z_l2: -0.0391
      Episode_Reward/ang_vel_xy_l2: -0.1408
     Episode_Reward/dof_torques_l2: -0.0911
         Episode_Reward/dof_acc_l2: -0.2076
     Episode_Reward/action_rate_l2: -0.1947
      Episode_Reward/feet_air_time: -0.0014
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5021
Metrics/base_velocity/error_vel_xy: 0.4136
Metrics/base_velocity/error_vel_yaw: 0.5363
      Episode_Termination/time_out: 1.9167
  Episode_Termination/base_contact: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 53676000
                    Iteration time: 1.18s
                      Time elapsed: 00:27:33
                               ETA: 00:00:11

################################################################################
                     [1m Learning iteration 1491/1500 [0m                     

                       Computation: 31597 steps/s (collection: 1.086s, learning 0.054s)
             Mean action noise std: 0.79
          Mean value_function loss: 0.0218
               Mean surrogate loss: -0.0148
                 Mean entropy loss: 13.9642
                       Mean reward: 19.40
               Mean episode length: 925.54
Episode_Reward/track_lin_vel_xy_exp: 1.0722
Episode_Reward/track_ang_vel_z_exp: 0.4810
       Episode_Reward/lin_vel_z_l2: -0.0406
      Episode_Reward/ang_vel_xy_l2: -0.1357
     Episode_Reward/dof_torques_l2: -0.0897
         Episode_Reward/dof_acc_l2: -0.2106
     Episode_Reward/action_rate_l2: -0.1852
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5041
Metrics/base_velocity/error_vel_xy: 0.4080
Metrics/base_velocity/error_vel_yaw: 0.5127
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 53712000
                    Iteration time: 1.14s
                      Time elapsed: 00:27:34
                               ETA: 00:00:09

################################################################################
                     [1m Learning iteration 1492/1500 [0m                     

                       Computation: 32507 steps/s (collection: 1.057s, learning 0.050s)
             Mean action noise std: 0.79
          Mean value_function loss: 0.0275
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 13.9737
                       Mean reward: 18.40
               Mean episode length: 908.20
Episode_Reward/track_lin_vel_xy_exp: 1.0138
Episode_Reward/track_ang_vel_z_exp: 0.4706
       Episode_Reward/lin_vel_z_l2: -0.0434
      Episode_Reward/ang_vel_xy_l2: -0.1335
     Episode_Reward/dof_torques_l2: -0.0914
         Episode_Reward/dof_acc_l2: -0.1956
     Episode_Reward/action_rate_l2: -0.1824
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4986
Metrics/base_velocity/error_vel_xy: 0.4746
Metrics/base_velocity/error_vel_yaw: 0.5325
      Episode_Termination/time_out: 1.9167
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 53748000
                    Iteration time: 1.11s
                      Time elapsed: 00:27:35
                               ETA: 00:00:08

################################################################################
                     [1m Learning iteration 1493/1500 [0m                     

                       Computation: 31426 steps/s (collection: 1.096s, learning 0.050s)
             Mean action noise std: 0.79
          Mean value_function loss: 0.0205
               Mean surrogate loss: -0.0156
                 Mean entropy loss: 13.9732
                       Mean reward: 18.15
               Mean episode length: 891.13
Episode_Reward/track_lin_vel_xy_exp: 1.0682
Episode_Reward/track_ang_vel_z_exp: 0.4871
       Episode_Reward/lin_vel_z_l2: -0.0393
      Episode_Reward/ang_vel_xy_l2: -0.1268
     Episode_Reward/dof_torques_l2: -0.0918
         Episode_Reward/dof_acc_l2: -0.1839
     Episode_Reward/action_rate_l2: -0.1802
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4975
Metrics/base_velocity/error_vel_xy: 0.4166
Metrics/base_velocity/error_vel_yaw: 0.4863
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 53784000
                    Iteration time: 1.15s
                      Time elapsed: 00:27:36
                               ETA: 00:00:07

################################################################################
                     [1m Learning iteration 1494/1500 [0m                     

                       Computation: 32186 steps/s (collection: 1.066s, learning 0.053s)
             Mean action noise std: 0.79
          Mean value_function loss: 0.0256
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 13.9676
                       Mean reward: 17.89
               Mean episode length: 889.39
Episode_Reward/track_lin_vel_xy_exp: 1.0122
Episode_Reward/track_ang_vel_z_exp: 0.4647
       Episode_Reward/lin_vel_z_l2: -0.0416
      Episode_Reward/ang_vel_xy_l2: -0.1360
     Episode_Reward/dof_torques_l2: -0.0902
         Episode_Reward/dof_acc_l2: -0.1974
     Episode_Reward/action_rate_l2: -0.1827
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4926
Metrics/base_velocity/error_vel_xy: 0.4480
Metrics/base_velocity/error_vel_yaw: 0.5347
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 53820000
                    Iteration time: 1.12s
                      Time elapsed: 00:27:37
                               ETA: 00:00:06

################################################################################
                     [1m Learning iteration 1495/1500 [0m                     

                       Computation: 31955 steps/s (collection: 1.072s, learning 0.054s)
             Mean action noise std: 0.79
          Mean value_function loss: 0.0237
               Mean surrogate loss: -0.0154
                 Mean entropy loss: 13.9685
                       Mean reward: 16.27
               Mean episode length: 847.16
Episode_Reward/track_lin_vel_xy_exp: 0.9207
Episode_Reward/track_ang_vel_z_exp: 0.4184
       Episode_Reward/lin_vel_z_l2: -0.0396
      Episode_Reward/ang_vel_xy_l2: -0.1207
     Episode_Reward/dof_torques_l2: -0.0816
         Episode_Reward/dof_acc_l2: -0.1880
     Episode_Reward/action_rate_l2: -0.1664
      Episode_Reward/feet_air_time: -0.0010
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4955
Metrics/base_velocity/error_vel_xy: 0.4036
Metrics/base_velocity/error_vel_yaw: 0.4778
      Episode_Termination/time_out: 1.8333
  Episode_Termination/base_contact: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 53856000
                    Iteration time: 1.13s
                      Time elapsed: 00:27:38
                               ETA: 00:00:05

################################################################################
                     [1m Learning iteration 1496/1500 [0m                     

                       Computation: 31317 steps/s (collection: 1.096s, learning 0.053s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0234
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 13.9669
                       Mean reward: 16.52
               Mean episode length: 826.55
Episode_Reward/track_lin_vel_xy_exp: 1.0303
Episode_Reward/track_ang_vel_z_exp: 0.4562
       Episode_Reward/lin_vel_z_l2: -0.0443
      Episode_Reward/ang_vel_xy_l2: -0.1306
     Episode_Reward/dof_torques_l2: -0.0789
         Episode_Reward/dof_acc_l2: -0.1867
     Episode_Reward/action_rate_l2: -0.1722
      Episode_Reward/feet_air_time: -0.0011
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.4975
Metrics/base_velocity/error_vel_xy: 0.3717
Metrics/base_velocity/error_vel_yaw: 0.4726
      Episode_Termination/time_out: 1.2917
  Episode_Termination/base_contact: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 53892000
                    Iteration time: 1.15s
                      Time elapsed: 00:27:39
                               ETA: 00:00:04

################################################################################
                     [1m Learning iteration 1497/1500 [0m                     

                       Computation: 32462 steps/s (collection: 1.058s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0215
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 13.9568
                       Mean reward: 16.94
               Mean episode length: 845.70
Episode_Reward/track_lin_vel_xy_exp: 1.0872
Episode_Reward/track_ang_vel_z_exp: 0.4873
       Episode_Reward/lin_vel_z_l2: -0.0426
      Episode_Reward/ang_vel_xy_l2: -0.1375
     Episode_Reward/dof_torques_l2: -0.0855
         Episode_Reward/dof_acc_l2: -0.2083
     Episode_Reward/action_rate_l2: -0.1868
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5036
Metrics/base_velocity/error_vel_xy: 0.4126
Metrics/base_velocity/error_vel_yaw: 0.5154
      Episode_Termination/time_out: 1.5417
  Episode_Termination/base_contact: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 53928000
                    Iteration time: 1.11s
                      Time elapsed: 00:27:41
                               ETA: 00:00:03

################################################################################
                     [1m Learning iteration 1498/1500 [0m                     

                       Computation: 31613 steps/s (collection: 1.087s, learning 0.051s)
             Mean action noise std: 0.78
          Mean value_function loss: 0.0227
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.9551
                       Mean reward: 17.29
               Mean episode length: 847.15
Episode_Reward/track_lin_vel_xy_exp: 1.0685
Episode_Reward/track_ang_vel_z_exp: 0.4677
       Episode_Reward/lin_vel_z_l2: -0.0391
      Episode_Reward/ang_vel_xy_l2: -0.1277
     Episode_Reward/dof_torques_l2: -0.0833
         Episode_Reward/dof_acc_l2: -0.1792
     Episode_Reward/action_rate_l2: -0.1765
      Episode_Reward/feet_air_time: -0.0013
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5062
Metrics/base_velocity/error_vel_xy: 0.3624
Metrics/base_velocity/error_vel_yaw: 0.4901
      Episode_Termination/time_out: 1.4583
  Episode_Termination/base_contact: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 53964000
                    Iteration time: 1.14s
                      Time elapsed: 00:27:42
                               ETA: 00:00:02

################################################################################
                     [1m Learning iteration 1499/1500 [0m                     

                       Computation: 32712 steps/s (collection: 1.050s, learning 0.051s)
             Mean action noise std: 0.79
          Mean value_function loss: 0.0231
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 13.9615
                       Mean reward: 18.04
               Mean episode length: 868.68
Episode_Reward/track_lin_vel_xy_exp: 1.1167
Episode_Reward/track_ang_vel_z_exp: 0.4788
       Episode_Reward/lin_vel_z_l2: -0.0405
      Episode_Reward/ang_vel_xy_l2: -0.1314
     Episode_Reward/dof_torques_l2: -0.0819
         Episode_Reward/dof_acc_l2: -0.1915
     Episode_Reward/action_rate_l2: -0.1799
      Episode_Reward/feet_air_time: -0.0012
Episode_Reward/flat_orientation_l2: 0.0000
     Episode_Reward/dof_pos_limits: 0.0000
         Curriculum/terrain_levels: 5.5011
Metrics/base_velocity/error_vel_xy: 0.3354
Metrics/base_velocity/error_vel_yaw: 0.4802
      Episode_Termination/time_out: 1.6667
  Episode_Termination/base_contact: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 54000000
                    Iteration time: 1.10s
                      Time elapsed: 00:27:43
                               ETA: 00:00:01

urceAsset </UsdPrimvarReader_float2.info:mdl:sourceAsset> with value @UsdPreviewSurface.mdl@.

2025-06-03 06:50:10 [10,416ms] [Warning] [omni.usd] Warning: in GetNodeDiscoveryResults at line 136 of /builds/omniverse/usd-ci/USD/pxr/usd/usdShade/shaderDefUtils.cpp -- Unable to resolve info:sourceAsset </UsdPrimvarReader_float3.info:mdl:sourceAsset> with value @UsdPreviewSurface.mdl@.

2025-06-03 06:50:10 [10,416ms] [Warning] [omni.usd] Warning: in GetNodeDiscoveryResults at line 136 of /builds/omniverse/usd-ci/USD/pxr/usd/usdShade/shaderDefUtils.cpp -- Unable to resolve info:sourceAsset </UsdPrimvarReader_float4.info:mdl:sourceAsset> with value @UsdPreviewSurface.mdl@.

2025-06-03 06:50:10 [10,416ms] [Warning] [omni.usd] Warning: in GetNodeDiscoveryResults at line 136 of /builds/omniverse/usd-ci/USD/pxr/usd/usdShade/shaderDefUtils.cpp -- Unable to resolve info:sourceAsset </UsdPrimvarReader_int.info:mdl:sourceAsset> with value @UsdPreviewSurface.mdl@.

2025-06-03 06:50:10 [10,416ms] [Warning] [omni.usd] Warning: in GetNodeDiscoveryResults at line 136 of /builds/omniverse/usd-ci/USD/pxr/usd/usdShade/shaderDefUtils.cpp -- Unable to resolve info:sourceAsset </UsdPrimvarReader_string.info:mdl:sourceAsset> with value @UsdPreviewSurface.mdl@.

2025-06-03 06:50:10 [10,416ms] [Warning] [omni.usd] Warning: in GetNodeDiscoveryResults at line 136 of /builds/omniverse/usd-ci/USD/pxr/usd/usdShade/shaderDefUtils.cpp -- Unable to resolve info:sourceAsset </UsdPrimvarReader_normal.info:mdl:sourceAsset> with value @UsdPreviewSurface.mdl@.

2025-06-03 06:50:10 [10,416ms] [Warning] [omni.usd] Warning: in GetNodeDiscoveryResults at line 136 of /builds/omniverse/usd-ci/USD/pxr/usd/usdShade/shaderDefUtils.cpp -- Unable to resolve info:sourceAsset </UsdPrimvarReader_point.info:mdl:sourceAsset> with value @UsdPreviewSurface.mdl@.

2025-06-03 06:50:10 [10,416ms] [Warning] [omni.usd] Warning: in GetNodeDiscoveryResults at line 136 of /builds/omniverse/usd-ci/USD/pxr/usd/usdShade/shaderDefUtils.cpp -- Unable to resolve info:sourceAsset </UsdPrimvarReader_vector.info:mdl:sourceAsset> with value @UsdPreviewSurface.mdl@.

2025-06-03 06:50:10 [10,416ms] [Warning] [omni.usd] Warning: in GetNodeDiscoveryResults at line 136 of /builds/omniverse/usd-ci/USD/pxr/usd/usdShade/shaderDefUtils.cpp -- Unable to resolve info:sourceAsset </UsdPrimvarReader_matrix.info:mdl:sourceAsset> with value @UsdPreviewSurface.mdl@.

2025-06-03 06:50:10 [10,416ms] [Warning] [omni.usd] Warning: in GetNodeDiscoveryResults at line 136 of /builds/omniverse/usd-ci/USD/pxr/usd/usdShade/shaderDefUtils.cpp -- Unable to resolve info:sourceAsset </UsdTransform2d.info:mdl:sourceAsset> with value @UsdPreviewSurface.mdl@.

